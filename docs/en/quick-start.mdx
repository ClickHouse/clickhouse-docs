---
slug: /en/getting-started/quick-start
sidebar_label: Quick Start
sidebar_position: 1
keywords: [clickhouse, install, getting started, quick start]
pagination_next: 'en/getting-started/index'
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from '@theme/CodeBlock';

# ClickHouse Quick Start

:::tip
This page helps you set up open-source ClickHouse on your own machine. The fastest way to deploy ClickHouse and to get access to our exclusive SQL Console is to use ClickHouse Cloud.

New users get $300 in free trial credits. Click [here](https://clickhouse.cloud/signUp?loc=docs-quick-start) to sign up.
:::

##  1: Download the binary

ClickHouse runs natively on Linux, FreeBSD and macOS, and runs on Windows via the [WSL](https://learn.microsoft.com/en-us/windows/wsl/about).
The simplest way to download ClickHouse locally is to run the following `curl` command. It determines if your operating system is supported,
then downloads an appropriate ClickHouse binary:

  ```bash
  curl https://clickhouse.com/ | sh
  ```



## 2: Start the server

Run the following command to start the ClickHouse server:
  ```bash
  ./clickhouse server
  ```



## 3: Start the client

Use the `clickhouse-client` to connect to your ClickHouse service. Open a new Terminal, change directories to where your `clickhouse`
binary is saved, and run the following command:

```bash
./clickhouse client
```

You should see a smiling face as it connects to your service running on localhost:

  ```response
  my-host :)
  ```

## 4: Create a table

Use `CREATE TABLE` to define a new table. Typical SQL DDL commands work in ClickHouse with one addition - tables in ClickHouse require
an `ENGINE` clause. Use `MergeTree` to take advantage of the performance benefits of ClickHouse:

```sql
CREATE TABLE my_first_table
(
    user_id UInt32,
    message String,
    timestamp DateTime,
    metric Float32
)
ENGINE = MergeTree
PRIMARY KEY (user_id, timestamp)
```

## 5. Insert data

You can use the familiar `INSERT INTO TABLE` command with ClickHouse, but it is important to understand that each insert into a
`MergeTree` table causes a **part** (folder) to be created in storage. To minimize parts, bulk insert lots of rows at a time (tens of
thousands or even millions at once).

```sql
INSERT INTO my_first_table (user_id, message, timestamp, metric) VALUES
    (101, 'Hello, ClickHouse!',                                 now(),       -1.0    ),
    (102, 'Insert a lot of rows per batch',                     yesterday(), 1.41421 ),
    (102, 'Sort your data based on your commonly-used queries', today(),     2.718   ),
    (101, 'Granules are the smallest chunks of data read',      now() + 5,   3.14159 )
```

## 6. Query your new table

You can write a `SELECT` query just like you would with any SQL database:

 ```sql
  SELECT *
  FROM my_first_table
  ORDER BY timestamp
  ```
  Notice the response comes back in a nice table format:
   ```response
   ┌─user_id─┬─message────────────────────────────────────────────┬───────────timestamp─┬──metric─┐
   │     102 │ Insert a lot of rows per batch                     │ 2022-03-21 00:00:00 │ 1.41421 │
   │     102 │ Sort your data based on your commonly-used queries │ 2022-03-22 00:00:00 │   2.718 │
   │     101 │ Hello, ClickHouse!                                 │ 2022-03-22 14:04:09 │      -1 │
   │     101 │ Granules are the smallest chunks of data read      │ 2022-03-22 14:04:14 │ 3.14159 │
   └─────────┴────────────────────────────────────────────────────┴─────────────────────┴─────────┘

   4 rows in set. Elapsed: 0.008 sec.
   ```

## 7: Insert your own data

The next step is to get your current data into ClickHouse. We have lots of [table functions](/docs/en/sql-reference/table-functions/index.md)
and [integrations](/docs/en/integrations) for ingesting data. We have some examples in the tabs below, or check out our [Integrations](/docs/en/integrations)
for a long list of technologies that integrate with ClickHouse.

<Tabs groupId="read_data">
<TabItem value="S3" label="S3" default>

Use the [`s3` table function](/docs/en/sql-reference/table-functions/s3.md) to read files from S3. It's a table function - meaning that the
result is a table that can be:

1. used as the source of a `SELECT` query (allowing you to run ad-hoc queries and leave your data in S3), or...
2. insert the resulting table into a `MergeTree` table (when you are ready to move your data into ClickHouse)

An ad-hoc query looks like:

```sql
SELECT
   passenger_count,
   avg(toFloat32(total_amount))
FROM s3(
    'https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_0.gz',
    'TabSeparatedWithNames'
)
GROUP BY passenger_count
ORDER BY passenger_count;
```

Moving the data into a ClickHouse table looks like the following, where `nyc_taxi` is a `MergeTree` table:

```sql
INSERT INTO nyc_taxi
   SELECT * FROM s3(
    'https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_0.gz',
    'TabSeparatedWithNames'
)
SETTINGS input_format_allow_errors_num=25000;
```

View our [collection of AWS S3 documentation pages](/docs/en/integrations/data-ingestion/s3/index.md) for lots more details and examples of using S3 with ClickHouse.

</TabItem>
<TabItem value="GCS" label="GCS">

The [`s3` table function](/docs/en/sql-reference/table-functions/s3.md) used for reading data in AWS S3 also works on files in Google Cloud Storage. For example:

```sql
SELECT
   *
FROM s3(
  'https://storage.googleapis.com/my-bucket/trips.parquet',
  'MY_GCS_HMAC_KEY',
  'MY_GCS_HMAC_SECRET_KEY',
  'Parquet'
)
LIMIT 1000
```

Find more details on the [`s3` table function page](/docs/en/sql-reference/table-functions/s3.md).

</TabItem>
<TabItem value="URL" label="Web">

The [`url` table function](/docs/en/sql-reference/table-functions/url) reads files accessible from the web:

```sql
--By default, ClickHouse prevents redirects to protect from SSRF attacks.
--The URL below requires a redirect, so we must set max_http_get_redirects > 0.
SET max_http_get_redirects=10;

SELECT *
FROM url(
    'http://prod2.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-complete.csv',
    'CSV'
  );
```

Find more details on the [`url` table function page](/docs/en/sql-reference/table-functions/url).

</TabItem>
<TabItem value="local_file" label="Local">

Use the [`file` table engine](/docs/en/sql-reference/table-functions/file) to read a local file. For simplicity, copy the file to the `user_files` directory (which is
found in the directory where you downloaded the ClickHouse binary).

```sql
DESCRIBE TABLE file('comments.tsv')

Query id: 8ca9b2f9-65a2-4982-954a-890de710a336

┌─name──────┬─type────────────────────┬─default_type─┬─default_expression─┬─comment─┬─codec_expression─┬─ttl_expression─┐
│ id        │ Nullable(Int64)         │              │                    │         │                  │                │
│ type      │ Nullable(String)        │              │                    │         │                  │                │
│ author    │ Nullable(String)        │              │                    │         │                  │                │
│ timestamp │ Nullable(DateTime64(9)) │              │                    │         │                  │                │
│ comment   │ Nullable(String)        │              │                    │         │                  │                │
│ children  │ Array(Nullable(Int64))  │              │                    │         │                  │                │
└───────────┴─────────────────────────┴──────────────┴────────────────────┴─────────┴──────────────────┴────────────────┘
```

Notice ClickHouse infers the names and data types of your columns by analyzing a large batch of rows.
If ClickHouse can not determine the storage type from the filename, you can specify it as the second argument:

```sql
SELECT count()
FROM file(
  'comments.tsv',
  'TabSeparatedWithNames'
)
```

View the [`file` table function](/docs/en/sql-reference/table-functions/file) docs page for more details.

</TabItem>
<TabItem value="PostgreSQL" label="PostgreSQL">

Use the [`postgresql` table function](/en/sql-reference/table-functions/postgresql) to read data from a table in PostgreSQL:


```sql
SELECT *
FROM
   postgresql(
    'localhost:5432',
    'my_database',
    'my_table',
    'postgresql_user',
    'password')
;
```

View the [`postgresql` table function](/docs/en/sql-reference/table-functions/postgresql) docs page for more details.

</TabItem>
<TabItem value="MySQL" label="MySQL">

Use the [`mysql` table function](/docs/en/sql-reference/table-functions/mysql) to read data from a table in MySQL:


```sql
SELECT *
FROM
   mysql(
    'localhost:3306',
    'my_database',
    'my_table',
    'postgresql_user',
    'password')
;
```

View the [`mysql` table function](/docs/en/sql-reference/table-functions/mysql) docs page for more details.


</TabItem>
<TabItem value="Other DBMS" label="ODBC/JDBC">

ClickHouse can read data from any ODBC or JDBC data source:

```sql
SELECT *
FROM
   odbc(
    'DSN=mysqlconn',
    'my_database',
    'my_table'
  );
```

View the [`odbc` table function](/docs/en/sql-reference/table-functions/odbc) and the [`jdbc` table function](/docs/en/sql-reference/table-functions/jdbc) docs pages for more details.

</TabItem>
<TabItem value="messagequeue" label="Message Queues">

Message queues can stream data into ClickHouse using the corresponding table engine, including:

- **Kafka**: integrate with Kafka using the [`Kafka` table engine](/docs/en/engines/table-engines/integrations/kafka)
- **Amazon MSK**: integrate with [Amazon Managed Streaming for Apache Kafka (MSK)](/docs/en/integrations/kafka/cloud/amazon-msk/)
- **RabbitMQ**: integrate with RabbitMQ using the [`RabbitMQ` table engine](/docs/en/engines/table-engines/integrations/rabbitmq)

</TabItem>
<TabItem value="datalake" label="Data Lakes">

ClickHouse has table functions to read data from the following sources:

- **Hadoop**: integrate with Apache Hadoop using the [`hdfs` table function](/docs/en/sql-reference/table-functions/hdfs)
- **Hudi**: read from existing Apache Hudi tables in S3 using the [`hudi` table function](/docs/en/sql-reference/table-functions/hudi)
- **Iceberg**: read from existing Apache Iceberg tables in S3 using the [`iceberg` table function](/docs/en/sql-reference/table-functions/iceberg)
- **DeltaLake**: read from existing Delta Lake tables in S3 using the [`deltaLake` table function](/docs/en/sql-reference/table-functions/deltalake)

</TabItem>
<TabItem value="Other" label="Other">

Check out our [long list of ClickHouse integrations](/docs/en/integrations) to find how to connect your existing frameworks and data sources to ClickHouse.

</TabItem>
</Tabs>

## What's Next?

- Check out the [Advanced Tutorial](tutorial.md) which takes a much deeper dive into the key concepts and capabilities of ClickHouse
- Continue your learning by taking our free on-demand training courses at the [ClickHouse Academy](https://learn.clickhouse.com/visitor_class_catalog)
- We have a list of [example datasets](/docs/en/getting-started/example-datasets/) with instructions on how to insert them
- If your data is coming from an external source, view our [collection of integration guides](/docs/en/integrations/) for connecting to message queues, databases, pipelines and more
- If you are using a UI/BI visualization tool, view the [user guides for connecting a UI to ClickHouse](/docs/en/integrations/data-visualization/)
- The user guide on [primary keys](/docs/en/guides/best-practices/sparse-primary-indexes.md) is everything you need to know about primary keys and how to define them
