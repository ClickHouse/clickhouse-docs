---
slug: /getting-started/quick-start/oss
sidebar_label: 'OSS'
sidebar_position: 2
keywords: ['getting started', 'quick start', 'beginner-friendly']
title: 'ClickHouse OSS quick start'
description: 'ClickHouse Quick Start guide'
show_related_blogs: true
doc_type: tutorial
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from '@theme/CodeBlock';
import {VerticalStepper} from '@clickhouse/click-ui/bundled';

# ClickHouse OSS quick start

> In this quick start tutorial, we'll get you set up with OSS ClickHouse in 8
easy steps. You'll download an appropriate binary for your OS,
learn to run ClickHouse server, and use the ClickHouse client to create a table,
then insert data into it and run a query to select that data.

<VerticalStepper>

## Download ClickHouse {#download-the-binary}

ClickHouse runs natively on Linux, FreeBSD and macOS, and runs on Windows via
the [WSL](https://learn.microsoft.com/en-us/windows/wsl/about). The simplest way to download ClickHouse locally is to run the
following `curl` command. It determines if your operating system is supported,
then downloads an appropriate ClickHouse binary.

:::note
We recommend running the command below from a new and empty subdirectory as
some configuration files will be created in the directory the binary is located
in the first time ClickHouse server is run.
:::

```bash
curl https://clickhouse.com/ | sh
```

You should see:

```
Successfully downloaded the ClickHouse binary, you can run it as:
    ./clickhouse

You can also install it:
sudo ./clickhouse install
```

At this stage, you can ignore the prompt to run the `install` command.

:::note
For Mac users: If you are getting errors that the developer of the binary cannot
be verified, please see ["Fix the Developer Verification Error in MacOS"](https://clickhouse.com/docs/knowledgebase/fix-developer-verification-error-in-macos).
:::

## Start the server

Run the following command to start the ClickHouse server:

```bash
./clickhouse server
```

You should see the terminal fill up with logging. This is expected. In ClickHouse
the [default logging level](https://clickhouse.com/docs/knowledgebase/why_default_logging_verbose)
is set to `trace` rather than `warning`.

## Start the client

Use `clickhouse-client` to connect to your ClickHouse service. Open a new
terminal, change directories to where your `clickhouse` binary is saved, and
run the following command:

```bash
./clickhouse client
```

You should see a smiling face as it connects to your service running on localhost:

```response
my-host :)
```

## Create a table

Use `CREATE TABLE` to define a new table. Typical SQL DDL commands work in
ClickHouse with one addition - tables in ClickHouse require
an `ENGINE` clause. Use [`MergeTree`](/engines/table-engines/mergetree-family/mergetree)
to take advantage of the performance benefits of ClickHouse:

```sql
CREATE TABLE my_first_table
(
    user_id UInt32,
    message String,
    timestamp DateTime,
    metric Float32
)
ENGINE = MergeTree
PRIMARY KEY (user_id, timestamp)
```

## Insert data

You can use the familiar `INSERT INTO TABLE` command with ClickHouse, but it is
important to understand that each insert into a `MergeTree` table causes what we
call a **part** in ClickHouse to be created in storage. These ^^parts^^ later get
merged in the background by ClickHouse.

In ClickHouse, we try to bulk insert lots of rows at a time
(tens of thousands or even millions at once) to minimize the number of [**parts**](/parts)
that need to get merged in the background process.

In this guide, we won't worry about that just yet. Run the following command to
insert a few rows of data into your table:

```sql
INSERT INTO my_first_table (user_id, message, timestamp, metric) VALUES
    (101, 'Hello, ClickHouse!',                                 now(),       -1.0    ),
    (102, 'Insert a lot of rows per batch',                     yesterday(), 1.41421 ),
    (102, 'Sort your data based on your commonly-used queries', today(),     2.718   ),
    (101, 'Granules are the smallest chunks of data read',      now() + 5,   3.14159 )
```

## Query your new table

You can write a `SELECT` query just like you would with any SQL database:

```sql
SELECT *
FROM my_first_table
ORDER BY timestamp
```
Notice the response comes back in a nice table format:

```text
┌─user_id─┬─message────────────────────────────────────────────┬───────────timestamp─┬──metric─┐
│     102 │ Insert a lot of rows per batch                     │ 2022-03-21 00:00:00 │ 1.41421 │
│     102 │ Sort your data based on your commonly-used queries │ 2022-03-22 00:00:00 │   2.718 │
│     101 │ Hello, ClickHouse!                                 │ 2022-03-22 14:04:09 │      -1 │
│     101 │ Granules are the smallest chunks of data read      │ 2022-03-22 14:04:14 │ 3.14159 │
└─────────┴────────────────────────────────────────────────────┴─────────────────────┴─────────┘

4 rows in set. Elapsed: 0.008 sec.
```

## Insert your own data

The next step is to get your own data into ClickHouse. We have lots of [table functions](/sql-reference/table-functions/index.md)
and [integrations](/integrations) for ingesting data. We have some examples in the tabs
below, or you can check out our [Integrations](/integrations) page for a long list of
technologies that integrate with ClickHouse.

<Tabs groupId="read_data">
    <TabItem value="S3" label="S3" default>

        Use the [`s3` table function](/sql-reference/table-functions/s3.md) to
        read files from S3. It's a table function - meaning that the result is a table
        that can be:

        1. used as the source of a `SELECT` query (allowing you to run ad-hoc queries and
        leave your data in S3), or...
        2. insert the resulting table into a `MergeTree` table (when you are ready to
        move your data into ClickHouse)

        An ad-hoc query looks like:

        ```sql
        SELECT
        passenger_count,
        avg(toFloat32(total_amount))
        FROM s3(
        'https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_0.gz',
        'TabSeparatedWithNames'
        )
        GROUP BY passenger_count
        ORDER BY passenger_count;
        ```

        Moving the data into a ClickHouse table looks like the following, where
        `nyc_taxi` is a `MergeTree` table:

        ```sql
        INSERT INTO nyc_taxi
        SELECT * FROM s3(
        'https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_0.gz',
        'TabSeparatedWithNames'
        )
        SETTINGS input_format_allow_errors_num=25000;
        ```

        View our [collection of AWS S3 documentation pages](/integrations/data-ingestion/s3/index.md) for lots more details and examples of using S3 with ClickHouse.
        <br/>
    </TabItem>
    <TabItem value="GCS" label="GCS">

        The [`s3` table function](/sql-reference/table-functions/s3.md) used for
        reading data in AWS S3 also works on files in Google Cloud Storage.

        For example:

        ```sql
        SELECT
        *
        FROM s3(
        'https://storage.googleapis.com/my-bucket/trips.parquet',
        'MY_GCS_HMAC_KEY',
        'MY_GCS_HMAC_SECRET_KEY',
        'Parquet'
        )
        LIMIT 1000
        ```

        Find more details on the [`s3` table function page](/sql-reference/table-functions/s3.md).
        <br/>
    </TabItem>
    <TabItem value="URL" label="Web">

        The [`url` table function](/sql-reference/table-functions/url) reads
        files accessible from the web:

        ```sql
        --By default, ClickHouse prevents redirects to protect from SSRF attacks.
        --The URL below requires a redirect, so we must set max_http_get_redirects > 0.
        SET max_http_get_redirects=10;

        SELECT *
        FROM url(
        'http://prod2.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-complete.csv',
        'CSV'
        );
        ```

        Find more details on the [`url` table function page](/sql-reference/table-functions/url).
        <br/>
    </TabItem>
    <TabItem value="local_file" label="Local">

        Use the [`file` table engine](/sql-reference/table-functions/file) to
        read a local file. For simplicity, copy the file to the `user_files` directory
        (which is found in the directory where you downloaded the ClickHouse binary).

        ```sql
        DESCRIBE TABLE file('comments.tsv')

        Query id: 8ca9b2f9-65a2-4982-954a-890de710a336

        ┌─name──────┬─type────────────────────┐
        │ id        │ Nullable(Int64)         │
        │ type      │ Nullable(String)        │
        │ author    │ Nullable(String)        │
        │ timestamp │ Nullable(DateTime64(9)) │
        │ comment   │ Nullable(String)        │
        │ children  │ Array(Nullable(Int64))  │
        └───────────┴─────────────────────────┘
        ```

        Notice ClickHouse infers the names and data types of your columns by analyzing a
        large batch of rows. If ClickHouse can not determine the file format from the
        filename, you can specify it as the second argument:

        ```sql
        SELECT count()
        FROM file(
        'comments.tsv',
        'TabSeparatedWithNames'
        )
        ```

        View the [`file` table function](/sql-reference/table-functions/file)
        docs page for more details.
        <br/>
    </TabItem>
    <TabItem value="PostgreSQL" label="PostgreSQL">

        Use the [`postgresql` table function](/sql-reference/table-functions/postgresql)
        to read data from a table in PostgreSQL:

        ```sql
        SELECT *
        FROM
        postgresql(
        'localhost:5432',
        'my_database',
        'my_table',
        'postgresql_user',
        'password')
        ;
        ```

        View the [`postgresql` table function](/sql-reference/table-functions/postgresql)
        docs page for more details.
        <br/>
    </TabItem>
    <TabItem value="MySQL" label="MySQL">

        Use the [`mysql` table function](/sql-reference/table-functions/mysql)
        to read data from a table in MySQL:

        ```sql
        SELECT *
        FROM
        mysql(
        'localhost:3306',
        'my_database',
        'my_table',
        'mysql_user',
        'password')
        ;
        ```

        View the [`mysql` table function](/sql-reference/table-functions/mysql)
        docs page for more details.
        <br/>
    </TabItem>
    <TabItem value="Other DBMS" label="ODBC/JDBC">

        ClickHouse can read data from any ODBC or JDBC data source:

        ```sql
        SELECT *
        FROM
        odbc(
        'DSN=mysqlconn',
        'my_database',
        'my_table'
        );
        ```

        View the [`odbc` table function](/sql-reference/table-functions/odbc)
        and the [`jdbc` table function](/sql-reference/table-functions/jdbc) docs
        pages for more details.
        <br/>
    </TabItem>
    <TabItem value="messagequeue" label="Message Queues">

        Message queues can stream data into ClickHouse using the corresponding table
        engine, including:

        - **Kafka**: integrate with Kafka using the [`Kafka` table engine](/engines/table-engines/integrations/kafka)
        - **Amazon MSK**: integrate with [Amazon Managed Streaming for Apache Kafka (MSK)](/integrations/kafka/cloud/amazon-msk/)
        - **RabbitMQ**: integrate with RabbitMQ using the [`RabbitMQ` table engine](/engines/table-engines/integrations/rabbitmq)
        <br/>
    </TabItem>
    <TabItem value="datalake" label="Data Lakes">

        ClickHouse has table functions to read data from the following sources:

        - **Hadoop**: integrate with Apache Hadoop using the [`hdfs` table function](/sql-reference/table-functions/hdfs)
        - **Hudi**: read from existing Apache Hudi tables in S3 using the [`hudi` table function](/sql-reference/table-functions/hudi)
        - **Iceberg**: read from existing Apache Iceberg tables in S3 using the [`iceberg` table function](/sql-reference/table-functions/iceberg)
        - **DeltaLake**: read from existing Delta Lake tables in S3 using the [`deltaLake` table function](/sql-reference/table-functions/deltalake)
        <br/>
    </TabItem>
    <TabItem value="Other" label="Other">

        Check out our [long list of ClickHouse integrations](/integrations) to find how to connect your existing frameworks and data sources to ClickHouse.
        <br/>
    </TabItem>
</Tabs>

## Explore

- Check out our [Core Concepts](/managing-data/core-concepts) section to learn some of the fundamentals of how ClickHouse works under the hood.
- Check out the [Advanced Tutorial](tutorial.md) which takes a much deeper dive into the key concepts and capabilities of ClickHouse.
- Continue your learning by taking our free on-demand training courses at the [ClickHouse Academy](https://learn.clickhouse.com/visitor_class_catalog).
- We have a list of [example datasets](/getting-started/example-datasets/) with instructions on how to insert them.
- If your data is coming from an external source, view our [collection of integration guides](/integrations/) for connecting to message queues, databases, pipelines and more.
- If you are using a UI/BI visualization tool, view the [user guides for connecting a UI to ClickHouse](/integrations/data-visualization/).
- The user guide on [primary keys](/guides/best-practices/sparse-primary-indexes.md) is everything you need to know about primary keys and how to define them.

</VerticalStepper>
