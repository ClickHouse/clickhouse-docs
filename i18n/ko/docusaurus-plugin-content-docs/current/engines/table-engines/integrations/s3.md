---
'description': '이 엔진은 Amazon S3 생태계와의 통합을 제공합니다. HDFS 엔진과 유사하지만 S3 전용 기능을 제공합니다.'
'sidebar_label': 'S3'
'sidebar_position': 180
'slug': '/engines/table-engines/integrations/s3'
'title': 'S3 테이블 엔진'
'doc_type': 'reference'
---



# S3 테이블 엔진

이 엔진은 [Amazon S3](https://aws.amazon.com/s3/) 생태계와 통합을 제공합니다. 이 엔진은 [HDFS](/engines/table-engines/integrations/hdfs) 엔진과 유사하지만 S3 전용 기능을 제공합니다.

## 예시 {#example}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE=S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'gzip')
    SETTINGS input_format_with_names_use_header = 0;

INSERT INTO s3_engine_table VALUES ('one', 1), ('two', 2), ('three', 3);

SELECT * FROM s3_engine_table LIMIT 2;
```

```text
┌─name─┬─value─┐
│ one  │     1 │
│ two  │     2 │
└──────┴───────┘
```

## 테이블 생성 {#creating-a-table}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE = S3(path [, NOSIGN | aws_access_key_id, aws_secret_access_key,] format, [compression], [partition_strategy], [partition_columns_in_data_file])
    [PARTITION BY expr]
    [SETTINGS ...]
```

### 엔진 매개변수 {#parameters}

- `path` — 파일로의 경로가 포함된 버킷 URL. 읽기 전용 모드에서는 다음 와일드카드를 지원합니다: `*`, `**`, `?`, `{abc,def}` 및 `{N..M}` 여기서 `N`, `M` — 숫자, `'abc'`, `'def'` — 문자열입니다. 자세한 내용은 [아래](#wildcards-in-path)를 참조하십시오.
- `NOSIGN` - 자격 증명 대신 이 키워드가 제공되면 모든 요청이 서명되지 않습니다.
- `format` — 파일의 [형식](/sql-reference/formats#formats-overview).
- `aws_access_key_id`, `aws_secret_access_key` - [AWS](https://aws.amazon.com/) 계정 사용자의 장기 자격 증명. 이러한 자격 증명을 사용하여 요청을 인증할 수 있습니다. 매개변수는 선택 사항입니다. 자격 증명이 지정되지 않은 경우 구성 파일에서 사용됩니다. 자세한 내용은 [S3를 데이터 저장소로 사용하는 방법](../mergetree-family/mergetree.md#table_engine-mergetree-s3)을 참조하십시오.
- `compression` — 압축 유형. 지원되는 값: `none`, `gzip/gz`, `brotli/br`, `xz/LZMA`, `zstd/zst`. 매개변수는 선택 사항입니다. 기본적으로 파일 확장자에 따라 자동으로 압축을 감지합니다.
- `partition_strategy` – 옵션: `WILDCARD` 또는 `HIVE`. `WILDCARD`는 경로에 `{_partition_id}`가 필요하며, 이 부분은 파티션 키로 대체됩니다. `HIVE`는 와일드카드를 허용하지 않고, 경로가 테이블 루트라고 가정하며, Snowflake ID를 파일 이름으로 사용하고 파일 형식을 확장자로 가진 Hive 스타일의 파티션 디렉토리를 생성합니다. 기본값은 `WILDCARD`입니다.
- `partition_columns_in_data_file` - `HIVE` 파티션 전략과 함께 사용됩니다. ClickHouse에 데이터 파일에 파티션 컬럼이 작성될 것으로 예상하는지를 알려줍니다. 기본값은 `false`입니다.
- `storage_class_name` - 옵션: `STANDARD` 또는 `INTELLIGENT_TIERING`, [AWS S3 인텔리전트 티어링](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/)을 지정할 수 있습니다.

### 데이터 캐시 {#data-cache}

`S3` 테이블 엔진은 로컬 디스크에서 데이터 캐싱을 지원합니다. 파일 시스템 캐시 구성 옵션과 사용법은 이 [섹션](/operations/storing-data.md/#using-local-cache)에서 확인하십시오. 캐싱은 경로와 저장 객체의 ETag에 따라 수행되므로 ClickHouse는 오래된 캐시 버전을 읽지 않습니다.

캐싱을 활성화하려면 설정 `filesystem_cache_name = '<name>'` 및 `enable_filesystem_cache = 1`을 사용하십시오.

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
SETTINGS filesystem_cache_name = 'cache_for_s3', enable_filesystem_cache = 1;
```

구성 파일에서 캐시를 정의하는 방법은 두 가지가 있습니다.

1. ClickHouse 구성 파일에 다음 섹션을 추가합니다:

```xml
<clickhouse>
    <filesystem_caches>
        <cache_for_s3>
            <path>path to cache directory</path>
            <max_size>10Gi</max_size>
        </cache_for_s3>
    </filesystem_caches>
</clickhouse>
```

2. ClickHouse `storage_configuration` 섹션의 캐시 구성을 재사용합니다. [여기에서 설명됨](/operations/storing-data.md/#using-local-cache)

### PARTITION BY {#partition-by}

`PARTITION BY` — 선택 사항입니다. 대부분의 경우 파티션 키가 필요하지 않으며, 필요할 경우 일반적으로는 월 단위보다 세부적인 파티션 키는 필요하지 않습니다. 파티션은 쿼리 속도를 높이지 않습니다 (ORDER BY 표현과 대조적). 너무 세부적인 파티션을 사용하면 안 됩니다. 클라이언트 식별자나 이름으로 데이터를 파티션하지 마십시오 (대신, 클라이언트 식별자나 이름을 ORDER BY 표현의 첫 번째 컬럼으로 만드십시오).

월별 파티션을 위해서는 `toYYYYMM(date_column)` 표현을 사용하십시오. 여기서 `date_column`은 [Date](/sql-reference/data-types/date.md) 유형의 날짜가 있는 컬럼입니다. 파티션 이름은 `"YYYYMM"` 형식을 가집니다.

#### 파티션 전략 {#partition-strategy}

`WILDCARD` (기본값): 파일 경로의 `{_partition_id}` 와일드카드를 실제 파티션 키로 대체합니다. 읽기는 지원되지 않습니다.

`HIVE`는 읽기 및 쓰기 위한 Hive 스타일 파티셔닝을 구현합니다. 읽기는 재귀적인 glob 패턴을 사용하여 구현되며 `SELECT * FROM s3('table_root/**.parquet')`와 동등합니다. 쓰기는 다음 형식을 사용하여 파일을 생성합니다: `<prefix>/<key1=val1/key2=val2...>/<snowflakeid>.<toLower(file_format)>`.

노트: `HIVE` 파티션 전략을 사용할 때 `use_hive_partitioning` 설정은 영향을 주지 않습니다.

`HIVE` 파티션 전략의 예시:

```sql
arthur :) CREATE TABLE t_03363_parquet (year UInt16, country String, counter UInt8)
ENGINE = S3(s3_conn, filename = 't_03363_parquet', format = Parquet, partition_strategy='hive')
PARTITION BY (year, country);

arthur :) INSERT INTO t_03363_parquet VALUES
    (2022, 'USA', 1),
    (2022, 'Canada', 2),
    (2023, 'USA', 3),
    (2023, 'Mexico', 4),
    (2024, 'France', 5),
    (2024, 'Germany', 6),
    (2024, 'Germany', 7),
    (1999, 'Brazil', 8),
    (2100, 'Japan', 9),
    (2024, 'CN', 10),
    (2025, '', 11);

arthur :) select _path, * from t_03363_parquet;

    ┌─_path──────────────────────────────────────────────────────────────────────┬─year─┬─country─┬─counter─┐
 1. │ test/t_03363_parquet/year=2100/country=Japan/7329604473272971264.parquet   │ 2100 │ Japan   │       9 │
 2. │ test/t_03363_parquet/year=2024/country=France/7329604473323302912.parquet  │ 2024 │ France  │       5 │
 3. │ test/t_03363_parquet/year=2022/country=Canada/7329604473314914304.parquet  │ 2022 │ Canada  │       2 │
 4. │ test/t_03363_parquet/year=1999/country=Brazil/7329604473289748480.parquet  │ 1999 │ Brazil  │       8 │
 5. │ test/t_03363_parquet/year=2023/country=Mexico/7329604473293942784.parquet  │ 2023 │ Mexico  │       4 │
 6. │ test/t_03363_parquet/year=2023/country=USA/7329604473319108608.parquet     │ 2023 │ USA     │       3 │
 7. │ test/t_03363_parquet/year=2025/country=/7329604473327497216.parquet        │ 2025 │         │      11 │
 8. │ test/t_03363_parquet/year=2024/country=CN/7329604473310720000.parquet      │ 2024 │ CN      │      10 │
 9. │ test/t_03363_parquet/year=2022/country=USA/7329604473298137088.parquet     │ 2022 │ USA     │       1 │
10. │ test/t_03363_parquet/year=2024/country=Germany/7329604473306525696.parquet │ 2024 │ Germany │       6 │
11. │ test/t_03363_parquet/year=2024/country=Germany/7329604473306525696.parquet │ 2024 │ Germany │       7 │
    └────────────────────────────────────────────────────────────────────────────┴──────┴─────────┴─────────┘
```

### 파티션화된 데이터 쿼리하기 {#querying-partitioned-data}

이 예시는 ClickHouse와 MinIO를 통합한 [docker compose recipe](https://github.com/ClickHouse/examples/tree/5fdc6ff72f4e5137e23ea075c88d3f44b0202490/docker-compose-recipes/recipes/ch-and-minio-S3)를 사용합니다. 엔드포인트 및 인증 값을 변경하여 S3를 사용하여 동일한 쿼리를 재현할 수 있어야 합니다.

`ENGINE` 구성의 S3 엔드포인트가 `{_partition_id}`라는 매개변수를 S3 객체(파일 이름)의 일부로 사용한다는 점과 SELECT 쿼리가 해당 결과 객체 이름(예: `test_3.csv`)을 선택한다는 점에 유의하십시오.

:::note
예제에서 보여준 바와 같이, 현재 파티션화된 S3 테이블의 쿼리는 직접 지원되지 않지만 S3 테이블 함수를 사용해 개별 파티션을 쿼리함으로써 가능합니다.

S3에 파티션화된 데이터 쓰기의 주요 사용 사례는 해당 데이터를 다른 ClickHouse 시스템으로 전송할 수 있도록 하는 것입니다 (예: 온프레미스 시스템에서 ClickHouse Cloud로 이동). ClickHouse 데이터 세트는 종종 매우 크며, 네트워크 신뢰성이 때때로 불완전하기 때문에 데이터 세트를 부분으로 전송하는 것이 의미가 있습니다. 따라서 파티션화된 쓰기 작업이 필요합니다.
:::

#### 테이블 생성 {#create-the-table}
```sql
CREATE TABLE p
(
    `column1` UInt32,
    `column2` UInt32,
    `column3` UInt32
)
ENGINE = S3(
-- highlight-next-line
           'http://minio:10000/clickhouse//test_{_partition_id}.csv',
           'minioadmin',
           'minioadminpassword',
           'CSV')
PARTITION BY column3
```

#### 데이터 삽입 {#insert-data}
```sql
INSERT INTO p VALUES (1, 2, 3), (3, 2, 1), (78, 43, 45)
```

#### 파티션 3에서 선택 {#select-from-partition-3}

:::tip
이 쿼리는 S3 테이블 기능을 사용합니다.
:::

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│  1 │  2 │  3 │
└────┴────┴────┘
```

#### 파티션 1에서 선택 {#select-from-partition-1}
```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_1.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│  3 │  2 │  1 │
└────┴────┴────┘
```

#### 파티션 45에서 선택 {#select-from-partition-45}
```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_45.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│ 78 │ 43 │ 45 │
└────┴────┴────┘
```

#### 제한 사항 {#limitation}

`Select * from p`를 시도할 수 있지만, 위에서 언급한 바와 같이 이 쿼리는 실패합니다; 이전 쿼리를 사용하십시오.

```sql
SELECT * FROM p
```
```response
Received exception from server (version 23.4.1):
Code: 48. DB::Exception: Received from localhost:9000. DB::Exception: Reading from a partitioned S3 storage is not implemented yet. (NOT_IMPLEMENTED)
```

## 데이터 삽입 {#inserting-data}

행은 새 파일에만 삽입될 수 있음을 주의하십시오. 병합 주기나 파일 분할 작업이 없습니다. 파일이 작성된 후에는 이후 삽입이 실패합니다. 이를 피하려면 `s3_truncate_on_insert` 및 `s3_create_new_file_on_insert` 설정을 사용할 수 있습니다. 자세한 내용은 [여기에서]( /integrations/s3#inserting-data) 확인하십시오.

## 가상 컬럼 {#virtual-columns}

- `_path` — 파일 경로. 유형: `LowCardinality(String)`.
- `_file` — 파일 이름. 유형: `LowCardinality(String)`.
- `_size` — 파일 크기(바이트 단위). 유형: `Nullable(UInt64)`. 크기가 알려지지 않은 경우, 값은 `NULL`입니다.
- `_time` — 파일의 마지막 수정 시간. 유형: `Nullable(DateTime)`. 시간이 알려지지 않은 경우, 값은 `NULL`입니다.
- `_etag` — 파일의 ETag. 유형: `LowCardinality(String)`. ETag가 알려지지 않은 경우, 값은 `NULL`입니다.
- `_tags` — 파일의 태그. 유형: `Map(String, String)`. 태그가 없는 경우, 값은 빈 맵 `{}`입니다.

가상 컬럼에 대한 자세한 내용은 [여기에서](../../../engines/table-engines/index.md#table_engines-virtual_columns) 확인하세요.

## 구현 세부정보 {#implementation-details}

- 읽기 및 쓰기를 병렬로 처리할 수 있습니다.
- 지원되지 않음:
  - `ALTER` 및 `SELECT...SAMPLE` 작업.
  - 인덱스.
  - [제로 복사](../../../operations/storing-data.md#zero-copy) 복제는 가능하지만 지원되지 않습니다.

  :::note 제로 복사 복제는 프로덕션을 위해 준비되지 않았습니다.
  Zero-copy replication은 ClickHouse 버전 22.8 이상에서 기본적으로 비활성화되어 있습니다. 이 기능은 프로덕션 사용을 권장하지 않습니다.
  :::

## 경로의 와일드카드 {#wildcards-in-path}

`path` 인수는 bash와 유사한 와일드카드를 사용하여 여러 파일을 지정할 수 있습니다. 파일로 처리되려면 파일이 존재하고 전체 경로 패턴과 일치해야 합니다. 파일 목록은 `SELECT` 중에 결정됩니다( `CREATE` 순간에는 결정되지 않음).

- `*` — `/`를 제외한 임의의 문자 수를 대체하며 빈 문자열도 포함됩니다.
- `**` — `/`를 포함한 임의의 문자 수를 대체하며 빈 문자열도 포함됩니다.
- `?` — 단일 문자를 대체합니다.
- `{some_string,another_string,yet_another_one}` — 문자열 `'some_string', 'another_string', 'yet_another_one'` 중 하나를 대체합니다.
- `{N..M}` — N부터 M까지의 범위에 있는 숫자를 대체하며 양쪽 경계를 포함합니다. N과 M은 선행 0을 가질 수 있습니다. 예를 들면, `000..078`.

`{}`가 포함된 구문은 [remote](../../../sql-reference/table-functions/remote.md) 테이블 함수와 유사합니다.

:::note
파일 목록에 선행 0이 포함된 숫자 범위가 있는 경우 각 자릿수에 대해 중괄호를 사용한 구조를 사용하거나 `?`를 사용해야 합니다.
:::

**와일드카드 예시 1**

`file-000.csv`, `file-001.csv`, ... , `file-999.csv`라는 이름의 파일로 테이블 생성:

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/my_folder/file-{000..999}.csv', 'CSV');
```

**와일드카드 예시 2**

다음 URIs를 가진 CSV 형식의 여러 파일이 S3에 있다고 가정합니다:

- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_3.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_3.csv'

모든 여섯 파일로 구성된 테이블을 만드는 방법은 여러 가지가 있습니다:

1. 파일 접미사의 범위를 지정합니다:

```sql
CREATE TABLE table_with_range (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_{1..3}', 'CSV');
```

2. `some_file_` 접두사를 가진 모든 파일을 가져옵니다(두 폴더에 해당 접두사를 가진 추가 파일이 없어야 함):

```sql
CREATE TABLE table_with_question_mark (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_?', 'CSV');
```

3. 두 폴더의 모든 파일을 가져옵니다(모든 파일은 쿼리에 설명된 형식 및 스키마를 충족해야 합니다):

```sql
CREATE TABLE table_with_asterisk (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/*', 'CSV');
```

## 저장 설정 {#storage-settings}

- [s3_truncate_on_insert](/operations/settings/settings.md#s3_truncate_on_insert) - 삽입 전에 파일을 자르는 것을 허용합니다. 기본적으로 비활성화되어 있습니다.
- [s3_create_new_file_on_insert](/operations/settings/settings.md#s3_create_new_file_on_insert) - 형식에 접미사가 있는 경우 각 삽입 시 새 파일을 만드는 것을 허용합니다. 기본적으로 비활성화되어 있습니다.
- [s3_skip_empty_files](/operations/settings/settings.md#s3_skip_empty_files) - 읽는 동안 빈 파일을 스킵하는 것을 허용합니다. 기본적으로 활성화되어 있습니다.

## S3 관련 설정 {#settings}

다음 설정은 쿼리 실행 전에 설정할 수 있으며 구성 파일에 배치할 수 있습니다.

- `s3_max_single_part_upload_size` — 단일 파트 업로드를 사용하여 S3에 업로드할 객체의 최대 크기. 기본값은 `32Mb`입니다.
- `s3_min_upload_part_size` — [S3 Multipart upload](https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html) 중 다중 파트 업로드에 대한 최소 파트 크기. 기본값은 `16Mb`입니다.
- `s3_max_redirects` — 허용되는 S3 리디렉션의 최대 수. 기본값은 `10`입니다.
- `s3_single_read_retries` — 단일 읽기 중 최대 재시도 횟수. 기본값은 `4`입니다.
- `s3_max_put_rps` — 제한 적용 전에 초당 최대 PUT 요청 수. 기본값은 `0` (무제한)입니다.
- `s3_max_put_burst` — 초당 요청 한도에 도달하기 전에 동시에 발행할 수 있는 최대 요청 수. 기본적으로 (`0` 값)은 `s3_max_put_rps`와 같습니다.
- `s3_max_get_rps` — 제한 적용 전에 초당 최대 GET 요청 수. 기본값은 `0` (무제한)입니다.
- `s3_max_get_burst` — 초당 요청 한도에 도달하기 전에 동시에 발행할 수 있는 최대 요청 수. 기본적으로 (`0` 값)은 `s3_max_get_rps`와 같습니다.
- `s3_upload_part_size_multiply_factor` - `s3_min_upload_part_size`를 단일 쓰기에서 S3로 업로드된 각 파트마다 이 배수로 곱합니다. 기본값은 `2`입니다.
- `s3_upload_part_size_multiply_parts_count_threshold` - 이 수의 파트가 S3에 업로드될 때마다 `s3_min_upload_part_size`가 `s3_upload_part_size_multiply_factor`로 곱해집니다. 기본값은 `500`입니다.
- `s3_max_inflight_parts_for_one_file` - 하나의 객체에 대해 동시에 실행할 수 있는 PUT 요청의 수를 제한합니다. 이 수는 제한되어야 합니다. 값 `0`은 무제한을 의미합니다. 기본값은 `20`입니다. 각 비행 중 파트는 첫 번째 `s3_upload_part_size_multiply_factor` 파트에 대해 `s3_min_upload_part_size` 크기의 버퍼가 있으며, 파일이 충분히 클 때 추가됩니다. 기본 설정으로 업로드된 하나의 파일은 8G 미만의 파일에 대해 `320Mb`를 초과하지 않게 소비합니다. 더 큰 파일의 경우 소비량이 증가합니다.

보안 고려 사항: 악의적인 사용자가 임의의 S3 URL을 지정할 수 있는 경우, SSRF(https://en.wikipedia.org/wiki/Server-side_request_forgery) 공격을 방지하기 위해 `s3_max_redirects`는 0으로 설정해야 하며, 또는 대안으로 `remote_host_filter`를 서버 구성에서 지정해야 합니다.

## 엔드포인트 기반 설정 {#endpoint-settings}

다음 설정은 주어진 엔드포인트에 대해 구성 파일에 지정될 수 있습니다(이는 URL의 정확한 접두사로 일치합니다):

- `endpoint` — 엔드포인트의 접두사를 지정합니다. 필수입니다.
- `access_key_id` 및 `secret_access_key` — 주어진 엔드포인트에 사용할 자격 증명을 지정합니다. 선택 사항입니다.
- `use_environment_credentials` — `true`로 설정하면 S3 클라이언트는 주어진 엔드포인트에 대한 자격 증명을 환경 변수 및 [Amazon EC2](https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud) 메타데이터에서 얻으려고 시도합니다. 선택 사항이며 기본값은 `false`입니다.
- `region` — S3 리전 이름을 지정합니다. 선택 사항입니다.
- `use_insecure_imds_request` — `true`로 설정하면 S3 클라이언트는 Amazon EC2 메타데이터에서 자격 증명을 얻는 동안 불안전한 IMDS 요청을 사용합니다. 선택 사항이며 기본값은 `false`입니다.
- `expiration_window_seconds` — 만료 기반 자격 증명이 만료되었는지 확인하기 위한 유예 기간. 선택 사항이며 기본값은 `120`입니다.
- `no_sign_request` - 모든 자격 증명을 무시하여 요청이 서명되지 않도록 합니다. 공개 버킷에 접근하는 데 유용합니다.
- `header` — 지정된 HTTP 헤더를 주어진 엔드포인트에 대한 요청에 추가합니다. 선택 사항이며 여러 번 지정할 수 있습니다.
- `access_header` - 다른 소스에서 자격 증명이 없는 경우 주어진 엔드포인트에 대한 요청에 지정된 HTTP 헤더를 추가합니다.
- `server_side_encryption_customer_key_base64` — 지정된 경우, SSE-C 암호화가 적용된 S3 개체에 접근하기 위한 필수 헤더가 설정됩니다. 선택 사항입니다.
- `server_side_encryption_kms_key_id` - 지정된 경우, [SSE-KMS 암호화](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html)가 적용된 S3 객체에 접근하기 위한 필수 헤더가 설정됩니다. 빈 문자열이 지정된 경우 AWS 관리 S3 키가 사용됩니다. 선택 사항입니다.
- `server_side_encryption_kms_encryption_context` - `server_side_encryption_kms_key_id`와 함께 지정된 경우 지정된 암호화 컨텍스트 헤더가 SSE-KMS에 설정됩니다. 선택 사항입니다.
- `server_side_encryption_kms_bucket_key_enabled` - `server_side_encryption_kms_key_id`와 함께 지정된 경우 SSE-KMS에 S3 버킷 키를 활성화하는 헤더가 설정됩니다. 선택 사항이며 `true` 또는 `false`일 수 있으며 기본값은 아무것도 설정되지 않았습니다(버킷 수준 설정과 일치).
- `max_single_read_retries` — 단일 읽기 중 최대 재시도 횟수. 기본값은 `4`입니다. 선택 사항입니다.
- `max_put_rps`, `max_put_burst`, `max_get_rps` 및 `max_get_burst` - 특정 엔드포인트에 대해 사용되는 제한 설정(위 설명 참조). 선택 사항입니다.

**예시:**

```xml
<s3>
    <endpoint-name>
        <endpoint>https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/</endpoint>
        <!-- <access_key_id>ACCESS_KEY_ID</access_key_id> -->
        <!-- <secret_access_key>SECRET_ACCESS_KEY</secret_access_key> -->
        <!-- <region>us-west-1</region> -->
        <!-- <use_environment_credentials>false</use_environment_credentials> -->
        <!-- <use_insecure_imds_request>false</use_insecure_imds_request> -->
        <!-- <expiration_window_seconds>120</expiration_window_seconds> -->
        <!-- <no_sign_request>false</no_sign_request> -->
        <!-- <header>Authorization: Bearer SOME-TOKEN</header> -->
        <!-- <server_side_encryption_customer_key_base64>BASE64-ENCODED-KEY</server_side_encryption_customer_key_base64> -->
        <!-- <server_side_encryption_kms_key_id>KMS_KEY_ID</server_side_encryption_kms_key_id> -->
        <!-- <server_side_encryption_kms_encryption_context>KMS_ENCRYPTION_CONTEXT</server_side_encryption_kms_encryption_context> -->
        <!-- <server_side_encryption_kms_bucket_key_enabled>true</server_side_encryption_kms_bucket_key_enabled> -->
        <!-- <max_single_read_retries>4</max_single_read_retries> -->
    </endpoint-name>
</s3>
```

## 아카이브 작업하기 {#working-with-archives}

S3에 다음 URIs를 가진 여러 아카이브 파일이 있다고 가정합니다:

- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip'

이 아카이브에서 데이터 추출은 ::를 사용하여 가능합니다. Globs는 URL 부분과 아카이브 내부 파일 이름 부분 모두에서 사용할 수 있습니다.

```sql
SELECT *
FROM s3(
   'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-1{0..2}.csv.zip :: *.csv'
);
```

:::note
ClickHouse는 세 가지 아카이브 형식을 지원합니다:
ZIP
TAR
7Z
ZIP와 TAR 아카이브는 지원되는 모든 저장소 위치에서 접근할 수 있지만 7Z 아카이브는 ClickHouse가 설치된 로컬 파일 시스템에서만 읽을 수 있습니다.
:::

## 공개 버킷 접근하기 {#accessing-public-buckets}

ClickHouse는 여러 다양한 소스에서 자격 증명을 가져오려고 합니다. 때때로 이로 인해 공개 버킷에 접근할 때 클라이언트가 `403` 오류 코드를 반환하는 문제가 발생할 수 있습니다. 이 문제는 `NOSIGN` 키워드를 사용하여 해결할 수 있습니다. 이 키워드는 클라이언트가 모든 자격 증명을 무시하고 요청에 서명하지 않도록 강제합니다.

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv', NOSIGN, 'CSVWithNames');
```

## 성능 최적화 {#optimizing-performance}

s3 기능의 성능 최적화에 대한 자세한 내용은 [상세 가이드]( /integrations/s3/performance)를 참조하십시오.

## 참고 {#see-also}

- [s3 테이블 함수](../../../sql-reference/table-functions/s3.md)
- [S3와 ClickHouse 통합](/integrations/s3)
