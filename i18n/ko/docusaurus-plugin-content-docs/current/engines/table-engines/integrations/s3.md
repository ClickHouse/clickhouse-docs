---
description: '이 엔진은 Amazon S3 생태계와의 통합을 제공합니다. HDFS 엔진과 유사하지만 S3 특화 기능을 제공합니다.'
sidebar_label: 'S3'
sidebar_position: 180
slug: /engines/table-engines/integrations/s3
title: 'S3 테이블 엔진'
doc_type: 'reference'
---



# S3 테이블 엔진 \{#s3-table-engine\}

이 엔진은 [Amazon S3](https://aws.amazon.com/s3/) 에코시스템과 통합을 제공합니다. 이 엔진은 [HDFS](/engines/table-engines/integrations/hdfs) 엔진과 유사하지만 S3에 특화된 기능을 제공합니다.



## 예제 \{#example\}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE=S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'gzip')
    SETTINGS input_format_with_names_use_header = 0;

INSERT INTO s3_engine_table VALUES ('one', 1), ('two', 2), ('three', 3);

SELECT * FROM s3_engine_table LIMIT 2;
```

```text
┌─name─┬─value─┐
│ one  │     1 │
│ two  │     2 │
└──────┴───────┘
```


## 테이블 생성 \{#creating-a-table\}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE = S3(path [, NOSIGN | aws_access_key_id, aws_secret_access_key,] format, [compression], [partition_strategy], [partition_columns_in_data_file])
    [PARTITION BY expr]
    [SETTINGS ...]
```

### Engine parameters \{#parameters\}

* `path` — 파일이 위치한 버킷 URL과 경로입니다. 읽기 전용 모드에서 다음 와일드카드를 지원합니다: `*`, `**`, `?`, `{abc,def}`, `{N..M}`. 여기서 `N`, `M`은 숫자이고, `'abc'`, `'def'`는 문자열입니다. 자세한 내용은 [아래](#wildcards-in-path)를 참조하십시오.
* `NOSIGN` - 이 키워드를 자격 증명 위치에 지정하면 모든 요청에 서명이 적용되지 않습니다.
* `format` — 파일의 [format](/sql-reference/formats#formats-overview)입니다.
* `aws_access_key_id`, `aws_secret_access_key` - [AWS](https://aws.amazon.com/) 계정 사용자의 장기 자격 증명입니다. 이 값을 사용하여 요청을 인증할 수 있습니다. 선택적인 매개변수입니다. 자격 증명이 지정되지 않으면 설정 파일에 있는 값이 사용됩니다. 자세한 내용은 [Using S3 for Data Storage](../mergetree-family/mergetree.md#table_engine-mergetree-s3)를 참조하십시오.
* `compression` — 압축 유형입니다. 지원되는 값: `none`, `gzip/gz`, `brotli/br`, `xz/LZMA`, `zstd/zst`. 선택적인 매개변수입니다. 기본적으로 파일 확장자를 기준으로 압축 형식을 자동으로 감지합니다.
* `partition_strategy` – 옵션: `WILDCARD` 또는 `HIVE`. `WILDCARD`는 경로에 `{_partition_id}`가 포함되어 있어야 하며, 이 값이 파티션 키로 대체됩니다. `HIVE`는 와일드카드를 허용하지 않으며, 경로를 테이블 루트로 간주하고, 파일 이름은 Snowflake ID, 확장자는 파일 포맷인 Hive 스타일 파티션 디렉터리를 생성합니다. 기본값은 `WILDCARD`입니다.
* `partition_columns_in_data_file` - `HIVE` 파티션 전략에서만 사용됩니다. 데이터 파일에 파티션 컬럼이 기록되어 있을 것으로 ClickHouse가 예상해야 하는지 여부를 지정합니다. 기본값은 `false`입니다.
* `storage_class_name` - 옵션: `STANDARD` 또는 `INTELLIGENT_TIERING`이며, [AWS S3 Intelligent Tiering](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/)을 지정할 수 있습니다.

### Data cache \{#data-cache\}

`S3` 테이블 엔진은 로컬 디스크에 데이터 캐시를 지원합니다.
파일 시스템 캐시 설정 옵션과 사용 방법은 이 [섹션](/operations/storing-data.md/#using-local-cache)을 참조하십시오.
캐싱은 경로와 스토리지 오브젝트의 ETag에 따라 수행되므로 ClickHouse는 오래된 캐시 버전을 읽지 않습니다.

캐싱을 활성화하려면 `filesystem_cache_name = '<name>'` 및 `enable_filesystem_cache = 1` 설정을 사용하십시오.

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
SETTINGS filesystem_cache_name = 'cache_for_s3', enable_filesystem_cache = 1;
```

구성 파일에서 캐시를 정의하는 방법은 두 가지가 있습니다.

1. ClickHouse 구성 파일에 다음 섹션을 추가합니다:

```xml
<clickhouse>
    <filesystem_caches>
        <cache_for_s3>
            <path>path to cache directory</path>
            <max_size>10Gi</max_size>
        </cache_for_s3>
    </filesystem_caches>
</clickhouse>
```

2. ClickHouse `storage_configuration` 섹션의 [여기에서 설명한](/operations/storing-data.md/#using-local-cache) 캐시 구성(및 그에 따른 캐시 스토리지)을 재사용합니다.

### PARTITION BY \{#partition-by\}

`PARTITION BY` — 선택 사항입니다. 대부분의 경우 파티션 키가 필요하지 않으며, 필요하더라도 일반적으로 월 단위보다 더 세밀한 파티션 키는 필요하지 않습니다. 파티셔닝은 쿼리를 빠르게 하지 않습니다(ORDER BY 표현식과는 대조적입니다). 지나치게 세밀한 파티셔닝은 절대 사용하지 않아야 합니다. 데이터는 클라이언트 식별자나 이름으로 파티셔닝하지 말고, 대신 클라이언트 식별자나 이름을 ORDER BY 표현식의 첫 번째 컬럼으로 두십시오.

월 단위로 파티셔닝하려면 `toYYYYMM(date_column)` 표현식을 사용하십시오. 여기서 `date_column`은 [Date](/sql-reference/data-types/date.md) 타입의 날짜를 가진 컬럼입니다. 이 경우 파티션 이름은 `"YYYYMM"` 형식을 사용합니다.

#### Partition strategy \{#partition-strategy\}

`WILDCARD`(기본값): 파일 경로의 `{_partition_id}` 와일드카드를 실제 파티션 키로 대체합니다. 읽기는 지원되지 않습니다.


`HIVE`는 읽기 및 쓰기에 대해 Hive 스타일 파티셔닝을 구현합니다. 읽기는 재귀적인 glob 패턴을 사용하여 구현되며, `SELECT * FROM s3('table_root/**.parquet')`와 동일합니다.
쓰기는 다음 형식으로 파일을 생성합니다: `<prefix>/<key1=val1/key2=val2...>/<snowflakeid>.<toLower(file_format)>`.

참고: `HIVE` 파티션 전략을 사용할 때는 `use_hive_partitioning` 설정이 아무런 영향을 미치지 않습니다.

`HIVE` 파티션 전략 예시:

```sql
arthur :) CREATE TABLE t_03363_parquet (year UInt16, country String, counter UInt8)
ENGINE = S3(s3_conn, filename = 't_03363_parquet', format = Parquet, partition_strategy='hive')
PARTITION BY (year, country);

arthur :) INSERT INTO t_03363_parquet VALUES
    (2022, 'USA', 1),
    (2022, 'Canada', 2),
    (2023, 'USA', 3),
    (2023, 'Mexico', 4),
    (2024, 'France', 5),
    (2024, 'Germany', 6),
    (2024, 'Germany', 7),
    (1999, 'Brazil', 8),
    (2100, 'Japan', 9),
    (2024, 'CN', 10),
    (2025, '', 11);

arthur :) select _path, * from t_03363_parquet;

    ┌─_path──────────────────────────────────────────────────────────────────────┬─year─┬─country─┬─counter─┐
 1. │ test/t_03363_parquet/year=2100/country=Japan/7329604473272971264.parquet   │ 2100 │ Japan   │       9 │
 2. │ test/t_03363_parquet/year=2024/country=France/7329604473323302912.parquet  │ 2024 │ France  │       5 │
 3. │ test/t_03363_parquet/year=2022/country=Canada/7329604473314914304.parquet  │ 2022 │ Canada  │       2 │
 4. │ test/t_03363_parquet/year=1999/country=Brazil/7329604473289748480.parquet  │ 1999 │ Brazil  │       8 │
 5. │ test/t_03363_parquet/year=2023/country=Mexico/7329604473293942784.parquet  │ 2023 │ Mexico  │       4 │
 6. │ test/t_03363_parquet/year=2023/country=USA/7329604473319108608.parquet     │ 2023 │ USA     │       3 │
 7. │ test/t_03363_parquet/year=2025/country=/7329604473327497216.parquet        │ 2025 │         │      11 │
 8. │ test/t_03363_parquet/year=2024/country=CN/7329604473310720000.parquet      │ 2024 │ CN      │      10 │
 9. │ test/t_03363_parquet/year=2022/country=USA/7329604473298137088.parquet     │ 2022 │ USA     │       1 │
10. │ test/t_03363_parquet/year=2024/country=Germany/7329604473306525696.parquet │ 2024 │ Germany │       6 │
11. │ test/t_03363_parquet/year=2024/country=Germany/7329604473306525696.parquet │ 2024 │ Germany │       7 │
    └────────────────────────────────────────────────────────────────────────────┴──────┴─────────┴─────────┘
```

### 파티션된 데이터 쿼리하기 \{#querying-partitioned-data\}

이 예제에서는 ClickHouse와 MinIO가 통합된 [docker compose recipe](https://github.com/ClickHouse/examples/tree/5fdc6ff72f4e5137e23ea075c88d3f44b0202490/docker-compose-recipes/recipes/ch-and-minio-S3)를 사용합니다. 엔드포인트와 인증 값을 S3에 맞게 변경하면 동일한 쿼리를 실행할 수 있습니다.

`ENGINE` 설정의 S3 엔드포인트는 S3 오브젝트(파일 이름)의 일부로 매개변수 토큰 `{_partition_id}`를 사용하며, SELECT 쿼리는 그 결과로 생성된 오브젝트 이름(예: `test_3.csv`)을 대상으로 실행된다는 점에 유의하십시오.


:::note
예시에서 볼 수 있듯이, 현재는 파티션된 S3 테이블에 대해
직접 쿼리를 실행하는 기능은 지원되지 않습니다. 그러나 S3 테이블 함수를 사용해
각 개별 파티션을 쿼리함으로써 동일한 작업을 수행할 수 있습니다.

S3에 파티션된 데이터를 쓰는 주요 사용 사례는
그 데이터를 다른 ClickHouse 시스템(예: 온프레미스 시스템에서 ClickHouse
Cloud로 이전하는 경우)으로 전송할 수 있도록 하는 것입니다. ClickHouse 데이터 세트는
대부분 매우 크고 네트워크 신뢰성이 항상 완벽하지는 않으므로, 데이터 세트를
부분 단위로 전송하는 것이 합리적이며, 이러한 이유로 파티션 단위 쓰기를 사용합니다.
:::

#### 테이블 생성 \{#create-the-table\}

```sql
CREATE TABLE p
(
    `column1` UInt32,
    `column2` UInt32,
    `column3` UInt32
)
ENGINE = S3(
-- highlight-next-line
           'http://minio:10000/clickhouse//test_{_partition_id}.csv',
           'minioadmin',
           'minioadminpassword',
           'CSV')
PARTITION BY column3
```

#### 데이터 삽입 \{#insert-data\}

```sql
INSERT INTO p VALUES (1, 2, 3), (3, 2, 1), (78, 43, 45)
```

#### 파티션 3에서 조회하기 \{#select-from-partition-3\}

:::tip
이 쿼리는 S3 테이블 함수(s3 table function)를 사용합니다.
:::

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│  1 │  2 │  3 │
└────┴────┴────┘
```

#### 파티션 1에서 조회하기 \{#select-from-partition-1\}

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_1.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│  3 │  2 │  1 │
└────┴────┴────┘
```

#### 파티션 45에서 조회하기 \{#select-from-partition-45\}

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_45.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│ 78 │ 43 │ 45 │
└────┴────┴────┘
```

#### 제한 사항 \{#limitation\}

자연스럽게 `Select * from p`를 실행해 보고 싶겠지만, 위에서 언급했듯이 이 쿼리는 실패합니다. 앞에서 사용한 쿼리를 사용하십시오.

```sql
SELECT * FROM p
```

```response
Received exception from server (version 23.4.1):
Code: 48. DB::Exception: Received from localhost:9000. DB::Exception: Reading from a partitioned S3 storage is not implemented yet. (NOT_IMPLEMENTED)
```


## 데이터 삽입 \{#inserting-data\}

행은 새로운 파일에만 삽입할 수 있습니다. 머지 주기나 파일 분할 작업은 수행되지 않습니다. 한 번 파일이 작성되면 이후 삽입은 실패합니다. 이를 피하려면 `s3_truncate_on_insert` 및 `s3_create_new_file_on_insert` 설정을 사용할 수 있습니다. 자세한 내용은 [여기](/integrations/s3#inserting-data)를 참조하십시오.



## 가상 컬럼 \{#virtual-columns\}

- `_path` — 파일 경로. 타입: `LowCardinality(String)`.
- `_file` — 파일 이름. 타입: `LowCardinality(String)`.
- `_size` — 파일 크기(바이트). 타입: `Nullable(UInt64)`. 크기를 알 수 없는 경우 값은 `NULL`입니다.
- `_time` — 파일의 마지막 수정 시간. 타입: `Nullable(DateTime)`. 시간을 알 수 없는 경우 값은 `NULL`입니다.
- `_etag` — 파일의 ETag. 타입: `LowCardinality(String)`. etag를 알 수 없는 경우 값은 `NULL`입니다.
- `_tags` — 파일의 태그. 타입: `Map(String, String)`. 태그가 존재하지 않는 경우 값은 빈 맵 `{}'입니다.

가상 컬럼에 대한 자세한 내용은 [여기](../../../engines/table-engines/index.md#table_engines-virtual_columns)를 참조하십시오.



## 구현 세부 사항 \{#implementation-details\}

- 읽기와 쓰기는 병렬로 가능합니다.
- 다음은 지원되지 않습니다.
  - `ALTER` 및 `SELECT...SAMPLE` 작업.
  - 인덱스.
  - [Zero-copy](../../../operations/storing-data.md#zero-copy) 복제는 가능하지만, 지원되지는 않습니다.

  :::note Zero-copy replication is not ready for production
  Zero-copy 복제는 ClickHouse 22.8 버전 이상에서 기본적으로 비활성화되어 있습니다. 이 기능은 프로덕션 환경에서의 사용을 권장하지 않습니다.
  :::



## 경로에서 와일드카드 사용 \{#wildcards-in-path\}

`path` 인수는 bash 스타일의 와일드카드를 사용하여 여러 파일을 지정할 수 있습니다. 처리되려면 파일이 실제로 존재해야 하며 전체 경로 패턴과 일치해야 합니다. 파일 목록은 `SELECT`를 실행하는 시점에 결정되며 (`CREATE` 시점에는 결정되지 않습니다).

* `*` — 빈 문자열을 포함하여 `/`를 제외한 임의의 문자 0개 이상을 대체합니다.
* `**` — 빈 문자열을 포함하여 `/`를 포함한 임의의 문자 0개 이상을 대체합니다.
* `?` — 임의의 한 문자를 대체합니다.
* `{some_string,another_string,yet_another_one}` — 문자열 `'some_string', 'another_string', 'yet_another_one'` 중 하나로 대체됩니다.
* `{N..M}` — N과 M을 포함한 N에서 M 사이의 임의의 숫자를 대체합니다. N과 M은 예를 들어 `000..078`처럼 앞에 0을 포함할 수 있습니다.

`{}`를 사용하는 구문은 [remote](../../../sql-reference/table-functions/remote.md) 테이블 함수와 유사합니다.

:::note
파일 목록에 앞에 0이 붙은 숫자 범위가 포함되어 있는 경우, 각 자릿수마다 중괄호를 사용하는 방식이나 `?`를 사용하십시오.
:::

**와일드카드 예시 1**

`file-000.csv`, `file-001.csv`, ... , `file-999.csv`라는 이름의 파일로 테이블을 생성합니다:

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/my_folder/file-{000..999}.csv', 'CSV');
```

**와일드카드 예시 2**

S3에 다음과 같은 URI를 가진 CSV 형식의 파일이 여러 개 있다고 가정합니다:

* &#39;https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some&#95;folder/some&#95;file&#95;1.csv&#39;
* &#39;https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some&#95;folder/some&#95;file&#95;2.csv&#39;
* &#39;https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some&#95;folder/some&#95;file&#95;3.csv&#39;
* &#39;https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another&#95;folder/some&#95;file&#95;1.csv&#39;
* &#39;https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another&#95;folder/some&#95;file&#95;2.csv&#39;
* &#39;https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another&#95;folder/some&#95;file&#95;3.csv&#39;

이 6개 파일 전체로 구성된 테이블을 만드는 방법은 여러 가지가 있습니다:

1. 파일 이름 접미사 범위를 지정합니다:

```sql
CREATE TABLE table_with_range (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_{1..3}', 'CSV');
```

2. `some_file_` 접두사가 붙은 모든 파일을 사용합니다(두 폴더 모두에 이 접두사를 가진 추가 파일이 존재하면 안 됩니다).

```sql
CREATE TABLE table_with_question_mark (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_?', 'CSV');
```

3. 두 폴더에 있는 모든 파일을 가져옵니다(모든 파일은 쿼리에서 설명된 포맷과 스키마를 충족해야 합니다).

```sql
CREATE TABLE table_with_asterisk (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/*', 'CSV');
```


## Storage settings \{#storage-settings\}

- [s3_truncate_on_insert](/operations/settings/settings.md#s3_truncate_on_insert) - 파일에 데이터를 삽입하기 전에 기존 파일을 잘라낼 수 있도록 합니다. 기본적으로 비활성화되어 있습니다.
- [s3_create_new_file_on_insert](/operations/settings/settings.md#s3_create_new_file_on_insert) - 포맷에 접미사가 있는 경우 각 삽입마다 새 파일을 생성할 수 있도록 합니다. 기본적으로 비활성화되어 있습니다.
- [s3_skip_empty_files](/operations/settings/settings.md#s3_skip_empty_files) - 읽는 동안 비어 있는 파일을 건너뛸 수 있도록 합니다. 기본적으로 활성화되어 있습니다.



## S3-related settings \{#settings\}

다음 설정은 쿼리 실행 전에 설정하거나 설정 파일에 포함할 수 있습니다.

- `s3_max_single_part_upload_size` — S3로 단일 파트 업로드(singlepart upload)를 사용할 때 업로드할 수 있는 객체의 최대 크기입니다. 기본값은 `32Mb`입니다.
- `s3_min_upload_part_size` — [S3 Multipart upload](https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html)를 통한 멀티파트 업로드 시 업로드할 파트의 최소 크기입니다. 기본값은 `16Mb`입니다.
- `s3_max_redirects` — 허용되는 S3 리다이렉트 홉의 최대 개수입니다. 기본값은 `10`입니다.
- `s3_single_read_retries` — 단일 읽기(single read) 시도 횟수의 최대값입니다. 기본값은 `4`입니다.
- `s3_max_put_rps` — 스로틀링(throttling)이 걸리기 전의 초당 최대 PUT 요청 수입니다. 기본값은 `0` (무제한)입니다.
- `s3_max_put_burst` — 초당 요청 한도에 도달하기 전에 동시에 보낼 수 있는 최대 요청 개수입니다. 기본값(`0` 값)에서는 `s3_max_put_rps`와 같습니다.
- `s3_max_get_rps` — 스로틀링이 걸리기 전의 초당 최대 GET 요청 수입니다. 기본값은 `0` (무제한)입니다.
- `s3_max_get_burst` — 초당 요청 한도에 도달하기 전에 동시에 보낼 수 있는 최대 요청 개수입니다. 기본값(`0` 값)에서는 `s3_max_get_rps`와 같습니다.
- `s3_upload_part_size_multiply_factor` - 단일 쓰기에서 `s3_multiply_parts_count_threshold`개의 파트가 S3로 업로드될 때마다 `s3_min_upload_part_size`에 곱해지는 계수입니다. 기본값은 `2`입니다.
- `s3_upload_part_size_multiply_parts_count_threshold` - 이 개수만큼의 파트가 S3로 업로드될 때마다 `s3_min_upload_part_size`는 `s3_upload_part_size_multiply_factor`만큼 곱해집니다. 기본값은 `500`입니다.
- `s3_max_inflight_parts_for_one_file` - 하나의 객체에 대해 동시에 실행될 수 있는 put 요청 수를 제한합니다. 이 값은 제한하는 것이 바람직합니다. 값이 `0`이면 무제한을 의미합니다. 기본값은 `20`입니다. 각 진행 중인(in-flight) 파트는 처음 `s3_upload_part_size_multiply_factor`개의 파트에 대해서는 `s3_min_upload_part_size` 크기의 버퍼를 가지며, 파일이 충분히 클 경우 더 큰 버퍼를 사용합니다. 자세한 내용은 `upload_part_size_multiply_factor`를 참조하십시오. 기본 설정에서는 크기가 `8G` 미만인 파일 하나를 업로드할 때 최대 `320Mb`까지만 사용합니다. 더 큰 파일의 경우 소비량이 더 커집니다.

보안 관련 주의 사항: 악의적인 사용자가 임의의 S3 URL을 지정할 수 있는 경우, [SSRF](https://en.wikipedia.org/wiki/Server-side_request_forgery) 공격을 방지하려면 `s3_max_redirects`를 0으로 설정해야 합니다. 또는 서버 설정에서 `remote_host_filter`를 지정해야 합니다.



## 엔드포인트 기반 설정 \{#endpoint-settings\}

다음 설정은 지정된 엔드포인트에 대해 설정 파일에서 지정할 수 있습니다(지정된 엔드포인트는 URL의 정확한 접두사와 일치하여 매칭됩니다).

* `endpoint` — 엔드포인트의 접두사를 지정합니다. 필수입니다.
* `access_key_id` 및 `secret_access_key` — 지정된 엔드포인트에 사용할 자격 증명을 지정합니다. 선택 사항입니다.
* `use_environment_credentials` — `true`로 설정하면, 지정된 엔드포인트에 대해 S3 클라이언트가 환경 변수와 [Amazon EC2](https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud) 메타데이터에서 자격 증명을 가져오려고 시도합니다. 선택 사항이며 기본값은 `false`입니다.
* `region` — S3 리전 이름을 지정합니다. 선택 사항입니다.
* `use_insecure_imds_request` — `true`로 설정하면, S3 클라이언트가 Amazon EC2 메타데이터에서 자격 증명을 가져올 때 보안이 적용되지 않은 IMDS 요청을 사용합니다. 선택 사항이며 기본값은 `false`입니다.
* `expiration_window_seconds` — 만료 시간을 기준으로 하는 자격 증명이 만료되었는지 확인하기 위한 유예 기간입니다. 선택 사항이며 기본값은 `120`입니다.
* `no_sign_request` - 모든 자격 증명을 무시하여 요청에 서명이 적용되지 않도록 합니다. 공개 버킷에 접근할 때 유용합니다.
* `header` — 지정된 HTTP 헤더를 해당 엔드포인트로 전송되는 요청에 추가합니다. 선택 사항이며, 여러 번 지정할 수 있습니다.
* `access_header` - 다른 소스에서 제공되는 자격 증명이 없을 때, 지정된 HTTP 헤더를 해당 엔드포인트로 전송되는 요청에 추가합니다.
* `server_side_encryption_customer_key_base64` — 지정된 경우, SSE-C 암호화가 적용된 S3 객체에 접근하는 데 필요한 헤더가 설정됩니다. 선택 사항입니다.
* `server_side_encryption_kms_key_id` - 지정된 경우, [SSE-KMS 암호화](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html)가 적용된 S3 객체에 접근하는 데 필요한 헤더가 설정됩니다. 빈 문자열이 지정되면 AWS 관리 S3 키가 사용됩니다. 선택 사항입니다.
* `server_side_encryption_kms_encryption_context` - `server_side_encryption_kms_key_id`와 함께 지정된 경우, SSE-KMS용 암호화 컨텍스트 헤더가 설정됩니다. 선택 사항입니다.
* `server_side_encryption_kms_bucket_key_enabled` - `server_side_encryption_kms_key_id`와 함께 지정된 경우, SSE-KMS용 S3 버킷 키를 활성화하는 헤더가 설정됩니다. 선택 사항이며, `true` 또는 `false`가 될 수 있고, 기본값은 설정되지 않은 상태(버킷 수준 설정과 일치)입니다.
* `max_single_read_retries` — 단일 읽기 동안 시도할 최대 횟수입니다. 기본값은 `4`입니다. 선택 사항입니다.
* `max_put_rps`, `max_put_burst`, `max_get_rps` 및 `max_get_burst` - 특정 엔드포인트에 대해, 쿼리 단위가 아닌 엔드포인트 단위로 사용할 속도 제한(스로틀링) 설정입니다(위 설명을 참조하십시오). 선택 사항입니다.

**예시:**

```xml
<s3>
    <endpoint-name>
        <endpoint>https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/</endpoint>
        <!-- <access_key_id>ACCESS_KEY_ID</access_key_id> -->
        <!-- <secret_access_key>SECRET_ACCESS_KEY</secret_access_key> -->
        <!-- <region>us-west-1</region> -->
        <!-- <use_environment_credentials>false</use_environment_credentials> -->
        <!-- <use_insecure_imds_request>false</use_insecure_imds_request> -->
        <!-- <expiration_window_seconds>120</expiration_window_seconds> -->
        <!-- <no_sign_request>false</no_sign_request> -->
        <!-- <header>Authorization: Bearer SOME-TOKEN</header> -->
        <!-- <server_side_encryption_customer_key_base64>BASE64-ENCODED-KEY</server_side_encryption_customer_key_base64> -->
        <!-- <server_side_encryption_kms_key_id>KMS_KEY_ID</server_side_encryption_kms_key_id> -->
        <!-- <server_side_encryption_kms_encryption_context>KMS_ENCRYPTION_CONTEXT</server_side_encryption_kms_encryption_context> -->
        <!-- <server_side_encryption_kms_bucket_key_enabled>true</server_side_encryption_kms_bucket_key_enabled> -->
        <!-- <max_single_read_retries>4</max_single_read_retries> -->
    </endpoint-name>
</s3>
```


## 아카이브 작업하기 \{#working-with-archives\}

S3에 다음 URI를 가진 여러 아카이브 파일이 있다고 가정합니다.

* &#39;https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip&#39;
* &#39;https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip&#39;
* &#39;https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip&#39;

이러한 아카이브에서 데이터는 :: 구문을 사용하여 추출할 수 있습니다. 글롭(glob) 패턴은 URL 부분과 :: 뒤의 부분(아카이브 내부의 파일 이름을 지정하는 부분) 모두에 사용할 수 있습니다.

```sql
SELECT *
FROM s3(
   'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-1{0..2}.csv.zip :: *.csv'
);
```

:::note
ClickHouse는 다음과 같은 3가지 아카이브 형식을 지원합니다.
ZIP
TAR
7Z
ZIP 및 TAR 아카이브는 지원되는 모든 스토리지 위치에서 사용 가능하지만, 7Z 아카이브는 ClickHouse가 설치된 로컬 파일 시스템에서만 읽을 수 있습니다.
:::


## 퍼블릭 버킷에 접근하기 \{#accessing-public-buckets\}

ClickHouse는 여러 유형의 소스에서 자격 증명을 가져오려고 시도합니다.
때때로 퍼블릭으로 설정된 일부 버킷에 접근할 때 문제가 발생하여 클라이언트가 `403` 오류 코드를 반환할 수 있습니다.
`NOSIGN` 키워드를 사용하면 이 문제를 피할 수 있으며, 클라이언트가 모든 자격 증명을 무시하고 요청에 서명하지 않도록 강제합니다.

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv', NOSIGN, 'CSVWithNames');
```


## 성능 최적화 \{#optimizing-performance\}

`s3` 함수의 성능 최적화에 대해서는 [상세 가이드](/integrations/s3/performance)를 참조하십시오.



## 같이 보기 \{#see-also\}

- [S3 테이블 함수](../../../sql-reference/table-functions/s3.md)
- [ClickHouse와 S3 연동](/integrations/s3)
