---
'slug': '/academic_overview'
'title': '아키텍처 개요'
'description': '문서 버전의 2024 VLDB 논문'
'keywords':
- 'architecture'
'show_related_blogs': true
'doc_type': 'guide'
---

import useBrokenLinks from "@docusaurus/useBrokenLinks";
import image_01 from '@site/static/images/managing-data/core-concepts/_vldb2024_1_Figure_0.png'
import image_02 from '@site/static/images/managing-data/core-concepts/_vldb2024_2_Figure_0.png'
import image_03 from '@site/static/images/managing-data/core-concepts/_vldb2024_2_Figure_5.png'
import image_04 from '@site/static/images/managing-data/core-concepts/_vldb2024_3_Figure_7.png'
import image_05 from '@site/static/images/managing-data/core-concepts/_vldb2024_4_Figure_6.png'
import image_06 from '@site/static/images/managing-data/core-concepts/_vldb2024_5_Figure_8.png'
import image_07 from '@site/static/images/managing-data/core-concepts/_vldb2024_6_Figure_0.png'
import image_08 from '@site/static/images/managing-data/core-concepts/_vldb2024_7_Figure_1.png'
import image_09 from '@site/static/images/managing-data/core-concepts/_vldb2024_8_Figure_7.png'
import image_10 from '@site/static/images/managing-data/core-concepts/_vldb2024_10_Figure_14.png'
import image_11 from '@site/static/images/managing-data/core-concepts/_vldb2024_10_Figure_0.png'
import image_12 from '@site/static/images/managing-data/core-concepts/_vldb2024_10_Figure_12.png'
import image_13 from '@site/static/images/managing-data/core-concepts/_vldb2024_10_Figure_13.png'
import Image from '@theme/IdealImage';


<!-- needed as docusaurus cannot resolve links to span ids, we need a custom span -->
export function Anchor(props) {
    useBrokenLinks().collectAnchor(props.id);
    return <span style={{scrollMarginTop: "var(--ifm-navbar-height)"}} {...props}/>;
}

This is the web version of our [VLDB 2024 scientific paper](https://www.vldb.org/pvldb/vol17/p3731-schulze.pdf). We also [blogged](https://clickhouse.com/blog/first-clickhouse-research-paper-vldb-lightning-fast-analytics-for-everyone) about its background and journey, and recommend watching the VLDB 2024 presentation by ClickHouse CTO and creator, Alexey Milovidov:

<iframe width="1024" height="576" src="https://www.youtube.com/embed/7QXKBKDOkJE?si=5uFerjqPSXQWqDkF" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
## ABSTRACT {#abstract}

지난 수십 년 동안 저장되고 분석되는 데이터의 양은 기하급수적으로 증가했습니다. 산업 전반에 걸쳐 기업들은 이러한 데이터를 기반으로 제품을 개선하고, 성과를 평가하며, 비즈니스에 중요한 결정을 내리기 시작했습니다. 그러나 데이터 볼륨이 인터넷 규모로 증가함에 따라 기업들은 역사적 데이터와 새로운 데이터를 비용 효율적이고 확장 가능한 방식으로 관리해야 했으며, 동시에 높은 동시 쿼리를 사용해 이를 분석하고 실시간 지연 시간(예: 사용 사례에 따라 1초 미만)을 기대해야 했습니다.

이 문서에서는 고속 데이터 수집 비율로 페타바이트 규모 데이터세트에 대한 고성능 분석을 위해 설계된 인기 있는 오픈소스 OLAP 데이터베이스 ClickHouse의 개요를 제시합니다. ClickHouse의 저장소 계층은 전통적인 로그 구조 병합(LSM) 트리를 기반으로 한 데이터 형식과 배경에서 역사적 데이터의 지속적인 변환(예: 집계, 아카이빙)을 위한 새로운 기술을 결합합니다. 쿼리는 편리한 SQL 방언으로 작성되며, 선택적 코드 컴파일과 함께 최신 벡터화된 쿼리 실행 엔진에 의해 처리됩니다. ClickHouse는 irrelevant data를 평가하지 않기 위해 공격적으로 프루닝 기술을 사용합니다. 다른 데이터 관리 시스템은 테이블 함수, ^^table engine^^, 또는 데이터베이스 엔진 수준에서 통합할 수 있습니다. 실제 벤치마크는 ClickHouse가 시장에서 가장 빠른 분석 데이터베이스 중 하나임을 보여줍니다.
## 1 INTRODUCTION  {#1-introduction}

이 문서에서는 수조 행과 수백 개의 컬럼에 대한 고성능 분석 쿼리를 위해 설계된 ClickHouse라는 컬럼형 OLAP 데이터베이스를 설명합니다. ClickHouse는 웹 스케일 로그 파일 데이터에 대한 필터와 집계 연산자로 2009년에 [시작](https://clickhou.se/evolution) 되었으며, 2016년에 오픈 소스되었습니다. [Figure 1](#page-1-0)은 이 문서에서 설명하는 주요 기능이 ClickHouse에 도입된 시점을 보여줍니다.

ClickHouse는 현대 분석 데이터 관리의 다섯 가지 주요 문제를 해결하기 위해 설계되었습니다:

1. **높은 수집률의 거대한 데이터 세트**. 웹 분석, 금융 및 전자 상거래와 같은 산업에서 데이터 기반 애플리케이션은 엄청나고 지속적으로 증가하는 데이터 규모가 특징입니다. 이러한 거대한 데이터 세트를 처리하기 위해, 분석 데이터베이스는 효율적인 인덱싱 및 압축 전략을 제공할 뿐만 아니라, 개별 서버가 수십 테라바이트의 저장 용량으로 제한되므로 여러 노드에 대한 데이터 분배를 허용해야 합니다. 또한, 최근 데이터는 일반적으로 역사적 데이터보다 실시간 통찰력에 보다 관련성이 높습니다. 따라서 분석 데이터베이스는 일관되게 높은 비율로 새 데이터를 수집하거나 갑자기 대량으로 수집할 수 있어야 할 뿐만 아니라, 병렬 보고 쿼리를 늦추지 않고 역사적 데이터를 지속적으로 "비우선순위화" (예: 집계, 아카이빙) 할 수 있어야 합니다.

2. **저지연이 기대되는 많은 동시 쿼리**. 쿼리는 일반적으로 비정기적(예: 탐색적 데이터 분석) 또는 반복적(예: 주기적인 대시보드 쿼리)으로 분류할 수 있습니다. 사용 사례가 대화형일수록 쿼리 지연이 낮아져야 하며, 쿼리 최적화 및 실행에 도전이 됩니다. 반복 쿼리는 또한 워크로드에 따라 물리적 데이터베이스 레이아웃을 조정할 기회를 제공합니다. 결과적으로, 데이터베이스는 빈번한 쿼리를 최적화할 수 있는 프루닝 기술을 제공해야 합니다. 쿼리 우선 순위에 따라 데이터베이스는 CPU, 메모리, 디스크 및 네트워크 I/O와 같은 공유 시스템 리소스에 대해 동등하거나 우선순위가 있는 접근을 부여해야 하며, 비록 많은 수의 쿼리가 동시에 실행되는 경우에도 그렇습니다.

3. **다양한 데이터 저장소, 저장 위치 및 형식**. 기존 데이터 아키텍처와 통합하기 위해, 현대의 분석 데이터베이스는 모든 시스템, 위치 또는 형식에서 외부 데이터를 읽고 쓸 수 있는 높은 개방성을 가져야 합니다.

4. **성능 검토를 지원하는 편리한 쿼리 언어**. OLAP 데이터베이스의 실제 사용은 추가적인 "부드러운" 요구 사항을 제기합니다. 예를 들어, 틈새 프로그래밍 언어 대신 사용자는 종종 중첩 데이터 타입과 광범위한 일반, 집계 및 윈도우 함수를 갖춘 표현력이 풍부한 SQL 방언으로 데이터베이스와 상호 작용하는 것을 선호합니다. 분석 데이터베이스는 또한 시스템 또는 개별 쿼리의 성능을 조사하기 위한 정교한 도구를 제공해야 합니다.

5. **산업 수준의 견고함 및 다양한 배포**. 일반 하드웨어는 신뢰할 수 없으므로, 데이터베이스는 노드 실패에 대한 견고성을 위해 데이터 복제를 제공해야 합니다. 또한 데이터베이스는 오래된 노트북에서 강력한 서버에 이르기까지 모든 하드웨어에서 실행되어야 합니다. 마지막으로, JVM 기반 프로그램의 가비지 수집 오버헤드를 피하고 베어메탈 성능(예: SIMD)을 가능하게 하기 위해 데이터베이스는 이상적으로 타겟 플랫폼을 위한 네이티브 바이너리로 배포됩니다.

<Anchor id="page-1-0"/><Image img={image_01} size="lg" alt="Image 01"/>

Figure 1: ClickHouse 타임라인.
## 2 ARCHITECTURE {#2-architecture}

<Anchor id="page-2-0"/><Image img={image_02} size="lg" alt="Image 02"/>

Figure 2: ClickHouse 데이터베이스 엔진의 고수준 아키텍처.

[Figure 2](#page-2-0)에서 볼 수 있듯이 ClickHouse 엔진은 세 가지 주요 계층으로 나뉩니다: 쿼리 처리 계층(섹션 [4)](#page-6-0)에서 설명), 저장소 계층(섹션 [3)](#page-1-1), 통합 계층(섹션 [5)](#page-9-0). 이 외에도 접근 계층은 사용자 세션 및 다양한 프로토콜을 통한 응용 프로그램과의 통신을 관리합니다. 스레딩, 캐싱, 역할 기반 접근 제어, 백업 및 지속적인 모니터링을 위한 직교 구성 요소가 있습니다. ClickHouse는 C++로 의존성 없이 단일 스태틱 링크 바이너리로 구축되었습니다.

쿼리 처리는 수신 쿼리를 구문 분석하고 논리계획 및 물리계획을 구성 및 최적화하며 실행하는 전통적인 패러다임을 따릅니다. ClickHouse는 MonetDB/X100 [\[11\]](#page-12-0)와 유사한 벡터화된 실행 모델을 사용하며, 기회에 따라 코드 컴파일을 결합합니다 [\[53\]](#page-13-0). 쿼리는 기능이 풍부한 SQL 방언, PRQL [\[76\]](#page-13-1) 또는 Kusto의 KQL [\[50\]](#page-13-2)로 작성할 수 있습니다.

저장소 계층은 테이블 데이터의 형식과 위치를 캡슐화하는 다양한 테이블 엔진으로 구성됩니다. 테이블 엔진은 세 가지 카테고리로 나눌 수 있습니다: 첫 번째 카테고리는 ClickHouse의 기본 영속성 형식을 나타내는 ^^MergeTree^^* 테이블 엔진입니다. LSM 트리 [\[60\]](#page-13-3)를 기반으로 하여, 테이블은 수평으로 정렬된 ^^parts^^로 나뉘며, 배경 프로세스에 의해 지속적으로 병합됩니다. 개별 ^^MergeTree^^* 테이블 엔진은 입력 ^^parts^^에서 행을 병합하는 방식이 다릅니다. 예를 들어, 행은 집계되거나 구식인 경우 교체될 수 있습니다.

두 번째 카테고리는 특별 목적의 테이블 엔진으로, 쿼리 실행을 가속화하거나 분산하는 데 사용됩니다. 이 카테고리에는 딕셔너리라고 불리는 인메모리 키-값 테이블 엔진이 포함됩니다. [딕셔너리](https://clickhou.se/dictionaries)는 내부 또는 외부 데이터 소스에 대해 주기적으로 실행되는 쿼리의 결과를 캐시합니다. 이는 데이터의 오래된 상태가 허용될 수 있는 시나리오에서 접근 지연 시간을 크게 줄입니다. 특별 목적의 테이블 엔진의 다른 예로는 임시 테이블에 사용되는 순수 인메모리 엔진과 투명한 데이터 샤딩을 위한 ^^Distributed table^^ 엔진이 있습니다 (아래 참조).

세 번째 카테고리의 테이블 엔진은 관계형 데이터베이스(예: PostgreSQL, MySQL), 발행/구독 시스템(예: Kafka, RabbitMQ [\[24\]](#page-12-1)), 또는 키/값 저장소(예: Redis)와의 양방향 데이터 교환을 위한 가상 테이블 엔진입니다. 가상 엔진은 데이터 레이크(예: Iceberg, DeltaLake, Hudi [\[36\]](#page-12-2)) 또는 객체 저장소의 파일(예: AWS S3, Google GCP)과도 상호작용할 수 있습니다.

ClickHouse는 스케일 가능성과 가용성을 위해 여러 ^^cluster^^ 노드에 걸쳐 테이블의 샤딩과 복제를 지원합니다. 샤딩은 샤딩 표현식에 따라 테이블을 일련의 테이블 샤드로 분할합니다. 개별 샤드는 상호 독립적인 테이블이며, 일반적으로 서로 다른 노드에 위치합니다. 클라이언트는 샤드를 직접 읽고 쓸 수 있으며, 즉 이를 별도의 테이블로 취급하거나 모든 테이블 샤드의 글로벌 뷰를 제공하는 Distributed 특별 ^^table engine^^을 사용할 수 있습니다. 샤딩의 주요 목적은 개별 노드의 용량을 초과하는 데이터 세트를 처리하는 것입니다(일반적으로 수십 테라바이트의 데이터 기준). 샤딩의 또 다른 용도는 테이블에 대한 읽기-쓰기 부하를 여러 노드에 분산시켜 부하 균형을 이루는 것입니다. 이러한 것과는 별개로, ^^shard^^는 노드 실패에 대한 내성을 위해 여러 노드에서 복제할 수 있습니다. 이를 위해 각 Merge-Tree* ^^table engine^^은 해당 ReplicatedMergeTree* 엔진을 가지며, 이는 Raft 합의 [\[59\]](#page-13-4) 기반의 다중 마스터 조정 방식을 사용하여 모든 ^^shard^^가 항상 구성 가능한 수의 복제본을 가지도록 보장합니다. 섹션 [3.6](#page-5-0)에서 복제 메커니즘에 대해 자세히 설명합니다. 예를 들어, [Figure 2](#page-2-0)는 두 개의 샤드가 있는 테이블을 나타내며, 각 샤드는 두 노드에 복제됩니다.

마지막으로, ClickHouse 데이터베이스 엔진은 온프레미스, 클라우드, 독립 실행형 또는 프로세스 내 모드로 운영할 수 있습니다. 온프레미스 모드에서는 사용자가 ClickHouse를 로컬에 단일 서버 또는 다중 노드 ^^cluster^^로 샤딩 및/또는 복제와 함께 설정합니다. 클라이언트는 네이티브, MySQL, PostgreSQL의 바이너리 통신 프로토콜 또는 HTTP REST API를 통해 데이터베이스와 통신합니다. 클라우드 모드는 완전 관리 및 자동 확장 DBaaS 제안인 ClickHouse Cloud로 표현됩니다. 본 문서는 온프레미스 모드에 중점을 두고 있지만, ClickHouse Cloud의 아키텍처를 후속 출판물에서 설명할 계획입니다. [독립 실행형 모드](https://clickhou.se/local-fastest-tool)는 ClickHouse를 파일을 분석하고 변환하기 위한 명령줄 유틸리티로 전환하여 cat 및 grep과 같은 Unix 도구에 대한 SQL 기반 대안으로 만듭니다. 이는 사전 구성 없이도 가능하지만, 독립 실행형 모드는 단일 서버에 제한됩니다. 최근에는 Jupyter 노트북 [\[37\]](#page-12-4)와 Pandas 데이터프레임 [\[61\]](#page-13-5) 같은 인터랙티브 데이터 분석 사용 사례를 위해 chDB [\[15\]](#page-12-3)라는 프로세스 내 모드가 개발되었습니다. DuckDB [\[67\]](#page-13-6)에서 영감을 받아, [chDB](https://clickhou.se/chdb-rocket-engine)는 ClickHouse를 고성능 OLAP 엔진으로 호스트 프로세스에 포함시킵니다. 다른 모드에 비해, 이는 데이터베이스 엔진과 애플리케이션 간에 데이터 소스와 결과를 복사 없이 효율적으로 전달할 수 있게 해줍니다.

## <Anchor id="page-1-1"/>3 STORAGE LAYER {#3-storage-layer}

이 섹션에서는 ClickHouse의 기본 저장 형식으로서 ^^MergeTree^^* 테이블 엔진에 대해 논의합니다. 우리는 이들의 디스크 상의 표현을 설명하고 ClickHouse에서의 세 가지 데이터 프루닝 기술에 대해 논의합니다. 이후 우리는 동시 삽입에 영향을 주지 않고 데이터를 지속적으로 변환하는 병합 전략을 제시합니다. 마지막으로 업데이트 및 삭제, 데이터 중복 제거, 데이터 복제 및 ACID 준수에 대해 설명합니다.
### <Anchor id="page-2-2"/>3.1 On-Disk Format {#3-1-on-disk-format}

^^MergeTree^^* ^^table engine^^의 각 테이블은 불변 테이블 ^^parts^^의 모음으로 구성됩니다. 파트는 행 집합이 테이블에 삽입될 때마다 생성됩니다. ^^Parts^^는 중앙 카탈로그에 대한 추가 조회 없이도 콘텐츠를 해석하는 데 필요한 모든 메타데이터를 포함하므로 자족적인 속성을 가지고 있습니다. 테이블당 ^^parts^^ 수를 낮게 유지하기 위해, 배경 병합 작업이 주기적으로 여러 개의 작은 ^^parts^^를 더 큰 파트로 병합하여 구성 가능한 파트 크기(기본값 150 GB)에 도달할 때까지 수행됩니다. ^^Parts^^는 테이블의 ^^primary key^^ 컬럼(섹션 [3.2)](#page-3-0)로 정렬되므로, 효율적인 k-way merge sort [\[40\]](#page-12-5)가 병합에 사용됩니다. 소스 ^^parts^^는 비활성으로 표시되며, 참조 수가 제로로 떨어지면 곧 삭제됩니다. 즉 더 이상의 쿼리가 이로부터 읽지 않습니다.

행은 두 가지 모드로 삽입될 수 있습니다: 동기 삽입 모드에서는 각 INSERT 문이 새로운 파트를 생성하고 이를 테이블에 추가합니다. 병합 오버헤드를 최소화하기 위해 데이터베이스 클라이언트는 튜플을 대량으로 삽입하도록 권장됩니다. 예를 들어, 한 번에 20,000행을 삽입하는 방식입니다. 그러나 클라이언트 측의 배치로 인한 지연은 데이터를 실시간 분석해야 할 경우 종종 용납되기 어렵습니다. 예를 들어, 가시성 사용 사례에는 종종 수천 개의 모니터링 에이전트가 이벤트 및 메트릭 데이터의 소량을 지속적으로 전송해야 합니다. 이러한 시나리오에서는 ClickHouse가 여러 개의 INSERT에서 행을 버퍼링하고 버퍼의 크기가 구성 가능한 임계값을 초과하거나 타임아웃이 만료될 때까지 새로운 파트를 생성하는 비동기 삽입 모드를 이용할 수 있습니다.

<Anchor id="page-2-1"/><Image img={image_03} size="lg" alt="Image 03"/>

Figure 3: ^^MergeTree^^*-엔진 테이블을 위한 삽입 및 병합.

[Figure 3](#page-2-1)은 ^^MergeTree^^*-엔진 테이블에 대한 네 개의 동기 및 두 개의 비동기 삽입을 나타냅니다. 두 번의 병합으로 활성 ^^parts^^의 수가 처음 다섯에서 두로 줄어들었습니다.

LSM 트리 [\[58\]](#page-13-7) 및 다양한 데이터베이스에서의 이들의 구현 [\[13,](#page-12-6) [26,](#page-12-7) [56\]](#page-13-8)와 비교해서, ClickHouse는 모든 ^^parts^^를 동등하게 취급하므로 계층 구조로 정렬하지 않습니다. 그 결과, 병합은 이제 같은 수준의 ^^parts^^로 제한되지 않습니다. 이는 ^^parts^^의 암묵적인 시간적 순서를 포기하므로, tombstones에 기반하지 않은 업데이트 및 삭제에 대한 대체 메커니즘이 필요합니다(섹션 [3.4)](#page-4-0). ClickHouse는 삽입을 디스크에 직접 기록하며, 다른 LSM-트리 기반 저장소는 일반적으로 앞서 쓰기 로그를 사용합니다(섹션 [3.7)](#page-5-1).

하나의 파트는 디스크의 디렉터리에 해당하며, 각 컬럼에 대해 하나의 파일을 포함합니다. 최적화로서, 작은 파트(기본값 10 MB 미만)의 컬럼은 읽기 및 쓰기를 위한 공간 지역성을 높이기 위해 단일 파일에 연속적으로 저장됩니다. 하나의 파트의 행은 또한 논리적으로 8192레코드 그룹으로 나뉘며, 이를 granule이라고 부릅니다. ^^granule^^은 ClickHouse에서 스캔 및 인덱스 조회 연산자에 의해 처리되는 가장 작은 불가분의 데이터 단위를 나타냅니다. 그러나 디스크상의 데이터의 읽기와 쓰기는 ^^granule^^ 수준에서 이루어지지 않고, 컬럼 내의 여러 인접한 granules를 결합하는 블록의 세분화 수준에서 수행됩니다. 새로운 블록은 구성 가능한 바이트 크기가 지정된 ^^block^^당(기본값 1 MB) 형태로 형성되며, ^^block^^ 내의 granules 수는 가변적이며 컬럼의 데이터 타입 및 분포에 따라 달라집니다. 블록은 또한 크기와 I/O 비용을 줄이기 위해 압축됩니다. 기본적으로 ClickHouse는 일반 목적 압축 알고리즘으로 LZ4 [\[75\]](#page-13-9)를 사용하지만, 사용자는 Gorilla [\[63\]](#page-13-10) 또는 FPC [\[12\]](#page-12-8)와 같은 특수 코덱을 부여하여 부동 소수점 데이터에 사용할 수 있습니다. 압축 알고리즘은 또한 연쇄로 사용될 수 있습니다. 예를 들어, 먼저 델타 코딩 [\[23\]](#page-12-9)을 사용하여 숫자 값의 논리적 중복성을 줄이고, 그런 다음 무거운 압축을 수행하고, 마지막으로 AES 코덱을 사용하여 데이터를 암호화하는 방식이 가능합니다. 블록은 디스크에서 메모리로 로드될 때 즉시 압축 해제됩니다. ClickHouse는 또한 압축에도 불구하고 각각의 granule에 대한 빠른 임의 접근을 가능하게 하기 위해 각 컬럼에 대해 매핑을 추가로 저장합니다. 이 매핑은 각 ^^granule^^의 id와 컬럼 파일 내의 압축된 ^^block^^의 오프셋 및 압축되지 않은 ^^block^^ 내의 ^^granule^^의 오프셋과 관련이 있습니다.

컬럼은 또한 ^^dictionary^^-인코딩 [\[2,](#page-12-10) [77,](#page-13-11) [81\]](#page-13-12) 되거나 두 가지 특수 래퍼 데이터 타입을 사용하여 Nullable하게 만들 수 있습니다: LowCardinality(T)는 원래 컬럼 값을 정수 ID로 대체하므로 적은 고유 값에 대한 저장 오버헤드가 크게 감소합니다. Nullable(T)는 컬럼 T에 NULL 여부를 나타내는 내부 비트맵을 추가합니다.

마지막으로, 테이블은 임의의 파티셔닝 표현식을 사용하여 범위, 해시 또는 라운드 로빈으로 파티셔닝할 수 있습니다. ClickHouse는 파티션 프루닝을 가능하게 하도록 각 파티션에 대해 파티셔닝 표현식의 최소 및 최대 값을 추가로 저장합니다. 사용자는 선택적으로 고급 컬럼 통계(예: HyperLogLog [\[30\]](#page-12-11) 또는 t-digest [\[28\]](#page-12-12) 통계)를 생성하여 기수 추정치를 제공할 수 있습니다.
### <Anchor id="page-3-0"/>3.2 Data Pruning {#3-2-data-pruning}

대부분의 사용 사례에서 단일 쿼리에 응답하기 위해 페타바이트의 데이터를 스캔하는 것은 너무 느리고 비쌉니다. ClickHouse는 검색 중 과반수의 행을 건너뛰도록 허용하는 세 가지 데이터 프루닝 기술을 지원하여 쿼리를 크게 속도 향상시킬 수 있습니다.

먼저, 사용자는 테이블에 대해 **^^primary key^^ 인덱스**를 정의할 수 있습니다. ^^primary key^^ 컬럼은 각 파트 내 행의 정렬 순서를 결정하며, 즉 인덱스는 로컬로 클러스터링됩니다. ClickHouse는 또한 각 ^^granule^^의 첫 번째 행의 ^^primary key^^ 컬럼 값과 ^^granule^^ id 간의 매핑을 각 파트에 대해 추가로 저장하므로 인덱스는 희소합니다 [\[31\]](#page-12-13). 결과 데이터 구조는 일반적으로 메모리에 완전히 남도록 충분히 작습니다. 예를 들어, 810만 행을 인덱싱하기 위해서는 단 1000개의 항목이 필요합니다. ^^primary key^^의 주요 목적은 자주 필터링되는 컬럼에 대해 평등 및 범위 프레딕트를 이진 검색을 사용하여 평가하는 것이며 순차 스캔(섹션 [4.4)](#page-7-0)을 대신합니다. 지역 정렬은 또한 파트 병합 및 쿼리 최적화를 위해 활용될 수 있습니다. 예를 들어, 정렬된 컬럼이 ^^primary key^^ 컬럼의 접두사를 형성하면, 물리적 실행 계획에서 정렬 연산자를 제거하는 데 활용할 수 있습니다.

[Figure 4](#page-3-1)는 페이지 인상 통계에 대한 테이블의 EventTime 컬럼에 대한 ^^primary key^^ 인덱스를 나타냅니다. 쿼리에서 범위 프레딕트에 일치하는 granule은 EventTime을 순차적으로 스캔하는 대신 ^^primary key^^ 인덱스를 사용하여 이진 검색하여 찾을 수 있습니다.

<Anchor id="page-3-1"/><Image img={image_04} size="lg" alt="Image 04"/>

Figure 4: ^^primary key^^ 인덱스으로 필터 평가하기.

둘째, 사용자는 **테이블 프로젝션**을 생성할 수 있습니다. 즉, 주 테이블의 ^^primary key^^ [\[71\]](#page-13-13)와 다른 방식으로 정렬된 동일한 행을 포함하는 대안 버전의 테이블입니다. 프로젝션은 기본 테이블의 ^^primary key^^와 다른 컬럼을 필터링하는 쿼리 속도를 높일 수 있지만, 삽입, 병합 및 공간 소비에 대한 오버헤드는 증가합니다. 기본적으로 프로젝션은 기존 ^^parts^^에서가 아니라 주 테이블에 새로 삽입된 ^^parts^^에서 지연적으로 채워집니다. 사용자가 ^^projection^^을 완전히 실체화하지 않는 한, 쿼리 최적화기는 주 테이블에서 읽거나 프로젝션에서 읽는 것 사이를 선택합니다. 특정 부분에 대해 프로젝션이 존재하지 않으면, 쿼리 실행은 해당 기본 테이블 부분으로 되돌아갑니다.

셋째, **스킵 인덱스**는 프로젝션에 대한 경량 대안을 제공합니다. 스킵 인덱스의 아이디어는 여러 개의 연속된 granules 수준에서 소량의 메타데이터를 저장하여 irrelevant rows를 스캔하지 않도록 하는 것입니다. 스킵 인덱스는 임의의 인덱스 표현식 및 구성 가능한 granularity (즉, ^^skipping index^^ 블록 내의 granules 수)에 대해 생성할 수 있습니다. 사용 가능한 ^^skipping index^^ 유형에는 다음이 포함됩니다: 
1. 최소-최대 인덱스 [\[51\]](#page-13-14)은 각 인덱스 ^^block^^에 대해 인덱스 표현식의 최소 및 최대 값을 저장합니다. 이 인덱스 유형은 로컬로 클러스터링된 데이터에 대해 잘 동작합니다; 절대적인 범위가 작습니다. 
2. 집합 인덱스는 구성 가능한 수의 고유 인덱스 ^^block^^ 값을 저장합니다. 이 인덱스는 "응집된" 값이 적은 데이터와 함께 사용되는 것이 가장 좋습니다. 
3. Bloom 필터 인덱스 [\[9\]](#page-12-14)는 구성 가능한 거짓 긍정률을 가진 행, 토큰 또는 n-그램 값을 위해 빌드됩니다. 이러한 인덱스는 텍스트 검색을 지원하며 [\[73\]](#page-13-15) 하지만 최소-최대 및 집합 인덱스와는 달리 범위 또는 음수 프레딕트에 사용할 수 없습니다.

### <Anchor id="page-4-3"/>3.3 Merge-time Data Transformation {#3-3-merge-time-data-transformation}

비즈니스 인텔리전스 및 가시성 사용 사례는 종종 지속적으로 높은 속도의 데이터 또는 대량으로 생성되는 데이터를 처리해야 합니다. 또한, 최근 생성된 데이터는 일반적으로 역사적 데이터보다 의미 있는 실시간 통찰력에 더 관련성이 높습니다. 이러한 사용 사례는 데이터베이스가 높은 데이터 수집 비율을 유지하면서 집계 또는 데이터 노화와 같은 기술을 통해 역사적 데이터의 볼륨을 지속적으로 줄일 수 있도록 요구합니다. ClickHouse는 다양한 병합 전략을 사용하여 기존 데이터의 지속적인 점진적 변환을 허용합니다. 병합 시 데이터 변환은 INSERT 문장 성능에 영향을 미치지 않습니다. 그러나 테이블이 원하지 않는(예: 구식 또는 비집계된) 값을 포함하지 않도록 보장할 수는 없습니다. 필요한 경우 모든 병합 시간 변환은 SELECT 문에 FINAL 키워드를 지정하여 쿼리 시간에 적용될 수 있습니다.

**Replacing merges**는 소속 파트의 생성 타임스탬프에 따라 튜플의 가장 최근에 삽입된 버전만 유지하며, 이전 버전은 삭제됩니다. 튜플은 동일한 ^^primary key^^ 컬럼 값을 가질 때 동등한 것으로 간주됩니다. 어떤 튜플이 보존되는지에 대한 명시적인 제어를 위해, 비교를 위해 특별 버전 열을 지정할 수도 있습니다. Replacing merges는 일반적으로 병합 시간 업데이트 메커니즘으로 사용되거나, 삽입 시간 데이터 중복 제거의 대안으로 사용됩니다(섹션 [3.5)](#page-5-2).

**Aggregating merges**는 동등한 ^^primary key^^ 컬럼 값을 가진 행을 집계된 행으로 축소합니다. 비^^primary key^^ 컬럼은 요약 값을 보유하는 부분적인 집계 상태여야 합니다. 두 개의 부분 집계 상태(예: avg()을 위한 총합 및 수)는 새로운 부분 집계 상태로 결합됩니다. Aggregating merges는 일반 테이블 대신 물리화된 뷰에서 일반적으로 사용됩니다. 물리화된 뷰는 소스 테이블을 대상으로 하는 변환 쿼리 기반으로 채워집니다. 다른 데이터베이스와 달리 ClickHouse는 물리화된 뷰를 주기적으로 소스 테이블의 전체 내용으로 새로 고치지 않습니다. 물리화된 뷰는 오히려 새로운 파트가 소스 테이블에 삽입될 때 변환 쿼리의 결과로 점진적으로 업데이트됩니다.

[Figure 5](#page-4-1)는 페이지 인상 통계에 대한 테이블에서 정의된 ^^materialized view^^를 보여줍니다. 소스 테이블에 새로 삽입된 ^^parts^^에 대해 변환 쿼리는 지역별로 최대 및 평균 지연 시간을 계산하고 결과를 ^^materialized view^^에 삽입합니다. 집계 함수 avg()와 max()는 실제 결과 대신 부분 집계 상태를 반환하는 -State를 사용합니다. ^^materialized view^^에 대해 정의된 집계 병합은 서로 다른 ^^parts^^에서 부분 집계 상태를 지속적으로 결합합니다. 최종 결과를 얻으려면 사용자가 avg()와 max()를 사용하여 ^^materialized view^^에서 부분 집계 상태를 통합하여야 합니다 (-Merge 확장).

<Anchor id="page-4-1"/><Image img={image_05} size="lg" alt="Image 05"/>

Figure 5: 물리화된 뷰의 집계 병합.

**^^TTL^^ (time-to-live) merges**는 역사적 데이터의 노화를 제공합니다. 삭제 및 집계 병합과 달리, ^^TTL^^ 병합은 한 번에 하나의 파트만 처리합니다. ^^TTL^^ 병합은 트리거 및 작업이 포함된 규칙으로 정의됩니다. 트리거는 각 행의 타임스탬프를 계산하여 ^^TTL^^ 병합이 실행되는 시간과 비교하는 표현식입니다. 이를 통해 사용자는 행 세분화에서 작업을 제어할 수 있지만, 주어진 조건을 만족하는 모든 행을 확인하고 전체 파트에 대해 작업을 실행하는 것이 충분하다는 것을 알게 되었습니다. 가능한 작업에는 1. 파트를 다른 볼륨으로 이동(예: 더 저렴하고 느린 저장소) 2. 파트 재압축(예: 보다 무거운 코덱으로) 3. 파트 삭제 4. 롤업, 즉 집계 키 및 집계 함수를 사용하여 행을 집계하는 것이 포함됩니다.

예를 들어, [Listing 1.](#page-4-2)에서의 로깅 테이블 정의를 고려하겠습니다. ClickHouse는 타임스탬프 컬럼 값이 일주일 이상 된 ^^parts^^를 느리지만 저렴한 S3 객체 저장소로 이동할 것입니다.
<Anchor id="page-4-2"/>
```
1 CREATE TABLE tab ( ts DateTime , msg String )
2 ENGINE MergeTree PRIMARY KEY ts
3 TTL ( ts + INTERVAL 1 WEEK ) TO VOLUME 's3 '
```
Listing 1: 1주일 후 객체 저장소로 파트 이동.
### <Anchor id="page-4-0"/>3.4 Updates and Deletes {#3-4-updates-and-deletes}

^^MergeTree^^* 테이블 엔진의 설계는 추가 전용 워크로드를 선호하지만, 일부 사용 사례에서는 때때로 기존 데이터를 수정해야 합니다. 데이터 업데이트 또는 삭제를 위한 두 가지 접근 방식이 있으며, 두 접근 방식 모두 병렬 삽입을 ^^block^^하지 않습니다.

**Mutations**는 테이블의 모든 ^^parts^^를 제자리에서 다시 작성합니다. 테이블(삭제) 또는 컬럼(업데이트)이 일시적으로 크기가 두 배로 늘어나는 것을 방지하기 위해 이 작업은 비원자적입니다. 즉, 병렬 SELECT 문이 변형된 및 비변형된 ^^parts^^를 읽을 수 있습니다. 변형은 작업의 끝에서 데이터가 물리적으로 변경되도록 보장합니다. 삭제 변형은 여전히 모든 컬럼을 모든 ^^parts^^에서 다시 작성하므로 비용이 많이 듭니다.

대안으로 **경량 삭제**는 행이 삭제된 여부를 나타내는 내부 비트맵 컬럼을 업데이트합니다. ClickHouse는 SELECT 쿼리에 비트맵 컬럼에 대한 추가 필터를 덧붙여서 삭제된 행을 결과에서 제외합니다. 삭제된 행은 추후에 일정 시간에 정기적인 병합에 의해 물리적으로 제거됩니다. 컬럼 수에 따라 경량 삭제는 변형보다 훨씬 빠를 수 있지만, SELECT의 속도는 느려질 수 있습니다.

동일한 테이블에서 수행되는 업데이트 및 삭제 작업은 일반적으로 드물고 논리적 충돌을 피하기 위해 직렬화되는 것으로 예상됩니다.
### <Anchor id="page-5-2"/>3.5 Idempotent Inserts {#3-5-idempotent-inserts}

실제로 자주 발생하는 문제는 클라이언트가 데이터베이스에 테이블에 삽입하기 위해 데이터 전송 후 연결 시간이 초과된 경우 어떻게 처리해야 하는지입니다. 이 경우 클라이언트는 데이터가 성공적으로 삽입되었는지 여부를 구분하기 어렵습니다. 이 문제는 전통적으로 클라이언트가 서버로 데이터를 재전송하고 ^^primary key^^ 또는 고유 제약 조건에 의존하여 중복 삽입을 거부하도록 해결됩니다. 데이터베이스는 이진 트리[\[39,](#page-12-15) [68\]](#page-13-16), 레디스 트리[\[45,](#page-13-17) 또는 해시 테이블[\[29\]](#page-12-16)을 기반으로 하는 인덱스 구조를 빠르게 사용하여 필요한 포인트 조회를 수행합니다. 이러한 데이터 구조는 모든 튜플을 인덱싱하므로, 대량의 데이터 세트와 높은 수집률에 대해서는 공간 및 업데이트 오버헤드가 큰 부담이 됩니다.

ClickHouse는 각 삽입이 궁극적으로 파트를 생성한다는 사실을 기반으로 보다 경량 대안을 제공합니다. 더 구체적으로, 서버는 마지막 N개의 삽입된 ^^parts^^ (예: N=100)의 해시를 유지하고, 알려진 해시를 가진 ^^parts^^의 재삽입을 무시합니다. 비복제 및 복제 테이블에 대한 해시는 각각 Keeper에 로컬로 저장됩니다. 그 결과 삽입은 단일 원자적이므로 클라이언트는 시간 초과 후 동일한 행 배치를 재전송해도 서버가 중복 제거를 처리하도록 할 수 있습니다. 중복 제거 프로세스에 대한 더 많은 제어를 위해 클라이언트는 선택적으로 파트 해시로 작용하는 삽입 토큰을 제공할 수 있습니다. 해시 기반 중복 제거는 새로운 행의 해시를 해시하는 것과 관련된 오버헤드가 발생하지만, 해시를 저장하고 비교하는 비용은 무시할만한 수준입니다.

### <Anchor id="page-5-0"/>3.6 데이터 복제 {#3-6-data-replication}

복제는 고가용성(노드 장애에 대한 내구성)의 전제 조건이지만, 로드 밸런싱과 제로 다운타임 업그레이드에도 사용됩니다 [\[14\]](#page-12-17). ClickHouse에서 복제는 테이블 상태의 개념에 기반을 두고 있으며, 여기에는 테이블의 일련의 테이블 ^^parts^^ (섹션 [3.1](#page-2-2))과 컬럼 이름 및 유형과 같은 테이블 메타데이터가 포함됩니다. 노드는 세 가지 작업을 사용하여 테이블의 상태를 진행시킵니다: 1. 삽입은 상태에 새로운 파트를 추가하고, 2. 병합은 상태에 새로운 파트를 추가하고 기존 ^^parts^^를 삭제하며, 3. 변이와 DDL 문장은 ^^parts^^를 추가하고/또는 ^^parts^^를 삭제하고/또는 테이블 메타데이터를 변경합니다. 작업은 단일 노드에서 로컬로 수행되며, 글로벌 복제 로그에서 상태 전환의 시퀀스로 기록됩니다.

복제 로그는 일반적으로 세 개의 ClickHouse Keeper 프로세스의 앙상블에 의해 유지되며, 이들은 Raft 합의 알고리즘 [\[59\]](#page-13-4)를 사용하여 ClickHouse 노드의 ^^클러스터^^에 대한 분산 및 내결함성이 있는 조정 계층을 제공합니다. 모든 ^^클러스터^^ 노드는 처음에 복제 로그의 동일한 위치를 가리킵니다. 노드가 로컬 삽입, 병합, 변이 및 DDL 문장을 실행하는 동안, 복제 로그는 다른 모든 노드에서 비동기적으로 재생됩니다. 결과적으로, 복제된 테이블은 결국 일관성만 보장되며, 즉 노드는 최신 상태로 수렴하는 동안 일시적으로 이전 테이블 상태를 읽을 수 있습니다. 앞서 언급한 대부분의 작업은 노드의 과반수(예: 절대 다수 또는 모든 노드)가 새로운 상태를 수용할 때까지 동기적으로 실행될 수 있습니다.

예를 들어 [그림 6](#page-5-3)은 세 개의 ClickHouse 노드로 구성된 ^^클러스터^^ 내에서 처음에 빈 복제 테이블을 보여줍니다. 노드 1은 먼저 두 개의 삽입 문을 수신하고 이를 복제 로그에 기록합니다( 1 2 ). 다음으로 노드 2는 첫 번째 로그 항목을 재생하여 이를 가져오고( 3 ) 노드 1에서 새로운 파트를 다운로드합니다( 4 ), 반면 노드 3은 두 개의 로그 항목을 모두 재생합니다( 3 4 5 6 ). 마지막으로 노드 3은 두 개의 ^^parts^^를 새로운 파트로 병합하고 입력 ^^parts^^를 삭제하며 복제 로그에 병합 항목을 기록합니다( 7 ).

<Anchor id="page-5-3"/><Image img={image_06} size="lg" alt="Image 06"/>

그림 6: 세 개의 노드로 구성된 ^^클러스터^^에서의 복제.

동기화를 가속화하기 위한 세 가지 최적화가 존재합니다: 첫째, ^^클러스터^^에 추가된 새로운 노드들은 복제 로그를 처음부터 재생하는 대신, 마지막 복제 로그 항목을 기록한 노드의 상태를 단순히 복사합니다. 둘째, 병합은 로컬에서 반복하여 재생하거나 다른 노드에서 결과 파트를 가져옴으로써 수행됩니다. 정확한 동작은 구성 가능하며 CPU 소비와 네트워크 I/O의 균형을 맞출 수 있습니다. 예를 들어, 교차 데이터 센터 복제는 운영 비용을 최소화하기 위해 일반적으로 로컬 병합을 선호합니다. 셋째, 노드는 상호 독립적인 복제 로그 항목을 병렬로 재생합니다. 여기에는 예를 들어, 연속적으로 같은 테이블에 삽입된 새로운 ^^parts^^의 가져오기 또는 서로 다른 테이블에 대한 작업이 포함됩니다. 

### <Anchor id="page-5-1"/>3.7 ACID 준수 {#3-7-acid-compliance}

동시 읽기 및 쓰기 작업의 성능을 극대화하기 위해 ClickHouse는 가능한 한 많이 latch를 피합니다. 쿼리는 쿼리 시작 시 생성된 모든 ^^parts^^의 스냅샷에 대해 실행됩니다. 이는 병렬 INSERT 또는 병합에 의해 삽입된 새로운 ^^parts^^가 실행에 참여하지 않도록 보장합니다 (섹션 [3.1](#page-2-2)). 여러 ^^parts^^가 동시에 수정되거나 제거되는 것을 방지하기 위해(섹션 [3.4](#page-4-0)), 쿼리 기간 동안 처리된 ^^parts^^의 참조 카운트가 증가합니다. 공식적으로, 이는 버전이 있는 ^^parts^^를 기반으로 한 MVCC 변형에 의해 실현된 스냅샷 격리와 일치합니다 [\[6\]](#page-12-18). 결과적으로, 일반적으로 문장은 ACID 준수를 충족하지 않지만 스냅샷이 생성될 때 동시 쓰기가 단일 파트에만 영향을 주는 드문 경우를 제외합니다.

실제로, ClickHouse의 쓰기 중심 의사 결정 사용 사례는 전원 장애가 발생할 경우 새로운 데이터가 손실될 위험을 작게 받아들입니다. 데이터베이스는 이를 활용하여 기본적으로 새로 삽입된 ^^parts^^를 디스크에 커밋(fsсync)하지 않음으로써 커널이 ^^원자성^^을 포기하는 비용을 감수하여 쓰기를 배치할 수 있도록 허용합니다.

## <Anchor id="page-6-0"/>4 쿼리 처리 레이어 {#4-query-processing-layer}

<Anchor id="page-6-1"/><Image img={image_07} size="lg" alt="Image 07"/>

그림 7: SIMD 유닛, 코어 및 노드 간의 병렬화.

[그림 7](#page-6-1)에서 설명된 바와 같이 ClickHouse는 데이터 요소, 데이터 청크 및 테이블 샤드 수준에서 쿼리를 병렬화합니다. 여러 데이터 요소는 SIMD 명령어를 사용하여 연산자 내에서 동시에 처리될 수 있습니다. 단일 노드에서 쿼리 엔진은 여러 스레드에서 동시에 연산자를 실행합니다. ClickHouse는 MonetDB/X100 [\[11\]](#page-12-0)과 동일한 벡터화 모델을 사용합니다. 즉, 연산자는 개별 행이 아닌 여러 행(데이터 청크)을 생성하고 전달하며 소비하여 가상 함수 호출의 오버헤드를 최소화합니다. 원본 테이블이 불연속적으로 테이블 샤드로 분할되면 여러 노드가 샤드를 동시에 스캔할 수 있습니다. 결과적으로 모든 하드웨어 리소스가 완전히 활용되며, 쿼리 처리는 노드를 추가하여 수평적으로 확장되거나 코어를 추가하여 수직적으로 확장될 수 있습니다.

이 섹션의 나머지 부분은 먼저 데이터 요소, 데이터 청크 및 ^^샤드^^ 세분화 수준에서의 병렬 처리를 자세히 설명합니다. 그런 다음 쿼리 성능을 극대화하기 위한 주요 최적화를 선택적으로 제시합니다. 마지막으로 ClickHouse가 동시 쿼리 존재 하에서 공유 시스템 리소스를 관리하는 방법에 대해 논의합니다.

### 4.1 SIMD 병렬화 {#4-1-simd-parallelization}

연산자 간에 여러 행을 전달하는 것은 벡터화의 기회를 만듭니다. 벡터화는 수동으로 작성된 내장함수 [\[64,](#page-13-18) [80\]](#page-13-19) 또는 컴파일러 자동 벡터화 [\[25\]](#page-12-19) 기반입니다. 벡터화의 이점을 누리는 코드는 서로 다른 컴퓨팅 커널로 컴파일됩니다. 예를 들어, 쿼리 연산자의 내부 열렬한 루프는 비벡터화된 커널, 자동 벡터화된 AVX2 커널 및 수동 벡터화된 AVX-512 커널의 관점에서 구현될 수 있습니다. 가장 빠른 커널은 [런타임에 선택됩니다](https://clickhou.se/cpu-dispatch) cpuid 명령어를 기반으로 합니다. 이 접근 방식은 ClickHouse가 SSE 4.2를 최소 요구 사항으로 하는 15년 된 시스템에서 실행될 수 있도록 하면서도 최신 하드웨어에서 상당한 속도 향상을 제공합니다.

### 4.2 멀티 코어 병렬화 {#4-2-multi-core-parallelization}

<Anchor id="page-7-1"/><Image img={image_08} size="lg" alt="Image 08"/>

그림 8: 세 개의 차선이 있는 물리적 운영자 계획.

ClickHouse는 SQL 쿼리를 물리적 계획의 유향 그래프로 변환하는 전통적인 접근 방식 [\[31\]](#page-12-13)을 따릅니다. 운영자 계획의 입력은 데이터가 네이티브 형식 또는 지원되는 제3자 형식으로 읽는 특별한 소스 연산자에 의해 표현됩니다(섹션 [5](#page-9-0) 참조). 마찬가지로, 특별한 싱크 연산자는 결과를 원하는 출력 형식으로 변환합니다. 물리적 운영자 계획은 쿼리 컴파일 시간에 최대 작업자 스레드 수(기본적으로 코어 수)와 소스 테이블 크기를 기반으로 독립 실행 경로로 해체됩니다. 경로는 병렬 연산자가 처리할 데이터 를 비겹치도록 분해합니다. 병렬 처리의 기회를 극대화하기 위해 경로는 가능한 한 늦게 병합됩니다.

예를 들어 [그림 8](#page-7-1)의 노드 1 상자는 페이지 인쇄 통계 테이블에 대한 전형적인 OLAP 쿼리의 연산자 그래프를 보여줍니다. 첫 번째 단계에서는 소스 테이블의 세 개의 불연속 범위가 동시에 필터링됩니다. Repartition 교환 연산자는 첫 번째 및 두 번째 단계 간에 결과 청크를 동적으로 라우팅하여 처리 스레드가 균등하게 활용되도록 합니다. 스캔된 범위의 선택성이 크게 다를 경우 첫 번째 단계 이후 경로가 불균형해질 수 있습니다. 두 번째 단계에서는 필터를 통과한 행이 RegionID로 그룹화됩니다. 집계 연산자는 RegionID를 그룹화 컬럼으로 사용하고 avg()에 대한 부분 집계 상태로서 그룹당 합계 및 카운트를 유지합니다. 지역 집계 결과는 GroupStateMerge 연산자에 의해 글로벌 집계 결과로 병합됩니다. 이 연산자는 또한 파이프라인 브레이커로, 집계 결과가 완전히 계산된 후에야 세 번째 단계가 시작될 수 있습니다. 세 번째 단계에서는 결과 그룹이 먼저 Distribute 교환 연산자에 의해 세 개의 동일하게 큰 불연속 파티션으로 나누어지며, 이어서 AvgLatency를 기준으로 정렬됩니다. 정렬은 세 단계로 수행됩니다: 첫째, ChunkSort 연산자가 각 파티션의 개별 청크를 정렬합니다. 둘째, StreamSort 연산자는 로컬 정렬된 결과를 유지하여 수신하는 정렬된 청크와 함께 2-way 병합 정렬을 사용하여 결합합니다. 마지막으로, MergeSort 연산자가 k-way 정렬을 사용하여 로컬 결과를 결합하여 최종 결과를 얻습니다.

연산자는 상태 기계이며 입력 및 출력 포트를 통해 서로 연결됩니다. 연산자의 세 가지 가능한 상태는 need-chunk, ready, done입니다. need-chunk에서 ready로 이동하려면 청크가 연산자의 입력 포트에 배치됩니다. ready에서 done으로 이동하려면 연산자가 입력 청크를 처리하고 출력 청크를 생성합니다. done에서 need-chunk로 이동하려면 출력 청크가 연산자의 출력 포트에서 제거됩니다. 두 개의 연결된 연산자의 첫 번째 및 세 번째 상태 전이는 결합된 단계에서만 수행될 수 있습니다. 소스 연산자(싱크 연산자)는 오직 ready와 done 상태(need-chunk와 done)만 가집니다.

작업자 스레드는 지속적으로 물리적 연산자 계획을 탐색하고 상태 전환을 수행합니다. CPU 캐시를 더 뜨겁게 유지하기 위해 계획은 동일한 스레드가 동일한 경로에서 연속적인 연산자를 처리해야 한다는 힌트를 포함합니다. 병렬 처리는 단계 내에서 불연속적인 입력 간의 수평적으로 발생하며(예: [그림 8](#page-7-1)에서 Aggregate 연산자는 동시에 실행됩니다) 파이프라인 브레이커로 분리되지 않은 단계 간의 수직적으로 발생합니다(예: [그림 8](#page-7-1)에서 같은 경로의 Filter와 Aggregate 연산자는 동시에 실행될 수 있습니다). 새로운 쿼리가 시작되거나 동시 쿼리가 완료될 때 오버 및 언더 구독을 피하기 위해, 병렬성의 정도는 쿼리가 시작될 때 지정된 쿼리를 위한 작업자 스레드의 수(최대 수) 사이에서 중간 쿼리에서 변경될 수 있습니다(섹션 [4.5](#page-9-1) 참조).

연산자는 또한 런타임에서 쿼리 실행에 두 가지 방식으로 영향을 줄 수 있습니다. 첫째, 연산자는 동적으로 새로운 연산자를 생성하고 연결할 수 있습니다. 이는 주로 메모리 소비가 구성 가능한 임계값을 초과할 때 쿼리를 취소하는 대신 외부 집계, 정렬 또는 조인 알고리즘으로 전환하는 데 사용됩니다. 둘째, 연산자는 작업자 스레드가 비동기 대기열로 이동하도록 요청할 수 있습니다. 이렇게 하면 원격 데이터를 기다리는 동안 작업자 스레드를 보다 효율적으로 사용할 수 있습니다.

ClickHouse의 쿼리 실행 엔진 및 조각 기반 병렬성 [\[44\]](#page-12-20)은 경로가 일반적으로 서로 다른 코어/NUMA 소켓에서 실행되고 작업자 스레드가 다른 경로에서 작업을 빼앗을 수 있다는 점에서 유사합니다. 또한 중앙 스케줄링 구성 요소는 없으며, 대신 작업자 스레드는 연산자 계획을 지속적으로 탐색하여 개별적으로 작업을 선택합니다. 조각 기반 병렬성과 달리 ClickHouse는 최대 병렬성 정도를 계획에 포함시키고 기본 조각 크기인 약 100,000행과 비교하여 소스 테이블을 부분화하는 데 더 큰 범위를 사용합니다. 이로 인해 경우에 따라 정체가 발생할 수 있지만(예: 서로 다른 경로에서 필터 연산자의 런타임이 크게 다를 때) 우리는 Repartition과 같은 교환 연산자의 자유로운 사용이 최소한 단계 간의 불균형이 축적되는 것을 피할 수 있다는 것을 발견했습니다.

### 4.3 멀티 노드 병렬화 {#4-3-multi-node-parallelization}

쿼리의 원본 테이블이 샤드되면 쿼리를 수신한 노드(시작 노드)의 쿼리 최적화기가 가능한 한 많은 작업을 다른 노드에서 수행하려고 시도합니다. 다른 노드에서의 결과는 쿼리 계획의 다양한 지점에 통합될 수 있습니다. 쿼리에 따라 원격 노드는 다음과 같은 방식으로 작업을 수행할 수 있습니다: 1. 원본 테이블 컬럼을 시작 노드로 스트리밍, 2. 원본 컬럼을 필터링하고 필터링된 행을 전송, 3. 필터 및 집계 단계를 실행하고 부분 집계 상태를 가진 로컬 결과 그룹을 전송, 또는 4. 필터링, 집계 및 정렬을 포함하여 전체 쿼리를 실행합니다.

[그림 8](#page-7-1)의 노드 2 ... N은 히트 테이블의 샤드를 보유한 다른 노드에서 실행된 계획 조각을 보여줍니다. 이러한 노드는 로컬 데이터를 필터링하고 그룹화한 후 결과를 시작 노드로 전송합니다. 노드 1에서 GroupStateMerge 연산자는 로컬 및 원격 결과를 병합한 후 결과 그룹을 최종적으로 정렬합니다.

### <Anchor id="page-7-0"/>4.4 전체 성능 최적화 {#4-4-holistic-performance-optimization}

이 섹션에서는 쿼리 실행의 다양한 단계에 적용되는 주요 성능 최적화를 선택적으로 제시합니다.

**쿼리 최적화**. 첫 번째 최적화 집합은 쿼리의 AST에서 얻은 의미적 쿼리 표현 위에 적용됩니다. 이러한 최적화의 예로는 상수 접기(예: concat(lower('a'), upper('b'))는 'aB'가 됨), 특정 집계 함수에서 스칼라 추출(예: sum(a*2)는 2 * sum(a)로 변환됨), 공통 하위 표현 제거, 그리고 동등 필터의 논리합을 IN-리스트로 변환하는 것(예: x=c OR x=d는 x IN (c,d)로 변환됨) 등이 있습니다. 최적화된 의미적 쿼리 표현은 이후 논리적 운영자 계획으로 변환됩니다. 논리적 계획 위에 적용되는 최적화는 필터 푸시다운, 함수 평가 및 정렬 단계 재배치 등을 포함하며, 이는 더 비용이 많이 드는 것이 무엇인지에 따라 결정됩니다. 마지막으로, 논리 쿼리 계획은 물리적 운영자 계획으로 변환됩니다. 이 변환은 관련 테이블 엔진의 특이점을 활용할 수 있습니다. 예를 들어, ^^MergeTree^^*-^^테이블 엔진^^의 경우, ORDER BY 컬럼이 ^^기본 키^^의 접두사를 형성하는 경우 데이터는 디스크 순서로 읽을 수 있으며, 정렬 연산자는 계획에서 제거할 수 있습니다. 또한, 집계에서 그룹화 컬럼이 ^^기본 키^^의 접두사를 형성하는 경우 ClickHouse는 정렬 집계를 사용할 수 있습니다 [\[33\]](#page-12-21), 즉 사전 정렬된 입력에서 동일한 값의 집계 실행을 직접 집계합니다. 해시 집계에 비해, 정렬 집계는 메모리 집약도가 훨씬 낮으며, 집계 값은 실행이 완료된 후 즉시 다음 연산자로 전달될 수 있습니다.

**쿼리 컴파일**. ClickHouse는 [LLVM 기반 쿼리 컴파일](https://clickhou.se/jit)을 사용하여 인접한 계획 연산자를 동적으로 융합합니다 [\[38,](#page-12-22) [53\]](#page-13-0). 예를 들어, 표현식 a * b + c + 1은 세 개의 연산자 대신 하나의 연산자로 결합될 수 있습니다. 표현식 외에도 ClickHouse는 그룹화(GROUP BY)에서 여러 집계 함수를 동시에 평가하기 위해, 정렬 시 여러 정렬 키에 대해서도 컴파일을 사용합니다. 쿼리 컴파일은 가상 호출 수를 줄이고 데이터를 레지스터나 CPU 캐시에 유지하며 분기 예측기를 돕습니다. 또한, 런타임 컴파일은 논리적 최적화 및 컴파일러에서 구현된 peephole 최적화와 같은 풍부한 최적화 세트를 가능하게 하며, 가장 빠른 로컬 CPU 명령에 접근할 수 있게 합니다. 컴파일은 동일한 정규, 집계 또는 정렬 표현이 서로 다른 쿼리에 의해 설정한 구성 가능한 횟수 이상으로 실행될 때만 시작됩니다. 컴파일된 쿼리 연산자는 캐시되며 미래의 쿼리에서 재사용될 수 있습니다.[7]

**^^기본 키^^ 인덱스 평가**. ClickHouse는 WHERE 조건이 정규형의 접합(normal form)에서 필터 조건의 하위 집합이 ^^기본 키^^ 컬럼의 접두사를 구성하는 경우 ^^기본 키^^ 인덱스를 사용하여 평가합니다. ^^기본 키^^ 인덱스는 키 값의 사전순으로 정렬된 범위를 왼쪽에서 오른쪽으로 분석합니다. ^^기본 키^^ 컬럼에 해당하는 필터 조건은 삼진법 논리를 사용하여 평가됩니다 - 이들은 모두 참이거나, 모두 거짓이거나, 또는 범위 내의 값에 대해 혼합되어 참/거짓입니다. 후자의 경우 범위는 재귀적으로 분석되는 하위 범위로 나뉩니다. 필터 조건에서의 함수에 대한 추가 최적화가 존재합니다. 먼저, 함수는 그들의 단조성을 설명하는 특성이 있습니다. 예를 들어, toDayOfMonth(date)는 한 달 내에서 부분적으로 단조적입니다. 단조성 특성은 정렬된 입력 키 값 범위에서 함수가 정렬된 결과를 생성하는지 추론할 수 있게 합니다. 둘째, 일부 함수는 주어진 함수 결과의 역이미지를 계산할 수 있습니다. 이는 필터 조건의 상수 비교를 키 컬럼에 대한 함수 호출로 대체하는 데 사용됩니다. 예를 들어, toYear(k) = 2024는 k >= 2024-01-01 && k < 2025-01-01로 대체될 수 있습니다.

**데이터 스킵**. ClickHouse는 [3.2](#page-3-0) 섹션에서 제시된 데이터 구조를 사용하여 쿼리 실행 중 데이터 읽기를 피하려고 합니다. 또한 필터는 휴리스틱 및(선택 사항인) 컬럼 통계 기반으로 내림차순으로 선택성이 추정된 순서로 서로 다른 컬럼에서 순차적으로 평가됩니다. 적어도 하나의 일치하는 행이 포함된 데이터 청크만이 다음 술어로 전달됩니다. 이는 술어에서 술어로 갈수록 읽는 데이터 양과 수행해야 할 계산 수를 점진적으로 줄입니다. 최적화는 고선택성 술어가 하나 이상 존재할 때만 적용됩니다. 그렇지 않으면, 모든 술어가 병렬로 평가될 경우 쿼리의 지연 시간이 악화될 수 있습니다.

**해시 테이블**. 해시 테이블은 집계 및 해시 조인에 필수적인 데이터 구조입니다. 적절한 해시 테이블 유형을 선택하는 것은 성능에 매우 중요합니다. ClickHouse는 해시 함수, 할당자, 셀 유형 및 크기 조정 정책을 변형 포인트로 하여 다양한 해시 테이블을 [인스턴스화](https://clickhou.se/hashtables)합니다(2024년 3월 기준 30개 이상). 그룹화 컬럼의 데이터 유형, 예상 해시 테이블 카디널리티 및 기타 요인에 따라 각 쿼리 연산자에 대해 가장 빠른 해시 테이블이 개별적으로 선택됩니다. 해시 테이블에 대해 구현된 추가 최적화에는 다음이 포함됩니다:

- 256개의 하위 테이블을 포함하는 2단계 레이아웃(해시의 첫 번째 바이트 기반)으로 큰 키 세트 지원,
- 문자열 길이가 다른 네 개의 하위 테이블과 다양한 해시 함수를 가진 문자열 해시 테이블 [\[79\]](#page-13-20),
- 키가 몇 개뿐인 경우 키를 직접 버킷 인덱스로 사용하는 조회 테이블(즉, 해싱 없음),
- 비교가 비싼 경우 더 빠른 충돌 해결을 위한 내장 해시 값을 가진 값(예: 문자열, AST),
- 불필요한 크기 조정을 피하기 위한 런타임 통계에서 예측한 크기를 기반으로 해시 테이블 생성,
- 단일 메모리 슬랩에서 동일한 생성/파괴 생애 주기를 가진 여러 개의 작은 해시 테이블 할당,
- 해시 테이블의 재사용을 위한 즉각적인 정리 및 해시 맵 및 셀 버전 카운터 사용,
- CPU 프리페치(__builtin_prefetch) 사용하여 키 해싱 후 값 검색 속도 향상.

**조인**. ClickHouse는 원래 조인을 단순히 지원했기 때문에 많은 사용 사례가 역사적으로 비정규화된 테이블에 의존했습니다. 오늘날 데이터베이스는 SQL에서 제공되는 모든 조인 유형(내부, 왼쪽/오른쪽/전체 외부, 교차, as-of)을 제공하며, 해시 조인(나이브, 그레이스), 정렬-병합 조인, 빠른 키-값 조회를 위한 인덱스 조인과 같은 다양한 조인 알고리즘도 제공합니다(일반적으로 딕셔너리).

조인은 가장 비용이 많이 드는 데이터베이스 작업 중 하나이므로 고전적인 조인 알고리즘의 병렬 변형을 제공하는 것이 중요합니다. 이상적으로는 구성 가능한 공간/시간 무역 오프도 있어야 합니다. ClickHouse는 [\[7\]](#page-12-23)에서 블로킹이 없는 공유 파티션 알고리즘을 구현합니다. 예를 들어 [그림 9](#page-8-3)의 쿼리는 페이지 히트 통계 테이블에 대한 자기 조인을 통해 사용자가 URL 간에 어떻게 이동하는지를 계산합니다. 조인의 빌드 단계는 소스 테이블의 세 개의 불연속 범위를 포함하는 세 가지 경로로 나뉩니다. 전역 해시 테이블 대신에 파티션 해시 테이블이 사용됩니다. (일반적으로 세 개의) 작업자 스레드는 해시 함수를 모듈로 계산하여 빌드 측의 각 입력 행에 대한 대상 파티션을 결정합니다. 해시 테이블 파티션에 대한 접근은 Gather 교환 연산자를 사용하여 동기화됩니다. 프로브 단계는 입력 튜플의 대상 파티션을 유사하게 찾습니다. 이 알고리즘은 각 튜플마다 두 개의 추가 해시 계산을 도입하지만, 해시 테이블 파티션 수에 따라 빌드 단계의 락 경합을 크게 줄입니다.

<Anchor id="page-8-3"/><Image img={image_09} size="lg" alt="Image 09"/>

그림 9: 세 개의 해시 테이블 파티션을 가진 병렬 해시 조인.
### <Anchor id="page-9-1"/>4.5 작업 부하 격리 {#4-5-workload-isolation}

ClickHouse는 사용자들이 쿼리를 작업 부하 클래스로 분리할 수 있도록 동시성 제어, 메모리 사용 제한 및 I/O 스케줄링을 제공합니다. 특정 작업 부하 클래스에 대해 공유 리소스(CPU 코어, DRAM, 디스크 및 네트워크 I/O)에 제한을 설정하여 이러한 쿼리가 다른 중요한 비즈니스 쿼리에 영향을 미치지 않도록 보장합니다.

동시성 제어는 동시 쿼리가 많을 때 스레드의 오버 구독을 방지합니다. 보다 구체적으로, 쿼리당 작업자 스레드 수는 사용 가능한 CPU 코어 수에 대한 비율에 따라 동적으로 조정됩니다.

ClickHouse는 서버, 사용자 및 쿼리 수준에서 메모리 할당의 바이트 크기를 추적하여 유연한 메모리 사용 제한 설정을 허용합니다. 메모리 초과 할당은 쿼리가 보장된 메모리를 초과하여 추가 무료 메모리를 사용할 수 있도록 하며, 다른 쿼리에 대한 메모리 한계를 보장합니다. 또한 집계, 정렬 및 조인 절에서의 메모리 사용량을 제한할 수 있어, 메모리 한계를 초과할 경우 외부 알고리즘으로 되돌아가게 됩니다.

마지막으로, I/O 스케줄링은 사용자들이 최대 대역폭, 요청 중 및 정책(예: FIFO, SFC [\[32\]](#page-12-24))에 따라 작업 부하 클래스를 위해 로컬 및 원격 디스크 접근을 제한할 수 있도록 합니다.
### <Anchor id="page-9-0"/>5 통합 레이어 {#5-integration-layer}

실시간 의사 결정 애플리케이션은 종종 여러 위치에서 데이터에 대한 효율적이고 저지연 접근에 의존합니다. OLAP 데이터베이스에서 외부 데이터를 사용할 수 있게 만드는 두 가지 접근 방식이 있습니다. 푸시 기반 데이터 접근에서는 서드파티 구성 요소가 데이터베이스와 외부 데이터 저장소를 연결합니다. 이의 예는 원격 데이터를 목적 시스템으로 푸시하는 전문 ETL 도구입니다. 풀 기반 모델에서는 데이터베이스 자체가 원격 데이터 소스에 연결하여 데이터를 쿼리하기 위해 로컬 테이블로 가져오거나 원격 시스템으로 데이터를 내보냅니다. 푸시 기반 접근 방식은 더 다재다능하고 일반적이지만 더 큰 아키텍처 풋프린트와 확장성 병목을 수반합니다. 반면, 데이터베이스 내에서의 원격 연결은 로컬 데이터와 원격 데이터 간의 조인과 같은 흥미로운 기능을 제공하면서 전체 아키텍처를 단순하게 유지하고 통찰력을 얻는 시간을 줄입니다.

이 섹션의 나머지 부분에서는 ClickHouse에서 원격 지역의 데이터에 접근하기 위한 풀 기반 데이터 통합 방법을 탐구합니다. SQL 데이터베이스에서 원격 연결의 아이디어는 새롭지 않다는 점에 주목합니다. 예를 들어, 2001년에 도입되어 2011년부터 PostgreSQL이 구현한 SQL/MED 표준 [\[35\]](#page-12-25)는 외부 데이터를 관리하기 위한 통합 인터페이스로 외부 데이터 래퍼를 제안합니다. ClickHouse는 다른 데이터 저장소 및 저장 형식과 최대 상호 운용성을 요구하는 설계 목표가 있습니다. 2024년 3월 현재, ClickHouse는 모든 분석 데이터베이스에서 가장 많은 내장 데이터 통합 옵션을 제공하는 것으로 우리의 지식의 한계를 넘어설 수 있습니다.

외부 연결. ClickHouse는 ODBC, MySQL, PostgreSQL, SQLite, Kafka, Hive, MongoDB, Redis, S3/GCP/Azure 객체 스토어 및 다양한 데이터 레이크를 포함하여 외부 시스템 및 저장 위치와의 연결을 위한 [50개 이상의](https://clickhou.se/query-integrations) 통합 테이블 함수 및 엔진을 제공합니다. 우리는 이를 다음 보너스 그림에서 보여준 카테고리로 추가 분류합니다(원본 vldb 문서의 일부 아님).

<Anchor id="bonus-figure"/><Image img={image_10} size="lg" alt="Image 10"/>

보너스 그림: ClickBench의 상호 운용성 옵션.

일시적 접근과 통합 **테이블 함수**. 테이블 함수는 탐색적 즉석 쿼리를 위해 원격 데이터를 읽기 위해 SELECT 쿼리의 FROM 절에서 호출할 수 있습니다. 대안으로, INSERT INTO TABLE FUNCTION 문을 사용하여 원격 저장소에 데이터를 기록하는 데 사용할 수 있습니다.

지속적 접근. 원격 데이터 저장소 및 처리 시스템과의 영구 연결을 생성하기 위한 세 가지 방법이 존재합니다.

첫째, 통합 **테이블 엔진**은 MySQL 테이블과 같은 원격 데이터 소스를 영구 로컬 테이블로 나타냅니다. 사용자는 CREATE TABLE AS 문법을 사용하여 테이블 정의를 저장하고, SELECT 쿼리 및 테이블 함수를 결합합니다. 원격 컬럼의 하위 집합만 참조하도록 사용자 지정 스키마를 지정하거나, 스키마 추론을 사용하여 컬럼 이름과 동등한 ClickHouse 유형을 자동으로 결정하는 것이 가능합니다. 우리는 또한 수동 및 능동적인 런타임 동작을 구분합니다: 수동 테이블 엔진은 쿼리를 원격 시스템으로 전달하고 결과로 로컬 프록시 테이블을 채웁니다. 반면에 능동 테이블 엔진은 원격 시스템에서 주기적으로 데이터를 가져오거나 PostgreSQL의 논리 복제 프로토콜과 같은 원격 변경 사항에 구독합니다. 결과적으로 로컬 테이블은 원격 테이블의 전체 복사본을 포함합니다.

둘째, 통합 **데이터베이스 엔진**은 원격 데이터 저장소의 테이블 스키마에 있는 모든 테이블을 ClickHouse에 매핑합니다. 이전과 달리 일반적으로 원격 데이터 저장소는 관계형 데이터베이스일 것을 요구하며, DDL 문에 대한 제한된 지원을 제공합니다.

셋째, **딕셔너리**는 해당하는 통합 테이블 함수나 엔진을 사용하여 가능한 모든 데이터 소스에 대해 임의 쿼리를 사용하여 채울 수 있습니다. 런타임 동작은 Active이므로 원격 스토리지에서 일정 간격으로 데이터를 끌어옵니다.

데이터 형식. 제3자 시스템과 상호작용하기 위해 현대의 분석 데이터베이스는 모든 형식의 데이터를 처리할 수 있어야 합니다. ClickHouse는 네이티브 형식 외에 [90개 이상의](https://clickhou.se/query-formats) 형식을 지원하며, 여기에는 CSV, JSON, Parquet, Avro, ORC, Arrow 및 Protobuf가 포함됩니다. 각 형식은 ClickHouse가 읽을 수 있는 입력 형식, ClickHouse가 내보낼 수 있는 출력 형식, 또는 두 가지 모두가 될 수 있습니다. Parquet와 같은 일부 분석 지향 형식은 쿼리 처리와 통합되어 있으며, 즉 옵티마이저는 내장된 통계를 활용할 수 있으며, 필터는 압축된 데이터에서 직접 평가됩니다.

호환성 인터페이스. ClickHouse는 기본 이진 와이어 프로토콜 및 HTTP 외에 클라이언트가 MySQL 또는 PostgreSQL 와이어 프로토콜과 호환되는 인터페이스를 통해 ClickHouse와 상호작용할 수 있도록 합니다. 이 호환성 기능은 공급업체가 아직 네이티브 ClickHouse 연결을 구현하지 않은 독점 응용 프로그램(예: 특정 비즈니스 인사이트 도구)에서의 접근을 활성화하는 데 유용합니다. 

## 6 성능을 특징으로 {#6-performance-as-a-feature}

이 섹션에서는 성능 분석을 위한 내장 도구를 제시하고 실제 및 벤치마크 쿼리를 사용하여 성능을 평가합니다.

### 6.1 내장 성능 분석 도구 {#6-1-built-in-performance-analysis-tools}

개별 쿼리나 백그라운드 작업에서 성능 저하를 조사하기 위한 다양한 도구가 제공됩니다. 사용자는 시스템 테이블을 기반으로 한 일관된 인터페이스를 통해 모든 도구와 상호작용합니다.

**서버 및 쿼리 메트릭**. 활성 파트 수, 네트워크 처리량, 캐시 적중률과 같은 서버 수준의 통계는 읽은 블록 수 또는 인덱스 사용 통계와 같은 쿼리별 통계로 보완됩니다. 메트릭은 구성 가능한 간격에서 동기적으로(요청 시) 또는 비동기적으로 계산됩니다.

**샘플링 프로파일러**. 서버 스레드의 호출 스택은 샘플링 프로파일러를 이용하여 수집할 수 있습니다. 결과는 옵션에 따라 flamegraph 시각화 도구와 같은 외부 도구로 내보낼 수 있습니다.

**OpenTelemetry 통합**. OpenTelemetry는 여러 데이터 처리 시스템에서 데이터 행을 추적하기 위한 개방 표준입니다 [\[8\]](#page-12-26). ClickHouse는 모든 쿼리 처리 단계에 대해 구성 가능한 세분성으로 OpenTelemetry 로그 스팬을 생성할 수 있으며, 다른 시스템으로부터 OpenTelemetry 로그 스팬을 수집하고 분석할 수도 있습니다.

**쿼리 설명**. 다른 데이터베이스와 마찬가지로 SELECT 쿼리는 쿼리의 AST, 논리적 및 물리적 운영자 계획, 실행 시간 동작에 대한 세부 정보를 제공하기 위해 EXPLAIN으로 선행될 수 있습니다.

### 6.2 벤치마크 {#6-2-benchmarks}

벤치마크는 현실적이지 못하다는 비판을 받아왔지만 [\[10,](#page-12-27) [52,](#page-13-22) [66,](#page-13-23) [74\]](#page-13-24) 여전히 데이터베이스의 강점과 약점을 식별하는 데 유용합니다. 다음에서는 ClickHouse의 성능을 평가하기 위해 벤치마크가 어떻게 사용되는지 논의합니다.
#### 6.2.1 비정규화된 테이블 {#6-2-1-denormalized-tables}

비정규화된 사실 테이블에 대한 필터 및 집계 쿼리는 역사적으로 ClickHouse의 주요 사용 사례를 나타냅니다. 우리는 ClickBench의 실행 시간을 보고하는데, 이는 클릭 스트림 및 트래픽 분석에 사용되는 즉시 및 주기적 보고 쿼리를 시뮬레이션하는 전형적인 워크로드입니다. 이 벤치마크는 1억 개의 익명화된 페이지 조회가 있는 테이블에 대해 43개의 쿼리로 구성되어 있으며, 이는 웹의 가장 큰 분석 플랫폼 중 하나에서 제공됩니다. 2024년 6월 기준으로 45개 이상의 상용 및 연구 데이터베이스에 대한 측정값(냉/열 실행 시간, 데이터 가져오기 시간, 디스크 크기)을 보여주는 온라인 대시보드 [\[17\]](#page-12-28)이 있습니다. 결과는 공개적으로 사용 가능한 데이터 세트와 쿼리를 기반으로 한 독립 기여자들에 의해 제출됩니다 [\[16\]](#page-12-29). 쿼리는 순차적 및 인덱스 스캔 접근 경로를 테스트하며 CPU-, IO- 또는 메모리 바운드 관계형 연산자를 정기적으로 노출합니다.

[Figure 10](#page-10-0)은 분석에 자주 사용되는 데이터베이스에서 모든 ClickBench 쿼리를 순차적으로 실행하는 데 대한 총 상대 냉 및 열 실행 시간을 보여줍니다. 측정값은 16 vCPUs, 32 GB RAM 및 5000 IOPS / 1000 MiB/s 디스크가 있는 단일 노드 AWS EC2 c6a.4xlarge 인스턴스에서 가져왔습니다. Redshift ([ra3.4xlarge](https://clickhou.se/redshift-sizes), 12 vCPUs, 96 GB RAM) 및 Snowfake ([warehouse size S](https://clickhou.se/snowflake-sizes): 2x8 vCPUs, 2x16 GB RAM)와 비교 가능한 시스템을 사용했습니다. 물리적 데이터베이스 설계는 아주 가볍게 조정되며, 예를 들어 기본 키를 지정하지만 개별 컬럼의 압축을 변경하거나, 프로젝션을 생성하거나, 스킵 인덱스를 생성하지 않습니다. 각 냉 쿼리 실행 전에 Linux 페이지 캐시를 플러시하지만, 데이터베이스나 운영 체제 조정은 하지 않습니다. 모든 쿼리에 대해, 데이터베이스 간 가장 빠른 실행 시간이 기준으로 사용됩니다. 다른 데이터베이스의 상대 쿼리 실행 시간은 ( + 10)/(_ + 10)으로 계산됩니다. 데이터베이스의 총 상대 실행 시간은 쿼리 당 비율의 기하 평균입니다. 연구 데이터베이스 Umbra [\[54\]](#page-13-25)이 최고의 열 실행 시간을 달성하는 반면, ClickHouse는 모든 다른 프로덕션 등급 데이터베이스보다 열 및 냉 실행 시간에서 우수합니다.

<Anchor id="page-10-0"/><Image img={image_11} size="lg" alt="Image 11"/>

Figure 10: ClickBench의 상대 냉 및 열 실행 시간.

보다 다양한 워크로드에서 SELECT의 성능을 추적하기 위해, 우리는 [use](https://clickhou.se/performance-over-years)라 불리는 네 가지 벤치마크 조합인 VersionsBench [\[19\]](#page-12-30)을 사용합니다. 이 벤치마크는 새 릴리스가 발표될 때마다 성능을 평가하기 위해 매월 한 번 실행 [\[20\]](#page-12-31)되어 성능 저하를 초래할 수 있는 코드 변경 사항을 식별합니다: 개별 벤치마크에는: 1. ClickBench(위에 설명됨), 2. 15 MgBench [\[21\]](#page-12-32) 쿼리, 3. 6억 행이 있는 비정규화된 스타 스키마 벤치마크 [\[57\]](#page-13-26) 사실 테이블에 대한 13개의 쿼리. 4. 34억 행이 있는 [NYC Taxi Rides](https://clickhou.se/nyc-taxi-rides-benchmark)에 대한 4개의 쿼리 [\[70\]](#page-13-27).

[Figure 11](#page-10-5)은 2018년 3월부터 2024년 3월까지 77개 ClickHouse 버전의 VersionsBench 실행 시간 변화를 보여줍니다. 개별 쿼리의 상대 실행 시간 차이를 보정하기 위해, 모든 버전 간 최소 쿼리 실행 시간을 비율로 하여 기하 평균을 사용하여 실행 시간을 정규화합니다. VersionBench의 성능은 지난 6년 동안 1.72배 향상되었습니다. 장기 지원(LTS) 릴리스의 날짜는 x축에 표시되어 있습니다. 일부 기간 동안 성능이 일시적으로 악화되었지만, LTS 릴리스는 일반적으로 이전 LTS 버전과 비교할 수 있거나 더 나은 성능을 보입니다. 2022년 8월의 유의미한 개선은 [4.4.](#page-7-0) 섹션에서 설명한 열 단위 필터 평가 기술 때문입니다.

<Anchor id="page-10-5"/><Image img={image_12} size="lg" alt="Image 12"/>

Figure 11: 2018-2024 VersionsBench의 상대 열 실행 시간.
#### 6.2.2 정규화된 테이블 {#6-2-2-normalized-tables}

고전 창고에서 데이터는 종종 스타 또는 스노우플레이크 스키마를 사용하여 모델링됩니다. 우리는 TPC-H 쿼리(스케일 팩터 100)의 실행 시간을 제시하지만, 정규화된 테이블은 ClickHouse의 새로운 사용 사례임을 언급합니다. [Figure 12](#page-10-6)는 [4.4.](#page-7-0) 섹션에서 설명한 병렬 해시 조인 알고리즘을 기반으로 한 TPC-H 쿼리의 열 실행 시간을 보여줍니다. 측정값은 64 vCPUs, 128 GB RAM 및 5000 IOPS / 1000 MiB/s 디스크가 있는 단일 노드 AWS EC2 c6i.16xlarge 인스턴스에서 가져왔습니다. 다섯 번의 실행 중 가장 빠른 것을 기록했습니다. 참조를 위해, 우리는 비슷한 크기의 Snowfake 시스템(warehouse size L, 8x8 vCPUs, 8x16 GB RAM)에서 동일한 측정을 수행했습니다. 열에는 11개의 쿼리 결과가 제외되어 있습니다: 쿼리 Q2, Q4, Q13, Q17 및 Q20-22는 ClickHouse v24.6 기준으로 지원되지 않는 상관 서브쿼리를 포함하고 있습니다. 쿼리 Q7-Q9 및 Q19는 실행 가능한 실행 시간을 달성하기 위해 조인에 대해 조인 재배치 및 조인 프레디케이트 푸시다운과 같은 확장된 플랜 수준 최적화에 의존합니다 (둘 다 ClickHouse v24.6 기준으로 누락됨). 자동 서브쿼리 비상관화 및 조인에 대한 더 나은 최적화 지원은 2024년 구현 예정입니다 [\[18\]](#page-12-33). 남은 11개의 쿼리 중 5개의 쿼리는 ClickHouse에서 더 빠르게 실행되었고(6개의 쿼리는 Snowfake에서), 앞서 언급한 최적화가 성능에 중요하다는 것이 알려져 있으므로 [\[27\]](#page-12-34), 구현 이후 이 쿼리의 실행 시간을 더욱 개선할 것으로 기대하고 있습니다.

<Anchor id="page-10-6"/><Image img={image_13} size="lg" alt="Image 13"/>

Figure 12: TPC-H 쿼리에 대한 열 실행 시간(초).
## 7 관련 연구 {#7-related-work}

분석 데이터베이스는 최근 수십 년간 큰 학술적 및 상업적 관심을 받아왔습니다 [\[1\]](#page-12-35). Sybase IQ [\[48\]](#page-13-28), Teradata [\[72\]](#page-13-29), Vertica [\[42\]](#page-12-36), Greenplum [\[47\]](#page-13-30)과 같은 초기 시스템은 비싼 배치 ETL 작업 및 온프레미스 특성으로 인해 제한된 탄력성으로 특징지어졌습니다. 2010년대 초반에는, 클라우드 네이티브 데이터 웨어하우스 및 데이터베이스 서비스(DBaaS)의 출현(Snowfake [\[22\]](#page-12-37), BigQuery [\[49\]](#page-13-31), Redshift [\[4\]](#page-12-38))이 조직의 분석 비용과 복잡성을 획기적으로 줄였으며, 높은 가용성 및 자동 리소스 스케일링의 혜택을 누리게 되었습니다. 더 최근에는, 분석 실행 커널(예: Photon [\[5\]](#page-12-39) 및 Velox [\[62\]](#page-13-32))이 다양한 분석, 스트리밍 및 기계 학습 애플리케이션에 사용하기 위한 공동 수정된 데이터 처리를 제공합니다.

ClickHouse와 목표 및 설계 원칙 면에서 가장 유사한 데이터베이스는 Druid [\[78\]](#page-13-33)와 Pinot [\[34\]](#page-12-40)입니다. 두 시스템 모두 높은 데이터 수집률로 실시간 분석을 목표로 합니다. ClickHouse와 마찬가지로 테이블은 세그먼트라는 수평 ^^parts^^로 분할됩니다. ClickHouse는 더 작은 ^^parts^^를 지속적으로 병합하며 선택적으로 [3.3의 기술](#page-4-3)을 사용해 데이터 양을 줄이나, Druid와 Pinot에서는 ^^parts^^가 영구적으로 변경 불가능합니다. 또한, Druid와 Pinot는 테이블을 생성, 변형 및 검색하기 위해 특수화된 노드가 필요한 반면, ClickHouse는 이러한 작업을 위해 단일 이진 파일을 사용합니다.

Snowfake [\[22\]](#page-12-37)은 공유 디스크 아키텍처를 기반으로 한 인기 있는 상용 클라우드 데이터 웨어하우스입니다. 마이크로 파티션으로 테이블을 분할하는 접근 방식은 ClickHouse의 ^^parts^^ 개념과 유사합니다. Snowfake는 영속성을 위해 하이브리드 PAX 페이지 [\[3\]](#page-12-41)을 사용하고, ClickHouse의 저장 형식은 엄격하게 컬럼형입니다. Snowfake는 또한 자동으로 생성된 경량 인덱스를 사용하여 지역 캐싱 및 데이터 프루닝을 강조하여 우수한 성능을 제공합니다 [\[31,](#page-12-13) [51\]](#page-13-14). ClickHouse에서의 기본 키처럼 사용자는 선택적으로 클러스터링 인덱스를 생성하여 동일한 값으로 데이터를 co-locate할 수 있습니다.

Photon [\[5\]](#page-12-39)와 Velox [\[62\]](#page-13-32)는 복잡한 데이터 관리 시스템의 구성 요소로 사용하도록 설계된 쿼리 실행 엔진입니다. 두 시스템 모두 쿼리 계획이 입력으로 전달되어 로컬 노드에서 Parquet(Photon) 또는 Arrow(Velox) 파일에서 실행됩니다 [\[46\]](#page-13-34). ClickHouse는 이러한 일반 형식을 소비하고 생성할 수 있지만, 저장을 위해 기본 파일 형식을 선호합니다. Velox와 Photon은 쿼리 계획을 최적화하지 않지만(Velox는 기본 표현 최적화를 수행), 데이터 특성에 따라 동적으로 컴퓨팅 커널을 전환하는 런타임 적응 기술을 사용합니다. 유사하게, ClickHouse의 계획 연산자는 쿼리 메모리 사용량을 기반으로 외부 집계 또는 조인 연산자로 전환하기 위해 런타임에 다른 연산자를 생성할 수 있습니다. Photon 논문은 코드 생성 설계 [\[38,](#page-12-22) [41,](#page-12-42) [53\]](#page-13-0)이 해석된 벡터화된 설계 [\[11\]](#page-12-0)보다 개발 및 디버그하기 더 어렵다고 지적합니다. Velox의 코드 생성에 대한 지원은 런타임 생성 C++ 코드에서 생성된 공유 라이브러리를 구축하고 링크하는 반면, ClickHouse는 요청에 따라 LLVM의 컴파일 API와 직접 상호작용합니다.

DuckDB [\[67\]](#page-13-6)는 호스트 프로세스에 내장되도록 설계되었지만, 쿼리 최적화 및 트랜잭션도 제공합니다. OLAP 쿼리와 가끔의 OLTP 문장이 혼합된 형태로 설계되었습니다. 따라서 DuckDB는 하이브리드 워크로드에서 좋은 성능을 달성하기 위해 정렬 유지 사전 또는 참조 프레임 [\[2\]](#page-12-10)와 같은 경량 압축 방법을 사용하는 DataBlocks [\[43\]](#page-12-43) 저장 형식을 선택했습니다. 반대로 ClickHouse는 추가되기만 하는 사용 사례, 즉 업데이트 및 삭제가 없거나 드문 경우에 최적화되어 있습니다. 블록은 LZ4와 같은 중량 기술을 사용하여 압축되며, 사용자가 빈번한 쿼리를 가속화하기 위해 데이터 프루닝을 관대하게 사용하고, 나머지 쿼리에서는 I/O 비용이 압축 해제 비용을 초월한다고 가정합니다. DuckDB는 Hyper의 MVCC 방식 [\[55\]](#page-13-35)을 기반으로 한 직렬화 가능한 트랜잭션을 제공하는 반면, ClickHouse는 스냅샷 격리만 제공합니다.
## 8 결론 및 전망 {#8-conclusion-and-outlook}

우리는 오픈 소스, 고성능 OLAP 데이터베이스인 ClickHouse의 아키텍처를 제시했습니다. 쓰기 최적화된 스토리지 레이어와 최첨단 벡터화된 쿼리 엔진을 기반으로, ClickHouse는 페타바이트 규모의 데이터 세트에 대해 높은 수집률로 실시간 분석을 가능하게 합니다. ClickHouse는 백그라운드에서 비동기적으로 데이터를 병합하고 변형함으로써 데이터 유지 관리와 병렬 삽입을 효율적으로 분리합니다. 그 스토리지 레이어는 스파스 기본 인덱스, 스킵 인덱스 및 ^^프로젝션^^ 테이블을 사용하여 공격적인 데이터 프루닝을 가능케 합니다. 우리는 ClickHouse의 업데이트 및 삭제 구현, 항등성 삽입 및 고가용성을 위한 노드 간 데이터 복제 방식을 설명했습니다. 쿼리 처리 레이어는 많은 기술을 사용하여 쿼리를 최적화하고, 모든 서버와 ^^클러스터^^ 리소스에 걸쳐 실행을 병렬화합니다. 통합 테이블 엔진과 함수는 다른 데이터 관리 시스템 및 데이터 형식과 원활하게 상호작용할 수 있는 편리한 방법을 제공합니다. 벤치마크를 통해 ClickHouse가 시장에서 가장 빠른 분석 데이터베이스 중 하나임을 보여주었고, 수년간 ClickHouse의 실세계 배포에서 일반 쿼리의 성능에 상당한 개선을 보여주었습니다.

2024년 계획된 모든 기능 및 개선 사항은 공개 로드맵 [\[18\]](#page-12-33)에서 확인할 수 있습니다. 계획된 개선 사항에는 사용자 트랜잭션 지원, PromQL [\[69\]](#page-13-36)이라는 대체 쿼리 언어, 반구조적 데이터(예: JSON)를 위한 새로운 데이터 유형, 조인의 계획 수준 최적화 개선, 경량 삭제를 보완하기 위한 경량 업데이트 구현이 포함됩니다.
## 감사의 말씀 {#acknowledgements}

버전 24.6에 따라, SELECT * FROM system.contributors는 ClickHouse에 기여한 1994명의 개인을 반환합니다. ClickHouse Inc.의 전체 엔지니어링 팀과 ClickHouse의 놀라운 오픈 소스 커뮤니티에 그들의 노력과 헌신에 감사드립니다.
## REFERENCES {#references}

- <Anchor id="page-12-35"/>[1] Daniel Abadi, Peter Boncz, Stavros Harizopoulos, Stratos Idreaos, and Samuel Madden. 2013. 현대 컬럼 지향 데이터베이스 시스템의 설계 및 구현. https://doi.org/10.1561/9781601987556
- <Anchor id="page-12-10"/>[2] Daniel Abadi, Samuel Madden, and Miguel Ferreira. 2006. 컬럼 지향 데이터베이스 시스템에서 압축과 실행의 통합. In Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data (SIGMOD '06). 671–682. https://doi.org/10.1145/1142473.1142548
- <Anchor id="page-12-41"/>[3] Anastassia Ailamaki, David J. DeWitt, Mark D. Hill, and Marios Skounakis. 2001. 캐시 성능을 위한 관계 엮기. In Proceedings of the 27th International Conference on Very Large Data Bases (VLDB '01). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 169–180.
- <Anchor id="page-12-38"/>[4] Nikos Armenatzoglou, Sanuj Basu, Naga Bhanoori, Mengchu Cai, Naresh Chainani, Kiran Chinta, Venkatraman Govindaraju, Todd J. Green, Monish Gupta, Sebastian Hillig, Eric Hotinger, Yan Leshinksy, Jintian Liang, Michael McCreedy, Fabian Nagel, Ippokratis Pandis, Panos Parchas, Rahul Pathak, Orestis Polychroniou, Foyzur Rahman, Gaurav Saxena, Gokul Soundararajan, Sriram Subramanian, and Doug Terry. 2022. Amazon Redshift 재구성. In Proceedings of the 2022 International Conference on Management of Data (Philadelphia, PA, USA) (SIGMOD '22). Association for Computing Machinery, New York, NY, USA, 2205–2217. https://doi.org/10.1145/3514221.3526045
- <Anchor id="page-12-39"/>[5] Alexander Behm, Shoumik Palkar, Utkarsh Agarwal, Timothy Armstrong, David Cashman, Ankur Dave, Todd Greenstein, Shant Hovsepian, Ryan Johnson, Arvind Sai Krishnan, Paul Leventis, Ala Luszczak, Prashanth Menon, Mostafa Mokhtar, Gene Pang, Sameer Paranjpye, Greg Rahn, Bart Samwel, Tom van Bussel, Herman van Hovell, Maryann Xue, Reynold Xin, and Matei Zaharia. 2022. Photon: 호수 시스템을 위한 빠른 쿼리 엔진 (SIGMOD '22). Association for Computing Machinery, New York, NY, USA, 2326–2339. [https://doi.org/10.1145/3514221.](https://doi.org/10.1145/3514221.3526054) [3526054](https://doi.org/10.1145/3514221.3526054)
- <Anchor id="page-12-18"/>[6] Philip A. Bernstein and Nathan Goodman. 1981. 분산 데이터베이스 시스템에서의 동시성 제어. ACM Computing Survey 13, 2 (1981), 185–221. https://doi.org/10.1145/356842.356846
- <Anchor id="page-12-23"/>[7] Spyros Blanas, Yinan Li, and Jignesh M. Patel. 2011. 다중 코어 CPU를 위한 메인 메모리 해시 조인 알고리즘의 설계 및 평가. In Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data (Athens, Greece) (SIGMOD '11). Association for Computing Machinery, New York, NY, USA, 37–48. https://doi.org/10.1145/1989323.1989328
- <Anchor id="page-12-26"/><Anchor id="page-12-14"/>[8] Daniel Gomez Blanco. 2023. 실용적인 OpenTelemetry. Springer Nature.
- [9] Burton H. Bloom. 1970. 허용 가능한 오류와 함께 해시 코딩의 공간/시간 절충. Commun. ACM 13, 7 (1970), 422–426. [https://doi.org/10.1145/362686.](https://doi.org/10.1145/362686.362692) [362692](https://doi.org/10.1145/362686.362692)
- <Anchor id="page-12-27"/>[10] Peter Boncz, Thomas Neumann, and Orri Erling. 2014. TPC-H 분석: 영향력 있는 벤치마크에서의 숨겨진 메시지와 배운 교훈. In Performance Characterization and Benchmarking. 61–76. [https://doi.org/10.1007/978-3-319-](https://doi.org/10.1007/978-3-319-04936-6_5) [04936-6_5](https://doi.org/10.1007/978-3-319-04936-6_5)
- <Anchor id="page-12-0"/>[11] Peter Boncz, Marcin Zukowski, and Niels Nes. 2005. MonetDB/X100: 하이퍼 파이프라인 쿼리 실행. In CIDR.
- <Anchor id="page-12-8"/>[12] Martin Burtscher and Paruj Ratanaworabhan. 2007. 배수 정밀도 부동 소수점 데이터의 높은 처리량 압축. In Data Compression Conference (DCC). 293–302. https://doi.org/10.1109/DCC.2007.44
- <Anchor id="page-12-6"/>[13] Jef Carpenter and Eben Hewitt. 2016. Cassandra: 확실한 가이드 (2판). O'Reilly Media, Inc.
- <Anchor id="page-12-17"/>[14] Bernadette Charron-Bost, Fernando Pedone, and André Schiper (Eds.). 2010. 복제: 이론과 실제. Springer-Verlag.
- <Anchor id="page-12-3"/>[15] chDB. 2024. chDB - 내장 OLAP SQL 엔진. 2024-06-20에 https://github.com/chdb-io/chdb에서 검색.
- <Anchor id="page-12-29"/>[16] ClickHouse. 2024. ClickBench: 분석용 데이터베이스를 위한 벤치마크. 2024-06-20에 https://github.com/ClickHouse/ClickBench에서 검색.
- <Anchor id="page-12-28"/>[17] ClickHouse. 2024. ClickBench: 비교 측정. 2024-06-20에 https://benchmark.clickhouse.com에서 검색.
- <Anchor id="page-12-33"/>[18] ClickHouse. 2024. ClickHouse 로드맵 2024 (GitHub). 2024-06-20에 https://github.com/ClickHouse/ClickHouse/issues/58392에서 검색.
- <Anchor id="page-12-30"/>[19] ClickHouse. 2024. ClickHouse 버전 벤치마크. 2024-06-20에 https://github.com/ClickHouse/ClickBench/tree/main/versions에서 검색.
- <Anchor id="page-12-31"/>[20] ClickHouse. 2024. ClickHouse 버전 벤치마크 결과. 2024-06-20에 https://benchmark.clickhouse.com/versions/에서 검색.
- <Anchor id="page-12-32"/>[21] Andrew Crotty. 2022. MgBench. 2024-06-20에 [https://github.com/](https://github.com/andrewcrotty/mgbench) [andrewcrotty/mgbench](https://github.com/andrewcrotty/mgbench)에서 검색.
- <Anchor id="page-12-37"/>[22] Benoit Dageville, Thierry Cruanes, Marcin Zukowski, Vadim Antonov, Artin Avanes, Jon Bock, Jonathan Claybaugh, Daniel Engovatov, Martin Hentschel, Jiansheng Huang, Allison W. Lee, Ashish Motivala, Abdul Q. Munir, Steven Pelley, Peter Povinec, Greg Rahn, Spyridon Triantafyllis, and Philipp Unterbrunner. 2016. Snowflake 탄력적 데이터 웨어하우스. In Proceedings of the 2016 International Conference on Management of Data (San Francisco, California, USA) (SIGMOD '16). Association for Computing Machinery, New York, NY, USA, 215–226. [https:](https://doi.org/10.1145/2882903.2903741) [//doi.org/10.1145/2882903.2903741](https://doi.org/10.1145/2882903.2903741)
- <Anchor id="page-12-9"/>[23] Patrick Damme, Annett Ungethüm, Juliana Hildebrandt, Dirk Habich, and Wolfgang Lehner. 2019. 포괄적인 실험 조사에서 경량 정수 압축 알고리즘에 대한 비용 기반 선택 전략으로의 전환. ACM Trans. Database Syst. 44, 3, Article 9 (2019), 46 페이지. https://doi.org/10.1145/3323991
- <Anchor id="page-12-1"/>[24] Philippe Dobbelaere and Kyumars Sheykh Esmaili. 2017. Kafka 대 RabbitMQ: 두 산업 기준 게시/구독 구현의 비교 연구: 산업 논문 (DEBS '17). Association for Computing Machinery, New York, NY, USA, 227–238. https://doi.org/10.1145/3093742.3093908
- <Anchor id="page-12-19"/>[25] LLVM documentation. 2024. LLVM에서의 자동 벡터화. 2024-06-20에 https://llvm.org/docs/Vectorizers.html에서 검색.
- <Anchor id="page-12-7"/>[26] Siying Dong, Andrew Kryczka, Yanqin Jin, and Michael Stumm. 2021. RocksDB: 대규모 애플리케이션에 서비스를 제공하는 키-값 저장소에서의 개발 우선 순위의 진화. ACM Transactions on Storage 17, 4, Article 26 (2021), 32 페이지. https://doi.org/10.1145/3483840
- <Anchor id="page-12-34"/>[27] Markus Dreseler, Martin Boissier, Tilmann Rabl, and Matthias Ufacker. 2020. TPC-H 병목 지점 정량화 및 최적화. Proc. VLDB Endow. 13, 8 (2020), 1206–1220. https://doi.org/10.14778/3389133.3389138
- <Anchor id="page-12-12"/>[28] Ted Dunning. 2021. t-digest: 분포의 효율적인 추정. Software Impacts 7 (2021). https://doi.org/10.1016/j.simpa.2020.100049
- <Anchor id="page-12-16"/>[29] Martin Faust, Martin Boissier, Marvin Keller, David Schwalb, Holger Bischof, Katrin Eisenreich, Franz Färber, and Hasso Plattner. 2016. SAP HANA에서 해시 인덱스를 이용한 발자국 감소 및 고유성 강화. In Database and Expert Systems Applications. 137–151. [https://doi.org/10.1007/978-3-319-44406-](https://doi.org/10.1007/978-3-319-44406-2_11) [2_11](https://doi.org/10.1007/978-3-319-44406-2_11)
- <Anchor id="page-12-11"/>[30] Philippe Flajolet, Eric Fusy, Olivier Gandouet, and Frederic Meunier. 2007. HyperLogLog: 거의 최적의 카디널리티 추정 알고리즘 분석. In AofA: 알고리즘의 분석, Vol. DMTCS Proceedings vol. AH, 2007 Conference on Analysis of Algorithms (AofA 07). Discrete Mathematics and Theoretical Computer Science, 137–156. https://doi.org/10.46298/dmtcs.3545
- <Anchor id="page-12-13"/>[31] Hector Garcia-Molina, Jefrey D. Ullman, and Jennifer Widom. 2009. 데이터베이스 시스템 - 완전한 책 (2판).
- <Anchor id="page-12-24"/>[32] Pawan Goyal, Harrick M. Vin, and Haichen Chen. 1996. 시작 시간 공정 큐잉: 통합 서비스 패킷 스위칭 네트워크를 위한 스케줄링 알고리즘. 26, 4 (1996), 157–168. https://doi.org/10.1145/248157.248171
- <Anchor id="page-12-21"/>[33] Goetz Graefe. 1993. 대형 데이터베이스를 위한 쿼리 평가 기법. ACM Comput. Surv. 25, 2 (1993), 73–169. https://doi.org/10.1145/152610.152611
- <Anchor id="page-12-40"/>[34] Jean-François Im, Kishore Gopalakrishna, Subbu Subramaniam, Mayank Shrivastava, Adwait Tumbde, Xiaotian Jiang, Jennifer Dai, Seunghyun Lee, Neha Pawar, Jialiang Li, and Ravi Aringunram. 2018. Pinot: 5억 사용자 를 위한 실시간 OLAP. In Proceedings of the 2018 International Conference on Management of Data (Houston, TX, USA) (SIGMOD '18). Association for Computing Machinery, New York, NY, USA, 583–594. https://doi.org/10.1145/3183713.3190661
- <Anchor id="page-12-25"/>[35] ISO/IEC 9075-9:2001 2001. 정보 기술 - 데이터베이스 언어 - SQL - 9부: 외부 데이터 관리 (SQL/MED). 표준. 국제 표준화 기구.
- <Anchor id="page-12-2"/>[36] Paras Jain, Peter Kraft, Conor Power, Tathagata Das, Ion Stoica, and Matei Zaharia. 2023. 호수 저장 시스템 분석 및 비교. CIDR.
- <Anchor id="page-12-4"/>[37] Project Jupyter. 2024. Jupyter 노트북. 2024-06-20에 [https:](https://jupyter.org/) [//jupyter.org/](https://jupyter.org/)에서 검색.
- <Anchor id="page-12-22"/>[38] Timo Kersten, Viktor Leis, Alfons Kemper, Thomas Neumann, Andrew Pavlo, and Peter Boncz. 2018. 컴파일 및 벡터화된 쿼리에 대해 항상 알고 싶었던 모든 것. Proc. VLDB Endow. 11, 13 (2018년 9월), 2209–2222. https://doi.org/10.14778/3275366.3284966
- <Anchor id="page-12-15"/>[39] Changkyu Kim, Jatin Chhugani, Nadathur Satish, Eric Sedlar, Anthony D. Nguyen, Tim Kaldewey, Victor W. Lee, Scott A. Brandt, and Pradeep Dubey. 2010. FAST: 현대 CPU 및 GPU에서 빠른 아키텍처 민감 트리 검색. In Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data (Indianapolis, Indiana, USA) (SIGMOD '10). Association for Computing Machinery, New York, NY, USA, 339–350. https://doi.org/10.1145/1807167.1807206
- <Anchor id="page-12-5"/>[40] Donald E. Knuth. 1973. 컴퓨터 프로그래밍의 예술, 제3권: 정렬 및 검색. Addison-Wesley.
- <Anchor id="page-12-42"/>[41] André Kohn, Viktor Leis, and Thomas Neumann. 2018. 컴파일된 쿼리의 적응 실행. In 2018 IEEE 제34회 국제 데이터 공학 회의 (ICDE). 197–208. https://doi.org/10.1109/ICDE.2018.00027
- <Anchor id="page-12-36"/>[42] Andrew Lamb, Matt Fuller, Ramakrishna Varadarajan, Nga Tran, Ben Vandiver, Lyric Doshi, and Chuck Bear. 2012. Vertica 분석 데이터베이스: C-Store 7년 후. Proc. VLDB Endow. 5, 12 (2012년 8월), 1790–1801. [https://doi.org/10.](https://doi.org/10.14778/2367502.2367518) [14778/2367502.2367518](https://doi.org/10.14778/2367502.2367518)
- <Anchor id="page-12-43"/>[43] Harald Lang, Tobias Mühlbauer, Florian Funke, Peter A. Boncz, Thomas Neumann, and Alfons Kemper. 2016. 데이터 블록: 벡터화 및 컴파일을 모두 사용하는 압축 저장소에서의 하이브리드 OLTP 및 OLAP. In Proceedings of the 2016 International Conference on Management of Data (San Francisco, California, USA) (SIGMOD '16). Association for Computing Machinery, New York, NY, USA, 311–326. https://doi.org/10.1145/2882903.2882925
- <Anchor id="page-12-20"/>[44] Viktor Leis, Peter Boncz, Alfons Kemper, and Thomas Neumann. 2014. 모서리 기반 병렬 처리: 다중 코어 시대를 위한 NUMA 인식 쿼리 평가 프레임워크. In Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data (Snowbird, Utah, USA) (SIGMOD '14). Association for Computing Machinery, New York, NY, USA, 743–754. [https://doi.org/10.1145/2588555.](https://doi.org/10.1145/2588555.2610507) [2610507](https://doi.org/10.1145/2588555.2610507)
- <Anchor id="page-13-17"/>[45] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. 적응형 레디스 트리: 주 메모리 데이터베이스를 위한 ARTful 인덱싱. In 2013 IEEE 제29회 국제 데이터 공학 회의 (ICDE). 38–49. [https://doi.org/10.1109/ICDE.](https://doi.org/10.1109/ICDE.2013.6544812) [2013.6544812](https://doi.org/10.1109/ICDE.2013.6544812)
- <Anchor id="page-13-34"/>[46] Chunwei Liu, Anna Pavlenko, Matteo Interlandi, and Brandon Haynes. 2023. 분석 DBMS용 공용 열린 형식에 대한 심층 분석. 16, 11 (2023년 7월), 3044–3056. https://doi.org/10.14778/3611479.3611507
- <Anchor id="page-13-30"/>[47] Zhenghua Lyu, Huan Hubert Zhang, Gang Xiong, Gang Guo, Haozhou Wang, Jinbao Chen, Asim Praveen, Yu Yang, Xiaoming Gao, Alexandra Wang, Wen Lin, Ashwin Agrawal, Junfeng Yang, Hao Wu, Xiaoliang Li, Feng Guo, Jiang Wu, Jesse Zhang, and Venkatesh Raghavan. 2021. Greenplum: 트랜잭션 및 분석 워크로드를 위한 하이브리드 데이터베이스 (SIGMOD '21). Association for Computing Machinery, New York, NY, USA, 2530–2542. [https:](https://doi.org/10.1145/3448016.3457562) [//doi.org/10.1145/3448016.3457562](https://doi.org/10.1145/3448016.3457562)
- <Anchor id="page-13-28"/>[48] Roger MacNicol and Blaine French. 2004. Sybase IQ 멀티플렉스 - 분석을 위해 설계됨. In Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30 (Toronto, Canada) (VLDB '04). VLDB Endowment, 1227–1230.
- <Anchor id="page-13-31"/>[49] Sergey Melnik, Andrey Gubarev, Jing Jing Long, Geofrey Romer, Shiva Shivakumar, Matt Tolton, Theo Vassilakis, Hossein Ahmadi, Dan Delorey, Slava Min, Mosha Pasumansky, and Jef Shute. 2020. Dremel: 웹 스케일에서의 상호작용 SQL 분석 10년. Proc. VLDB Endow. 13, 12 (2020년 8월), 3461–3472. https://doi.org/10.14778/3415478.3415568
- <Anchor id="page-13-2"/>[50] Microsoft. 2024. Kusto 쿼리 언어. 2024-06-20에 [https:](https://github.com/microsoft/Kusto-Query-Language) [//github.com/microsoft/Kusto-Query-Language](https://github.com/microsoft/Kusto-Query-Language)에서 검색.
- <Anchor id="page-13-14"/>[51] Guido Moerkotte. 1998. 소형 물리화 집계: 데이터 웨어하우징을 위한 경량 인덱스 구조. In Proceedings of the 24rd International Conference on Very Large Data Bases (VLDB '98). 476–487.
- <Anchor id="page-13-22"/>[52] Jalal Mostafa, Sara Wehbi, Suren Chilingaryan, and Andreas Kopmann. 2022. SciTS: 과학 실험 및 산업 사물인터넷에서 시간 시계열 데이터베이스를 위한 벤치마크. In Proceedings of the 34th International Conference on Scientific and Statistical Database Management (SSDBM '22). Article 12. [https:](https://doi.org/10.1145/3538712.3538723) [//doi.org/10.1145/3538712.3538723](https://doi.org/10.1145/3538712.3538723)
- <Anchor id="page-13-0"/>[53] Thomas Neumann. 2011. 현대 하드웨어를 위한 효율적인 쿼리 계획의 효율적인 컴파일. Proc. VLDB Endow. 4, 9 (2011년 6월), 539–550. [https://doi.org/10.14778/](https://doi.org/10.14778/2002938.2002940) [2002938.2002940](https://doi.org/10.14778/2002938.2002940)
- <Anchor id="page-13-25"/>[54] Thomas Neumann and Michael J. Freitag. 2020. Umbra: 인 메모리 성능을 가진 디스크 기반 시스템. In 10th Conference on Innovative Data Systems Research, CIDR 2020, Amsterdam, Netherlands, January 12-15, 2020, Online Proceedings. www.cidrdb.org. [http://cidrdb.org/cidr2020/papers/p29-neumann](http://cidrdb.org/cidr2020/papers/p29-neumann) [cidr20.pdf](http://cidrdb.org/cidr2020/papers/p29-neumann-cidr20.pdf)
- <Anchor id="page-13-35"/>[55] Thomas Neumann, Tobias Mühlbauer, and Alfons Kemper. 2015. 메인 메모리 데이터베이스 시스템을 위한 빠른 직렬화 가능한 다중 버전 동시성 제어. In Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data (Melbourne, Victoria, Australia) (SIGMOD '15). Association for Computing Machinery, New York, NY, USA, 677–689. [https://doi.org/10.1145/2723372.](https://doi.org/10.1145/2723372.2749436) [2749436](https://doi.org/10.1145/2723372.2749436)
- <Anchor id="page-13-8"/>[56] LevelDB on GitHub. 2024. LevelDB. 2024-06-20에 [https://github.](https://github.com/google/leveldb) [com/google/leveldb](https://github.com/google/leveldb)에서 검색.
- <Anchor id="page-13-26"/>[57] Patrick O'Neil, Elizabeth O'Neil, Xuedong Chen, and Stephen Revilak. 2009. 스타 스키마 벤치마크 및 증강 사실 테이블 인덱싱. In Performance Evaluation and Benchmarking. Springer Berlin Heidelberg, 237–252. [https:](https://doi.org/10.1007/978-3-642-10424-4_17) [//doi.org/10.1007/978-3-642-10424-4_17](https://doi.org/10.1007/978-3-642-10424-4_17)
- <Anchor id="page-13-7"/>[58] Patrick E. O'Neil, Edward Y. C. Cheng, Dieter Gawlick, and Elizabeth J. O'Neil. 1996. 로그 구조 병합 트리 (LSM-tr리). Acta Informatica 33 (1996), 351–385. https://doi.org/10.1007/s002360050048
- <Anchor id="page-13-4"/>[59] Diego Ongaro and John Ousterhout. 2014. 이해할 수 있는 합의 알고리즘을 찾아서. In Proceedings of the 2014 USENIX Conference on USENIX Annual Technical Conference (USENIX ATC'14). 305–320. [https://doi.org/doi/10.](https://doi.org/doi/10.5555/2643634.2643666) [5555/2643634.2643666](https://doi.org/doi/10.5555/2643634.2643666)
- <Anchor id="page-13-3"/>[60] Patrick O'Neil, Edward Cheng, Dieter Gawlick, and Elizabeth O'Neil. 1996. 로그 구조 병합 트리 (LSM-트리). Acta Inf. 33, 4 (1996), 351–385. [https:](https://doi.org/10.1007/s002360050048) [//doi.org/10.1007/s002360050048](https://doi.org/10.1007/s002360050048)
- <Anchor id="page-13-5"/>[61] Pandas. 2024. Pandas 데이터 프레임. 2024-06-20에 [https://pandas.](https://pandas.pydata.org/) [pydata.org/](https://pandas.pydata.org/)에서 검색.
- <Anchor id="page-13-32"/>[62] Pedro Pedreira, Orri Erling, Masha Basmanova, Kevin Wilfong, Laith Sakka, Krishna Pai, Wei He, and Biswapesh Chattopadhyay. 2022. Velox: 메타의 통합 실행 엔진. Proc. VLDB Endow. 15, 12 (2022년 8월), 3372–3384. [https:](https://doi.org/10.14778/3554821.3554829) [//doi.org/10.14778/3554821.3554829](https://doi.org/10.14778/3554821.3554829)
- <Anchor id="page-13-10"/>[63] Tuomas Pelkonen, Scott Franklin, Justin Teller, Paul Cavallaro, Qi Huang, Justin Meza, and Kaushik Veeraraghavan. 2015. 고릴라: 빠르고 확장 가능한 인메모리 시계열 데이터베이스. Proceedings of the VLDB Endowment 8, 12 (2015), 1816–1827. https://doi.org/10.14778/2824032.2824078
- <Anchor id="page-13-18"/>[64] Orestis Polychroniou, Arun Raghavan, and Kenneth A. Ross. 2015. 인메모리 데이터베이스를 위한 SIMD 벡터화 재고. In Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data (SIGMOD '15). 1493–1508. https://doi.org/10.1145/2723372.2747645
- <Anchor id="page-13-21"/>[65] PostgreSQL. 2024. PostgreSQL - 외부 데이터 래퍼. 2024-06-20에 https://wiki.postgresql.org/wiki/Foreign_data_wrappers에서 검색.
- <Anchor id="page-13-23"/>[66] Mark Raasveldt, Pedro Holanda, Tim Gubner, and Hannes Mühleisen. 2018. 공정한 벤치마킹이 고려되는 어려움: 데이터베이스 성능 테스트의 일반적인 함정. In Proceedings of the Workshop on Testing Database Systems (Houston, TX, USA) (DBTest'18). Article 2, 6 페이지. https://doi.org/10.1145/3209950.3209955
- <Anchor id="page-13-6"/>[67] Mark Raasveldt and Hannes Mühleisen. 2019. DuckDB: 내장 가능한 분석 데이터베이스 (SIGMOD '19). Association for Computing Machinery, New York, NY, USA, 1981–1984. https://doi.org/10.1145/3299869.3320212
- <Anchor id="page-13-16"/>[68] Jun Rao and Kenneth A. Ross. 1999. 주 메모리에서 의사 결정 지원을 위한 캐시 인식 인덱싱. In Proceedings of the 25th International Conference on Very Large Data Bases (VLDB '99). San Francisco, CA, USA, 78–89.
- <Anchor id="page-13-36"/>[69] Navin C. Sabharwal and Piyush Kant Pandey. 2020. 프로메테우스 쿼리 언어 (PromQL) 사용. In Monitoring Microservices and Containerized Applications. https://doi.org/10.1007/978-1-4842-6216-0_5
- <Anchor id="page-13-27"/>[70] Todd W. Schneider. 2022. 뉴욕시 택시 및 임대 차량 데이터. 2024-06-20에 https://github.com/toddwschneider/nyc-taxi-data에서 검색.
- <Anchor id="page-13-13"/>[71] Mike Stonebraker, Daniel J. Abadi, Adam Batkin, Xuedong Chen, Mitch Cherniack, Miguel Ferreira, Edmond Lau, Amerson Lin, Sam Madden, Elizabeth O'Neil, Pat O'Neil, Alex Rasin, Nga Tran, and Stan Zdonik. 2005. C-Store: 컬럼 지향 DBMS. In Proceedings of the 31st International Conference on Very Large Data Bases (VLDB '05). 553–564.
- <Anchor id="page-13-29"/>[72] Teradata. 2024. Teradata 데이터베이스. 2024-06-20에 [https://www.](https://www.teradata.com/resources/datasheets/teradata-database) [teradata.com/resources/datasheets/teradata-database](https://www.teradata.com/resources/datasheets/teradata-database)에서 검색.
- <Anchor id="page-13-15"/>[73] Frederik Transier. 2010. 인메모리 텍스트 검색 엔진을 위한 알고리즘 및 데이터 구조. 박사학위 논문. https://doi.org/10.5445/IR/1000015824
- <Anchor id="page-13-24"/>[74] Adrian Vogelsgesang, Michael Haubenschild, Jan Finis, Alfons Kemper, Viktor Leis, Tobias Muehlbauer, Thomas Neumann, and Manuel Then. 2018. 현실적: 벤치마크가 실제 세계를 대표하지 못하는 방법. In Proceedings of the Workshop on Testing Database Systems (Houston, TX, USA) (DBTest'18). Article 1, 6 페이지. https://doi.org/10.1145/3209950.3209952
- <Anchor id="page-13-9"/>[75] LZ4 website. 2024. LZ4. 2024-06-20에 https://lz4.org/에서 검색.
- <Anchor id="page-13-11"/><Anchor id="page-13-1"/>[76] PRQL website. 2024. PRQL. 2024-06-20에 https://prql-lang.org에서 검색 [77] Till Westmann, Donald Kossmann, Sven Helmer, and Guido Moerkotte. 2000. 압축된 데이터베이스의 구현 및 성능. SIGMOD Rec.
- <Anchor id="page-13-33"/>29, 3 (2000년 9월), 55–67. https://doi.org/10.1145/362084.362137 [78] Fangjin Yang, Eric Tschetter, Xavier Léauté, Nelson Ray, Gian Merlino, and Deep Ganguli. 2014. Druid: 실시간 분석 데이터 저장소. In Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data (Snowbird, Utah, USA) (SIGMOD '14). Association for Computing Machinery, New York, NY, USA, 157–168. https://doi.org/10.1145/2588555.2595631
- <Anchor id="page-13-20"/>[79] Tianqi Zheng, Zhibin Zhang, and Xueqi Cheng. 2020. SAHA: 분석 데이터베이스를 위한 문자열 적응 해시 테이블. Applied Sciences 10, 6 (2020). [https:](https://doi.org/10.3390/app10061915) [//doi.org/10.3390/app10061915](https://doi.org/10.3390/app10061915)
- <Anchor id="page-13-19"/>[80] Jingren Zhou and Kenneth A. Ross. 2002. SIMD 명령어를 사용하여 데이터베이스 작업 구현. In Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data (SIGMOD '02). 145–156. [https://doi.org/10.](https://doi.org/10.1145/564691.564709) [1145/564691.564709](https://doi.org/10.1145/564691.564709)
- <Anchor id="page-13-12"/>[81] Marcin Zukowski, Sandor Heman, Niels Nes, and Peter Boncz. 2006. 슈퍼 스칼라 RAM-CPU 캐시 압축. In Proceedings of the 22nd International Conference on Data Engineering (ICDE '06). 59. [https://doi.org/10.1109/ICDE.](https://doi.org/10.1109/ICDE.2006.150) [2006.150](https://doi.org/10.1109/ICDE.2006.150)
