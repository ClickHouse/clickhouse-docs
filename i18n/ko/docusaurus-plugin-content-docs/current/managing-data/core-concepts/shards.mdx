---
'slug': '/shards'
'title': '테이블 샤드 및 복제본'
'description': 'ClickHouse에서 테이블 샤드 및 복제본이란 무엇인가?'
'keywords':
- 'shard'
- 'shards'
- 'sharding'
- 'replica'
- 'replicas'
'doc_type': 'guide'
---

import image_01 from '@site/static/images/managing-data/core-concepts/shards_01.png'
import image_02 from '@site/static/images/managing-data/core-concepts/shards_02.png'
import image_03 from '@site/static/images/managing-data/core-concepts/shards_03.png'
import image_04 from '@site/static/images/managing-data/core-concepts/shards_04.png'
import image_05 from '@site/static/images/managing-data/core-concepts/shards_replicas_01.png'
import Image from '@theme/IdealImage';

<br/>
:::note
이 주제는 ClickHouse Cloud에 적용되지 않으며, 이곳에서 [Parallel Replicas](/docs/deployment-guides/parallel-replicas)는 전통적인 공유-무 지식 ClickHouse 클러스터의 여러 샤드처럼 작동하며, 객체 스토리지는 [기존의 복제본을 대체](https://clickhouse.com/blog/clickhouse-cloud-boosts-performance-with-sharedmergetree-and-lightweight-updates#shared-object-storage-for-data-availability)하여 높은 가용성과 내결함성을 보장합니다.
:::

## ClickHouse에서 테이블 샤드란 무엇인가? {#what-are-table-shards-in-clickhouse}

전통적인 [공유-무](https://en.wikipedia.org/wiki/Shared-nothing_architecture) ClickHouse 클러스터에서, 샤딩은 ① 데이터가 단일 서버의 용량을 초과할 때 또는 ② 단일 서버가 데이터를 처리하는 데 너무 느릴 때 사용됩니다. 다음 그림은 [uk_price_paid_simple](/parts) 테이블이 단일 기계의 용량을 초과하는 사례 ①을 보여줍니다:

<Image img={image_01} size="lg" alt='SHARDS'/>

<br/>

이런 경우 데이터를 여러 ClickHouse 서버에 테이블 샤드 형태로 분할할 수 있습니다:

<Image img={image_02} size="lg" alt='SHARDS'/>

<br/>

각 ^^샤드^^는 데이터의 하위 집합을 보유하고 정규 ClickHouse 테이블처럼 독립적으로 쿼리될 수 있습니다. 그러나 쿼리는 데이터 분포에 따라 유효한 사용 사례일 수 있는 해당 하위 집합만 처리합니다. 일반적으로 [분산 테이블](/docs/engines/table-engines/special/distributed) (대개 서버당 하나)가 전체 데이터 세트의 통합된 뷰를 제공합니다. 실제로 데이터는 저장하지 않고 **SELECT** 쿼리를 모든 샤드로 포워딩하며, 결과를 조합하고 **INSERTS**를 라우팅하여 데이터를 고르게 분배합니다.

## 분산 테이블 생성 {#distributed-table-creation}

**SELECT** 쿼리 포워딩 및 **INSERT** 라우팅을 설명하기 위해, [What are table parts](/parts) 예제 테이블이 두 개의 ClickHouse 서버의 두 샤드에 분할된 사례를 살펴보겠습니다. 먼저, 이 설정에 해당하는 **^^Distributed table^^**을 생성하기 위한 DDL 문장을 보여줍니다:

```sql
CREATE TABLE uk.uk_price_paid_simple_dist ON CLUSTER test_cluster
(
    date Date,
    town LowCardinality(String),
    street LowCardinality(String),
    price UInt32
)
ENGINE = Distributed('test_cluster', 'uk', 'uk_price_paid_simple', rand())
```

`ON CLUSTER` 절은 DDL 문장을 [분산 DDL 문장](/docs/sql-reference/distributed-ddl)으로 만들어 ClickHouse에 `test_cluster` [클러스터 정의](/architecture/replication/#configure-clickhouse-servers)에 나열된 모든 서버에서 테이블을 생성하도록 지시합니다. 분산 DDL은 [클러스터 아키텍처](/architecture/horizontal-scaling)에서 추가적인 [Keeper](https://clickhouse.com/clickhouse/keeper) 컴포넌트를 필요로 합니다.

[분산 엔진 매개변수](/docs/engines/table-engines/special/distributed#distributed-parameters)에서는 ^^클러스터^^ 이름(`test_cluster`), 샤드 대상 테이블의 데이터베이스 이름(`uk`), 샤드 대상 테이블의 이름(`uk_price_paid_simple`), 그리고 INSERT 라우팅을 위한 **샤딩 키**를 지정합니다. 이 예제에서는 [rand](/sql-reference/functions/random-functions#rand) 함수를 사용하여 행을 샤드에 무작위로 할당합니다. 그러나 사용 사례에 따라 복잡한 표현도 샤딩 키로 사용할 수 있습니다. 다음 섹션에서는 INSERT 라우팅 작동 방식을 설명합니다.

## INSERT 라우팅 {#insert-routing}

아래 다이어그램은 ClickHouse에서 ^^분산 테이블^^로의 INSERT가 처리되는 방식을 보여줍니다:

<Image img={image_03} size="lg" alt='SHARDS'/>

<br/>

① ^^분산 테이블^^을 목표로 하는 INSERT(단일 행)가 클릭하우스 서버로 전송됩니다. 이는 직접 또는 로드 밸런서를 통해 이루어질 수 있습니다.

② INSERT의 각 행(우리는 예제로 하나의 행만 보겠습니다)에 대해 ClickHouse는 샤딩 키(rand())를 평가하고, 그 결과를 ^^샤드^^ 서버 수로 나눈 나머지를 계산하여 해당 서버 ID를 결정합니다(ID는 0부터 시작하여 1씩 증가함). 그런 다음 해당 행은 전달되어 ③ 해당 서버의 테이블 ^^샤드^^에 삽입됩니다.

다음 섹션에서는 SELECT 포워딩 작동 방식을 설명합니다.

## SELECT 포워딩 {#select-forwarding}

이 다이어그램은 ClickHouse의 ^^분산 테이블^^에서 SELECT 쿼리가 처리되는 방식을 보여줍니다:

<Image img={image_04} size="lg" alt='SHARDS'/>

<br/>

① ^^분산 테이블^^을 대상으로 하는 SELECT 집계 쿼리가 해당 ClickHouse 서버로 전송되며, 이는 직접 또는 로드 밸런서를 통해 이루어질 수 있습니다.

② ^^Distributed table^^은 대상 테이블의 샤드를 호스팅하는 모든 서버에 쿼리를 포워딩하며, 각 ClickHouse 서버는 로컬 집계 결과를 **병렬로** 계산합니다.

그런 다음, 처음 목표로 한 ^^분산 테이블^^을 호스팅하는 ClickHouse 서버가 ③ 모든 로컬 결과를 수집하고, ④ 이를 최종 글로벌 결과로 병합하며, ⑤ 쿼리 발송자에게 반환합니다.

## ClickHouse에서 테이블 복제본이란 무엇인가? {#what-are-table-replicas-in-clickhouse}

ClickHouse의 복제는 **데이터 무결성**과 **페일오버**를 보장하기 위해 여러 서버에 **^^샤드^^ 데이터**의 **복사본**을 유지합니다. 하드웨어 오류가 불가피하기 때문에, 복제는 각 ^^샤드^^에 여러 복제본이 있도록 하여 데이터 손실을 방지합니다. 쓰기는 직접 또는 [분산 테이블](#distributed-table-creation)을 통해 어떤 ^^복제본^^으로도 보낼 수 있으며, 이 경우 작업을 위해 ^^복제본^^이 선택됩니다. 변경 사항은 자동으로 다른 복제본으로 전파됩니다. 오류나 유지 관리가 발생했을 때, 다른 복제본에서 데이터는 여전히 이용 가능하며, 실패한 호스트가 복구되면 자동으로 동기화되어 최신 상태를 유지합니다.

복제를 위해서는 [클러스터 아키텍처](/architecture/horizontal-scaling)에서 [Keeper](https://clickhouse.com/clickhouse/keeper) 컴포넌트가 필요함을 유의하십시오.

다음 다이어그램은 여섯 대의 서버로 구성된 ClickHouse ^^클러스터^^를 보여줍니다. 여기서 앞서 소개된 두 개의 테이블 샤드 `Shard-1`과 `Shard-2`는 각각 세 개의 복제본을 가지고 있습니다. 쿼리가 이 ^^클러스터^^에 전송됩니다:

<Image img={image_05} size="lg" alt='SHARDS'/>

<br/>

쿼리 처리 방식은 복제본이 없는 설정과 유사하며, 각 ^^샤드^^에서 단일 ^^복제본^^만 쿼리를 실행합니다.

> 복제본은 데이터 무결성과 페일오버를 보장할 뿐만 아니라 여러 쿼리를 서로 다른 복제본에서 병렬로 실행할 수 있게 하여 쿼리 처리량을 개선합니다.

① ^^분산 테이블^^을 목표로 하는 쿼리가 해당 ClickHouse 서버로 전송됩니다. 이는 직접 또는 로드 밸런서를 통해 이루어질 수 있습니다.

② ^^Distributed table^^은 각 ^^샤드^^에서 하나의 ^^복제본^^으로 쿼리를 포워딩하며, 각 ClickHouse 서버는 선택된 ^^복제본^^을 호스팅하여 병렬로 로컬 쿼리 결과를 계산합니다.

남은 부분은 복제본이 없는 설정에서와 [같은 방식](#select-forwarding)으로 작동하며, 위의 다이어그램에 나타나지 않습니다. 처음 목표로 한 ^^분산 테이블^^을 호스팅하는 ClickHouse 서버가 모든 로컬 결과를 수집하고, 이를 최종 글로벌 결과로 병합하며 쿼리 발송자에게 반환합니다.

ClickHouse는 ②에 대한 쿼리 포워딩 전략을 구성할 수 있음을 유의하십시오. 기본적으로—위의 다이어그램과는 달리—^^분산 테이블^^은 가능하다면 로컬 ^^복제본^^을 [선호합니다](/docs/operations/settings/settings#prefer_localhost_replica), 그러나 다른 로드 밸런싱 [전략](/docs/operations/settings/settings#load_balancing)도 사용될 수 있습니다.

## 더 많은 정보를 찾는 곳 {#where-to-find-more-information}

테이블 샤드와 복제본에 대한 이 고수준 소개 이상의 자세한 내용을 보려면 우리의 [배포 및 규모 확장 가이드](/docs/architecture/horizontal-scaling)를 확인하십시오.

ClickHouse의 샤드와 복제본에 대한 깊이 있는 다이빙을 위한 이 튜토리얼 비디오도 강력히 추천합니다:

<iframe width="1024" height="576" src="https://www.youtube.com/embed/vBjCJtw_Ei0?si=WqopTrnti6usCMRs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
