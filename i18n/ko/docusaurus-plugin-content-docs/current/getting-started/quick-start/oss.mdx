---
slug: /getting-started/quick-start/oss
sidebar_label: 'OSS'
sidebar_position: 2
keywords: ['시작하기', '퀵 스타트', '초보자용']
title: 'ClickHouse OSS 퀵 스타트'
description: 'ClickHouse 퀵 스타트 가이드'
show_related_blogs: true
doc_type: 'guide'
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from '@theme/CodeBlock';
import {VerticalStepper} from '@clickhouse/click-ui/bundled';

# ClickHouse OSS 빠른 시작 \{#clickhouse-oss-quick-start\}

> 이 빠른 시작 튜토리얼에서는 8단계로 OSS ClickHouse 환경을 준비하는 방법을 안내합니다. 사용 중인 OS에 맞는 바이너리를 다운로드하고, ClickHouse 서버를 실행하는 방법을 익힌 뒤, ClickHouse 클라이언트를 사용해 테이블을 생성하고 데이터 삽입 후 해당 데이터를 조회하는 쿼리를 실행합니다.

<VerticalStepper>
  ## ClickHouse 다운로드하기 \{#download-the-binary\}

  ClickHouse는 Linux, FreeBSD, macOS에서 기본적으로 실행되며, Windows에서는
  [WSL](https://learn.microsoft.com/en-us/windows/wsl/about)을 통해 실행됩니다. ClickHouse를 로컬에 다운로드하는 가장 간단한 방법은 다음
  `curl` 명령을 실행하는 것입니다. 이 명령은 운영 체제가 지원되는지 확인한 다음,
  master 브랜치에서 빌드된 적절한 ClickHouse 바이너리를 다운로드합니다.

  :::note
  ClickHouse 서버를 처음 실행하면 바이너리가 위치한 디렉터리에 일부 설정 파일이 생성되므로, 새로운 빈 하위 디렉터리에서 아래 명령을 실행하는 것을 권장합니다.

  아래 스크립트는 프로덕션 환경에서 ClickHouse를 설치하는 데 권장되는 방법이 아닙니다.
  프로덕션 인스턴스 설치를 원하시면 [설치 페이지](/install)를 참조하세요.
  :::

  ```bash
  curl https://clickhouse.com/ | sh
  ```

  다음과 같이 표시되어야 합니다:

  ```
  Successfully downloaded the ClickHouse binary, you can run it as:
      ./clickhouse

  You can also install it:
  sudo ./clickhouse install
  ```

  이 단계에서는 `install` 명령 실행 프롬프트를 무시하십시오.

  :::note
  Mac 사용자의 경우: 바이너리 개발자를 확인할 수 없다는 오류가 발생하면
  [&quot;MacOS에서 개발자 확인 오류 수정하기&quot;](https://clickhouse.com/docs/knowledgebase/fix-developer-verification-error-in-macos)를 참조하세요.
  :::

  ## 서버 시작 \{#start-the-server\}

  다음 명령을 실행하여 ClickHouse 서버를 시작하세요:

  ```bash
  ./clickhouse server
  ```

  터미널에 로그가 출력되는 것을 확인할 수 있습니다. 이는 정상적인 동작입니다. ClickHouse의
  [기본 로깅 레벨](https://clickhouse.com/docs/knowledgebase/why_default_logging_verbose)은
  `warning`이 아닌 `trace`로 설정되어 있습니다.

  ## 클라이언트 시작하기 \{#start-the-client\}

  `clickhouse-client`를 사용하여 ClickHouse 서비스에 연결하세요. 새 터미널을 열고 `clickhouse` 바이너리가 저장된 디렉터리로 이동한 다음 아래 명령을 실행하세요:

  ```bash
  ./clickhouse client
  ```

  localhost에서 실행 중인 서비스에 연결되면 웃는 얼굴이 표시됩니다:

  ```response
  my-host :)
  ```

  ## 테이블 생성하기 \{#create-a-table\}

  `CREATE TABLE`을 사용하여 새 테이블을 정의합니다. 일반적인 SQL DDL 명령은
  ClickHouse에서 작동하며, 한 가지 추가 사항이 있습니다. ClickHouse의 테이블에는
  `ENGINE` 절이 필요합니다. ClickHouse의 성능 이점을 활용하려면 [`MergeTree`](/engines/table-engines/mergetree-family/mergetree)를
  사용하세요:

  ```sql
  CREATE TABLE my_first_table
  (
      user_id UInt32,
      message String,
      timestamp DateTime,
      metric Float32
  )
  ENGINE = MergeTree
  PRIMARY KEY (user_id, timestamp)
  ```

  ## 데이터 삽입하기 \{#insert-data\}

  ClickHouse에서 익숙한 `INSERT INTO TABLE` 명령을 사용할 수 있지만,
  `MergeTree` 테이블에 삽입할 때마다 스토리지에 **파트**가 생성된다는 점을
  이해하는 것이 중요합니다. 이러한 ^^파트^^는 이후 ClickHouse에 의해 백그라운드에서
  병합됩니다.

  ClickHouse에서는 백그라운드 프로세스에서 병합해야 하는 [**파트**](/parts)의 수를 최소화하기 위해
  한 번에 많은 행(수만 개 또는 수백만 개)을 대량으로 삽입합니다.

  이 가이드에서는 아직 해당 부분을 다루지 않습니다. 다음 명령을 실행하여 테이블에 몇 개의 데이터 행을 삽입하세요:

  ```sql
  INSERT INTO my_first_table (user_id, message, timestamp, metric) VALUES
      (101, 'Hello, ClickHouse!',                                 now(),       -1.0    ),
      (102, 'Insert a lot of rows per batch',                     yesterday(), 1.41421 ),
      (102, 'Sort your data based on your commonly-used queries', today(),     2.718   ),
      (101, 'Granules are the smallest chunks of data read',      now() + 5,   3.14159 )
  ```

  ## 새 테이블 쿼리 실행하기 \{#query-your-new-table\}

  다른 SQL 데이터베이스와 마찬가지로 `SELECT` 쿼리를 작성할 수 있습니다:

  ```sql
  SELECT *
  FROM my_first_table
  ORDER BY timestamp
  ```

  응답이 깔끔한 테이블 형식으로 반환됩니다:

  ```text
  ┌─user_id─┬─message────────────────────────────────────────────┬───────────timestamp─┬──metric─┐
  │     102 │ Insert a lot of rows per batch                     │ 2022-03-21 00:00:00 │ 1.41421 │
  │     102 │ Sort your data based on your commonly-used queries │ 2022-03-22 00:00:00 │   2.718 │
  │     101 │ Hello, ClickHouse!                                 │ 2022-03-22 14:04:09 │      -1 │
  │     101 │ Granules are the smallest chunks of data read      │ 2022-03-22 14:04:14 │ 3.14159 │
  └─────────┴────────────────────────────────────────────────────┴─────────────────────┴─────────┘

  4 rows in set. Elapsed: 0.008 sec.
  ```

  ## 데이터 삽입하기 \{#insert-your-own-data\}

  다음 단계는 ClickHouse에 자체 데이터를 가져오는 것입니다. 데이터 수집을 위한 다양한 [테이블 함수](/sql-reference/table-functions/index.md)와 [통합](/integrations)을 제공합니다. 아래 탭에서 몇 가지 예제를 확인하시거나, ClickHouse와 통합되는 기술의 전체 목록을 보려면 [통합](/integrations) 페이지를 참조하십시오.

  <Tabs groupId="read_data">
    <TabItem value="S3" label="S3" default>
      [`s3` table function](/sql-reference/table-functions/s3.md)을(를) 사용하여
      S3에서 파일을 읽습니다. 이는 결과가 테이블인 table function이므로, 이 테이블은 다음과 같이 사용할 수 있습니다:

      1. `SELECT` 쿼리의 소스로 사용하여(데이터를 S3에 그대로 둔 채 애드혹(ad-hoc) 쿼리를 실행) 또는...
      2. 결과 테이블을 `MergeTree` 테이블에 삽입하여(데이터를 ClickHouse로
         옮길 준비가 되었을 때)

      애드혹(ad-hoc) 쿼리는 다음과 같습니다:

      ```sql
      SELECT
      passenger_count,
      avg(toFloat32(total_amount))
      FROM s3(
      'https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_0.gz',
      'TabSeparatedWithNames'
      )
      GROUP BY passenger_count
      ORDER BY passenger_count;
      ```

      데이터를 ClickHouse 테이블로 옮기는 과정은 다음과 같으며, 여기서
      `nyc_taxi`는 `MergeTree` 테이블입니다.

      ```sql
      INSERT INTO nyc_taxi
      SELECT * FROM s3(
      'https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_0.gz',
      'TabSeparatedWithNames'
      )
      SETTINGS input_format_allow_errors_num=25000;
      ```

      ClickHouse와 함께 S3를 사용하는 더 많은 세부 정보와 예시는 [AWS S3 문서 모음](/integrations/data-ingestion/s3/index.md)을 참고하십시오.

      <br />
    </TabItem>

    <TabItem value="GCS" label="GCS">
      AWS S3에서 데이터를 읽는 데 사용하는 [`s3` table function](/sql-reference/table-functions/s3.md)은
      Google Cloud Storage에 있는 파일에도 동일하게 동작합니다.

      예를 들어:

      ```sql
      SELECT
      *
      FROM s3(
      'https://storage.googleapis.com/my-bucket/trips.parquet',
      'MY_GCS_HMAC_KEY',
      'MY_GCS_HMAC_SECRET_KEY',
      'Parquet'
      )
      LIMIT 1000
      ```

      자세한 내용은 [`s3` 테이블 함수 페이지](/sql-reference/table-functions/s3.md)에서 확인하십시오.

      <br />
    </TabItem>

    <TabItem value="URL" label="웹">
      [`url` table function](/sql-reference/table-functions/url)은
      웹을 통해 접근할 수 있는 파일을 읽습니다:

      ```sql
      --By default, ClickHouse prevents redirects to protect from SSRF attacks.
      --The URL below requires a redirect, so we must set max_http_get_redirects > 0.
      SET max_http_get_redirects=10;

      SELECT *
      FROM url(
      'http://prod2.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-complete.csv',
      'CSV'
      );
      ```

      [`url` 테이블 함수 페이지](/sql-reference/table-functions/url)에서 자세한 내용을 확인할 수 있습니다.

      <br />
    </TabItem>

    <TabItem value="local_file" label="로컬">
      로컬 파일을 읽으려면 [`file` table engine](/sql-reference/table-functions/file)을(를)
      사용하십시오. 간단하게 하기 위해, 파일을 ClickHouse 바이너리를 다운로드한 디렉터리에 있는
      `user_files` 디렉터리로 복사합니다.

      ```sql
      DESCRIBE TABLE file('comments.tsv')

      Query id: 8ca9b2f9-65a2-4982-954a-890de710a336

      ┌─name──────┬─type────────────────────┐
      │ id        │ Nullable(Int64)         │
      │ type      │ Nullable(String)        │
      │ author    │ Nullable(String)        │
      │ timestamp │ Nullable(DateTime64(9)) │
      │ comment   │ Nullable(String)        │
      │ children  │ Array(Nullable(Int64))  │
      └───────────┴─────────────────────────┘
      ```

      ClickHouse는 대량의 행(batch)을 분석하여 컬럼 이름과 데이터 타입을 추론합니다. ClickHouse가
      파일 이름만으로 파일 포맷을 판단할 수 없는 경우, 두 번째 인수로 파일 포맷을 직접 지정할 수 있습니다:

      ```sql
      SELECT count()
      FROM file(
      'comments.tsv',
      'TabSeparatedWithNames'
      )
      ```

      자세한 내용은 [`file` table function](/sql-reference/table-functions/file)
      문서를 참조하십시오.

      <br />
    </TabItem>

    <TabItem value="PostgreSQL" label="PostgreSQL">
      [`postgresql` table function](/sql-reference/table-functions/postgresql)을 사용하여 PostgreSQL 테이블의 데이터를 읽습니다.

      ```sql
      SELECT *
      FROM
      postgresql(
      'localhost:5432',
      'my_database',
      'my_table',
      'postgresql_user',
      'password')
      ;
      ```

      자세한 내용은 [`postgresql` table function](/sql-reference/table-functions/postgresql)
      문서를 참조하십시오.

      <br />
    </TabItem>

    <TabItem value="MySQL" label="MySQL">
      MySQL 테이블에서 데이터를 읽으려면
      [`mysql` table function](/sql-reference/table-functions/mysql)을 사용합니다.

      ```sql
      SELECT *
      FROM
      mysql(
      'localhost:3306',
      'my_database',
      'my_table',
      'mysql_user',
      'password')
      ;
      ```

      자세한 내용은 [`mysql` table function](/sql-reference/table-functions/mysql)
      문서를 참조하십시오.

      <br />
    </TabItem>

    <TabItem value="다른 DBMS" label="ODBC/JDBC">
      ClickHouse는 어떤 ODBC 또는 JDBC 데이터 소스에서도 데이터를 읽을 수 있습니다.

      ```sql
      SELECT *
      FROM
      odbc(
      'DSN=mysqlconn',
      'my_database',
      'my_table'
      );
      ```

      [`odbc` table function](/sql-reference/table-functions/odbc)
      및 [`jdbc` table function](/sql-reference/table-functions/jdbc) 문서를
      참고하여 자세한 내용을 확인하십시오.

      <br />
    </TabItem>

    <TabItem value="메시지 큐" label="메시지 큐">
      메시지 큐는 해당 테이블 엔진을 사용하여 ClickHouse로 데이터를 스트리밍할 수 있습니다. 예를 들면 다음과 같습니다.

      * **Kafka**: [`Kafka` table engine](/engines/table-engines/integrations/kafka)을 사용해 Kafka와 연동합니다.
      * **Amazon MSK**: [Amazon Managed Streaming for Apache Kafka (MSK)](/integrations/kafka/cloud/amazon-msk/)와 연동합니다.
      * **RabbitMQ**: [`RabbitMQ` table engine](/engines/table-engines/integrations/rabbitmq)을 사용해 RabbitMQ와 연동합니다.

      <br />
    </TabItem>

    <TabItem value="데이터 레이크" label="데이터 레이크">
      ClickHouse에는 다음과 같은 소스에서 데이터를 읽는 테이블 함수가 있습니다:

      * **Hadoop**: [`hdfs` table function](/sql-reference/table-functions/hdfs)을(를) 사용하여 Apache Hadoop과 연동해 데이터를 읽습니다
      * **Hudi**: [`hudi` table function](/sql-reference/table-functions/hudi)을(를) 사용하여 S3에 있는 기존 Apache Hudi 테이블에서 데이터를 읽습니다
      * **Iceberg**: [`iceberg` table function](/sql-reference/table-functions/iceberg)을(를) 사용하여 S3에 있는 기존 Apache Iceberg 테이블에서 데이터를 읽습니다
      * **DeltaLake**: [`deltaLake` table function](/sql-reference/table-functions/deltalake)을(를) 사용하여 S3에 있는 기존 Delta Lake 테이블에서 데이터를 읽습니다

      <br />
    </TabItem>

    <TabItem value="기타" label="기타">
      기존 프레임워크와 데이터 소스를 ClickHouse에 연결하는 방법은 [다양한 ClickHouse 연동 목록](/integrations)을 참조하십시오.

      <br />
    </TabItem>
  </Tabs>

  ## 탐색하기 \{#explore\}

  * ClickHouse가 내부적으로 어떻게 동작하는지에 대한 기본 개념은 [Core Concepts](/managing-data/core-concepts) 섹션을 참조하십시오.
  * ClickHouse의 주요 개념과 기능을 보다 심층적으로 다루는 [고급 튜토리얼](tutorial.md)을 살펴보십시오.
  * 학습을 계속하려면 [ClickHouse Academy](https://learn.clickhouse.com/visitor_class_catalog)에서 제공하는 무료 온디맨드 교육 과정을 수강하십시오.
  * [예제 데이터세트](/getting-started/example-datasets/) 목록과 각 데이터세트를 삽입하는 방법에 대한 안내가 있습니다.
  * 데이터가 외부 소스로부터 수집되는 경우, 메시지 큐, 데이터베이스, 파이프라인 등과 연결하는 방법은 [통합 가이드 모음](/integrations/)을 참고하십시오.
  * UI/BI 시각화 도구를 사용하는 경우 [UI를 ClickHouse에 연결하는 사용자 가이드](/integrations/data-visualization/)를 참조하십시오.
  * [기본 키](/guides/best-practices/sparse-primary-indexes.md)에 대한 사용자 가이드는 기본 키와 이를 정의하는 방법에 대해 알아야 할 모든 내용을 담고 있습니다.
</VerticalStepper>