---
date: 2025-01-29
title: Восстановление после повреждения снимка Keeper
tags: ['Устранение неполадок']
keywords: ['Keeper', 'поврежденный снимок']
description: 'В этой статье описано, как восстановиться после повреждения снимка Keeper: как проявляется проблема, что такое снимок, где его найти и какие существуют стратегии восстановления.'
---

{frontMatter.description}

{/* truncate */}

<br />

<br />

Повреждённые или некорректные snapshots ClickHouse Keeper могут приводить к существенной нестабильности работы системы, например к несогласованности метаданных, переводу таблиц в режим только для чтения, исчерпанию ресурсов или сбоям при создании резервных копий. В этой статье рассматривается:

* [Что такое snapshots и где они хранятся](#overview)
* [Как проявляется проблема](#symptoms)
* [Возможные стратегии восстановления](#recovery-strategies) и что означает каждая из них


## Обзор снимков Keeper \{#overview\}

### Что такое snapshot? \{#what-is-snapshot\}

Snapshot — это сериализованное состояние внутренних данных Keeper (например, метаданных о кластерах, путях координации таблиц и конфигураций) в определённый момент времени. Snapshots крайне важны для повторной синхронизации узлов Keeper в кластере, восстановления метаданных при сбоях, а также для процессов запуска или перезапуска, которые зависят от заведомо корректного состояния Keeper.

### Где хранятся snapshots? \{#where-to-find-snapshots\}

Снимки состояния (snapshots) хранятся в виде файлов в локальной файловой системе узлов Keeper. По умолчанию они размещаются в `/var/lib/clickhouse/coordination/snapshots/` или по настраиваемому пути, указанному в параметре `snapshot_storage_path` в вашем файле `keeper_server.xml`. Снимки получают последовательные имена (например, snapshot.23), при этом более новые имеют более высокие номера.

Для многовузловых кластеров у каждого узла Keeper есть собственный каталог для снимков.

:::note
Согласованность снимков состояния между узлами критически важна для восстановления.
:::

## Ключевые симптомы и проявления повреждённых снапшотов Keeper \{#symptoms\}

В таблице ниже перечислены некоторые распространённые симптомы и проявления повреждённых снапшотов Keeper:

| **Category** | **Issue Type** | **What to look for** |
|---|---|---|
| **Operational Issues** | Read-Only Mode | Таблицы неожиданно переходят в режим только для чтения |
| | Query Failures | Постоянные ошибки выполнения запросов с ошибками `Coordination::Exception` |
| **Metadata Corruption** | Outdated Metadata | Удалённые таблицы продолжают отображаться; сбои операций из‑за устаревших метаданных |
| **Resource Overload** | System Resource Exhaustion | Узлы Keeper потребляют чрезмерное количество CPU, памяти или дискового пространства, что может приводить к простоям |
| | Disk Full | Диск заполняется во время создания снапшота |
| **Backup & Restore** | Backup Failures | Резервные копии не создаются из‑за отсутствующих или неконсистентных метаданных Keeper |
| **Snapshot Creation/Transfer** | Keeper Crash | Сбой Keeper в середине создания снапшота (ищите ошибки "SEGFAULT") |
| | Snapshot Transfer Corruption | Повреждение во время передачи снапшота между репликами |
| | Race Condition | Состояние гонки во время уплотнения логов — фоновый поток коммита обращается к удалённым логам |
| | Network Synchronization | Проблемы сети, не позволяющие синхронизировать снапшот с лидера на ведомые узлы |

**Индикаторы в логах:**

Перед диагностикой повреждения снапшота проверьте **логи Keeper** на наличие типичных шаблонов ошибок:

| **Log Type** | **What to Look For** |
|---|---|
| **Snapshot corruption errors** | • `Aborting because of failure to load from latest snapshot with index`<br/>• `Failure to load from latest snapshot with index {}: {}. Manual intervention is necessary for recovery`<br/>• `Failed to preprocess stored log at index {}, aborting to avoid inconsistent state`<br/>• Сбои сериализации/загрузки снапшота во время запуска |
| **Other Keeper issues** | • `Coordination::Exception`<br/>• `Zookeeper::Session Timeout`<br/>• Проблемы синхронизации или выборов лидера<br/>• Состояния гонки при уплотнении логов |

## Восстановление после повреждения снимков Keeper \{#recovery-strategies\}

Прежде чем вносить какие-либо изменения в файлы, всегда выполняйте:

1. Остановите все узлы Keeper, чтобы предотвратить дальнейшее повреждение
2. Создайте резервную копию, скопировав весь каталог координации в безопасное место
3. Проверьте кворум кластера, чтобы убедиться, что по крайней мере на одном узле хранятся корректные данные

---

### 1. Восстановление из существующей резервной копии \{#1-restore-from-an-existing-backup\}

Следуйте этому процессу, если:

- Повреждение метаданных Keeper или снапшота делает текущие данные невосстановимыми.
- Существует резервная копия с заведомо корректным состоянием Keeper.

Выполните следующие шаги, чтобы восстановить существующую резервную копию:

1. Найдите и проверьте самую новую резервную копию на согласованность метаданных.
2. Остановите сервисы ClickHouse и Keeper.
3. Замените повреждённые снапшоты и логи файлами из каталога резервной копии.
4. Перезапустите кластер Keeper и проверьте синхронизацию метаданных.

:::tip[Регулярно создавайте резервные копии]
Если резервные копии устарели, вы можете потерять недавние изменения в метаданных. Поэтому мы рекомендуем выполнять резервное копирование регулярно.
:::

---

### 2. Откат к более раннему снапшоту \{#2-rollback-to-an-older-snapshot\}

Следуйте этому процессу, когда:

- Последние снапшоты повреждены, но более ранние остаются пригодными к использованию.
- Инкрементальные журналы целы и позволяют выполнить согласованное восстановление.

Выполните шаги ниже, чтобы откатиться к более раннему снапшоту:

1. Найдите и выберите корректный более ранний снапшот (например, snapshot.19) в директории Keeper.
2. Удалите более новые снапшоты и журналы.
3. Перезапустите Keeper, чтобы он воспроизвел журналы и восстановил состояние метаданных.

:::warning[Риск десинхронизации метаданных]
Существует риск десинхронизации метаданных, если снапшоты и журналы отсутствуют или неполны.
:::

---

### 3. Восстановление метаданных с помощью SYSTEM RESTORE REPLICA \{#3-restore-metadata-using-system-restore-replica\}

Используйте эту процедуру, если:

* Метаданные Keeper потеряны или повреждены, но данные таблиц всё ещё существуют на диске
* Таблицы перешли в режим только для чтения из‑за отсутствующих метаданных ZooKeeper/Keeper
* Необходимо воссоздать метаданные в Keeper на основе локально доступных частей

Выполните следующие шаги для восстановления метаданных:

1. Убедитесь, что данные таблиц существуют локально в каталоге данных clickhouse-server, задаваемом параметром `<path>` в конфигурации (`/var/lib/clickhouse/data/` по умолчанию).

2. Для каждой затронутой таблицы выполните:

```sql
SYSTEM RESTART REPLICA [db.]table_name;
SYSTEM RESTORE REPLICA [db.]table_name;
```

3. Для восстановления на уровне базы данных (если используется движок базы данных Replicated):

```sql
SYSTEM RESTORE DATABASE REPLICA db_name;
```

4. Дождитесь завершения синхронизации:

```sql
SYSTEM SYNC REPLICA [db.]table_name;
```

5. Проверьте успешность восстановления, убедившись, что в `system.replicas` `is_readonly = 0`, и отслеживая `system.detached_parts`

:::info[Как это работает]
`SYSTEM RESTORE REPLICA` отсоединяет все существующие части, заново создает метаданные в Keeper (как будто это новая пустая таблица), затем повторно присоединяет все части. Это позволяет избежать повторной загрузки данных по сети.
:::

:::warning[Предварительные требования]
Это работает только в том случае, если локальные части данных не повреждены. Если данные также повреждены, вместо этого используйте стратегию №5 (перестроить кластер).
:::

***


### 4. Удаление и пересоздание метаданных реплики в Keeper \{#4-drop-and-recreate-replica-metadata-in-keeper\}

Следуйте этому процессу, если:

* Ошибка возникает на одной реплике кластера и у нее повреждены или несогласованы метаданные в Keeper
* Вы сталкиваетесь с ошибками вида «Part XXXXX intersects previous part YYYYY»
* Необходимо полностью сбросить метаданные Keeper для реплики, сохранив при этом локальные данные

Выполните следующие шаги, чтобы удалить и пересоздать метаданные:

1. На затронутой реплике отсоедините таблицу:

```sql
DETACH TABLE [db.]table_name;
```

2. Удалите метаданные реплики из Keeper (выполните на любой из реплик):

```sql
SYSTEM DROP REPLICA 'replica_name' FROM ZKPATH '/clickhouse/tables/{shard}/table_name';
```

Чтобы найти правильный путь в ZooKeeper:

```sql
SELECT zookeeper_path, replica_name FROM system.replicas WHERE table = 'table_name';
```

3. Снова подключите таблицу (она будет в режиме только для чтения):

```sql
ATTACH TABLE [db.]table_name;
```

4. Восстановите метаданные реплики:

```sql
SYSTEM RESTORE REPLICA [db.]table_name;
```

5. Синхронизация с другими репликами:

```sql
SYSTEM SYNC REPLICA [db.]table_name;
```

6. Проверьте `system.detached_parts` на всех репликах после восстановления

:::warning[&quot;Выполнить на всех затронутых репликах&quot;]
Если повреждение затрагивает несколько реплик, последовательно повторите эти шаги для каждой из них.
:::

:::tip[&quot;Для всей базы данных&quot;]
Если используется база данных Replicated, вместо этого вы можете использовать `SYSTEM DROP REPLICA ... FROM DATABASE db_name`.
:::

**Альтернатива: использование флага force&#95;restore&#95;data**

Для автоматического восстановления всех реплицируемых таблиц при запуске сервера:

1. Остановите сервер ClickHouse
2. Создайте флаг восстановления:

```bash
sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data
```

3. Запустите сервер ClickHouse
4. Сервер автоматически удалит флаг и восстановит все реплицированные таблицы
5. Отслеживайте логи, чтобы контролировать ход восстановления

Этот подход полезен, когда необходимо одновременно восстановить несколько таблиц.

***


### 5. Повторное создание кластера Keeper \{#5-rebuild-keeper-cluster\}

Следуйте этому процессу, когда:

- Нет доступных корректных снапшотов, журналов или резервных копий для восстановления.
- Необходимо воссоздать весь кластер Keeper и его метаданные.

Выполните следующие шаги для повторного создания кластера Keeper:

1. Полностью остановите кластеры ClickHouse и Keeper.
2. Сбросьте каждый узел Keeper, очистив каталоги снапшотов и журналов.
3. Инициализируйте один узел Keeper как лидера и постепенно добавляйте остальные узлы.
4. Повторно импортируйте метаданные, если они доступны из внешних источников.

:::warning[Длительный процесс]
Этот процесс занимает много времени и несёт риск длительного простоя. Потребуется полная реконструкция данных.
:::