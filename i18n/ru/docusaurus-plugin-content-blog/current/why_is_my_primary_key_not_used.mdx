---
title: Почему мой первичный ключ не используется? Как это проверить?
description: "Рассматривается распространённая причина, по которой первичный ключ не используется при сортировке, и способы это проверить"
date: 2024-12-12
tags: ['Производительность и оптимизации']
keywords: ['Первичный ключ']
---

{frontMatter.description}

{/* обрезать */}

## Проверка первичного ключа \{#checking-your-primary-key\}

Пользователи могут столкнуться с ситуациями, когда запрос выполняется медленнее ожидаемого, полагая, что сортировка или фильтрация выполняется по первичному ключу. В этой статье мы показываем, как убедиться, что ключ действительно используется, и разбираем типичные причины, по которым это не так.

## Создание таблицы \{#create-table\}

Рассмотрим следующую простую таблицу:

```sql
CREATE TABLE logs
(
    `code` LowCardinality(String),
    `timestamp` DateTime64(3)
)
ENGINE = MergeTree
ORDER BY (code, toUnixTimestamp(timestamp))
```

Обратите внимание, что в нашем ключе сортировки вторым полем используется `toUnixTimestamp(timestamp)`.

## Заполнение таблицы данными \{#populate-data\}

Заполните эту таблицу 100 млн строками:

```sql
INSERT INTO logs SELECT
 ['200', '404', '502', '403'][toInt32(randBinomial(4, 0.1)) + 1] AS code,
    now() + toIntervalMinute(number) AS timestamp
FROM numbers(100000000)

0 строк в наборе. Затрачено: 15.845 сек. Обработано 100.00 миллионов строк, 800.00 МБ (6.31 миллионов строк/с., 50.49 МБ/с.)

SELECT count()
FROM logs

┌───count()─┐
│ 100000000 │ -- 100.00 million
└───────────┘

1 строка в наборе. Затрачено: 0.002 сек.
```

## Базовая фильтрация \{#basic-filtering\}

Если отфильтровать по коду, в выводе отобразится количество строк, прошедших сканирование — `49.15 thousand`. Обратите внимание, что это подмножество общего объёма в 100 млн строк.

```sql
SELECT count() AS c
FROM logs
WHERE code = '200'

┌────────c─┐
│ 65607542 │ -- 65,61 миллиона
└──────────┘

1 строка в результате. Затрачено: 0,021 сек. Обработано 49,15 тыс. строк, 49,17 КБ (2,34 млн строк/с., 2,34 МБ/с.)
Пик использования памяти: 92,70 КiБ.
```

Кроме того, мы можем убедиться в использовании индекса с помощью оператора `EXPLAIN indexes=1`:

```sql
EXPLAIN indexes = 1
SELECT count() AS c
FROM logs
WHERE code = '200'

┌─explain────────────────────────────────────────────────────────────┐
│ Expression ((Project names + Projection))                          │
│   AggregatingProjection                                            │
│     Expression (Before GROUP BY)                                   │
│       Filter ((WHERE + Change column names to column identifiers)) │
│         ReadFromMergeTree (default.logs)                           │
│         Indexes:                                                   │
│           PrimaryKey                                               │
│             Keys:                                                  │
│               code                                                 │
│             Condition: (code in ['200', '200'])                    │
│             Parts: 3/3 │
│             Granules: 8012/12209 │
│     ReadFromPreparedSource (_minmax_count_projection)              │
└────────────────────────────────────────────────────────────────────┘
```

Обратите внимание, что количество просканированных гранул `8012` составляет лишь часть от общего числа `12209`. Выделенный ниже фрагмент подтверждает использование первичного ключа.

```bash
PrimaryKey
  Keys: 
   code 
```

Гранулы являются единицей обработки данных в ClickHouse, при этом каждая обычно содержит 8192 строки. Для получения дополнительной информации о гранулах и том, как выполняется их фильтрация, рекомендуем прочитать [это руководство](/guides/best-practices/sparse-primary-indexes#mark-files-are-used-for-locating-granules).

:::note
Фильтрация по ключам, расположенным позже в ключе сортировки, будет менее эффективной, чем по тем, которые находятся раньше в кортеже. Причины этого описаны [здесь](/guides/best-practices/sparse-primary-indexes#secondary-key-columns-can-not-be-inefficient)
:::

## Фильтрация по нескольким ключам \{#multi-key-filtering\}

Предположим, мы фильтруем по `code` и `timestamp`:

```sql
SELECT count()
FROM logs
WHERE (code = '200') AND (timestamp >= '2025-01-01 00:00:00') AND (timestamp <= '2026-01-01 00:00:00')

┌─count()─┐
│  689742 │
└─────────┘

1 row in set. Elapsed: 0.008 sec. Processed 712.70 thousand rows, 6.41 MB (88.92 million rows/s., 799.27 MB/s.)


EXPLAIN indexes = 1
SELECT count()
FROM logs
WHERE (code = '200') AND (timestamp >= '2025-01-01 00:00:00') AND (timestamp <= '2026-01-01 00:00:00')

┌─explain───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Expression ((Project names + Projection))                                                                                                                         │
│   Aggregating                                                                                                                                                     │
│     Expression (Before GROUP BY)                                                                                                                                  │
│       Expression                                                                                                                                                  │
│         ReadFromMergeTree (default.logs)                                                                                                                          │
│         Indexes:                                                                                                                                                  │
│           PrimaryKey                                                                                                                                              │
│             Keys:                                                                                                                                                 │
│               code                                                                                                                                                │
│               toUnixTimestamp(timestamp)                                                                                                                          │
│             Condition: and((toUnixTimestamp(timestamp) in (-Inf, 1767225600]), and((toUnixTimestamp(timestamp) in [1735689600, +Inf)), (code in ['200', '200']))) │
│             Parts: 3/3 │
│             Granules: 87/12209 │
└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘

13 rows in set. Elapsed: 0.002 sec.

```

В этом случае оба ключа сортировки используются для фильтрации строк, в результате нужно прочитать всего `87` гранул.

## Использование ключей при сортировке \{#using-keys-in-sorting\}

ClickHouse также может использовать сортировочные ключи для эффективной сортировки. В частности,

Если параметр [optimize&#95;read&#95;in&#95;order](/sql-reference/statements/select/order-by#optimization-of-data-reading) включён (по умолчанию он включён), сервер ClickHouse использует индекс таблицы и читает данные в порядке ключа ORDER BY. Это позволяет избежать чтения всех данных при заданном LIMIT. Таким образом, запросы по большим объёмам данных с малым лимитом обрабатываются быстрее. Подробности см. [здесь](/sql-reference/statements/select/order-by#optimization-of-data-reading) и [здесь](/docs/knowledgebase/async_vs_optimize_read_in_order#what-about-optimize_read_in_order).

Однако для этого требуется согласование используемых ключей.

Например, рассмотрим следующий запрос:

```sql
SELECT *
FROM logs
WHERE (code = '200') AND (timestamp >= '2025-01-01 00:00:00') AND (timestamp <= '2026-01-01 00:00:00')
ORDER BY timestamp ASC
LIMIT 10

┌─code─┬───────────────timestamp─┐
│ 200 │ 2025-01-01 00:00:01.000 │
│ 200 │ 2025-01-01 00:00:45.000 │
│ 200 │ 2025-01-01 00:01:01.000 │
│ 200 │ 2025-01-01 00:01:45.000 │
│ 200 │ 2025-01-01 00:02:01.000 │
│ 200 │ 2025-01-01 00:03:01.000 │
│ 200 │ 2025-01-01 00:03:45.000 │
│ 200 │ 2025-01-01 00:04:01.000 │
│ 200 │ 2025-01-01 00:05:45.000 │
│ 200 │ 2025-01-01 00:06:01.000 │
└──────┴─────────────────────────

10 rows in set. Elapsed: 0.009 sec. Processed 712.70 thousand rows, 6.41 MB (80.13 million rows/s., 720.27 MB/s.)
Peak memory usage: 125.50 KiB.
```

Мы можем убедиться, что оптимизация здесь не была задействована, с помощью `EXPLAIN pipeline`:

```sql
EXPLAIN PIPELINE
SELECT *
FROM logs
WHERE (code = '200') AND (timestamp >= '2025-01-01 00:00:00') AND (timestamp <= '2026-01-01 00:00:00')
ORDER BY timestamp ASC
LIMIT 10

┌─explain───────────────────────────────────────────────────────────────────────┐
│ (Expression)                                                                  │
│ ExpressionTransform                                                           │
│   (Limit)                                                                     │
│   Limit │
│     (Sorting)                                                                 │
│     MergingSortedTransform 12 → 1 │
│       MergeSortingTransform × 12 │
│         LimitsCheckingTransform × 12 │
│           PartialSortingTransform × 12 │
│             (Expression)                                                      │
│             ExpressionTransform × 12 │
│               (Expression)                                                    │
│               ExpressionTransform × 12 │
│                 (ReadFromMergeTree)                                           │
│                 MergeTreeSelect(pool: ReadPool, algorithm: Thread) × 12 0 → 1 │
└───────────────────────────────────────────────────────────────────────────────┘

Получено 15 строк. Затрачено: 0,004 сек.
```

Строка `MergeTreeSelect(pool: ReadPool, algorithm: Thread)` здесь не означает использование оптимизации, а указывает на стандартное чтение. Это происходит из-за того, что в сортировочном ключе таблицы используется `toUnixTimestamp(Timestamp)`, а **НЕ** `timestamp`. Устранение этого несоответствия позволяет решить проблему:

```sql
EXPLAIN PIPELINE
SELECT *
FROM logs
WHERE (code = '200') AND (timestamp >= '2025-01-01 00:00:00') AND (timestamp <= '2026-01-01 00:00:00')
ORDER BY toUnixTimestamp(timestamp) ASC
LIMIT 10

┌─explain──────────────────────────────────────────────────────────────────────────┐
│ (Expression)                                                                     │
│ ExpressionTransform                                                              │
│   (Limit)                                                                        │
│   Limit │
│     (Sorting)                                                                    │
│     MergingSortedTransform 3 → 1 │
│       BufferChunks × 3 │
│         (Expression)                                                             │
│         ExpressionTransform × 3 │
│           (Expression)                                                           │
│           ExpressionTransform × 3 │
│             (ReadFromMergeTree)                                                  │
│             MergeTreeSelect(pool: ReadPoolInOrder, algorithm: InOrder) × 3 0 → 1 │
└──────────────────────────────────────────────────────────────────────────────────┘

Получено 13 строк. Прошло: 0.003 сек.
```
