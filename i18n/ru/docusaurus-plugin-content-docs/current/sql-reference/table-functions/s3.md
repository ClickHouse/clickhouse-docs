---
description: 'Предоставляет табличный интерфейс для чтения и записи файлов в Amazon S3
  и Google Cloud Storage. Эта табличная функция аналогична функции hdfs, но предоставляет
  возможности, специфичные для S3.'
keywords: ['s3', 'gcs', 'bucket']
sidebar_label: 's3'
sidebar_position: 180
slug: /sql-reference/table-functions/s3
title: 'Табличная функция s3'
doc_type: 'reference'
---

import ExperimentalBadge from '@theme/badges/ExperimentalBadge';
import CloudNotSupportedBadge from '@theme/badges/CloudNotSupportedBadge';


# Табличная функция s3

Предоставляет табличный интерфейс для выборки и вставки файлов в [Amazon S3](https://aws.amazon.com/s3/) и [Google Cloud Storage](https://cloud.google.com/storage/). Эта табличная функция аналогична [функции hdfs](../../sql-reference/table-functions/hdfs.md), но предоставляет возможности, специфичные для S3.

Если у вас несколько реплик в кластере, вы можете использовать вместо неё функцию [s3Cluster](../../sql-reference/table-functions/s3Cluster.md) для параллелизации вставок.

При использовании табличной функции `s3` с [`INSERT INTO...SELECT`](../../sql-reference/statements/insert-into#inserting-the-results-of-select) данные читаются и вставляются в потоковом режиме. В памяти находится только несколько блоков данных, пока блоки непрерывно считываются из S3 и записываются в целевую таблицу.



## Синтаксис {#syntax}

```sql
s3(url [, NOSIGN | access_key_id, secret_access_key, [session_token]] [,format] [,structure] [,compression_method],[,headers], [,partition_strategy], [,partition_columns_in_data_file])
s3(named_collection[, option=value [,..]])
```

:::tip GCS
Табличная функция S3 интегрируется с Google Cloud Storage через GCS XML API и ключи HMAC. Подробнее об endpoint и HMAC см. в [документации Google по совместимости](https://cloud.google.com/storage/docs/interoperability).

Для GCS используйте ваш HMAC-ключ и HMAC-секрет вместо `access_key_id` и `secret_access_key`.
:::

**Параметры**

Табличная функция `s3` поддерживает следующие простые параметры:

| Параметр                                | Описание                                                                                                                                                                                                                                                                                                                                                                         |
| --------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `url`                                   | URL бакета с путём к файлу. Поддерживает следующие подстановочные символы в режиме только для чтения: `*`, `**`, `?`, `{abc,def}` и `{N..M}`, где `N`, `M` — числа, `'abc'`, `'def'` — строки. Подробнее см. [здесь](../../engines/table-engines/integrations/s3.md#wildcards-in-path).                                                                                        |
| `NOSIGN`                                | Если это ключевое слово указано вместо учётных данных, все запросы не будут подписываться.                                                                                                                                                                                                                                                                                      |
| `access_key_id` и `secret_access_key`   | Ключи, определяющие учётные данные для использования с указанной конечной точкой. Необязательно.                                                                                                                                                                                                                                                                                 |
| `session_token`                         | Токен сеанса для использования с указанными ключами. Необязательно при передаче ключей.                                                                                                                                                                                                                                                                                          |
| `format`                                | [Формат](/sql-reference/formats) файла.                                                                                                                                                                                                                                                                                                                                         |
| `structure`                             | Структура таблицы. Формат `'column1_name column1_type, column2_name column2_type, ...'`.                                                                                                                                                                                                                                                                                        |
| `compression_method`                    | Необязательный параметр. Поддерживаемые значения: `none`, `gzip` или `gz`, `brotli` или `br`, `xz` или `LZMA`, `zstd` или `zst`. По умолчанию метод сжатия определяется автоматически по расширению файла.                                                                                                                                                                      |
| `headers`                               | Необязательный параметр. Позволяет передавать заголовки в S3-запросе. Передаётся в формате `headers(key=value)`, например `headers('x-amz-request-payer' = 'requester')`.                                                                                                                                                                                                       |
| `partition_strategy`                    | Необязательный параметр. Поддерживаемые значения: `WILDCARD` или `HIVE`. `WILDCARD` требует наличия `{_partition_id}` в пути, который заменяется ключом партиции. `HIVE` не допускает подстановочные символы, предполагает, что путь является корнем таблицы, и генерирует партиционированные каталоги в стиле Hive с идентификаторами Snowflake в качестве имён файлов и форматом файла в качестве расширения. По умолчанию `WILDCARD`. |
| `partition_columns_in_data_file`        | Необязательный параметр. Используется только со стратегией партиционирования `HIVE`. Указывает ClickHouse, следует ли ожидать, что столбцы партиций будут записаны в файл данных. По умолчанию `false`.                                                                                                                                                                          |
| `storage_class_name`                    | Необязательный параметр. Поддерживаемые значения: `STANDARD` или `INTELLIGENT_TIERING`. Позволяет указать [AWS S3 Intelligent Tiering](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/). По умолчанию `STANDARD`.                                                                                                                                                |

:::note GCS
URL GCS имеет следующий формат, поскольку конечная точка для Google XML API отличается от JSON API:

```text
  https://storage.googleapis.com/<bucket>/<folder>/<filename(s)>
```

а не ~~https://storage.cloud.google.com~~.
:::

Аргументы также можно передавать с помощью [именованных коллекций](operations/named-collections.md). В этом случае `url`, `access_key_id`, `secret_access_key`, `format`, `structure`, `compression_method` работают так же, и поддерживаются некоторые дополнительные параметры:

| Аргумент                      | Описание                                                                                                                                                                                                                          |
| ----------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `filename`                    | Добавляется к url, если указан.                                                                                                                                                                                                   |
| `use_environment_credentials` | Включён по умолчанию, позволяет передавать дополнительные параметры через переменные окружения `AWS_CONTAINER_CREDENTIALS_RELATIVE_URI`, `AWS_CONTAINER_CREDENTIALS_FULL_URI`, `AWS_CONTAINER_AUTHORIZATION_TOKEN`, `AWS_EC2_METADATA_DISABLED`. |
| `no_sign_request`             | Отключён по умолчанию.                                                                                                                                                                                                            |
| `expiration_window_seconds`   | Значение по умолчанию — 120.                                                                                                                                                                                                      |


## Возвращаемое значение {#returned_value}

Таблица с указанной структурой для чтения или записи данных в указанный файл.


## Примеры {#examples}

Выборка первых 5 строк из таблицы из файла S3 `https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv`:

```sql
SELECT *
FROM s3(
   'https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv',
   'CSVWithNames'
)
LIMIT 5;
```

```response
┌───────Date─┬────Open─┬────High─┬─────Low─┬───Close─┬───Volume─┬─OpenInt─┐
│ 1984-09-07 │ 0.42388 │ 0.42902 │ 0.41874 │ 0.42388 │ 23220030 │       0 │
│ 1984-09-10 │ 0.42388 │ 0.42516 │ 0.41366 │ 0.42134 │ 18022532 │       0 │
│ 1984-09-11 │ 0.42516 │ 0.43668 │ 0.42516 │ 0.42902 │ 42498199 │       0 │
│ 1984-09-12 │ 0.42902 │ 0.43157 │ 0.41618 │ 0.41618 │ 37125801 │       0 │
│ 1984-09-13 │ 0.43927 │ 0.44052 │ 0.43927 │ 0.43927 │ 57822062 │       0 │
└────────────┴─────────┴─────────┴─────────┴─────────┴──────────┴─────────┘
```

:::note
ClickHouse использует расширения файлов для определения формата данных. Например, предыдущую команду можно было выполнить без указания `CSVWithNames`:

```sql
SELECT *
FROM s3(
   'https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv'
)
LIMIT 5;
```

ClickHouse также может автоматически определить метод сжатия файла. Например, если файл был сжат с расширением `.csv.gz`, ClickHouse автоматически распакует его.
:::

:::note
Файлы Parquet с именами вида `*.parquet.snappy` или `*.parquet.zstd` могут привести к неправильной интерпретации в ClickHouse и вызвать ошибки `TOO_LARGE_COMPRESSED_BLOCK` или `ZSTD_DECODER_FAILED`.
Это происходит потому, что ClickHouse попытается прочитать весь файл как данные, сжатые Snappy или ZSTD, тогда как в действительности Parquet применяет сжатие на уровне групп строк и столбцов.

Метаданные Parquet уже содержат информацию о сжатии для каждого столбца, поэтому расширение файла является избыточным.
В таких случаях можно просто использовать `compression_method = 'none'`:

```sql
SELECT *
FROM s3(
  'https://<my-bucket>.s3.<my-region>.amazonaws.com/path/to/my-data.parquet.snappy',
  compression_format = 'none'
);
```

:::


## Использование {#usage}

Предположим, что у нас есть несколько файлов со следующими URI на S3:

- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/some_prefix/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/some_prefix/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/some_prefix/some_file_3.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/some_prefix/some_file_4.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/another_prefix/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/another_prefix/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/another_prefix/some_file_3.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/another_prefix/some_file_4.csv'

Подсчитаем количество строк в файлах, заканчивающихся числами от 1 до 3:

```sql
SELECT count(*)
FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/my-test-bucket-768/{some,another}_prefix/some_file_{1..3}.csv', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32')
```

```text
┌─count()─┐
│      18 │
└─────────┘
```

Подсчитаем общее количество строк во всех файлах в этих двух каталогах:

```sql
SELECT count(*)
FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/my-test-bucket-768/{some,another}_prefix/*', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32')
```

```text
┌─count()─┐
│      24 │
└─────────┘
```

:::tip
Если список файлов содержит диапазоны чисел с ведущими нулями, используйте конструкцию с фигурными скобками для каждой цифры отдельно или используйте `?`.
:::

Подсчитаем общее количество строк в файлах с именами `file-000.csv`, `file-001.csv`, ... , `file-999.csv`:

```sql
SELECT count(*)
FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/my-test-bucket-768/big_prefix/file-{000..999}.csv', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32');
```

```text
┌─count()─┐
│      12 │
└─────────┘
```

Вставим данные в файл `test-data.csv.gz`:

```sql
INSERT INTO FUNCTION s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip')
VALUES ('test-data', 1), ('test-data-2', 2);
```

Вставим данные в файл `test-data.csv.gz` из существующей таблицы:

```sql
INSERT INTO FUNCTION s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip')
SELECT name, value FROM existing_table;
```

Шаблон \*\* может использоваться для рекурсивного обхода каталогов. Рассмотрим пример ниже — он получит все файлы из каталога `my-test-bucket-768` рекурсивно:

```sql
SELECT * FROM s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/**', 'CSV', 'name String, value UInt32', 'gzip');
```

Следующий запрос получает данные из всех файлов `test-data.csv.gz` из любой папки внутри каталога `my-test-bucket` рекурсивно:

```sql
SELECT * FROM s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/**/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip');
```


Примечание. В файле конфигурации сервера можно задать пользовательские сопоставления URL. Пример:

```sql
SELECT * FROM s3('s3://clickhouse-public-datasets/my-test-bucket-768/**/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip');
```

URL `'s3://clickhouse-public-datasets/my-test-bucket-768/**/test-data.csv.gz'` будет заменён на `'http://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/**/test-data.csv.gz'`

Пользовательский маппер можно добавить в `config.xml`:

```xml
<url_scheme_mappers>
   <s3>
      <to>https://{bucket}.s3.amazonaws.com</to>
   </s3>
   <gs>
      <to>https://{bucket}.storage.googleapis.com</to>
   </gs>
   <oss>
      <to>https://{bucket}.oss.aliyuncs.com</to>
   </oss>
</url_scheme_mappers>
```

Для production-сценариев рекомендуется использовать [именованные коллекции](operations/named-collections.md). Например:

```sql

CREATE NAMED COLLECTION creds AS
        access_key_id = '***',
        secret_access_key = '***';
SELECT count(*)
FROM s3(creds, url='https://s3-object-url.csv')
```


## Партиционированная запись {#partitioned-write}

### Стратегия партиционирования {#partition-strategy}

Поддерживается только для запросов INSERT.

`WILDCARD` (по умолчанию): Заменяет подстановочный символ `{_partition_id}` в пути к файлу на фактический ключ партиции.

`HIVE` реализует партиционирование в стиле Hive для чтения и записи. Генерирует файлы в следующем формате: `<prefix>/<key1=val1/key2=val2...>/<snowflakeid>.<toLower(file_format)>`.

**Пример стратегии партиционирования `HIVE`**

```sql
INSERT INTO FUNCTION s3(s3_conn, filename='t_03363_function', format=Parquet, partition_strategy='hive') PARTITION BY (year, country) SELECT 2020 as year, 'Russia' as country, 1 as id;
```

```result
SELECT _path, * FROM s3(s3_conn, filename='t_03363_function/**.parquet');

   ┌─_path──────────────────────────────────────────────────────────────────────┬─id─┬─country─┬─year─┐
1. │ test/t_03363_function/year=2020/country=Russia/7351295896279887872.parquet │  1 │ Russia  │ 2020 │
   └────────────────────────────────────────────────────────────────────────────┴────┴─────────┴──────┘
```

**Примеры стратегии партиционирования `WILDCARD`**

1. Использование идентификатора партиции в ключе создаёт отдельные файлы:

```sql
INSERT INTO TABLE FUNCTION
    s3('http://bucket.amazonaws.com/my_bucket/file_{_partition_id}.csv', 'CSV', 'a String, b UInt32, c UInt32')
    PARTITION BY a VALUES ('x', 2, 3), ('x', 4, 5), ('y', 11, 12), ('y', 13, 14), ('z', 21, 22), ('z', 23, 24);
```

В результате данные записываются в три файла: `file_x.csv`, `file_y.csv` и `file_z.csv`.

2. Использование идентификатора партиции в имени бакета создаёт файлы в разных бакетах:

```sql
INSERT INTO TABLE FUNCTION
    s3('http://bucket.amazonaws.com/my_bucket_{_partition_id}/file.csv', 'CSV', 'a UInt32, b UInt32, c UInt32')
    PARTITION BY a VALUES (1, 2, 3), (1, 4, 5), (10, 11, 12), (10, 13, 14), (20, 21, 22), (20, 23, 24);
```

В результате данные записываются в три файла в разных бакетах: `my_bucket_1/file.csv`, `my_bucket_10/file.csv` и `my_bucket_20/file.csv`.


## Доступ к публичным бакетам {#accessing-public-buckets}

ClickHouse пытается получить учетные данные из различных типов источников.
Иногда это может вызывать проблемы при доступе к публичным бакетам, что приводит к возврату клиентом кода ошибки `403`.
Эту проблему можно избежать, используя ключевое слово `NOSIGN`, которое заставляет клиент игнорировать все учетные данные и не подписывать запросы.

```sql
SELECT *
FROM s3(
   'https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv',
   NOSIGN,
   'CSVWithNames'
)
LIMIT 5;
```


## Использование учетных данных S3 (ClickHouse Cloud) {#using-s3-credentials-clickhouse-cloud}

Для закрытых бакетов пользователи могут передать `aws_access_key_id` и `aws_secret_access_key` в функцию. Например:

```sql
SELECT count() FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/mta/*.tsv', '<KEY>', '<SECRET>','TSVWithNames')
```

Это подходит для разового доступа или в случаях, когда учетные данные можно легко ротировать. Однако это не рекомендуется в качестве долгосрочного решения для регулярного доступа или при работе с конфиденциальными учетными данными. В таких случаях рекомендуется использовать ролевой доступ.

Ролевой доступ для S3 в ClickHouse Cloud описан [здесь](/cloud/data-sources/secure-s3#setup).

После настройки `roleARN` можно передать в функцию s3 через параметр `extra_credentials`. Например:

```sql
SELECT count() FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/mta/*.tsv','CSVWithNames',extra_credentials(role_arn = 'arn:aws:iam::111111111111:role/ClickHouseAccessRole-001'))
```

Дополнительные примеры можно найти [здесь](/cloud/data-sources/secure-s3#access-your-s3-bucket-with-the-clickhouseaccess-role)


## Работа с архивами {#working-with-archives}

Предположим, у нас есть несколько архивных файлов со следующими URI на S3:

- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip'

Извлечение данных из этих архивов возможно с помощью оператора `::`. Шаблоны подстановки (globs) можно использовать как в части URL, так и в части после `::` (которая отвечает за имя файла внутри архива).

```sql
SELECT *
FROM s3(
   'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-1{0..2}.csv.zip :: *.csv'
);
```

:::note
ClickHouse поддерживает три формата архивов:
ZIP
TAR
7Z
Архивы ZIP и TAR доступны из любого поддерживаемого хранилища, тогда как архивы 7Z можно читать только из локальной файловой системы, где установлен ClickHouse.
:::


## Вставка данных {#inserting-data}

Обратите внимание, что строки можно вставлять только в новые файлы. Циклы слияния и операции разделения файлов отсутствуют. После записи файла последующие вставки будут завершаться ошибкой. Подробнее см. [здесь](/integrations/s3#inserting-data).


## Виртуальные столбцы {#virtual-columns}

- `_path` — Путь к файлу. Тип: `LowCardinality(String)`. В случае архива отображает путь в формате: `"{path_to_archive}::{path_to_file_inside_archive}"`
- `_file` — Имя файла. Тип: `LowCardinality(String)`. В случае архива отображает имя файла внутри архива.
- `_size` — Размер файла в байтах. Тип: `Nullable(UInt64)`. Если размер файла неизвестен, значение — `NULL`. В случае архива отображает несжатый размер файла внутри архива.
- `_time` — Время последнего изменения файла. Тип: `Nullable(DateTime)`. Если время неизвестно, значение — `NULL`.


## Настройка use_hive_partitioning {#hive-style-partitioning}

Эта настройка указывает ClickHouse на необходимость парсинга файлов с партиционированием в стиле Hive при чтении. На запись не влияет. Для симметричного чтения и записи используйте аргумент `partition_strategy`.

Когда настройка `use_hive_partitioning` установлена в 1, ClickHouse обнаружит партиционирование в стиле Hive в пути (`/name=value/`) и позволит использовать столбцы партиций как виртуальные столбцы в запросе. Эти виртуальные столбцы будут иметь те же имена, что и в партиционированном пути, но с префиксом `_`.

**Пример**

```sql
SELECT * FROM s3('s3://data/path/date=*/country=*/code=*/*.parquet') WHERE date > '2020-01-01' AND country = 'Netherlands' AND code = 42;
```


## Доступ к бакетам с оплатой запросчиком {#accessing-requester-pays-buckets}

Для доступа к бакету с оплатой запросчиком необходимо передавать заголовок `x-amz-request-payer = requester` во всех запросах. Это достигается передачей параметра `headers('x-amz-request-payer' = 'requester')` в функцию s3. Например:

```sql
SELECT
    count() AS num_rows,
    uniqExact(_file) AS num_files
FROM s3('https://coiled-datasets-rp.s3.us-east-1.amazonaws.com/1trc/measurements-100*.parquet', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY', headers('x-amz-request-payer' = 'requester'))

┌───num_rows─┬─num_files─┐
│ 1110000000 │       111 │
└────────────┴───────────┘

1 row in set. Elapsed: 3.089 sec. Processed 1.09 billion rows, 0.00 B (353.55 million rows/s., 0.00 B/s.)
Peak memory usage: 192.27 KiB.
```


## Настройки хранилища {#storage-settings}

- [s3_truncate_on_insert](operations/settings/settings.md#s3_truncate_on_insert) — позволяет усекать файл перед вставкой в него данных. По умолчанию отключена.
- [s3_create_new_file_on_insert](operations/settings/settings.md#s3_create_new_file_on_insert) — позволяет создавать новый файл при каждой вставке, если формат имеет суффикс. По умолчанию отключена.
- [s3_skip_empty_files](operations/settings/settings.md#s3_skip_empty_files) — позволяет пропускать пустые файлы при чтении. По умолчанию включена.


## Вложенные схемы Avro {#nested-avro-schemas}

При чтении файлов Avro, содержащих **вложенные записи**, которые отличаются в разных файлах (например, в некоторых файлах есть дополнительное поле внутри вложенного объекта), ClickHouse может вернуть ошибку вида:

> The number of leaves in record doesn't match the number of elements in tuple...

Это происходит потому, что ClickHouse ожидает, что все структуры вложенных записей будут соответствовать одной схеме.  
Для решения этой проблемы можно:

- Использовать `schema_inference_mode='union'` для объединения различных схем вложенных записей, или
- Вручную согласовать вложенные структуры и включить параметр  
  `use_structure_from_insertion_table_in_table_functions=1`.

:::note[Примечание о производительности]
Использование `schema_inference_mode='union'` может занять больше времени на очень больших наборах данных S3, так как требуется сканирование каждого файла для определения схемы.
:::

**Пример**

```sql
INSERT INTO data_stage
SELECT
    id,
    data
FROM s3('https://bucket-name/*.avro', 'Avro')
SETTINGS schema_inference_mode='union';

```


## Связанные материалы {#related}

- [Движок S3](../../engines/table-engines/integrations/s3.md)
- [Интеграция S3 с ClickHouse](/integrations/s3)
