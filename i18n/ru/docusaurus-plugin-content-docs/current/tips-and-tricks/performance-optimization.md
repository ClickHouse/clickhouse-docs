---
sidebar_position: 1
slug: /community-wisdom/performance-optimization
sidebar_label: 'Оптимизация производительности'
doc_type: 'guide'
keywords: [
  'performance optimization',
  'query performance',
  'database tuning',
  'slow queries',
  'memory optimization',
  'cardinality analysis',
  'indexing strategies',
  'aggregation optimization',
  'sampling techniques',
  'database performance',
  'query analysis',
  'performance troubleshooting'
]
title: 'Уроки — оптимизация производительности'
description: 'Практические примеры стратегий оптимизации производительности'
---



# Оптимизация производительности: стратегии, проверенные сообществом {#performance-optimization}

_Это руководство является частью сборника практических решений, полученных на встречах сообщества. Больше реальных решений и рекомендаций можно найти, [выбрав конкретную проблему](./community-wisdom.md)._
_Возникли проблемы с материализованными представлениями? Ознакомьтесь с руководством по [материализованным представлениям](./materialized-views.md) от сообщества._
_Если вы сталкиваетесь с медленными запросами и хотите больше примеров, у нас также есть руководство по [оптимизации запросов](/optimize/query-optimization)._


## Упорядочивание по кардинальности (от низкой к высокой) {#cardinality-ordering}

Первичный индекс ClickHouse работает наиболее эффективно, когда столбцы с низкой кардинальностью располагаются первыми, что позволяет эффективно пропускать большие объёмы данных. Столбцы с высокой кардинальностью, расположенные далее в ключе, обеспечивают детальную сортировку внутри этих блоков. Начинайте со столбцов с небольшим количеством уникальных значений (например, status, category, country) и заканчивайте столбцами с большим количеством уникальных значений (например, user_id, timestamp, session_id).

Ознакомьтесь с дополнительной документацией по кардинальности и первичным индексам:

- [Выбор первичного ключа](/best-practices/choosing-a-primary-key)
- [Первичные индексы](/primary-indexes)


## Важность гранулярности времени {#time-granularity}

При использовании временных меток в предложении ORDER BY учитывайте компромисс между кардинальностью и точностью. Временные метки с точностью до микросекунд создают очень высокую кардинальность (почти одно уникальное значение на строку), что снижает эффективность разреженного первичного индекса ClickHouse. Округлённые временные метки создают более низкую кардинальность, что обеспечивает лучшее пропускание индекса, но при этом теряется точность для запросов, основанных на времени.

```sql runnable editable
-- Задание: Попробуйте различные функции времени, такие как toStartOfMinute или toStartOfWeek
-- Эксперимент: Сравните различия в кардинальности с вашими собственными данными временных меток
SELECT
    'Точность до микросекунд' as granularity,
    uniq(created_at) as unique_values,
    'Создаёт огромную кардинальность - плохо для ключа сортировки' as impact
FROM github.github_events
WHERE created_at >= '2024-01-01'
UNION ALL
SELECT
    'Точность до часа',
    uniq(toStartOfHour(created_at)),
    'Намного лучше для ключа сортировки - включает пропуск индексов'
FROM github.github_events
WHERE created_at >= '2024-01-01'
UNION ALL
SELECT
    'Точность до дня',
    uniq(toStartOfDay(created_at)),
    'Оптимально для отчётных запросов'
FROM github.github_events
WHERE created_at >= '2024-01-01';
```


## Фокусируйтесь на отдельных запросах, а не на средних значениях {#focus-on-individual-queries-not-averages}

При отладке производительности ClickHouse не полагайтесь на среднее время выполнения запросов или общие системные метрики. Вместо этого выясните, почему конкретные запросы выполняются медленно. Система может демонстрировать хорошую среднюю производительность, в то время как отдельные запросы страдают от нехватки памяти, неэффективной фильтрации или операций с высокой кардинальностью.

По словам Алексея, технического директора ClickHouse: _«Правильный подход — спросить себя, почему этот конкретный запрос обрабатывался пять секунд... Меня не волнует, что медиана и другие запросы обрабатываются быстро. Меня волнует только мой запрос»_

Когда запрос выполняется медленно, не ограничивайтесь анализом средних значений. Спросите: «Почему именно ЭТОТ запрос был медленным?» и изучите фактические паттерны использования ресурсов.


## Память и сканирование строк {#memory-and-row-scanning}

Sentry — это платформа отслеживания ошибок, ориентированная на разработчиков, которая обрабатывает миллиарды событий ежедневно от более чем 4 миллионов разработчиков. Их ключевое наблюдение: _«Кардинальность ключа группировки определяет потребление памяти в данной конкретной ситуации»_ — агрегации с высокой кардинальностью снижают производительность из-за исчерпания памяти, а не из-за сканирования строк.

Когда запросы завершаются с ошибкой, определите, связано ли это с проблемой памяти (слишком много групп) или проблемой сканирования (слишком много строк).

Запрос вида `GROUP BY user_id, error_message, url_path` создает отдельное состояние в памяти для каждой уникальной комбинации всех трех значений. При большом количестве пользователей, типов ошибок и URL-путей можно легко получить миллионы состояний агрегации, которые должны одновременно храниться в памяти.

В экстремальных случаях Sentry использует детерминированную выборку. 10%-ная выборка снижает использование памяти на 90%, сохраняя при этом точность около 5% для большинства агрегаций:

```sql
WHERE cityHash64(user_id) % 10 = 0  -- Всегда одни и те же 10% пользователей
```

Это гарантирует, что одни и те же пользователи появляются в каждом запросе, обеспечивая согласованные результаты в разных временных периодах. Ключевой момент: `cityHash64()` генерирует согласованные хеш-значения для одного и того же входа, поэтому `user_id = 12345` всегда будет хешироваться в одно и то же значение, гарантируя, что пользователь либо всегда присутствует в вашей 10%-ной выборке, либо никогда не присутствует — без колебаний между запросами.


## Оптимизация битовых масок от Sentry {#bit-mask-optimization}

При агрегации по столбцам с высокой кардинальностью (например, URL) каждое уникальное значение создаёт отдельное состояние агрегации в памяти, что приводит к её исчерпанию. Решение Sentry: вместо группировки по фактическим строкам URL выполнять группировку по булевым выражениям, которые сворачиваются в битовые маски.

Вот запрос, который вы можете попробовать на своих таблицах, если эта ситуация применима к вам:

```sql
-- Паттерн эффективной по памяти агрегации: каждое условие = одно целое число на группу
-- Ключевая идея: sumIf() создаёт ограниченное потребление памяти независимо от объёма данных
-- Память на группу: N целых чисел (N * 8 байт), где N = количество условий

SELECT
    your_grouping_column,

    -- Каждый sumIf создаёт ровно один целочисленный счётчик на группу
    -- Память остаётся постоянной независимо от того, сколько строк соответствует каждому условию
    sumIf(1, your_condition_1) as condition_1_count,
    sumIf(1, your_condition_2) as condition_2_count,
    sumIf(1, your_text_column LIKE '%pattern%') as pattern_matches,
    sumIf(1, your_numeric_column > threshold_value) as above_threshold,

    -- Сложные многоусловные агрегации также используют постоянный объём памяти
    sumIf(1, your_condition_1 AND your_text_column LIKE '%pattern%') as complex_condition_count,

    -- Стандартные агрегации для контекста
    count() as total_rows,
    avg(your_numeric_column) as average_value,
    max(your_timestamp_column) as latest_timestamp

FROM your_schema.your_table
WHERE your_timestamp_column >= 'start_date'
  AND your_timestamp_column < 'end_date'
GROUP BY your_grouping_column
HAVING condition_1_count > minimum_threshold
   OR condition_2_count > another_threshold
ORDER BY (condition_1_count + condition_2_count + pattern_matches) DESC
LIMIT 20
```

Вместо хранения каждой уникальной строки в памяти вы сохраняете ответы на вопросы об этих строках в виде целых чисел. Состояние агрегации становится ограниченным и компактным независимо от разнообразия данных.

От инженерной команды Sentry: «Эти тяжёлые запросы выполняются более чем в 10 раз быстрее, а потребление памяти в 100 раз ниже (и, что более важно, ограничено). Наши крупнейшие клиенты больше не сталкиваются с ошибками при поиске повторов, и теперь мы можем поддерживать клиентов произвольного размера без исчерпания памяти».


## Видеоматериалы {#video-sources}

- [Lost in the Haystack - Optimizing High Cardinality Aggregations](https://www.youtube.com/watch?v=paK84-EUJCA) - практический опыт оптимизации памяти от Sentry
- [ClickHouse Performance Analysis](https://www.youtube.com/watch?v=lxKbvmcLngo) - методология отладки от Алексея Миловидова
- [ClickHouse Meetup: Query Optimization Techniques](https://www.youtube.com/watch?v=JBomQk4Icjo) - стратегии оптимизации от сообщества

**Читайте далее**:

- [Руководство по оптимизации запросов](/optimize/query-optimization)
- [Материализованные представления: опыт сообщества](./materialized-views.md)
