---
title: 'Вставка данных в ClickHouse'
description: 'Как выполнять вставку данных в ClickHouse'
keywords: ['INSERT', 'Пакетная вставка']
sidebar_label: 'Вставка данных в ClickHouse'
slug: /guides/inserting-data
show_related_blogs: true
doc_type: 'guide'
---

import postgres_inserts from '@site/static/images/guides/postgres-inserts.png';
import Image from '@theme/IdealImage';


## Вставка данных в ClickHouse в сравнении с OLTP-базами данных {#inserting-into-clickhouse-vs-oltp-databases}

Будучи OLAP-базой данных (Online Analytical Processing), ClickHouse оптимизирован для высокой производительности и масштабируемости, что позволяет вставлять потенциально миллионы строк в секунду.
Это достигается за счёт сочетания высокопараллелизованной архитектуры и эффективного колоночного сжатия, но с компромиссами в отношении немедленной согласованности данных.
Более конкретно, ClickHouse оптимизирован для операций только на добавление и предоставляет только гарантии согласованности в конечном итоге.

В отличие от этого, OLTP-базы данных, такие как Postgres, специально оптимизированы для транзакционных вставок с полным соблюдением ACID, обеспечивая строгие гарантии согласованности и надёжности.
PostgreSQL использует MVCC (Multi-Version Concurrency Control) для обработки конкурентных транзакций, что подразумевает поддержание нескольких версий данных.
Эти транзакции могут потенциально затрагивать небольшое количество строк за раз, при этом возникают значительные накладные расходы из-за гарантий надёжности, ограничивающих производительность вставки.

Для достижения высокой производительности вставки при сохранении строгих гарантий согласованности пользователи должны придерживаться простых правил, описанных ниже, при вставке данных в ClickHouse.
Следование этим правилам поможет избежать проблем, с которыми пользователи обычно сталкиваются при первом использовании ClickHouse, когда пытаются применить стратегию вставки, работающую для OLTP-баз данных.


## Рекомендации по вставке данных {#best-practices-for-inserts}

### Вставка большими пакетами {#insert-in-large-batch-sizes}

По умолчанию каждая вставка, отправленная в ClickHouse, приводит к немедленному созданию части хранилища, содержащей данные из вставки вместе с другими метаданными, которые необходимо сохранить.
Поэтому отправка меньшего количества вставок, каждая из которых содержит больше данных, по сравнению с отправкой большего количества вставок, каждая из которых содержит меньше данных, сократит количество необходимых операций записи.
Как правило, мы рекомендуем вставлять данные достаточно большими пакетами, содержащими как минимум 1 000 строк за раз, и в идеале от 10 000 до 100 000 строк.
(Подробнее [здесь](https://clickhouse.com/blog/asynchronous-data-inserts-in-clickhouse#data-needs-to-be-batched-for-optimal-performance)).

Если большие пакеты невозможны, используйте асинхронные вставки, описанные ниже.

### Обеспечение согласованности пакетов для идемпотентных повторов {#ensure-consistent-batches-for-idempotent-retries}

По умолчанию вставки в ClickHouse являются синхронными и идемпотентными (т. е. многократное выполнение одной и той же операции вставки имеет тот же эффект, что и однократное выполнение).
Для таблиц семейства движков MergeTree ClickHouse по умолчанию автоматически [дедуплицирует вставки](https://clickhouse.com/blog/common-getting-started-issues-with-clickhouse#5-deduplication-at-insert-time).

Это означает, что вставки остаются устойчивыми в следующих случаях:

- 1. Если узел, получающий данные, испытывает проблемы, запрос вставки завершится по таймауту (или выдаст более конкретную ошибку) и не получит подтверждения.
- 2. Если данные были записаны узлом, но подтверждение не может быть возвращено отправителю запроса из-за сетевых прерываний, отправитель получит либо таймаут, либо сетевую ошибку.

С точки зрения клиента случаи (i) и (ii) могут быть трудноразличимы. Однако в обоих случаях неподтвержденную вставку можно просто немедленно повторить.
Пока повторный запрос вставки содержит те же данные в том же порядке, ClickHouse автоматически проигнорирует повторную вставку, если (неподтвержденная) исходная вставка была успешной.

### Вставка в таблицу MergeTree или распределенную таблицу {#insert-to-a-mergetree-table-or-a-distributed-table}

Мы рекомендуем вставлять данные непосредственно в таблицу MergeTree (или реплицируемую таблицу), балансируя запросы между набором узлов, если данные шардированы, и устанавливая `internal_replication=true`.
Это позволит ClickHouse реплицировать данные на любые доступные шарды реплик и обеспечить согласованность данных в конечном итоге.

Если балансировка нагрузки на стороне клиента неудобна, пользователи могут вставлять данные через [распределенную таблицу](/engines/table-engines/special/distributed), которая затем распределит записи между узлами. Опять же, рекомендуется установить `internal_replication=true`.
Однако следует отметить, что этот подход немного менее производителен, поскольку записи должны быть выполнены локально на узле с распределенной таблицей, а затем отправлены на шарды.

### Использование асинхронных вставок для небольших пакетов {#use-asynchronous-inserts-for-small-batches}

Существуют сценарии, в которых пакетирование на стороне клиента невозможно, например, случай мониторинга с сотнями или тысячами специализированных агентов, отправляющих логи, метрики, трассировки и т. д.
В этом сценарии передача данных в реальном времени является ключевой для обнаружения проблем и аномалий как можно быстрее.
Кроме того, существует риск всплесков событий в наблюдаемых системах, что может потенциально вызвать большие всплески памяти и связанные с этим проблемы при попытке буферизации данных мониторинга на стороне клиента.
Если невозможно вставлять большие пакеты, пользователи могут делегировать пакетирование ClickHouse, используя [асинхронные вставки](/best-practices/selecting-an-insert-strategy#asynchronous-inserts).

При асинхронных вставках данные сначала вставляются в буфер, а затем записываются в хранилище базы данных в 3 этапа, как показано на диаграмме ниже:

<Image img={postgres_inserts} size='md' alt='Вставки Postgres' />

При включенных асинхронных вставках ClickHouse:

(1) получает запрос вставки асинхронно.
(2) сначала записывает данные запроса в буфер в памяти.
(3) сортирует и записывает данные как часть в хранилище базы данных только при следующей очистке буфера.

До очистки буфера в нем могут накапливаться данные других асинхронных запросов вставки от того же или других клиентов.
Часть, созданная в результате очистки буфера, потенциально будет содержать данные из нескольких асинхронных запросов вставки.
В целом эти механизмы переносят пакетирование данных со стороны клиента на сторону сервера (экземпляр ClickHouse).

:::note
Обратите внимание, что данные недоступны для поиска запросами до их сброса в хранилище базы данных, и что очистка буфера настраивается.


Полную информацию о настройке асинхронных вставок можно найти [здесь](/optimize/asynchronous-inserts#enabling-asynchronous-inserts), а подробное описание — [здесь](https://clickhouse.com/blog/asynchronous-data-inserts-in-clickhouse).
:::

### Используйте официальные клиенты ClickHouse {#use-official-clickhouse-clients}

ClickHouse предоставляет клиенты для наиболее популярных языков программирования.
Они оптимизированы для корректного выполнения вставок и нативно поддерживают асинхронные вставки либо напрямую, как, например, в [клиенте Go](/integrations/go#async-insert), либо косвенно при включении в настройках на уровне запроса, пользователя или соединения.

Полный список доступных клиентов и драйверов ClickHouse см. в разделе [Клиенты и драйверы](/interfaces/cli).

### Предпочитайте нативный формат {#prefer-the-native-format}

ClickHouse поддерживает множество [входных форматов](/interfaces/formats) при вставке данных (и выполнении запросов).
Это существенное отличие от OLTP-баз данных, которое значительно упрощает загрузку данных из внешних источников — особенно в сочетании с [табличными функциями](/sql-reference/table-functions) и возможностью загрузки данных из файлов на диске.
Эти форматы идеально подходят для разовой загрузки данных и задач по работе с данными.

Для приложений, нацеленных на достижение оптимальной производительности вставки, рекомендуется использовать формат [Native](/interfaces/formats/Native).
Он поддерживается большинством клиентов (таких как Go и Python) и обеспечивает минимальную нагрузку на сервер, поскольку этот формат уже является колоночно-ориентированным.
Таким образом, ответственность за преобразование данных в колоночно-ориентированный формат возлагается на клиентскую сторону. Это важно для эффективного масштабирования вставок.

В качестве альтернативы можно использовать [формат RowBinary](/interfaces/formats/RowBinary) (используемый клиентом Java), если предпочтителен строчный формат — его обычно проще реализовать, чем формат Native.
Он более эффективен с точки зрения сжатия, сетевых издержек и обработки на сервере, чем альтернативные строчные форматы, такие как [JSON](/interfaces/formats/JSON).
Формат [JSONEachRow](/interfaces/formats/JSONEachRow) может быть рассмотрен для пользователей с меньшей пропускной способностью записи, стремящихся к быстрой интеграции. Следует учитывать, что этот формат создаст дополнительную нагрузку на процессор в ClickHouse при парсинге.

### Используйте HTTP-интерфейс {#use-the-http-interface}

В отличие от многих традиционных баз данных, ClickHouse поддерживает HTTP-интерфейс.
Его можно использовать как для вставки, так и для запроса данных с применением любого из вышеперечисленных форматов.
Это часто предпочтительнее нативного протокола ClickHouse, поскольку позволяет легко переключать трафик с помощью балансировщиков нагрузки.
Ожидаются незначительные различия в производительности вставки по сравнению с нативным протоколом, который создает немного меньше издержек.
Существующие клиенты используют один из этих протоколов (в некоторых случаях оба, например, клиент Go).
Нативный протокол позволяет легко отслеживать прогресс выполнения запроса.

Дополнительную информацию см. в разделе [HTTP-интерфейс](/interfaces/http).


## Базовый пример {#basic-example}

Вы можете использовать знакомую команду `INSERT INTO TABLE` в ClickHouse. Давайте вставим данные в таблицу, которую мы создали в руководстве по началу работы ["Создание таблиц в ClickHouse"](./creating-tables).

```sql
INSERT INTO helloworld.my_first_table (user_id, message, timestamp, metric) VALUES
    (101, 'Привет, ClickHouse!',                                           now(),       -1.0    ),
    (102, 'Вставляйте много строк за один пакет',                          yesterday(), 1.41421 ),
    (102, 'Сортируйте данные на основе часто используемых запросов',       today(),     2.718   ),
    (101, 'Гранулы — это наименьшие фрагменты данных для чтения',          now() + 5,   3.14159 )
```

Чтобы проверить, что это сработало, выполним следующий запрос `SELECT`:

```sql
SELECT * FROM helloworld.my_first_table
```

Результат:

```response
user_id message                                             timestamp           metric
101         Привет, ClickHouse!                                             2024-11-13 20:01:22     -1
101         Гранулы — это наименьшие фрагменты данных для чтения            2024-11-13 20:01:27 3.14159
102         Вставляйте много строк за один пакет                            2024-11-12 00:00:00 1.41421
102         Сортируйте данные на основе часто используемых запросов     2024-11-13 00:00:00     2.718
```


## Загрузка данных из Postgres {#loading-data-from-postgres}

Для загрузки данных из Postgres можно использовать:

- `ClickPipes` — инструмент ETL, специально разработанный для репликации баз данных PostgreSQL. Доступен в двух вариантах:
  - ClickHouse Cloud — через наш [управляемый сервис приёма данных](/integrations/clickpipes/postgres) в ClickPipes.
  - Самостоятельное развёртывание — через [проект с открытым исходным кодом PeerDB](https://github.com/PeerDB-io/peerdb).
- [Движок таблиц PostgreSQL](/integrations/postgresql#using-the-postgresql-table-engine) для прямого чтения данных, как показано в предыдущих примерах. Обычно подходит, если достаточно пакетной репликации на основе известной контрольной точки (например, временной метки) или если требуется однократная миграция. Этот подход масштабируется до десятков миллионов строк. При миграции более крупных наборов данных рекомендуется использовать несколько запросов, каждый из которых обрабатывает часть данных. Для каждой части можно использовать промежуточные таблицы, прежде чем их партиции будут перемещены в итоговую таблицу. Это позволяет повторять неудачные запросы. Подробнее об этой стратегии массовой загрузки см. здесь.
- Данные можно экспортировать из PostgreSQL в формате CSV. Затем их можно загрузить в ClickHouse из локальных файлов или через объектное хранилище с использованием табличных функций.

:::note Нужна помощь с загрузкой больших наборов данных?
Если вам нужна помощь с загрузкой больших наборов данных или вы столкнулись с ошибками при импорте данных в ClickHouse Cloud, свяжитесь с нами по адресу support@clickhouse.com, и мы окажем помощь.
:::


## Вставка данных из командной строки {#inserting-data-from-command-line}

**Предварительные требования**

- Вы [установили](/install) ClickHouse
- `clickhouse-server` запущен
- У вас есть доступ к терминалу с `wget`, `zcat` и `curl`

В этом примере показано, как вставить CSV-файл в ClickHouse из командной строки, используя clickhouse-client в пакетном режиме. Дополнительную информацию и примеры вставки данных через командную строку с использованием clickhouse-client в пакетном режиме см. в разделе [«Пакетный режим»](/interfaces/cli#batch-mode).

В качестве примера мы будем использовать [набор данных Hacker News](/getting-started/example-datasets/hacker-news), который содержит 28 миллионов строк данных Hacker News.

<VerticalStepper headerLevel="h3">
    
### Загрузка CSV {#download-csv}

Выполните следующую команду для загрузки CSV-версии набора данных из нашего публичного S3-бакета:

```bash
wget https://datasets-documentation.s3.eu-west-3.amazonaws.com/hackernews/hacknernews.csv.gz
```

Загрузка этого сжатого файла размером 4,6 ГБ, содержащего 28 млн строк, займет 5–10 минут.

### Создание таблицы {#create-table}

При запущенном `clickhouse-server` вы можете создать пустую таблицу со следующей схемой непосредственно из командной строки, используя `clickhouse-client` в пакетном режиме:

```bash
clickhouse-client <<'_EOF'
CREATE TABLE hackernews(
    `id` UInt32,
    `deleted` UInt8,
    `type` Enum('story' = 1, 'comment' = 2, 'poll' = 3, 'pollopt' = 4, 'job' = 5),
    `by` LowCardinality(String),
    `time` DateTime,
    `text` String,
    `dead` UInt8,
    `parent` UInt32,
    `poll` UInt32,
    `kids` Array(UInt32),
    `url` String,
    `score` Int32,
    `title` String,
    `parts` Array(UInt32),
    `descendants` Int32
)
ENGINE = MergeTree
ORDER BY id
_EOF
```

Если ошибок нет, таблица успешно создана. В приведенной выше команде одинарные кавычки используются вокруг разделителя heredoc (`_EOF`) для предотвращения интерполяции. Без одинарных кавычек потребовалось бы экранировать обратные кавычки вокруг имен столбцов.

### Вставка данных из командной строки {#insert-data-via-cmd}

Затем выполните приведенную ниже команду для вставки данных из ранее загруженного файла в вашу таблицу:

```bash
zcat < hacknernews.csv.gz | ./clickhouse client --query "INSERT INTO hackernews FORMAT CSV"
```

Поскольку данные сжаты, необходимо сначала распаковать файл с помощью такого инструмента, как `gzip`, `zcat` или аналогичного, а затем передать распакованные данные в `clickhouse-client` с соответствующим оператором `INSERT` и `FORMAT`.

:::note
При вставке данных с помощью clickhouse-client в интерактивном режиме можно позволить ClickHouse самостоятельно выполнить распаковку при вставке, используя предложение `COMPRESSION`. ClickHouse может автоматически определить тип сжатия по расширению файла, но вы также можете указать его явно.

В этом случае запрос на вставку будет выглядеть следующим образом:

```bash
clickhouse-client --query "INSERT INTO hackernews FROM INFILE 'hacknernews.csv.gz' COMPRESSION 'gzip' FORMAT CSV;"
```

:::

После завершения вставки данных вы можете выполнить следующую команду, чтобы увидеть количество строк в таблице `hackernews`:

```bash
clickhouse-client --query "SELECT formatReadableQuantity(count(*)) FROM hackernews"
28.74 million
```

### Вставка данных через командную строку с помощью curl {#insert-using-curl}

На предыдущих шагах вы сначала загрузили CSV-файл на локальную машину с помощью `wget`. Также возможно напрямую вставить данные из удаленного URL с помощью одной команды.

Выполните следующую команду для очистки данных из таблицы `hackernews`, чтобы вы могли вставить их снова без промежуточного шага загрузки на локальную машину:

```bash
clickhouse-client --query "TRUNCATE hackernews"
```

Теперь выполните:

```bash
curl https://datasets-documentation.s3.eu-west-3.amazonaws.com/hackernews/hacknernews.csv.gz | zcat | clickhouse-client --query "INSERT INTO hackernews FORMAT CSV"
```

Теперь вы можете выполнить ту же команду, что и ранее, чтобы убедиться, что данные были вставлены снова:

```bash
clickhouse-client --query "SELECT formatReadableQuantity(count(*)) FROM hackernews"
28.74 million
```

</VerticalStepper>
