---
slug: /data-modeling/denormalization
title: 'Денормализация данных'
description: 'Как использовать денормализацию для повышения производительности запросов'
keywords: ['data denormalization', 'denormalize', 'query optimization']
doc_type: 'guide'
---

import denormalizationDiagram from '@site/static/images/data-modeling/denormalization-diagram.png';
import denormalizationSchema from '@site/static/images/data-modeling/denormalization-schema.png';
import Image from '@theme/IdealImage';


# Денормализация данных

Денормализация данных — это подход в ClickHouse, при котором используются «плоские» таблицы для снижения задержки выполнения запросов за счёт отказа от JOIN-ов.



## Сравнение нормализованных и денормализованных схем {#comparing-normalized-vs-denormalized-schemas}

Денормализация данных — это намеренное обращение процесса нормализации для оптимизации производительности базы данных под конкретные паттерны запросов. В нормализованных базах данных данные разделяются на несколько связанных таблиц для минимизации избыточности и обеспечения целостности данных. Денормализация повторно вводит избыточность путем объединения таблиц, дублирования данных и включения вычисляемых полей в одну таблицу или меньшее количество таблиц — фактически перенося операции соединения (JOIN) с момента выполнения запроса на момент вставки данных.

Этот процесс снижает потребность в сложных соединениях при выполнении запросов и может значительно ускорить операции чтения, что делает его идеальным решением для приложений с высокой нагрузкой на чтение и сложными запросами. Однако это может увеличить сложность операций записи и обслуживания, поскольку любые изменения дублированных данных должны распространяться на все экземпляры для поддержания согласованности.

<Image
  img={denormalizationDiagram}
  size='lg'
  alt='Денормализация в ClickHouse'
/>

<br />

Распространенная техника, популяризированная NoSQL-решениями, заключается в денормализации данных при отсутствии поддержки `JOIN`, фактически сохраняя всю статистику или связанные строки в родительской строке в виде столбцов и вложенных объектов. Например, в схеме для блога мы можем хранить все комментарии (`Comments`) в виде массива (`Array`) объектов в соответствующих публикациях.


## Когда использовать денормализацию {#when-to-use-denormalization}

В общем случае рекомендуется применять денормализацию в следующих ситуациях:

- Денормализуйте таблицы, которые изменяются нечасто или для которых допустима задержка перед тем, как данные станут доступны для аналитических запросов, то есть данные можно полностью перезагрузить пакетно.
- Избегайте денормализации связей «многие ко многим». Это может привести к необходимости обновления множества строк при изменении одной исходной строки.
- Избегайте денормализации связей с высокой кардинальностью. Если каждая строка в таблице имеет тысячи связанных записей в другой таблице, их потребуется представить в виде `Array` — либо примитивного типа, либо кортежей. Как правило, массивы с более чем 1000 кортежей не рекомендуются.
- Вместо денормализации всех столбцов в виде вложенных объектов рассмотрите возможность денормализации только статистических данных с использованием материализованных представлений (см. ниже).

Не всю информацию необходимо денормализовать — только ключевую информацию, к которой требуется частый доступ.

Работа по денормализации может выполняться как в ClickHouse, так и на предыдущих этапах обработки данных, например, с использованием Apache Flink.


## Избегайте денормализации часто обновляемых данных {#avoid-denormalization-on-frequently-updated-data}

В ClickHouse денормализация является одним из нескольких способов оптимизации производительности запросов, но использовать её следует с осторожностью. Если данные обновляются часто и требуют обновления в режиме, близком к реальному времени, этот подход следует избегать. Используйте его, если основная таблица преимущественно пополняется новыми данными или может периодически перезагружаться пакетно, например ежедневно.

Основная проблема этого подхода — производительность записи и обновления данных. Точнее говоря, денормализация фактически переносит ответственность за объединение данных с момента выполнения запроса на момент загрузки данных. Хотя это может значительно улучшить производительность запросов, это усложняет процесс загрузки и означает, что конвейеры данных должны повторно вставлять строку в ClickHouse при изменении любой из строк, которые использовались для её формирования. Это может означать, что изменение в одной исходной строке потенциально требует обновления множества строк в ClickHouse. В сложных схемах, где строки были сформированы из сложных объединений, изменение одной строки во вложенном компоненте объединения потенциально может означать необходимость обновления миллионов строк.

Реализация этого в режиме реального времени часто нереалистична и требует значительных инженерных усилий из-за двух проблем:

1. Запуск правильных операций объединения при изменении строки таблицы. В идеале это не должно приводить к обновлению всех объектов для объединения — а только тех, которые были затронуты. Модификация объединений для эффективной фильтрации нужных строк и достижение этого при высокой пропускной способности требует внешних инструментов или инженерных решений.
1. Обновления строк в ClickHouse необходимо тщательно контролировать, что вносит дополнительную сложность.

<br />

Поэтому более распространён процесс пакетного обновления, при котором все денормализованные объекты периодически перезагружаются.


## Практические примеры денормализации {#practical-cases-for-denormalization}

Рассмотрим несколько практических примеров, когда денормализация может иметь смысл, а также случаи, когда предпочтительнее использовать альтернативные подходы.

Рассмотрим таблицу `Posts`, которая уже денормализована и содержит статистику, такую как `AnswerCount` и `CommentCount` — исходные данные предоставляются именно в таком виде. На практике может быть целесообразно нормализовать эту информацию, поскольку она, вероятно, будет часто изменяться. Многие из этих столбцов также доступны через другие таблицы, например, комментарии к посту доступны через столбец `PostId` и таблицу `Comments`. Для целей примера предполагаем, что посты перезагружаются в пакетном режиме.

Мы также рассматриваем только денормализацию других таблиц в `Posts`, поскольку считаем её основной таблицей для аналитики. Денормализация в обратном направлении также может быть уместна для некоторых запросов, при этом применяются те же соображения.

_Для каждого из следующих примеров предполагается, что существует запрос, требующий объединения обеих таблиц._

### Posts и Votes {#posts-and-votes}

Голоса за посты представлены в виде отдельных таблиц. Оптимизированная схема для этого показана ниже, а также команда вставки для загрузки данных:

```sql
CREATE TABLE votes
(
        `Id` UInt32,
        `PostId` Int32,
        `VoteTypeId` UInt8,
        `CreationDate` DateTime64(3, 'UTC'),
        `UserId` Int32,
        `BountyAmount` UInt8
)
ENGINE = MergeTree
ORDER BY (VoteTypeId, CreationDate, PostId)

INSERT INTO votes SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/votes/*.parquet')

0 rows in set. Elapsed: 26.272 sec. Processed 238.98 million rows, 2.13 GB (9.10 million rows/s., 80.97 MB/s.)
```

На первый взгляд, они могут быть кандидатами для денормализации в таблице постов. Однако у этого подхода есть несколько проблем.

Голоса часто добавляются к постам. Хотя со временем это может уменьшаться для отдельного поста, следующий запрос показывает, что у нас около 40 тыс. голосов в час по более чем 30 тыс. постов.

```sql
SELECT round(avg(c)) AS avg_votes_per_hr, round(avg(posts)) AS avg_posts_per_hr
FROM
(
        SELECT
        toStartOfHour(CreationDate) AS hr,
        count() AS c,
        uniq(PostId) AS posts
        FROM votes
        GROUP BY hr
)

┌─avg_votes_per_hr─┬─avg_posts_per_hr─┐
│               41759 │         33322 │
└──────────────────┴──────────────────┘
```

Это можно решить с помощью пакетной обработки, если допустима задержка, но это всё равно требует обработки обновлений, если только мы не перезагружаем все посты периодически (что вряд ли желательно).

Более проблематично то, что некоторые посты имеют чрезвычайно большое количество голосов:

```sql
SELECT PostId, concat('https://stackoverflow.com/questions/', PostId) AS url, count() AS c
FROM votes
GROUP BY PostId
ORDER BY c DESC
LIMIT 5

┌───PostId─┬─url──────────────────────────────────────────┬─────c─┐
│ 11227902 │ https://stackoverflow.com/questions/11227902 │ 35123 │
│   927386 │ https://stackoverflow.com/questions/927386   │ 29090 │
│ 11227809 │ https://stackoverflow.com/questions/11227809 │ 27475 │
│   927358 │ https://stackoverflow.com/questions/927358   │ 26409 │
│  2003515 │ https://stackoverflow.com/questions/2003515  │ 25899 │
└──────────┴──────────────────────────────────────────────┴───────┘
```

Основное наблюдение здесь заключается в том, что агрегированной статистики голосов для каждого поста было бы достаточно для большинства анализов — нам не нужно денормализовать всю информацию о голосах. Например, текущий столбец `Score` представляет собой такую статистику, т.е. общее количество положительных голосов минус отрицательные голоса. В идеале мы могли бы просто получать эту статистику во время выполнения запроса с помощью простого поиска (см. [словари](/dictionary)).

### Users и Badges {#users-and-badges}

Теперь рассмотрим наши таблицы `Users` и `Badges`:

<Image img={denormalizationSchema} size='lg' alt='Схема Users и Badges' />

<p></p>
Сначала вставим данные с помощью следующей команды:
<p></p>


```sql
CREATE TABLE users
(
    `Id` Int32,
    `Reputation` LowCardinality(String),
    `CreationDate` DateTime64(3, 'UTC') CODEC(Delta(8), ZSTD(1)),
    `DisplayName` String,
    `LastAccessDate` DateTime64(3, 'UTC'),
    `AboutMe` String,
    `Views` UInt32,
    `UpVotes` UInt32,
    `DownVotes` UInt32,
    `WebsiteUrl` String,
    `Location` LowCardinality(String),
    `AccountId` Int32
)
ENGINE = MergeTree
ORDER BY (Id, CreationDate)
```

```sql
CREATE TABLE badges
(
    `Id` UInt32,
    `UserId` Int32,
    `Name` LowCardinality(String),
    `Date` DateTime64(3, 'UTC'),
    `Class` Enum8('Gold' = 1, 'Silver' = 2, 'Bronze' = 3),
    `TagBased` Bool
)
ENGINE = MergeTree
ORDER BY UserId

INSERT INTO users SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/users.parquet')

0 rows in set. Elapsed: 26.229 sec. Processed 22.48 million rows, 1.36 GB (857.21 thousand rows/s., 51.99 MB/s.)

INSERT INTO badges SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/badges.parquet')

0 rows in set. Elapsed: 18.126 sec. Processed 51.29 million rows, 797.05 MB (2.83 million rows/s., 43.97 MB/s.)
```

Хотя пользователи могут часто получать значки, вряд ли этот набор данных потребуется обновлять чаще одного раза в день. Связь между значками и пользователями — один ко многим. Возможно, мы можем просто денормализовать значки в таблицу пользователей в виде списка кортежей? Хотя это возможно, быстрая проверка максимального количества значков на пользователя показывает, что это не оптимальное решение:

```sql
SELECT UserId, count() AS c FROM badges GROUP BY UserId ORDER BY c DESC LIMIT 5

┌─UserId─┬─────c─┐
│  22656 │ 19334 │
│   6309 │ 10516 │
│ 100297 │  7848 │
│ 157882 │  7574 │
│  29407 │  6512 │
└────────┴───────┘
```

Вероятно, нереалистично денормализовать 19 тысяч объектов в одну строку. Эту связь лучше оставить в виде отдельных таблиц или с добавлением статистики.

> Возможно, потребуется денормализовать статистику по значкам в таблицу пользователей, например, количество значков. Мы рассмотрим такой пример при использовании словарей для этого набора данных во время вставки данных.

### Posts и PostLinks {#posts-and-postlinks}

`PostLinks` связывают `Posts`, которые пользователи считают связанными или дублирующими друг друга. Следующий запрос показывает схему и команду загрузки:

```sql
CREATE TABLE postlinks
(
  `Id` UInt64,
  `CreationDate` DateTime64(3, 'UTC'),
  `PostId` Int32,
  `RelatedPostId` Int32,
  `LinkTypeId` Enum('Linked' = 1, 'Duplicate' = 3)
)
ENGINE = MergeTree
ORDER BY (PostId, RelatedPostId)

INSERT INTO postlinks SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/postlinks.parquet')

0 rows in set. Elapsed: 4.726 sec. Processed 6.55 million rows, 129.70 MB (1.39 million rows/s., 27.44 MB/s.)
```

Можно убедиться, что ни один пост не имеет чрезмерного количества ссылок, препятствующего денормализации:

```sql
SELECT PostId, count() AS c
FROM postlinks
GROUP BY PostId
ORDER BY c DESC LIMIT 5

┌───PostId─┬───c─┐
│ 22937618 │ 125 │
│  9549780 │ 120 │
│  3737139 │ 109 │
│ 18050071 │ 103 │
│ 25889234 │  82 │
└──────────┴─────┘
```

Аналогично, эти ссылки не являются событиями, которые происходят слишком часто:


```sql
SELECT
  round(avg(c)) AS avg_votes_per_hr,
  round(avg(posts)) AS avg_posts_per_hr
FROM
(
  SELECT
  toStartOfHour(CreationDate) AS hr,
  count() AS c,
  uniq(PostId) AS posts
  FROM postlinks
  GROUP BY hr
)

┌─avg_votes_per_hr─┬─avg_posts_per_hr─┐
│                54 │                    44     │
└──────────────────┴──────────────────┘
```

Мы используем это в качестве примера денормализации ниже.

### Пример простой статистики {#simple-statistic-example}

В большинстве случаев денормализация требует добавления одного столбца или статистики к родительской строке. Например, мы можем захотеть обогатить наши посты количеством дублирующихся постов, и для этого достаточно добавить столбец.

```sql
CREATE TABLE posts_with_duplicate_count
(
  `Id` Int32 CODEC(Delta(4), ZSTD(1)),
   ... -другие столбцы
   `DuplicatePosts` UInt16
) ENGINE = MergeTree
ORDER BY (PostTypeId, toDate(CreationDate), CommentCount)
```

Для заполнения этой таблицы мы используем `INSERT INTO SELECT`, объединяя статистику дубликатов с нашими постами.

```sql
INSERT INTO posts_with_duplicate_count SELECT
    posts.*,
    DuplicatePosts
FROM posts AS posts
LEFT JOIN
(
    SELECT PostId, countIf(LinkTypeId = 'Duplicate') AS DuplicatePosts
    FROM postlinks
    GROUP BY PostId
) AS postlinks ON posts.Id = postlinks.PostId
```

### Использование сложных типов для связей один-ко-многим {#exploiting-complex-types-for-one-to-many-relationships}

Для выполнения денормализации часто необходимо использовать сложные типы. Если денормализуется связь один-к-одному с небольшим количеством столбцов, пользователи могут просто добавить их как строки с исходными типами, как показано выше. Однако это часто нежелательно для больших объектов и невозможно для связей один-ко-многим.

В случае сложных объектов или связей один-ко-многим пользователи могут использовать:

- Именованные кортежи (Named Tuples) — позволяют представить связанную структуру в виде набора столбцов.
- Array(Tuple) или Nested — массив именованных кортежей, также известный как Nested, где каждый элемент представляет объект. Применимо к связям один-ко-многим.

В качестве примера мы демонстрируем денормализацию `PostLinks` в `Posts` ниже.

Каждый пост может содержать несколько ссылок на другие посты, как показано в схеме `PostLinks` ранее. В виде типа Nested мы можем представить эти связанные и дублирующиеся посты следующим образом:

```sql
SET flatten_nested=0
CREATE TABLE posts_with_links
(
  `Id` Int32 CODEC(Delta(4), ZSTD(1)),
   ... -другие столбцы
   `LinkedPosts` Nested(CreationDate DateTime64(3, 'UTC'), PostId Int32),
   `DuplicatePosts` Nested(CreationDate DateTime64(3, 'UTC'), PostId Int32),
) ENGINE = MergeTree
ORDER BY (PostTypeId, toDate(CreationDate), CommentCount)
```

> Обратите внимание на использование настройки `flatten_nested=0`. Мы рекомендуем отключить выравнивание вложенных данных.

Мы можем выполнить эту денормализацию с помощью запроса `INSERT INTO SELECT` с `OUTER JOIN`:

```sql
INSERT INTO posts_with_links
SELECT
    posts.*,
    arrayMap(p -> (p.1, p.2), arrayFilter(p -> p.3 = 'Linked' AND p.2 != 0, Related)) AS LinkedPosts,
    arrayMap(p -> (p.1, p.2), arrayFilter(p -> p.3 = 'Duplicate' AND p.2 != 0, Related)) AS DuplicatePosts
FROM posts
LEFT JOIN (
    SELECT
         PostId,
         groupArray((CreationDate, RelatedPostId, LinkTypeId)) AS Related
    FROM postlinks
    GROUP BY PostId
) AS postlinks ON posts.Id = postlinks.PostId

0 rows in set. Elapsed: 155.372 sec. Processed 66.37 million rows, 76.33 GB (427.18 thousand rows/s., 491.25 MB/s.)
Peak memory usage: 6.98 GiB.
```

> Обратите внимание на время выполнения. Нам удалось денормализовать 66 млн строк примерно за 2 минуты. Как мы увидим позже, эту операцию можно запланировать.


Обратите внимание на использование функции `groupArray` для агрегирования `PostLinks` в массив для каждого `PostId` перед выполнением соединения. Затем этот массив фильтруется в два подсписка: `LinkedPosts` и `DuplicatePosts`, при этом из внешнего соединения также исключаются любые пустые результаты.

Мы можем выбрать несколько строк, чтобы увидеть нашу новую денормализованную структуру:

```sql
SELECT LinkedPosts, DuplicatePosts
FROM posts_with_links
WHERE (length(LinkedPosts) > 2) AND (length(DuplicatePosts) > 0)
LIMIT 1
FORMAT Vertical

Row 1:
──────
LinkedPosts:    [('2017-04-11 11:53:09.583',3404508),('2017-04-11 11:49:07.680',3922739),('2017-04-11 11:48:33.353',33058004)]
DuplicatePosts: [('2017-04-11 12:18:37.260',3922739),('2017-04-11 12:18:37.260',33058004)]
```


## Оркестрация и планирование денормализации {#orchestrating-and-scheduling-denormalization}

### Пакетная обработка {#batch}

Применение денормализации требует процесса преобразования, в рамках которого она может быть выполнена и оркестрирована.

Выше мы показали, как ClickHouse может использоваться для выполнения этого преобразования после загрузки данных с помощью `INSERT INTO SELECT`. Этот подход подходит для периодических пакетных преобразований.

У пользователей есть несколько вариантов оркестрации этого процесса в ClickHouse, если приемлем периодический процесс пакетной загрузки:

- **[Обновляемые материализованные представления](/materialized-view/refreshable-materialized-view)** - Обновляемые материализованные представления могут использоваться для периодического планирования запроса с отправкой результатов в целевую таблицу. При выполнении запроса представление обеспечивает атомарное обновление целевой таблицы. Это встроенный в ClickHouse способ планирования такой работы.
- **Внешние инструменты** - Использование таких инструментов, как [dbt](https://www.getdbt.com/) и [Airflow](https://airflow.apache.org/), для периодического планирования преобразования. [Интеграция ClickHouse с dbt](/integrations/dbt) обеспечивает атомарное выполнение этого процесса: создается новая версия целевой таблицы, которая затем атомарно заменяет версию, обрабатывающую запросы (с помощью команды [EXCHANGE](/sql-reference/statements/exchange)).

### Потоковая обработка {#streaming}

Пользователи также могут выполнять это за пределами ClickHouse, до вставки данных, используя технологии потоковой обработки, такие как [Apache Flink](https://flink.apache.org/). Альтернативно, инкрементные [материализованные представления](/guides/developer/cascading-materialized-views) могут использоваться для выполнения этого процесса по мере вставки данных.
