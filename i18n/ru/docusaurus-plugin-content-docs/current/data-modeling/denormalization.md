---
slug: /data-modeling/denormalization
title: 'Денормализация данных'
description: 'Как использовать денормализацию для повышения производительности запросов'
keywords: ['денормализация данных', 'денормализовать', 'оптимизация запросов']
doc_type: 'guide'
---

import denormalizationDiagram from '@site/static/images/data-modeling/denormalization-diagram.png';
import denormalizationSchema from '@site/static/images/data-modeling/denormalization-schema.png';
import Image from '@theme/IdealImage';


# Денормализация данных

Денормализация данных — это подход в ClickHouse, при котором используются плоские таблицы для минимизации задержки выполнения запросов за счёт устранения необходимости в соединениях таблиц (JOIN).



## Сравнение нормализованных и денормализованных схем {#comparing-normalized-vs-denormalized-schemas}

Денормализация данных представляет собой намеренное обращение процесса нормализации для оптимизации производительности базы данных под конкретные паттерны запросов. В нормализованных базах данных информация разделена на несколько связанных таблиц для минимизации избыточности и обеспечения целостности данных. Денормализация вновь вводит избыточность путем объединения таблиц, дублирования данных и включения вычисляемых полей в одну таблицу или меньшее количество таблиц — фактически перенося операции соединения из времени выполнения запроса во время вставки.

Этот процесс снижает потребность в сложных соединениях во время выполнения запросов и может значительно ускорить операции чтения, что делает его идеальным для приложений с высокими требованиями к чтению и сложными запросами. Однако это может увеличить сложность операций записи и обслуживания, поскольку любые изменения дублированных данных должны быть распространены на все экземпляры для поддержания согласованности.

<Image
  img={denormalizationDiagram}
  size='lg'
  alt='Денормализация в ClickHouse'
/>

<br />

Распространенная техника, популяризированная NoSQL-решениями, заключается в денормализации данных при отсутствии поддержки `JOIN`, фактически сохраняя всю статистику или связанные строки в родительской строке в виде столбцов и вложенных объектов. Например, в примере схемы для блога мы можем хранить все комментарии (`Comments`) в виде массива (`Array`) объектов в соответствующих публикациях.


## Когда использовать денормализацию {#when-to-use-denormalization}

В общем случае мы рекомендуем применять денормализацию в следующих ситуациях:

- Денормализуйте таблицы, которые изменяются нечасто или для которых допустима задержка перед тем, как данные станут доступны для аналитических запросов, то есть данные могут быть полностью перезагружены пакетно.
- Избегайте денормализации связей «многие ко многим». Это может привести к необходимости обновления множества строк при изменении одной исходной строки.
- Избегайте денормализации связей с высокой кардинальностью. Если каждая строка в таблице имеет тысячи связанных записей в другой таблице, их потребуется представить в виде `Array` — либо примитивного типа, либо кортежей. Как правило, массивы с более чем 1000 кортежей не рекомендуются.
- Вместо денормализации всех столбцов в виде вложенных объектов рассмотрите возможность денормализации только статистических данных с использованием материализованных представлений (см. ниже).

Не всю информацию необходимо денормализовать — только ключевую информацию, к которой требуется частый доступ.

Работа по денормализации может выполняться как в ClickHouse, так и на предыдущих этапах обработки, например с использованием Apache Flink.


## Избегайте денормализации для часто обновляемых данных {#avoid-denormalization-on-frequently-updated-data}

В ClickHouse денормализация является одним из нескольких вариантов оптимизации производительности запросов, но применять её следует с осторожностью. Если данные обновляются часто и требуют обновления в режиме, близком к реальному времени, этот подход следует избегать. Используйте его, если основная таблица преимущественно пополняется новыми данными или может периодически перезагружаться пакетно, например ежедневно.

Основная проблема этого подхода — производительность записи и обновление данных. Точнее говоря, денормализация фактически переносит ответственность за объединение данных с момента выполнения запроса на момент загрузки данных. Хотя это может значительно улучшить производительность запросов, это усложняет процесс загрузки и означает, что конвейеры данных должны повторно вставлять строку в ClickHouse при изменении любой из строк, которые использовались для её формирования. Это может означать, что изменение одной исходной строки потенциально требует обновления множества строк в ClickHouse. В сложных схемах, где строки были сформированы из сложных объединений, изменение одной строки во вложенном компоненте объединения может потенциально означать необходимость обновления миллионов строк.

Достижение этого в режиме реального времени часто нереалистично и требует значительных инженерных усилий из-за двух проблем:

1. Запуск правильных операций объединения при изменении строки таблицы. В идеале это не должно приводить к обновлению всех объектов для объединения — а только тех, которые были затронуты. Модификация объединений для эффективной фильтрации нужных строк и достижение этого при высокой пропускной способности требует внешних инструментов или инженерных решений.
1. Обновления строк в ClickHouse необходимо тщательно контролировать, что вносит дополнительную сложность.

<br />

Поэтому более распространён процесс пакетного обновления, при котором все денормализованные объекты периодически перезагружаются.


## Практические примеры денормализации {#practical-cases-for-denormalization}

Рассмотрим несколько практических примеров, когда денормализация может иметь смысл, а также случаи, когда предпочтительнее использовать альтернативные подходы.

Рассмотрим таблицу `Posts`, которая уже денормализована и содержит статистику, такую как `AnswerCount` и `CommentCount` — исходные данные предоставляются именно в таком виде. На практике может потребоваться нормализовать эту информацию, поскольку она, вероятно, будет часто изменяться. Многие из этих столбцов также доступны через другие таблицы, например, комментарии к публикации доступны через столбец `PostId` и таблицу `Comments`. Для целей примера предполагается, что публикации перезагружаются в пакетном режиме.

Мы также рассматриваем только денормализацию других таблиц в таблицу `Posts`, поскольку считаем её основной таблицей для аналитики. Денормализация в обратном направлении также может быть уместна для некоторых запросов, при этом применяются те же соображения.

_Для каждого из следующих примеров предполагается наличие запроса, требующего объединения обеих таблиц._

### Posts и Votes {#posts-and-votes}

Голоса за публикации представлены в виде отдельных таблиц. Оптимизированная схема для этого показана ниже вместе с командой вставки для загрузки данных:

```sql
CREATE TABLE votes
(
        `Id` UInt32,
        `PostId` Int32,
        `VoteTypeId` UInt8,
        `CreationDate` DateTime64(3, 'UTC'),
        `UserId` Int32,
        `BountyAmount` UInt8
)
ENGINE = MergeTree
ORDER BY (VoteTypeId, CreationDate, PostId)

INSERT INTO votes SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/votes/*.parquet')

0 rows in set. Elapsed: 26.272 sec. Processed 238.98 million rows, 2.13 GB (9.10 million rows/s., 80.97 MB/s.)
```

На первый взгляд, эти данные могут быть кандидатами для денормализации в таблицу публикаций. Однако у этого подхода есть несколько проблем.

Голоса часто добавляются к публикациям. Хотя со временем их количество на одну публикацию может уменьшаться, следующий запрос показывает, что у нас около 40 тысяч голосов в час для более чем 30 тысяч публикаций.

```sql
SELECT round(avg(c)) AS avg_votes_per_hr, round(avg(posts)) AS avg_posts_per_hr
FROM
(
        SELECT
        toStartOfHour(CreationDate) AS hr,
        count() AS c,
        uniq(PostId) AS posts
        FROM votes
        GROUP BY hr
)

┌─avg_votes_per_hr─┬─avg_posts_per_hr─┐
│               41759 │         33322 │
└──────────────────┴──────────────────┘
```

Это можно решить с помощью пакетной обработки, если допустима задержка, но это всё равно требует обработки обновлений, если только мы не перезагружаем все публикации периодически (что вряд ли желательно).

Более проблематично то, что некоторые публикации имеют чрезвычайно большое количество голосов:

```sql
SELECT PostId, concat('https://stackoverflow.com/questions/', PostId) AS url, count() AS c
FROM votes
GROUP BY PostId
ORDER BY c DESC
LIMIT 5

┌───PostId─┬─url──────────────────────────────────────────┬─────c─┐
│ 11227902 │ https://stackoverflow.com/questions/11227902 │ 35123 │
│   927386 │ https://stackoverflow.com/questions/927386   │ 29090 │
│ 11227809 │ https://stackoverflow.com/questions/11227809 │ 27475 │
│   927358 │ https://stackoverflow.com/questions/927358   │ 26409 │
│  2003515 │ https://stackoverflow.com/questions/2003515  │ 25899 │
└──────────┴──────────────────────────────────────────────┴───────┘
```

Основное наблюдение здесь заключается в том, что агрегированной статистики голосов для каждой публикации было бы достаточно для большинства аналитических задач — нам не нужно денормализовать всю информацию о голосах. Например, текущий столбец `Score` представляет собой такую статистику, то есть общее количество положительных голосов минус отрицательные голоса. В идеале мы могли бы просто получать эту статистику во время выполнения запроса с помощью простого поиска (см. [словари](/dictionary)).

### Users и Badges {#users-and-badges}

Теперь рассмотрим наши таблицы `Users` и `Badges`:

<Image img={denormalizationSchema} size='lg' alt='Users and Badges schema' />

<p></p>
Сначала вставим данные с помощью следующей команды:
<p></p>


```sql
CREATE TABLE users
(
    `Id` Int32,
    `Reputation` LowCardinality(String),
    `CreationDate` DateTime64(3, 'UTC') CODEC(Delta(8), ZSTD(1)),
    `DisplayName` String,
    `LastAccessDate` DateTime64(3, 'UTC'),
    `AboutMe` String,
    `Views` UInt32,
    `UpVotes` UInt32,
    `DownVotes` UInt32,
    `WebsiteUrl` String,
    `Location` LowCardinality(String),
    `AccountId` Int32
)
ENGINE = MergeTree
ORDER BY (Id, CreationDate)
```

```sql
CREATE TABLE badges
(
    `Id` UInt32,
    `UserId` Int32,
    `Name` LowCardinality(String),
    `Date` DateTime64(3, 'UTC'),
    `Class` Enum8('Gold' = 1, 'Silver' = 2, 'Bronze' = 3),
    `TagBased` Bool
)
ENGINE = MergeTree
ORDER BY UserId

INSERT INTO users SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/users.parquet')

0 rows in set. Elapsed: 26.229 sec. Processed 22.48 million rows, 1.36 GB (857.21 thousand rows/s., 51.99 MB/s.)

INSERT INTO badges SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/badges.parquet')

0 rows in set. Elapsed: 18.126 sec. Processed 51.29 million rows, 797.05 MB (2.83 million rows/s., 43.97 MB/s.)
```

Хотя пользователи могут часто получать значки, вряд ли этот набор данных потребуется обновлять чаще одного раза в день. Связь между значками и пользователями имеет тип «один ко многим». Возможно, можно просто денормализовать значки в таблицу пользователей в виде списка кортежей? Хотя это возможно, быстрая проверка максимального количества значков на пользователя показывает, что это не лучшее решение:

```sql
SELECT UserId, count() AS c FROM badges GROUP BY UserId ORDER BY c DESC LIMIT 5

┌─UserId─┬─────c─┐
│  22656 │ 19334 │
│   6309 │ 10516 │
│ 100297 │  7848 │
│ 157882 │  7574 │
│  29407 │  6512 │
└────────┴───────┘
```

Денормализация 19 тысяч объектов в одну строку вряд ли реалистична. Эту связь лучше оставить в виде отдельных таблиц или добавить агрегированную статистику.

> При необходимости можно денормализовать статистику по значкам в таблицу пользователей, например, количество значков. Такой пример рассматривается при использовании словарей для этого набора данных во время вставки.

### Posts и PostLinks {#posts-and-postlinks}

`PostLinks` связывают публикации `Posts`, которые пользователи считают связанными или дублирующимися. Следующий запрос показывает схему и команду загрузки:

```sql
CREATE TABLE postlinks
(
  `Id` UInt64,
  `CreationDate` DateTime64(3, 'UTC'),
  `PostId` Int32,
  `RelatedPostId` Int32,
  `LinkTypeId` Enum('Linked' = 1, 'Duplicate' = 3)
)
ENGINE = MergeTree
ORDER BY (PostId, RelatedPostId)

INSERT INTO postlinks SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/postlinks.parquet')

0 rows in set. Elapsed: 4.726 sec. Processed 6.55 million rows, 129.70 MB (1.39 million rows/s., 27.44 MB/s.)
```

Можно убедиться, что ни одна публикация не имеет чрезмерного количества ссылок, препятствующих денормализации:

```sql
SELECT PostId, count() AS c
FROM postlinks
GROUP BY PostId
ORDER BY c DESC LIMIT 5

┌───PostId─┬───c─┐
│ 22937618 │ 125 │
│  9549780 │ 120 │
│  3737139 │ 109 │
│ 18050071 │ 103 │
│ 25889234 │  82 │
└──────────┴─────┘
```

Кроме того, эти ссылки не являются событиями, которые происходят слишком часто:


```sql
SELECT
  round(avg(c)) AS avg_votes_per_hr,
  round(avg(posts)) AS avg_posts_per_hr
FROM
(
  SELECT
  toStartOfHour(CreationDate) AS hr,
  count() AS c,
  uniq(PostId) AS posts
  FROM postlinks
  GROUP BY hr
)

┌─avg_votes_per_hr─┬─avg_posts_per_hr─┐
│                54 │                    44     │
└──────────────────┴──────────────────┘
```

Мы используем это в качестве примера денормализации ниже.

### Пример простой статистики {#simple-statistic-example}

В большинстве случаев денормализация требует добавления одного столбца или статистики к родительской строке. Например, мы можем захотеть обогатить наши посты количеством дублирующихся постов, и для этого достаточно добавить столбец.

```sql
CREATE TABLE posts_with_duplicate_count
(
  `Id` Int32 CODEC(Delta(4), ZSTD(1)),
   ... -другие столбцы
   `DuplicatePosts` UInt16
) ENGINE = MergeTree
ORDER BY (PostTypeId, toDate(CreationDate), CommentCount)
```

Для заполнения этой таблицы мы используем `INSERT INTO SELECT`, объединяя статистику дубликатов с нашими постами.

```sql
INSERT INTO posts_with_duplicate_count SELECT
    posts.*,
    DuplicatePosts
FROM posts AS posts
LEFT JOIN
(
    SELECT PostId, countIf(LinkTypeId = 'Duplicate') AS DuplicatePosts
    FROM postlinks
    GROUP BY PostId
) AS postlinks ON posts.Id = postlinks.PostId
```

### Использование сложных типов для связей один-ко-многим {#exploiting-complex-types-for-one-to-many-relationships}

Для выполнения денормализации часто необходимо использовать сложные типы. Если денормализуется связь один-к-одному с небольшим количеством столбцов, пользователи могут просто добавить их как строки с исходными типами, как показано выше. Однако это часто нежелательно для больших объектов и невозможно для связей один-ко-многим.

В случае сложных объектов или связей один-ко-многим пользователи могут использовать:

- Именованные кортежи (Named Tuples) — позволяют представить связанную структуру в виде набора столбцов.
- Array(Tuple) или Nested — массив именованных кортежей, также известный как Nested, где каждая запись представляет объект. Применимо к связям один-ко-многим.

В качестве примера ниже мы демонстрируем денормализацию `PostLinks` в `Posts`.

Каждый пост может содержать несколько ссылок на другие посты, как показано в схеме `PostLinks` ранее. В виде типа Nested мы можем представить эти связанные и дублирующиеся посты следующим образом:

```sql
SET flatten_nested=0
CREATE TABLE posts_with_links
(
  `Id` Int32 CODEC(Delta(4), ZSTD(1)),
   ... -другие столбцы
   `LinkedPosts` Nested(CreationDate DateTime64(3, 'UTC'), PostId Int32),
   `DuplicatePosts` Nested(CreationDate DateTime64(3, 'UTC'), PostId Int32),
) ENGINE = MergeTree
ORDER BY (PostTypeId, toDate(CreationDate), CommentCount)
```

> Обратите внимание на использование настройки `flatten_nested=0`. Мы рекомендуем отключить выравнивание вложенных данных.

Мы можем выполнить эту денормализацию, используя `INSERT INTO SELECT` с запросом `OUTER JOIN`:

```sql
INSERT INTO posts_with_links
SELECT
    posts.*,
    arrayMap(p -> (p.1, p.2), arrayFilter(p -> p.3 = 'Linked' AND p.2 != 0, Related)) AS LinkedPosts,
    arrayMap(p -> (p.1, p.2), arrayFilter(p -> p.3 = 'Duplicate' AND p.2 != 0, Related)) AS DuplicatePosts
FROM posts
LEFT JOIN (
    SELECT
         PostId,
         groupArray((CreationDate, RelatedPostId, LinkTypeId)) AS Related
    FROM postlinks
    GROUP BY PostId
) AS postlinks ON posts.Id = postlinks.PostId

0 rows in set. Elapsed: 155.372 sec. Processed 66.37 million rows, 76.33 GB (427.18 thousand rows/s., 491.25 MB/s.)
Peak memory usage: 6.98 GiB.
```

> Обратите внимание на время выполнения. Нам удалось денормализовать 66 млн строк примерно за 2 минуты. Как мы увидим позже, эту операцию можно запланировать.


Обратите внимание на использование функций `groupArray` для сворачивания `PostLinks` в массив для каждого `PostId` до выполнения соединения. Затем этот массив фильтруется на два подсписка: `LinkedPosts` и `DuplicatePosts`; при этом из них исключаются любые пустые результаты внешнего соединения.

Мы можем выбрать несколько строк, чтобы увидеть нашу новую денормализованную структуру:

```sql
SELECT LinkedPosts, DuplicatePosts
FROM posts_with_links
WHERE (length(LinkedPosts) > 2) AND (length(DuplicatePosts) > 0)
LIMIT 1
FORMAT Vertical

Row 1:
──────
LinkedPosts:    [('2017-04-11 11:53:09.583',3404508),('2017-04-11 11:49:07.680',3922739),('2017-04-11 11:48:33.353',33058004)]
DuplicatePosts: [('2017-04-11 12:18:37.260',3922739),('2017-04-11 12:18:37.260',33058004)]
```


## Оркестрация и планирование денормализации {#orchestrating-and-scheduling-denormalization}

### Пакетная обработка {#batch}

Применение денормализации требует процесса преобразования, в рамках которого она может быть выполнена и оркестрирована.

Выше мы показали, как ClickHouse может использоваться для выполнения этого преобразования после загрузки данных с помощью `INSERT INTO SELECT`. Этот подход подходит для периодических пакетных преобразований.

У пользователей есть несколько вариантов оркестрации этого процесса в ClickHouse при условии, что приемлем периодический процесс пакетной загрузки:

- **[Обновляемые материализованные представления](/materialized-view/refreshable-materialized-view)** - Обновляемые материализованные представления могут использоваться для периодического планирования запроса с отправкой результатов в целевую таблицу. При выполнении запроса представление обеспечивает атомарное обновление целевой таблицы. Это встроенный в ClickHouse способ планирования такой работы.
- **Внешние инструменты** - Использование таких инструментов, как [dbt](https://www.getdbt.com/) и [Airflow](https://airflow.apache.org/), для периодического планирования преобразования. [Интеграция ClickHouse с dbt](/integrations/dbt) обеспечивает атомарное выполнение этого процесса: создается новая версия целевой таблицы, которая затем атомарно заменяет версию, обрабатывающую запросы (с помощью команды [EXCHANGE](/sql-reference/statements/exchange)).

### Потоковая обработка {#streaming}

Пользователи также могут выполнять это за пределами ClickHouse, до вставки данных, используя потоковые технологии, такие как [Apache Flink](https://flink.apache.org/). В качестве альтернативы можно использовать инкрементные [материализованные представления](/guides/developer/cascading-materialized-views) для выполнения этого процесса по мере вставки данных.
