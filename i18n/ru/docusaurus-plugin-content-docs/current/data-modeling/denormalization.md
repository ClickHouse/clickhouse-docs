---
slug: /data-modeling/denormalization
title: Денормализация данных
description: Как использовать денормализацию для улучшения производительности запросов
keywords: ['денормализация данных', 'денормализовать', 'оптимизация запросов']
---

import denormalizationDiagram from '@site/static/images/data-modeling/denormalization-diagram.png';
import denormalizationSchema from '@site/static/images/data-modeling/denormalization-schema.png';


# Денормализация данных

Денормализация данных — это техника в ClickHouse, которая использует плоские таблицы для минимизации латентности запросов, избегая соединений.

## Сравнение нормализованных и денормализованных схем {#comparing-normalized-vs-denormalized-schemas}

Денормализация данных включает в себя намеренное обращение нормализационного процесса с целью оптимизации производительности базы данных для определенных паттернов запросов. В нормализованных базах данных данные разбиты на несколько связанных таблиц для минимизации избыточности и обеспечения целостности данных. Денормализация вновь вводит избыточность, объединяя таблицы, дублируя данные и добавляя вычисляемые поля в либо одну таблицу, либо меньшее количество таблиц — эффективно перемещая любые соединения из времени запроса в время вставки.

Этот процесс снижает потребность в сложных соединениях в момент запроса и может значительно ускорить операции чтения, что делает его идеальным для приложений с высокой нагрузкой на чтение и сложными запросами. Однако это может увеличить сложность операций записи и обслуживания, так как любые изменения дублированных данных должны быть распространены на все экземпляры для поддержания согласованности.

<img src={denormalizationDiagram} class="image" alt="Денормализация в ClickHouse" style={{width: '100%', background: 'none'}} />

<br />

Обычная техника, популяризированная решениями NoSQL, — это денормализация данных при отсутствии поддержки `JOIN`, эффективно храня все статистические данные или связанные строки на родительской строке в виде колонок и вложенных объектов. Например, в примерной схеме для блога мы можем хранить все `Comments` в виде `Array` объектов на соответствующих постах.

## Когда использовать денормализацию {#when-to-use-denormalization}

В общем, мы рекомендуем денормализацию в следующих случаях:

- Денормализуйте таблицы, которые изменяются редко или для которых допустима задержка, прежде чем данные будут доступны для аналитических запросов, т.е. данные могут быть полностью перезагружены партиями.
- Избегайте денормализации отношений многие-ко-многим. Это может привести к необходимости обновления множества строк, если одна строка источника изменится.
- Избегайте денормализации отношений с высокой кардинальностью. Если каждая строка в таблице имеет тысячи связанных записей в другой таблице, эти записи необходимо будет представлять в виде `Array` — либо примитивного типа, либо кортежей. Обычно не рекомендуется использовать массивы с более чем 1000 кортежами.
- Вместо того чтобы денормализовать все колонки в виде вложенных объектов, рассмотрите возможность денормализации только одной статистики с помощью материализованных представлений (см. ниже).

Не всю информацию нужно денормализовать — только ключевую информацию, к которой необходимо часто обращаться.

Работа по денормализации может выполняться либо в ClickHouse, либо на этапе подготовки данных, например, с использованием Apache Flink.

## Избегайте денормализации часто обновляемых данных {#avoid-denormalization-on-frequently-updated-data}

Для ClickHouse денормализация является одним из нескольких вариантов, которые пользователи могут использовать для оптимизации производительности запросов, но должна использоваться с осторожностью. Если данные обновляются часто и должны обновляться в режиме близком к реальному времени, этот подход следует избегать. Используйте его, если основная таблица в основном представляет собой только добавление или может периодически перезагружаться партиями, например, ежедневно.

Как подход, он страдает от одной основной проблемы — производительности записи и обновления данных. Более конкретно, денормализация эффективно смещает ответственность за соединение данных с времени запроса на время загрузки. Хотя это может значительно улучшить производительность запросов, это усложняет загрузку и означает, что конвейеры данных должны повторно вставлять строку в ClickHouse, если какая-либо из строк, которые использовались для её составления, изменяется. Это может означать, что изменение в одной строке источника потенциально означает, что многие строки в ClickHouse нужно обновлять. В сложных схемах, где строки были составлены из сложных соединений, изменение одной строки во вложенном компоненте соединения может потенциально привести к необходимости обновления миллионов строк.

Достижение этого в реальном времени часто нереалистично и требует значительных инженерных усилий из-за двух проблем:

1. Активация правильных операторов соединения, когда строка таблицы изменяется. Это не должно приводить к обновлению всех объектов для соединения — желательно обновлять только те, которые были затронуты. Изменение соединений для фильтрации правильных строк эффективно и достижение этого при высокой пропускной способности требует внешних инструментов или инженерии.
2. Обновление строк в ClickHouse требует тщательного управления, что вводит дополнительную сложность.

<br />

Таким образом, процесс пакетного обновления более распространен, когда все денормализованные объекты периодически перезагружаются.

## Практические случаи для денормализации {#practical-cases-for-denormalization}

Рассмотрим несколько практических примеров, где денормализация может иметь смысл, и другие случаи, когда альтернативные подходы более желательны.

Рассмотрим таблицу `Posts`, которая уже была денормализована с такой статистикой, как `AnswerCount` и `CommentCount` — исходные данные предоставляются в этой форме. На самом деле, мы можем захотеть нормализовать эту информацию, поскольку она, вероятно, будет часто изменяться. Многие из этих колонок также доступны через другие таблицы, например, комментарии к посту доступны через колонку `PostId` и таблицу `Comments`. Для целей примера предполагаем, что посты перезагружаются партийным процессом.

Мы также рассматриваем только денормализацию других таблиц в `Posts`, так как считаем эту таблицу нашей основной для аналитики. Денормализация в другую сторону также будет целесообразна для некоторых запросов, при этом применяются те же соображения, что и выше.

*Для каждого из следующих примеров предполагается, что существует запрос, который требует использования обеих таблиц в соединении.*

### Посты и голоса {#posts-and-votes}

Голоса за посты представлены в отдельных таблицах. Оптимизированная схема для этого показана ниже, а также команда вставки для загрузки данных:

```sql
CREATE TABLE votes
(
	`Id` UInt32,
	`PostId` Int32,
	`VoteTypeId` UInt8,
	`CreationDate` DateTime64(3, 'UTC'),
	`UserId` Int32,
	`BountyAmount` UInt8
)
ENGINE = MergeTree
ORDER BY (VoteTypeId, CreationDate, PostId)

INSERT INTO votes SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/votes/*.parquet')

0 rows in set. Elapsed: 26.272 sec. Processed 238.98 million rows, 2.13 GB (9.10 million rows/s., 80.97 MB/s.)
```

На первый взгляд, эти данные могут быть кандидатами для денормализации в таблицу постов. С этим подходом связано несколько проблем.

Голоса часто добавляются к постам. Хотя это может уменьшиться на пост в течение времени, следующий запрос показывает, что у нас около 40 тыс. голосов в час на 30 тыс. постов.

```sql
SELECT round(avg(c)) AS avg_votes_per_hr, round(avg(posts)) AS avg_posts_per_hr
FROM
(
	SELECT
    	toStartOfHour(CreationDate) AS hr,
    	count() AS c,
    	uniq(PostId) AS posts
	FROM votes
	GROUP BY hr
)

┌─avg_votes_per_hr─┬─avg_posts_per_hr─┐
│        	41759 │        	33322 │
└──────────────────┴──────────────────┘
```

Это можно решить пакетной обработкой, если допустима задержка, но это по-прежнему требует обработки обновлений, если мы периодически не перезагружаем все посты (что вряд ли будет желательным).

Еще более проблематично то, что у некоторых постов очень высокое количество голосов:

```sql
SELECT PostId, concat('https://stackoverflow.com/questions/', PostId) AS url, count() AS c
FROM votes
GROUP BY PostId
ORDER BY c DESC
LIMIT 5

┌───PostId─┬─url──────────────────────────────────────────┬─────c─┐
│ 11227902 │ https://stackoverflow.com/questions/11227902 │ 35123 │
│   927386 │ https://stackoverflow.com/questions/927386   │ 29090 │
│ 11227809 │ https://stackoverflow.com/questions/11227809 │ 27475 │
│   927358 │ https://stackoverflow.com/questions/927358   │ 26409 │
│  2003515 │ https://stackoverflow.com/questions/2003515  │ 25899 │
└──────────┴──────────────────────────────────────────────┴───────┘
```

Основное наблюдение здесь заключается в том, что агрегированные статистические данные голосов для каждого поста будут достаточны для большинства анализов — нам не нужно денормализовать всю информацию о голосах. Например, текущая колонка `Score` представляет такую статистику, т.е. всего голосов минус отрицательные голоса. В идеале мы просто могли бы получать эту статистику в момент запроса с помощью простого запроса (см. [словарей](/dictionary)).

### Пользователи и награды {#users-and-badges}

Теперь давайте рассмотрим наши `Users` и `Badges`:

<img src={denormalizationSchema} class="image" alt="Схема Пользователи и Награды" style={{width: '100%', background: 'none'}} />

<p></p>
Сначала вставим данные с помощью следующей команды:
<p></p>

```sql
CREATE TABLE users
(
    `Id` Int32,
    `Reputation` LowCardinality(String),
    `CreationDate` DateTime64(3, 'UTC') CODEC(Delta(8), ZSTD(1)),
    `DisplayName` String,
    `LastAccessDate` DateTime64(3, 'UTC'),
    `AboutMe` String,
    `Views` UInt32,
    `UpVotes` UInt32,
    `DownVotes` UInt32,
    `WebsiteUrl` String,
    `Location` LowCardinality(String),
    `AccountId` Int32
)
ENGINE = MergeTree
ORDER BY (Id, CreationDate)
```

```sql
CREATE TABLE badges
(
    `Id` UInt32,
    `UserId` Int32,
    `Name` LowCardinality(String),
    `Date` DateTime64(3, 'UTC'),
    `Class` Enum8('Gold' = 1, 'Silver' = 2, 'Bronze' = 3),
    `TagBased` Bool
)
ENGINE = MergeTree
ORDER BY UserId

INSERT INTO users SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/users.parquet')

0 rows in set. Elapsed: 26.229 sec. Processed 22.48 million rows, 1.36 GB (857.21 thousand rows/s., 51.99 MB/s.)

INSERT INTO badges SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/badges.parquet')

0 rows in set. Elapsed: 18.126 sec. Processed 51.29 million rows, 797.05 MB (2.83 миллиона строк/с., 43.97 MB/с.)
```

Хотя пользователи могут часто получать награды, вряд ли это набор данных, который нужно обновлять чаще, чем раз в день. Отношение между наградами и пользователями одно-ко-многим. Возможно, мы можем просто денормализовать награды к пользователям в виде списка кортежей? Хотя это возможно, быстрый обзор на предмет наибольшего количества наград на пользователя показывает, что это не идеальный подход:

```sql
SELECT UserId, count() AS c FROM badges GROUP BY UserId ORDER BY c DESC LIMIT 5

┌─UserId─┬─────c─┐
│  22656 │ 19334 │
│   6309 │ 10516 │
│ 100297 │  7848 │
│ 157882 │  7574 │
│  29407 │  6512 │
└────────┴───────┘
```

Вряд ли реалистично денормализовать 19 тыс. объектов в одну строку. Лучше оставить это отношение как отдельные таблицы или добавить статистику.

> Мы можем захотеть денормализовать статистику наград в пользователей, например, общее количество наград. Мы рассматриваем такой пример, когда используем словари для этого набора данных во время вставки.

### Посты и Постовые ссылки {#posts-and-postlinks}

`PostLinks` соединяют `Posts`, которые пользователи считают связанными или дублированными. Следующий запрос показывает схему и команду загрузки:

```sql
CREATE TABLE postlinks
(
  `Id` UInt64,
  `CreationDate` DateTime64(3, 'UTC'),
  `PostId` Int32,
  `RelatedPostId` Int32,
  `LinkTypeId` Enum('Linked' = 1, 'Duplicate' = 3)
)
ENGINE = MergeTree
ORDER BY (PostId, RelatedPostId)

INSERT INTO postlinks SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/postlinks.parquet')

0 rows in set. Elapsed: 4.726 sec. Processed 6.55 million rows, 129.70 MB (1.39 million rows/s., 27.44 MB/s.)
```

Мы можем подтвердить, что у постов нет чрезмерного количества ссылок, что препятствует денормализации:

```sql
SELECT PostId, count() AS c
FROM postlinks
GROUP BY PostId
ORDER BY c DESC LIMIT 5

┌───PostId─┬───c─┐
│ 22937618 │ 125 │
│  9549780 │ 120 │
│  3737139 │ 109 │
│ 18050071 │ 103 │
│ 25889234 │  82 │
└──────────┴─────┘
```

Подобным образом, эти ссылки не являются событиями, которые происходят слишком часто:

```sql
SELECT
  round(avg(c)) AS avg_votes_per_hr,
  round(avg(posts)) AS avg_posts_per_hr
FROM
(
  SELECT
  toStartOfHour(CreationDate) AS hr,
  count() AS c,
  uniq(PostId) AS posts
  FROM postlinks
  GROUP BY hr
)

┌─avg_votes_per_hr─┬─avg_posts_per_hr─┐
│      		 54 │      		 44	│
└──────────────────┴──────────────────┘
```

Мы используем это как наш пример денормализации ниже.

### Простой пример статистики {#simple-statistic-example}

В большинстве случаев денормализация требует добавления одной колонки или статистики к родительской строке. Например, мы можем просто захотеть обогатить наши посты количеством дублирующихся постов, и нам нужно просто добавить колонку.

```sql
CREATE TABLE posts_with_duplicate_count
(
  `Id` Int32 CODEC(Delta(4), ZSTD(1)),
   ... -другие колонки
   `DuplicatePosts` UInt16
) ENGINE = MergeTree
ORDER BY (PostTypeId, toDate(CreationDate), CommentCount)
```

Чтобы заполнить эту таблицу, мы используем `INSERT INTO SELECT`, соединяя нашу дублирующую статистику с нашими постами.

```sql
INSERT INTO posts_with_duplicate_count SELECT
    posts.*,
    DuplicatePosts
FROM posts AS posts
LEFT JOIN
(
    SELECT PostId, countIf(LinkTypeId = 'Duplicate') AS DuplicatePosts
    FROM postlinks
    GROUP BY PostId
) AS postlinks ON posts.Id = postlinks.PostId
```

### Использование сложных типов для отношений один-ко-многим {#exploiting-complex-types-for-one-to-many-relationships}

Чтобы выполнить денормализацию, нам часто нужно использовать сложные типы. Если денормализуется отношение один-к-одному с небольшим количеством колонок, пользователи могут просто добавлять их в виде строк с их оригинальными типами, как показано выше. Однако это часто нежелательно для крупных объектов и невозможно для отношений один-ко-многим.

В случаях сложных объектов или отношений один-ко-многим пользователи могут использовать:

- Именованные кортежи — это позволяет представлять связанную структуру в виде набора колонок.
- Array(Tuple) или Nested — массив именованных кортежей, также известных как Nested, где каждая запись представляет объект. Применимо к отношениям один-ко-многим.

В качестве примера мы демонстрируем денормализацию `PostLinks` в `Posts` ниже.

Каждый пост может содержать множество ссылок на другие посты, как показано в схеме `PostLinks` ранее. Как вложенный тип мы можем представить эти связанные и дублированные посты следующим образом:

```sql
SET flatten_nested=0
CREATE TABLE posts_with_links
(
  `Id` Int32 CODEC(Delta(4), ZSTD(1)),
   ... -другие колонки
   `LinkedPosts` Nested(CreationDate DateTime64(3, 'UTC'), PostId Int32),
   `DuplicatePosts` Nested(CreationDate DateTime64(3, 'UTC'), PostId Int32),
) ENGINE = MergeTree
ORDER BY (PostTypeId, toDate(CreationDate), CommentCount)
```

> Обратите внимание на использование настройки `flatten_nested=0`. Мы рекомендуем отключить выравнивание вложенных данных.

Мы можем выполнить эту денормализацию, используя `INSERT INTO SELECT` с запросом `OUTER JOIN`:

```sql
INSERT INTO posts_with_links
SELECT
    posts.*,
    arrayMap(p -> (p.1, p.2), arrayFilter(p -> p.3 = 'Linked' AND p.2 != 0, Related)) AS LinkedPosts,
    arrayMap(p -> (p.1, p.2), arrayFilter(p -> p.3 = 'Duplicate' AND p.2 != 0, Related)) AS DuplicatePosts
FROM posts
LEFT JOIN (
    SELECT
   	 PostId,
   	 groupArray((CreationDate, RelatedPostId, LinkTypeId)) AS Related
    FROM postlinks
    GROUP BY PostId
) AS postlinks ON posts.Id = postlinks.PostId

0 rows in set. Elapsed: 155.372 sec. Processed 66.37 million rows, 76.33 GB (427.18 thousand rows/s., 491.25 MB/s.)
Peak memory usage: 6.98 GiB.
```

> Обратите внимание на время выполнения. Нам удалось денормализовать 66 миллионов строк за около 2 минут. Как мы увидим позже, это операция, которую мы можем запланировать.

Обратите внимание на использование функции `groupArray`, чтобы свести `PostLinks` к массиву для каждого `PostId` перед соединением. Этот массив затем фильтруется на два подпроизведения: `LinkedPosts` и `DuplicatePosts`, которые также исключают любые пустые результаты из внешнего соединения.

Мы можем выбрать несколько строк, чтобы увидеть нашу новую денормализованную структуру:

```sql
SELECT LinkedPosts, DuplicatePosts
FROM posts_with_links
WHERE (length(LinkedPosts) > 2) AND (length(DuplicatePosts) > 0)
LIMIT 1
FORMAT Vertical

Row 1:
──────
LinkedPosts:	[('2017-04-11 11:53:09.583',3404508),('2017-04-11 11:49:07.680',3922739),('2017-04-11 11:48:33.353',33058004)]
DuplicatePosts: [('2017-04-11 12:18:37.260',3922739),('2017-04-11 12:18:37.260',33058004)]
```

## Оркестрация и планирование денормализации {#orchestrating-and-scheduling-denormalization}

### Пакет {#batch}

Использование денормализации требует процесса трансформации, в рамках которого это может быть выполнено и оркестрировано.

Мы показали выше, как ClickHouse можно использовать для выполнения этой трансформации после загрузки данных через `INSERT INTO SELECT`. Это подходит для периодических пакетных трансформаций.

У пользователей есть несколько вариантов для оркестрации этого в ClickHouse, если приемлем процесс периодической пакетной загрузки:

- **[Обновляемые материализованные представления](/materialized-view/refreshable-materialized-view)** — обновляемые материализованные представления могут использоваться для периодического планирования запроса, результаты которого отправляются в целевую таблицу. При выполнении запроса представление гарантирует атомарное обновление целевой таблицы. Это предоставляет нативный способ ClickHouse для планирования этой работы.
- **Внешние инструменты** — использование таких инструментов, как [dbt](https://www.getdbt.com/) и [Airflow](https://airflow.apache.org/) для периодического планирования трансформации. [Интеграция ClickHouse для dbt](/integrations/dbt) гарантирует, что это выполняется атомарно с новой версией целевой таблицы, созданной и затем атомарно замененной версией, получающей запросы (через команду [EXCHANGE](/sql-reference/statements/exchange)).

### Потоковая обработка {#streaming}

Пользователи могут также предпочесть выполнять это вне ClickHouse, перед вставкой, используя поточные технологии, такие как [Apache Flink](https://flink.apache.org/). В качестве альтернативы можно использовать инкрементальные [материализованные представления](/guides/developer/cascading-materialized-views) для выполнения этого процесса, когда данные вставляются.
