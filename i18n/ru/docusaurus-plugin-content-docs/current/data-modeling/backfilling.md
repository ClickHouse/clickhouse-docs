---
slug: /data-modeling/backfilling
title: 'Дозагрузка данных'
description: 'Как выполнять дозагрузку больших объемов данных в ClickHouse'
keywords: ['materialized views', 'backfilling', 'inserting data', 'resilient data load']
doc_type: 'guide'
---

import nullTableMV from '@site/static/images/data-modeling/null_table_mv.png';
import Image from '@theme/IdealImage';


# Дозагрузка данных

Независимо от того, являетесь ли вы новым пользователем ClickHouse или отвечаете за существующий деплоймент, рано или поздно вам понадобится дозаполнить таблицы историческими данными. В некоторых случаях это относительно просто, но задача усложняется, когда требуется заполнить материализованные представления. В этом руководстве описаны несколько подходов к решению этой задачи, которые вы можете адаптировать под свои сценарии.

:::note
В этом руководстве предполагается, что пользователи уже знакомы с концепцией [Incremental Materialized Views](/materialized-view/incremental-materialized-view) и [загрузки данных с использованием табличных функций, таких как s3 и gcs](/integrations/s3). Мы также рекомендуем ознакомиться с нашим руководством по [оптимизации производительности вставки из объектного хранилища](/integrations/s3/performance), рекомендации из которого можно применять к операциям вставки данных, описанным в этом руководстве.
:::



## Пример набора данных {#example-dataset}

В этом руководстве используется набор данных PyPI. Каждая строка в этом наборе данных представляет собой загрузку пакета Python с помощью такого инструмента, как `pip`.

Например, подмножество охватывает один день — `2024-12-17` и доступно публично по адресу `https://datasets-documentation.s3.eu-west-3.amazonaws.com/pypi/2024-12-17/`. Пользователи могут выполнить запрос:

```sql
SELECT count()
FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/pypi/2024-12-17/*.parquet')

┌────count()─┐
│ 2039988137 │ -- 2,04 миллиарда
└────────────┘

1 row in set. Elapsed: 32.726 sec. Processed 2.04 billion rows, 170.05 KB (62.34 million rows/s., 5.20 KB/s.)
Пиковое использование памяти: 239.50 MiB.
```

Полный набор данных для этого бакета содержит более 320 ГБ файлов parquet. В примерах ниже мы намеренно обращаемся к подмножествам, используя glob-шаблоны.

Предполагается, что пользователь потребляет поток этих данных, например, из Kafka или объектного хранилища, для данных после этой даты. Схема этих данных показана ниже:

```sql
DESCRIBE TABLE s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/pypi/2024-12-17/*.parquet')
FORMAT PrettyCompactNoEscapesMonoBlock
SETTINGS describe_compact_output = 1

┌─name───────────────┬─type────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ timestamp │ Nullable(DateTime64(6))                                                                                                                 │
│ country_code       │ Nullable(String)                                                                                                                        │
│ url │ Nullable(String)                                                                                                                        │
│ project            │ Nullable(String)                                                                                                                        │
│ file │ Tuple(filename Nullable(String), project Nullable(String), version Nullable(String), type Nullable(String))                             │
│ installer          │ Tuple(name Nullable(String), version Nullable(String))                                                                                  │
│ python             │ Nullable(String)                                                                                                                        │
│ implementation     │ Tuple(name Nullable(String), version Nullable(String))                                                                                  │
│ distro             │ Tuple(name Nullable(String), version Nullable(String), id Nullable(String), libc Tuple(lib Nullable(String), version Nullable(String))) │
│ system │ Tuple(name Nullable(String), release Nullable(String))                                                                                  │
│ cpu                │ Nullable(String)                                                                                                                        │
│ openssl_version    │ Nullable(String)                                                                                                                        │
│ setuptools_version │ Nullable(String)                                                                                                                        │
│ rustc_version      │ Nullable(String)                                                                                                                        │
│ tls_protocol       │ Nullable(String)                                                                                                                        │
│ tls_cipher         │ Nullable(String)                                                                                                                        │
└────────────────────┴─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

:::note
Полный набор данных PyPI, состоящий из более чем 1 триллиона строк, доступен в нашей публичной демонстрационной среде [clickpy.clickhouse.com](https://clickpy.clickhouse.com). Для получения дополнительной информации об этом наборе данных, включая то, как демонстрация использует материализованные представления для повышения производительности и как данные пополняются ежедневно, см. [здесь](https://github.com/ClickHouse/clickpy).
:::


## Сценарии загрузки исторических данных {#backfilling-scenarios}

Загрузка исторических данных обычно требуется, когда поток данных начинает обрабатываться с определенного момента времени. Эти данные вставляются в таблицы ClickHouse с использованием [инкрементных материализованных представлений](/materialized-view/incremental-materialized-view), которые срабатывают при вставке блоков. Эти представления могут преобразовывать данные перед вставкой или вычислять агрегаты и отправлять результаты в целевые таблицы для последующего использования в приложениях-потребителях.

Мы рассмотрим следующие сценарии:

1. **Загрузка исторических данных при существующем потоке данных** - Загружаются новые данные, и необходимо загрузить исторические данные. Эти исторические данные уже определены.
2. **Добавление материализованных представлений к существующим таблицам** - Необходимо добавить новые материализованные представления к конфигурации, для которой исторические данные уже загружены и данные уже поступают в потоковом режиме.

Мы предполагаем, что исторические данные будут загружаться из объектного хранилища. Во всех случаях мы стремимся избежать пауз при вставке данных.

Мы рекомендуем загружать исторические данные из объектного хранилища. По возможности данные следует экспортировать в формат Parquet для оптимальной производительности чтения и сжатия (уменьшение объема передачи по сети). Обычно предпочтителен размер файла около 150 МБ, но ClickHouse поддерживает более [70 форматов файлов](/interfaces/formats) и способен обрабатывать файлы любого размера.


## Использование дублирующих таблиц и представлений {#using-duplicate-tables-and-views}

Во всех сценариях мы используем концепцию «дублирующих таблиц и представлений». Эти таблицы и представления являются копиями тех, которые используются для потоковых данных в реальном времени, и позволяют выполнять обратное заполнение изолированно с простым способом восстановления в случае сбоя. Например, у нас есть следующая основная таблица `pypi` и материализованное представление, которое вычисляет количество загрузок для каждого проекта Python:

```sql
CREATE TABLE pypi
(
    `timestamp` DateTime,
    `country_code` LowCardinality(String),
    `project` String,
    `type` LowCardinality(String),
    `installer` LowCardinality(String),
    `python_minor` LowCardinality(String),
    `system` LowCardinality(String),
    `on` String
)
ENGINE = MergeTree
ORDER BY (project, timestamp)

CREATE TABLE pypi_downloads
(
    `project` String,
    `count` Int64
)
ENGINE = SummingMergeTree
ORDER BY project

CREATE MATERIALIZED VIEW pypi_downloads_mv TO pypi_downloads
AS SELECT
 project,
    count() AS count
FROM pypi
GROUP BY project
```

Заполним основную таблицу и связанное представление подмножеством данных:

```sql
INSERT INTO pypi SELECT *
FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/pypi/2024-12-17/1734393600-000000000{000..100}.parquet')

0 rows in set. Elapsed: 15.702 sec. Processed 41.23 million rows, 3.94 GB (2.63 million rows/s., 251.01 MB/s.)
Peak memory usage: 977.49 MiB.

SELECT count() FROM pypi

┌──count()─┐
│ 20612750 │ -- 20,61 миллиона
└──────────┘

1 row in set. Elapsed: 0.004 sec.

SELECT sum(count)
FROM pypi_downloads

┌─sum(count)─┐
│   20612750 │ -- 20.61 million
└────────────┘

1 row in set. Elapsed: 0.006 sec. Processed 96.15 thousand rows, 769.23 KB (16.53 million rows/s., 132.26 MB/s.)
Peak memory usage: 682.38 KiB.
```

Предположим, мы хотим загрузить другое подмножество `{101..200}`. Хотя мы могли бы вставить данные напрямую в `pypi`, мы можем выполнить это обратное заполнение изолированно, создав дублирующие таблицы.

В случае сбоя обратного заполнения мы не повлияем на основные таблицы и можем просто [очистить](/managing-data/truncate) дублирующие таблицы и повторить операцию.

Чтобы создать новые копии этих представлений, можно использовать конструкцию `CREATE TABLE AS` с суффиксом `_v2`:

```sql
CREATE TABLE pypi_v2 AS pypi

CREATE TABLE pypi_downloads_v2 AS pypi_downloads

CREATE MATERIALIZED VIEW pypi_downloads_mv_v2 TO pypi_downloads_v2
AS SELECT
 project,
    count() AS count
FROM pypi_v2
GROUP BY project
```

Заполним её вторым подмножеством примерно того же размера и подтвердим успешную загрузку.

```sql
INSERT INTO pypi_v2 SELECT *
FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/pypi/2024-12-17/1734393600-000000000{101..200}.parquet')

0 rows in set. Elapsed: 17.545 sec. Processed 40.80 million rows, 3.90 GB (2.33 million rows/s., 222.29 MB/s.)
Peak memory usage: 991.50 MiB.

SELECT count()
FROM pypi_v2

┌──count()─┐
│ 20400020 │ -- 20,40 миллиона
└──────────┘

1 row in set. Elapsed: 0.004 sec.

SELECT sum(count)
FROM pypi_downloads_v2

┌─sum(count)─┐
│   20400020 │ -- 20.40 million
└────────────┘

1 row in set. Elapsed: 0.006 sec. Processed 95.49 thousand rows, 763.90 KB (14.81 million rows/s., 118.45 MB/s.)
Peak memory usage: 688.77 KiB.
```


Если на каком-то этапе этой второй загрузки произошёл сбой, мы могли бы просто [truncate](/managing-data/truncate) таблицы `pypi_v2` и `pypi_downloads_v2` и повторить загрузку данных.

После завершения загрузки данных мы можем переместить данные из дублирующих таблиц в основные таблицы с помощью оператора [`ALTER TABLE MOVE PARTITION`](/sql-reference/statements/alter/partition#move-partition-to-table).

```sql
ALTER TABLE pypi_v2 MOVE PARTITION () TO pypi

0 строк в наборе. Прошло: 1.401 сек.

ALTER TABLE pypi_downloads_v2 MOVE PARTITION () TO pypi_downloads

0 строк в наборе. Прошло: 0.389 сек.
```

:::note Имена партиций
Приведённый выше вызов `MOVE PARTITION` использует имя партиции `()`. Оно обозначает единственную партицию для этой таблицы (которая не разбита на партиции). Для таблиц, разбитых на партиции, пользователям потребуется выполнить несколько вызовов `MOVE PARTITION` — по одному для каждой партиции. Имена текущих партиций можно получить из таблицы [`system.parts`](/operations/system-tables/parts), например: `SELECT DISTINCT partition FROM system.parts WHERE (table = 'pypi_v2')`.
:::

Теперь мы можем убедиться, что `pypi` и `pypi_downloads` содержат полные данные. Таблицы `pypi_downloads_v2` и `pypi_v2` можно безопасно удалить.

```sql
SELECT count()
FROM pypi

┌──count()─┐
│ 41012770 │ -- 41,01 миллиона
└──────────┘

Получена 1 строка. Затрачено: 0.003 сек.

SELECT sum(count)
FROM pypi_downloads

┌─sum(count)─┐
│   41012770 │ -- 41,01 миллиона
└────────────┘

Получена 1 строка. Затрачено: 0.007 сек. Обработано 191,64 тыс. строк, 1,53 МБ (27,34 млн строк/сек., 218,74 МБ/сек.)

SELECT count()
FROM pypi_v2
```

Важно, что операция `MOVE PARTITION` одновременно и легковесна (использует жёсткие ссылки), и атомарна, то есть либо завершается неудачей, либо успешно, без промежуточных состояний.

Мы активно используем этот механизм в описанных ниже сценариях догрузки данных.

Обратите внимание, что этот процесс требует от пользователей выбора размера каждой операции вставки.

Более крупные вставки, то есть большее число строк, означают меньшее количество операций `MOVE PARTITION`. Однако это должно быть сбалансировано с затратами на восстановление в случае неудачной вставки, например из‑за сетевого сбоя. Пользователи могут дополнить этот процесс пакетированием файлов для снижения риска. Это можно сделать либо с помощью диапазонных запросов, например `WHERE timestamp BETWEEN 2024-12-17 09:00:00 AND 2024-12-17 10:00:00`, либо с помощью glob-шаблонов. Например,

```sql
INSERT INTO pypi_v2 SELECT *
FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/pypi/2024-12-17/1734393600-000000000{101..200}.parquet')
INSERT INTO pypi_v2 SELECT *
FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/pypi/2024-12-17/1734393600-000000000{201..300}.parquet')
INSERT INTO pypi_v2 SELECT *
FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/pypi/2024-12-17/1734393600-000000000{301..400}.parquet')
--продолжается до загрузки всех файлов ИЛИ выполнения команды MOVE PARTITION
```

:::note
ClickPipes использует этот подход при загрузке данных из объектного хранилища, автоматически создавая дубликаты целевой таблицы и её материализованных представлений и избавляя пользователя от необходимости выполнять описанные выше шаги. Дополнительно, за счёт использования нескольких рабочих потоков, каждый из которых обрабатывает собственное подмножество данных (по glob-шаблонам) и имеет свои дублирующие таблицы, данные могут загружаться быстро с гарантированной семантикой exactly-once. Тем, кому это интересно, дополнительные подробности можно найти [в этой статье в блоге](https://clickhouse.com/blog/supercharge-your-clickhouse-data-loads-part3).
:::


## Сценарий 1: Обратная загрузка данных при существующем процессе приёма данных {#scenario-1-backfilling-data-with-existing-data-ingestion}

В этом сценарии предполагается, что данные для обратной загрузки находятся не в изолированном бакете, поэтому требуется фильтрация. Данные уже поступают, и можно определить столбец с временной меткой или монотонно возрастающий столбец, с которого необходимо выполнить обратную загрузку исторических данных.

Процесс состоит из следующих шагов:

1. Определите контрольную точку — временную метку или значение столбца, с которого необходимо восстановить исторические данные.
2. Создайте дубликаты основной таблицы и целевых таблиц для материализованных представлений.
3. Создайте копии всех материализованных представлений, указывающих на целевые таблицы, созданные на шаге (2).
4. Выполните вставку в дублированную основную таблицу, созданную на шаге (2).
5. Переместите все партиции из дублированных таблиц в их исходные версии. Удалите дублированные таблицы.

Например, предположим, что у нас загружены данные PyPI. Мы можем определить минимальную временную метку и, таким образом, нашу «контрольную точку».

```sql
SELECT min(timestamp)
FROM pypi

┌──────min(timestamp)─┐
│ 2024-12-17 09:00:00 │
└─────────────────────┘

1 row in set. Elapsed: 0.163 sec. Processed 1.34 billion rows, 5.37 GB (8.24 billion rows/s., 32.96 GB/s.)
Peak memory usage: 227.84 MiB.
```

Из приведённого выше результата видно, что необходимо загрузить данные до `2024-12-17 09:00:00`. Используя описанный ранее процесс, создаём дублированные таблицы и представления и загружаем подмножество данных с применением фильтра по временной метке.

```sql
CREATE TABLE pypi_v2 AS pypi

CREATE TABLE pypi_downloads_v2 AS pypi_downloads

CREATE MATERIALIZED VIEW pypi_downloads_mv_v2 TO pypi_downloads_v2
AS SELECT project, count() AS count
FROM pypi_v2
GROUP BY project

INSERT INTO pypi_v2 SELECT *
FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/pypi/2024-12-17/1734393600-*.parquet')
WHERE timestamp < '2024-12-17 09:00:00'

0 rows in set. Elapsed: 500.152 sec. Processed 2.74 billion rows, 364.40 GB (5.47 million rows/s., 728.59 MB/s.)
```

:::note
Фильтрация по столбцам временных меток в Parquet может быть очень эффективной. ClickHouse будет читать только столбец временной метки для определения полных диапазонов данных для загрузки, минимизируя сетевой трафик. Индексы Parquet, такие как min-max, также могут использоваться движком запросов ClickHouse.
:::

После завершения вставки можно переместить соответствующие партиции.

```sql
ALTER TABLE pypi_v2 MOVE PARTITION () TO pypi

ALTER TABLE pypi_downloads_v2 MOVE PARTITION () TO pypi_downloads
```

Если исторические данные находятся в изолированном бакете, указанный выше фильтр по времени не требуется. Если столбец времени или монотонный столбец недоступен, изолируйте исторические данные.

:::note Просто используйте ClickPipes в ClickHouse Cloud
Пользователям ClickHouse Cloud следует использовать ClickPipes для восстановления исторических резервных копий, если данные могут быть изолированы в отдельном бакете (и фильтр не требуется). Помимо распараллеливания загрузки с использованием нескольких воркеров, что сокращает время загрузки, ClickPipes автоматизирует описанный выше процесс — создание дублированных таблиц как для основной таблицы, так и для материализованных представлений.
:::


## Сценарий 2: Добавление материализованных представлений к существующим таблицам {#scenario-2-adding-materialized-views-to-existing-tables}

Нередко возникает необходимость добавить новые материализованные представления в систему, в которой уже накоплен значительный объём данных и продолжается вставка новых данных. В этом случае полезно иметь столбец с временной меткой или монотонно возрастающий столбец, который можно использовать для определения точки в потоке данных — это позволяет избежать приостановки приёма данных. В приведённых ниже примерах мы рассматриваем оба случая, отдавая предпочтение подходам, которые не требуют приостановки приёма данных.

:::note Избегайте POPULATE
Мы не рекомендуем использовать команду [`POPULATE`](/sql-reference/statements/create/view#materialized-view) для обратного заполнения материализованных представлений, за исключением небольших наборов данных, где приём данных приостановлен. Этот оператор может пропустить строки, вставленные в исходную таблицу после создания материализованного представления, когда заполнение уже завершено. Кроме того, populate обрабатывает все данные и уязвим к прерываниям или ограничениям памяти на больших наборах данных.
:::

### Доступен столбец с временной меткой или монотонно возрастающий столбец {#timestamp-or-monotonically-increasing-column-available}

В этом случае мы рекомендуем, чтобы новое материализованное представление включало фильтр, ограничивающий строки теми, у которых значение больше произвольной даты в будущем. Затем материализованное представление можно обратно заполнить с этой даты, используя исторические данные из основной таблицы. Подход к обратному заполнению зависит от размера данных и сложности связанного запроса.

Наш простейший подход включает следующие шаги:

1. Создать материализованное представление с фильтром, который учитывает только строки с временной меткой больше произвольного времени в ближайшем будущем.
2. Выполнить запрос `INSERT INTO SELECT`, который вставляет данные в целевую таблицу материализованного представления, читая из исходной таблицы с агрегирующим запросом представления.

Это можно дополнительно улучшить, обрабатывая подмножества данных на шаге (2) и/или используя дублирующую целевую таблицу для материализованного представления (присоединить партиции к оригинальной таблице после завершения вставки) для более простого восстановления после сбоя.

Рассмотрим следующее материализованное представление, которое вычисляет наиболее популярные проекты по часам.

```sql
CREATE TABLE pypi_downloads_per_day
(
    `hour` DateTime,
    `project` String,
    `count` Int64
)
ENGINE = SummingMergeTree
ORDER BY (project, hour)

CREATE MATERIALIZED VIEW pypi_downloads_per_day_mv TO pypi_downloads_per_day
AS SELECT
 toStartOfHour(timestamp) as hour,
 project,
    count() AS count
FROM pypi
GROUP BY
    hour,
 project
```

Хотя мы можем добавить целевую таблицу, перед добавлением материализованного представления мы изменяем его секцию `SELECT`, чтобы включить фильтр, который учитывает только строки с временной меткой больше произвольного времени в ближайшем будущем — в данном случае мы предполагаем, что `2024-12-17 09:00:00` находится через несколько минут в будущем.

```sql
CREATE MATERIALIZED VIEW pypi_downloads_per_day_mv TO pypi_downloads_per_day
AS SELECT
 toStartOfHour(timestamp) AS hour,
 project, count() AS count
FROM pypi WHERE timestamp >= '2024-12-17 09:00:00'
GROUP BY hour, project
```

После добавления этого представления мы можем обратно заполнить все данные для материализованного представления до этой даты.

Простейший способ сделать это — просто выполнить запрос из материализованного представления на основной таблице с фильтром, который игнорирует недавно добавленные данные, вставляя результаты в целевую таблицу представления через `INSERT INTO SELECT`. Например, для приведённого выше представления:

```sql
INSERT INTO pypi_downloads_per_day SELECT
 toStartOfHour(timestamp) AS hour,
 project,
    count() AS count
FROM pypi
WHERE timestamp < '2024-12-17 09:00:00'
GROUP BY
    hour,
 project

Ok.

0 rows in set. Elapsed: 2.830 sec. Processed 798.89 million rows, 17.40 GB (282.28 million rows/s., 6.15 GB/s.)
Peak memory usage: 543.71 MiB.
```

:::note
В приведённом выше примере наша целевая таблица имеет движок [SummingMergeTree](/engines/table-engines/mergetree-family/summingmergetree). В этом случае мы можем просто использовать наш исходный агрегирующий запрос. Для более сложных случаев использования, которые задействуют [AggregatingMergeTree](/engines/table-engines/mergetree-family/aggregatingmergetree), пользователи будут использовать функции `-State` для агрегатов. Пример этого можно найти [здесь](/integrations/s3/performance#be-aware-of-merges).
:::


В нашем случае это относительно лёгкая агрегация, которая завершается менее чем за 3 секунды и использует менее 600 МиБ памяти. Для более сложных или длительных агрегаций пользователи могут сделать этот процесс более устойчивым, используя описанный ранее подход с дублирующей таблицей, т.е. создать теневую целевую таблицу, например `pypi_downloads_per_day_v2`, вставить в неё данные и присоединить полученные партиции к `pypi_downloads_per_day`.

Часто запрос материализованного представления может быть более сложным (что неудивительно, иначе пользователи не стали бы использовать представление!) и потреблять ресурсы. В более редких случаях ресурсы для запроса превышают возможности сервера. Это подчёркивает одно из преимуществ материализованных представлений ClickHouse — они инкрементальны и не обрабатывают весь набор данных за один раз!

В этом случае у пользователей есть несколько вариантов:

1. Изменить запрос для заполнения диапазонов, например `WHERE timestamp BETWEEN 2024-12-17 08:00:00 AND 2024-12-17 09:00:00`, `WHERE timestamp BETWEEN 2024-12-17 07:00:00 AND 2024-12-17 08:00:00` и т.д.
2. Использовать [движок таблиц Null](/engines/table-engines/special/null) для заполнения материализованного представления. Это воспроизводит типичное инкрементальное заполнение материализованного представления, выполняя его запрос над блоками данных (настраиваемого размера).

Вариант (1) представляет собой простейший подход и часто является достаточным. Мы не приводим примеры для краткости.

Вариант (2) мы рассмотрим подробнее ниже.

#### Использование движка таблиц Null для заполнения материализованных представлений {#using-a-null-table-engine-for-filling-materialized-views}

[Движок таблиц Null](/engines/table-engines/special/null) предоставляет движок хранения, который не сохраняет данные (можно представить его как `/dev/null` в мире движков таблиц). Хотя это может показаться противоречивым, материализованные представления всё равно будут выполняться для данных, вставляемых в эту таблицу. Это позволяет создавать материализованные представления без сохранения исходных данных — избегая операций ввода-вывода и связанного с ними хранилища.

Важно отметить, что любые материализованные представления, присоединённые к этому движку таблиц, всё равно выполняются над блоками данных при их вставке — отправляя результаты в целевую таблицу. Эти блоки имеют настраиваемый размер. Хотя более крупные блоки потенциально могут быть более эффективными (и быстрее обрабатываться), они потребляют больше ресурсов (в основном памяти). Использование этого движка таблиц означает, что мы можем строить материализованное представление инкрементально, т.е. по одному блоку за раз, избегая необходимости хранить всю агрегацию в памяти.

<Image img={nullTableMV} size='md' alt='Денормализация в ClickHouse' />

<br />

Рассмотрим следующий пример:

```sql
CREATE TABLE pypi_v2
(
    `timestamp` DateTime,
    `project` String
)
ENGINE = Null

CREATE MATERIALIZED VIEW pypi_downloads_per_day_mv_v2 TO pypi_downloads_per_day
AS SELECT
 toStartOfHour(timestamp) as hour,
 project,
    count() AS count
FROM pypi_v2
GROUP BY
    hour,
 project
```

Здесь мы создаём таблицу Null `pypi_v2` для получения строк, которые будут использоваться для построения материализованного представления. Обратите внимание, как мы ограничиваем схему только необходимыми столбцами. Наше материализованное представление выполняет агрегацию над строками, вставляемыми в эту таблицу (по одному блоку за раз), отправляя результаты в целевую таблицу `pypi_downloads_per_day`.

:::note
Мы использовали `pypi_downloads_per_day` в качестве целевой таблицы. Для дополнительной устойчивости пользователи могут создать дублирующую таблицу `pypi_downloads_per_day_v2` и использовать её в качестве целевой таблицы представления, как показано в предыдущих примерах. После завершения вставки партиции из `pypi_downloads_per_day_v2` могут быть, в свою очередь, перемещены в `pypi_downloads_per_day`. Это позволит восстановиться в случае, если вставка завершится неудачно из-за проблем с памятью или прерываний сервера, т.е. мы просто очищаем `pypi_downloads_per_day_v2`, настраиваем параметры и повторяем попытку.
:::

Чтобы заполнить это материализованное представление, мы просто вставляем соответствующие данные для заполнения в `pypi_v2` из `pypi`.

```sql
INSERT INTO pypi_v2 SELECT timestamp, project FROM pypi WHERE timestamp < '2024-12-17 09:00:00'

0 rows in set. Elapsed: 27.325 sec. Processed 1.50 billion rows, 33.48 GB (54.73 million rows/s., 1.23 GB/s.)
Peak memory usage: 639.47 MiB.
```

Обратите внимание, что использование памяти здесь составляет `639.47 MiB`.


##### Настройка производительности и ресурсов {#tuning-performance--resources}

Несколько факторов определяют производительность и потребление ресурсов в описанном выше сценарии. Перед началом настройки рекомендуем ознакомиться с механизмом вставки данных, подробно описанным в разделе [Using Threads for Reads](/integrations/s3/performance#using-threads-for-reads) руководства [Optimizing for S3 Insert and Read Performance guide](/integrations/s3/performance). Вкратце:

- **Параллелизм чтения** — количество потоков, используемых для чтения. Управляется параметром [`max_threads`](/operations/settings/settings#max_threads). В ClickHouse Cloud это значение определяется размером инстанса и по умолчанию равно количеству vCPU. Увеличение этого значения может улучшить производительность чтения за счёт большего потребления памяти.
- **Параллелизм вставки** — количество потоков, используемых для вставки данных. Управляется параметром [`max_insert_threads`](/operations/settings/settings#max_insert_threads). В ClickHouse Cloud это значение определяется размером инстанса (от 2 до 4), в OSS установлено значение 1. Увеличение этого значения может улучшить производительность за счёт большего потребления памяти.
- **Размер блока вставки** — данные обрабатываются в цикле, где они извлекаются, парсятся и формируются в блоки вставки в памяти на основе [ключа партиционирования](/engines/table-engines/mergetree-family/custom-partitioning-key). Эти блоки сортируются, оптимизируются, сжимаются и записываются в хранилище как новые [куски данных](/parts). Размер блока вставки, управляемый параметрами [`min_insert_block_size_rows`](/operations/settings/settings#min_insert_block_size_rows) и [`min_insert_block_size_bytes`](/operations/settings/settings#min_insert_block_size_bytes) (несжатые данные), влияет на использование памяти и дисковый ввод-вывод. Большие блоки потребляют больше памяти, но создают меньше кусков, снижая нагрузку на ввод-вывод и фоновые слияния. Эти параметры представляют минимальные пороговые значения (достижение любого из них первым инициирует сброс).
- **Размер блока материализованного представления** — помимо описанной выше механики для основной вставки, перед вставкой в материализованные представления блоки также объединяются для более эффективной обработки. Размер этих блоков определяется параметрами [`min_insert_block_size_bytes_for_materialized_views`](/operations/settings/settings#min_insert_block_size_bytes_for_materialized_views) и [`min_insert_block_size_rows_for_materialized_views`](/operations/settings/settings#min_insert_block_size_rows_for_materialized_views). Большие блоки позволяют более эффективно обрабатывать данные за счёт большего потребления памяти. По умолчанию эти параметры принимают значения параметров исходной таблицы [`min_insert_block_size_rows`](/operations/settings/settings#min_insert_block_size_rows) и [`min_insert_block_size_bytes`](/operations/settings/settings#min_insert_block_size_bytes) соответственно.

Для повышения производительности можно следовать рекомендациям, изложенным в разделе [Tuning Threads and Block Size for Inserts](/integrations/s3/performance#tuning-threads-and-block-size-for-inserts) руководства [Optimizing for S3 Insert and Read Performance guide](/integrations/s3/performance). В большинстве случаев не требуется изменять `min_insert_block_size_bytes_for_materialized_views` и `min_insert_block_size_rows_for_materialized_views` для повышения производительности. Если эти параметры изменяются, используйте те же рекомендации, что и для `min_insert_block_size_rows` и `min_insert_block_size_bytes`.

Для минимизации потребления памяти можно поэкспериментировать с этими параметрами. Это неизбежно снизит производительность. Используя предыдущий запрос, приведём примеры ниже.

Снижение `max_insert_threads` до 1 уменьшает потребление памяти.

```sql
INSERT INTO pypi_v2
SELECT
    timestamp,
 project
FROM pypi
WHERE timestamp < '2024-12-17 09:00:00'
SETTINGS max_insert_threads = 1

0 строк в наборе. Затрачено: 27.752 сек. Обработано 1.50 млрд строк, 33.48 ГБ (53.89 млн строк/с., 1.21 ГБ/с.)
Пиковое использование памяти: 506.78 МиБ.
```

Можно ещё больше снизить потребление памяти, уменьшив параметр `max_threads` до 1.

```sql
INSERT INTO pypi_v2
SELECT timestamp, project
FROM pypi
WHERE timestamp < '2024-12-17 09:00:00'
SETTINGS max_insert_threads = 1, max_threads = 1

Ok.

0 строк в наборе. Затрачено: 43.907 сек. Обработано 1.50 млрд строк, 33.48 ГБ (34.06 млн строк/с., 762.54 МБ/с.)
Пиковое использование памяти: 272.53 МиБ.
```


Наконец, мы можем дополнительно снизить потребление памяти, установив `min_insert_block_size_rows` в 0 (отключает его как определяющий фактор размера блока) и `min_insert_block_size_bytes` в 10485760 (10 МиБ).

```sql
INSERT INTO pypi_v2
SELECT
    timestamp,
 project
FROM pypi
WHERE timestamp < '2024-12-17 09:00:00'
SETTINGS max_insert_threads = 1, max_threads = 1, min_insert_block_size_rows = 0, min_insert_block_size_bytes = 10485760

0 rows in set. Elapsed: 43.293 sec. Processed 1.50 billion rows, 33.48 GB (34.54 million rows/s., 773.36 MB/s.)
Peak memory usage: 218.64 MiB.
```

Наконец, имейте в виду, что уменьшение размеров блоков приводит к созданию большего количества частей и увеличивает нагрузку на процесс слияния. Как обсуждалось [здесь](/integrations/s3/performance#be-aware-of-merges), эти настройки следует изменять с осторожностью.

### Отсутствие временной метки или монотонно возрастающего столбца {#no-timestamp-or-monotonically-increasing-column}

Описанные выше процессы предполагают наличие у пользователя временной метки или монотонно возрастающего столбца. В некоторых случаях это просто недоступно. В таком случае мы рекомендуем следующий процесс, который использует многие из ранее описанных шагов, но требует приостановки загрузки данных.

1. Приостановите вставки в основную таблицу.
2. Создайте дубликат основной целевой таблицы, используя синтаксис `CREATE AS`.
3. Присоедините партиции из исходной целевой таблицы к дубликату с помощью [`ALTER TABLE ATTACH`](/sql-reference/statements/alter/partition#attach-partitionpart). **Примечание:** Эта операция присоединения отличается от использованной ранее операции перемещения. Хотя она использует жёсткие ссылки, данные в исходной таблице сохраняются.
4. Создайте новые материализованные представления.
5. Возобновите вставки. **Примечание:** Вставки будут обновлять только целевую таблицу, но не дубликат, который будет ссылаться только на исходные данные.
6. Выполните обратное заполнение материализованного представления, применяя тот же процесс, который использовался выше для данных с временными метками, используя дубликат таблицы в качестве источника.

Рассмотрим следующий пример с использованием PyPI и нашего предыдущего нового материализованного представления `pypi_downloads_per_day` (предположим, что мы не можем использовать временную метку):

```sql
SELECT count() FROM pypi

┌────count()─┐
│ 2039988137 │ -- 2,04 миллиарда
└────────────┘

1 row in set. Elapsed: 0.003 sec.

-- (1) Приостанавливаем вставки
-- (2) Создаём дубликат целевой таблицы

CREATE TABLE pypi_v2 AS pypi

SELECT count() FROM pypi_v2

┌────count()─┐
│ 2039988137 │ -- 2,04 миллиарда
└────────────┘

1 row in set. Elapsed: 0.004 sec.

-- (3) Присоединяем партиции из исходной целевой таблицы к дубликату.

ALTER TABLE pypi_v2
 (ATTACH PARTITION tuple() FROM pypi)

-- (4) Создаём новые материализованные представления

CREATE TABLE pypi_downloads_per_day
(
    `hour` DateTime,
    `project` String,
    `count` Int64
)
ENGINE = SummingMergeTree
ORDER BY (project, hour)

CREATE MATERIALIZED VIEW pypi_downloads_per_day_mv TO pypi_downloads_per_day
AS SELECT
 toStartOfHour(timestamp) as hour,
 project,
    count() AS count
FROM pypi
GROUP BY
    hour,
 project

-- (5) Возобновляем вставки. Здесь мы имитируем это, вставляя одну строку.

INSERT INTO pypi SELECT *
FROM pypi
LIMIT 1

SELECT count() FROM pypi

┌────count()─┐
│ 2039988138 │ -- 2,04 миллиарда
└────────────┘

1 row in set. Elapsed: 0.003 sec.

-- обратите внимание, что pypi_v2 содержит то же количество строк, что и раньше

SELECT count() FROM pypi_v2
┌────count()─┐
│ 2039988137 │ -- 2,04 миллиарда
└────────────┘

-- (6) Выполняем обратное заполнение представления, используя резервную копию pypi_v2

INSERT INTO pypi_downloads_per_day SELECT
 toStartOfHour(timestamp) as hour,
 project,
    count() AS count
FROM pypi_v2
GROUP BY
    hour,
 project

0 rows in set. Elapsed: 3.719 sec. Processed 2.04 billion rows, 47.15 GB (548.57 million rows/s., 12.68 GB/s.)
```


DROP TABLE pypi&#95;v2;

```

На предпоследнем шаге мы заполняем `pypi_downloads_per_day`, используя простой подход `INSERT INTO SELECT`, описанный [ранее](#timestamp-or-monotonically-increasing-column-available). Этот процесс также можно оптимизировать с помощью подхода с таблицей Null, описанного [выше](#using-a-null-table-engine-for-filling-materialized-views), с опциональным использованием дублирующей таблицы для повышения отказоустойчивости.

Хотя эта операция требует приостановки вставок, промежуточные операции обычно выполняются быстро — это минимизирует прерывание потока данных.
```
