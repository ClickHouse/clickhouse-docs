---
slug: /deployment-guides/parallel-replicas
title: Параллельные Реплики
keywords: ['параллельная реплика']
description: 'В этом руководстве мы сначала обсудим, как ClickHouse распределяет запрос на несколько шардов через распределенные таблицы, а затем, как запрос может использовать несколько реплик для его выполнения.'
---

import BetaBadge from '@theme/badges/BetaBadge';
import image_1 from '@site/static/images/deployment-guides/parallel-replicas-1.png'
import image_2 from '@site/static/images/deployment-guides/parallel-replicas-2.png'
import image_3 from '@site/static/images/deployment-guides/parallel-replicas-3.png'
import image_4 from '@site/static/images/deployment-guides/parallel-replicas-4.png'
import image_5 from '@site/static/images/deployment-guides/parallel-replicas-5.png'
import image_6 from '@site/static/images/deployment-guides/parallel-replicas-6.png'
import image_7 from '@site/static/images/deployment-guides/parallel-replicas-7.png'
import image_8 from '@site/static/images/deployment-guides/parallel-replicas-8.png'
import image_9 from '@site/static/images/deployment-guides/parallel-replicas-9.png'

<BetaBadge/>
## Введение {#introduction}

ClickHouse обрабатывает запросы чрезвычайно быстро, но как эти запросы распределяются и параллелизируются на нескольких серверах? 

> В этом руководстве мы сначала обсудим, как ClickHouse распределяет запрос на несколько шардов через распределенные таблицы, а затем, как запрос может использовать несколько реплик для его выполнения.

## Шардированная архитектура {#sharded-architecture}

В архитектуре с полным разделением ресурса кластеры обычно разделены на несколько шардов, каждый из которых содержит подмножество общих данных. Распределенная таблица располагается поверх этих шардов, предоставляя унифицированный вид на полные данные.

Чтения могут быть направлены к локальной таблице. Выполнение запроса происходит только на указанном шарде, или он может быть направлен к распределенной таблице, и в этом случае каждый шард выполнит заданные запросы. Сервер, на котором был запрошен доступ к распределенной таблице, агрегирует данные и отвечает клиенту:

<img src={image_1} alt="шардированная архитектура"/>

На рисунке выше визуализируется, что происходит, когда клиент запрашивает распределенную таблицу:

<ol className="docs-ordered-list">
    <li>
        Запрос select отправляется к распределенной таблице на узле произвольно (по стратегии round-robin или после маршрутизации к конкретному серверу балансировщиком нагрузки). Этот узел теперь будет выступать в роли координатора.
    </li>
    <li>
        Узел находит каждый шард, который должен выполнить запрос, с помощью информации, указанной в распределенной таблице, и запрос отправляется на каждый шард.
    </li>
    <li>
        Каждый шард считывает, фильтрует и агрегирует данные локально, а затем отправляет обратно агрегированное состояние координатору.
    </li>
    <li>
        Координирующий узел объединяет данные и затем отправляет ответ обратно клиенту.
    </li>
</ol>

Когда мы добавляем реплики в процесс, процесс довольно похож, с единственным отличием, что только одна реплика из каждого шарда будет выполнять запрос. Это означает, что больше запросов может быть обработано параллельно.

## Не шардированная архитектура {#non-sharded-architecture}

ClickHouse Cloud имеет совершенно другую архитектуру, чем та, что представлена выше. (Смотрите ["Архитектура ClickHouse Cloud"](https://clickhouse.com/docs/cloud/reference/architecture) для получения более подробной информации). С разделением вычислений и хранения, и с практически бесконечным количеством хранилища, потребность в шардах становится менее важной.

На рисунке ниже показана архитектура ClickHouse Cloud:

<img src={image_2} alt="не шардированная архитектура"/>

Эта архитектура позволяет нам добавлять и удалять реплики практически мгновенно, обеспечивая очень высокую масштабируемость кластера. Кластер ClickHouse Keeper (показан справа) обеспечивает наличие единого источника правды для метаданных. Реплики могут получать метаданные из кластера ClickHouse Keeper и все поддерживают одни и те же данные. Сами данные хранятся в объектном хранилище, а кэш SSD позволяет нам ускорить запросы.

Но как мы теперь можем распределить выполнение запретов между несколькими серверами? В шардированной архитектуре это было довольно очевидно, так как каждый шард мог фактически выполнить запрос на подмножестве данных. Как это работает, когда нет шардирования?

## Введение параллельных реплик {#introducing-parallel-replicas}

Чтобы параллелизовать выполнение запроса через несколько серверов, нам сначала нужно назначить один из наших серверов координатором. Координатор — это тот, кто создает список задач, которые необходимо выполнить, обеспечивает их выполнение, агрегирует их и возвращает результат клиенту. Как и в большинстве распределенных систем, эту роль исполняет узел, который получает первоначальный запрос. Нам также нужно определить единицу работы. В шардированной архитектуре единицей работы является шард, подмножество данных. С параллельными репликами мы будем использовать небольшую долю таблицы, называемую [гранулами](/docs/guides/best-practices/sparse-primary-indexes#data-is-organized-into-granules-for-parallel-data-processing), в качестве единицы работы.

Теперь давайте посмотрим, как это работает на практике с помощью рисунка ниже:

<img src={image_3} alt="Параллельные реплики"/>

С параллельными репликами:

<ol className="docs-ordered-list">
    <li>
        Запрос от клиента отправляется к одному узлу после прохождения через балансировщик нагрузки. Этот узел становится координатором для этого запроса.
    </li>
    <li>
        Узел анализирует индекс каждой части и выбирает правильные части и гранулы для обработки.
    </li>
    <li>
        Координатор делит нагрузку на набор гранул, которые могут быть назначены различным репликам.
    </li>
    <li>
        Каждый набор гранул обрабатывается соответствующими репликами, и агрегируемое состояние отправляется координатору по завершении.
    </li>
    <li>
        Наконец, координатор объединяет все результаты от реплик и затем возвращает ответ клиенту.
    </li>
</ol>

Шаги выше описывают, как работают параллельные реплики в теории. Однако на практике есть множество факторов, которые могут помешать такой логике работать идеально:

<ol className="docs-ordered-list">
    <li>
        Некоторые реплики могут быть недоступны.
    </li>
    <li>
        Репликация в ClickHouse асинхронна, некоторые реплики могут не иметь одинаковых частей в определенный момент времени.
    </li>
    <li>
        Задержка между репликами должна как-то обрабатываться.
    </li>
    <li>
        Кэш файловой системы варьируется от реплики к реплике в зависимости от активности на каждой реплике, что означает, что случайное распределение задач может привести к менее оптимальной производительности с учетом локальности кэша.
    </li>
</ol>

Мы рассматриваем, как эти факторы преодолеваются в следующих разделах.

### Объявления {#announcements}

Чтобы решить (1) и (2) из списка выше, мы ввели концепцию объявления. Давайте визуализируем, как это работает, используя рисунок ниже:

<img src={image_4} alt="Объявления"/>

<ol className="docs-ordered-list">
    <li>
        Запрос от клиента отправляется к одному узлу после прохождения через балансировщик нагрузки. Узел становится координатором для этого запроса.
    </li>
    <li>
        Координирующий узел отправляет запрос для получения объявлений от всех реплик в кластере. Реплики могут иметь слегка разные представления текущего набора частей для таблицы. В результате нам нужно собрать эту информацию, чтобы избежать неправильных решений о расписании.
    </li>
    <li>
        Координирующий узел затем использует объявления для определения набора гранул, которые могут быть назначены различным репликам. Здесь, например, мы видим, что никакие гранулы из части 3 не были назначены реплике 2, поскольку эта реплика не предоставила эту часть в своем объявлении. Также обратите внимание, что никаких задач не было назначено реплике 3, потому что реплика не предоставила объявления.
    </li>
    <li>
        После того как каждая реплика обработала запрос на своем подмножестве гранул и агрегируемое состояние было отправлено обратно координатору, координатор объединяет результаты, и ответ отправляется клиенту.
    </li>
</ol>

### Динамическое координирование {#dynamic-coordination}

Чтобы решить проблему задержки на конце, мы добавили динамическое координирование. Это означает, что все гранулы не отправляются реплике в одном запросе, а каждая реплика сможет запрашивать новую задачу (набор гранул для обработки) у координатора. Координатор предоставит реплике набор гранул на основе полученного объявления.

Предположим, что мы находимся на этапе в процессе, когда все реплики отправили объявление со всеми частями.

На рисунке ниже визуализируется, как работает динамическое координирование:

<img src={image_5} alt="Динамическое координирование - часть 1"/>

<ol className="docs-ordered-list">
    <li>
        Реплики сообщают координатору, что они могут обрабатывать задачи, они также могут указать, сколько работы могут обработать.
    </li>
    <li>
        Координатор назначает задачи репликам.
    </li>
</ol>

<img src={image_6} alt="Динамическое координирование - часть 2"/>

<ol className="docs-ordered-list">
    <li>
        Реплики 1 и 2 могут быстро завершить свою задачу. Они запрашивают новую задачу у координатора.
    </li>
    <li>
        Координатор назначает новые задачи репликам 1 и 2.
    </li>
</ol>

<img src={image_7} alt="Динамическое координирование - часть 3"/>

<ol className="docs-ordered-list">
    <li>
        Все реплики теперь завершили обработку своей задачи. Они запрашивают дополнительные задачи.
    </li>
    <li>
        Координатор, используя объявления, проверяет, какие задачи еще нужно обработать, но нет оставшихся задач.
    </li>
    <li>
        Координатор сообщает репликам, что все было обработано. Он теперь объединит все агрегируемые состояния и ответит на запрос.
    </li>
</ol>

### Управление локальностью кэша {#managing-cache-locality}

Последней потенциальной проблемой является то, как мы обрабатываем локальность кэша. Если запрос выполняется несколько раз, как мы можем гарантировать, что одна и та же задача будет направлена на одну и ту же реплику? В предыдущем примере у нас были следующие назначенные задачи:

<table>
    <thead>
        <tr>
            <th></th>
            <th>Реплика 1</th>
            <th>Реплика 2</th>
            <th>Реплика 3</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Часть 1</td>
            <td>g1, g6, g7</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
        <tr>
            <td>Часть 2</td>
            <td>g1</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
        <tr>
            <td>Часть 3</td>
            <td>g1, g6</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
    </tbody>
</table>

Чтобы гарантировать, что одни и те же задачи назначаются одним и тем же репликам и могут извлечь выгоду из кэша, выполняется две вещи. Вычисляется хэш части + набор гранул (задача). Применяется модуль количества реплик для назначения задачи.

На бумаге это звучит хорошо, но на практике внезапная нагрузка на одну реплику или ухудшение сети могут вызвать задержку на конце, если одна и та же реплика постоянно используется для выполнения определенных задач. Если `max_parallel_replicas` меньше числа реплик, то случайные реплики выбираются для выполнения запроса.

### Кража задач {#task-stealing}

Если какая-то реплика обрабатывает задачи медленнее других, другие реплики попытаются "похитить" задачи, которые принципиально принадлежат этой реплике по хэшу, чтобы уменьшить задержку в конце.

### Ограничения {#limitations}

У этой функции есть известные ограничения, основные из которых документированы в этом разделе.

:::note
Если вы нашли проблему, которая не входит в перечисленные ограничения, и подозреваете, что параллельная реплика является причиной, пожалуйста, создайте проблему на GitHub с помощью метки `comp-parallel-replicas`.
:::

| Ограничение                                    | Описание                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
|-----------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Сложные запросы                                 | В настоящее время параллельная реплика работает довольно хорошо для простых запросов. Сложные слои, такие как CTE, подзапросы, JOIN, сложные запросы и т.д., могут иметь негативное влияние на производительность запроса.                                                                                                                                                                                                                                                                                   |
| Маленькие запросы                                 | Если вы выполняете запрос, который не обрабатывает много строк, его выполнение на нескольких репликах может не дать лучшего времени производительности, поскольку время сети для координации между репликами может привести к дополнительным циклам в выполнении запроса. Вы можете ограничить эти проблемы, используя настройку: [`parallel_replicas_min_number_of_rows_per_replica`](/docs/operations/settings/settings#parallel_replicas_min_number_of_rows_per_replica).  |
| Параллельные реплики отключены с FINAL      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| Данные с высокой кардинальностью и сложная агрегация | Высококардинальная агрегация, которая требует отправки большого количества данных, может значительно замедлить ваши запросы.                                                                                                                                                                                                                                                                                                                                                                     |
| Совместимость с новым анализатором           | Новый анализатор может значительно замедлить или ускорить выполнение запросов в конкретных сценариях.                                                                                                                                                                                                                                                                                                                                                                       |

## Настройки, связанные с параллельными репликами {#settings-related-to-parallel-replicas}

| Настройка                                            | Описание                                                                                                                                                                                                                                                         |
|----------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `enable_parallel_replicas`                         | `0`: отключено<br/> `1`: включено <br/>`2`: Принудительно использовать параллельную реплику, выдаст исключение, если не используется.                                                                                                                                                          |
| `cluster_for_parallel_replicas`                    | Имя кластера, которое будет использоваться для параллельной репликации; если вы используете ClickHouse Cloud, используйте `default`.                                                                                                                                                                 |
| `max_parallel_replicas`                            | Максимальное количество реплик, которые будут использоваться для выполнения запроса на нескольких репликах, если указано число меньшее, чем количество реплик в кластере, узлы будут выбраны случайным образом. Это значение также можно переопределить для учета горизонтального масштабирования. |
| `parallel_replicas_min_number_of_rows_per_replica` | Помогает ограничить количество используемых реплик в зависимости от количества строк, которые необходимо обработать, количество используемых реплик определяется по формуле: <br/> `предполагаемые строки для чтения` / `минимальное количество строк на реплику`.                                                               |
| `allow_experimental_analyzer`                      | `0`: использовать старый анализатор<br/> `1`: использовать новый анализатор. <br/><br/>Поведение параллельных реплик может измениться в зависимости от используемого анализатора.                                                                                                                                    |
## Исследование проблем с параллельными репликами {#investigating-issues-with-parallel-replicas}

Вы можете проверить, какие настройки используются для каждого запроса в таблице 
[`system.query_log`](/docs/operations/system-tables/query_log). Вы также можете
посмотреть таблицу [`system.events`](/docs/operations/system-tables/events)
чтобы увидеть все события, которые произошли на сервере, и вы можете использовать 
функцию таблицы [`clusterAllReplicas`](/docs/sql-reference/table-functions/cluster) для просмотра таблиц на всех репликах
(если вы пользователь облака, используйте `default`).

```sql title="Запрос"
SELECT
   hostname(),
   *
FROM clusterAllReplicas('default', system.events)
WHERE event ILIKE '%ParallelReplicas%'
```
<details>
<summary>Ответ</summary>
```response title="Ответ"
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasHandleRequestMicroseconds      │   438 │ Время, затраченное на обработку запросов на метки от реплик                                               │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   558 │ Время, затраченное на обработку объявлений реплик                                                         │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasReadUnassignedMarks            │   240 │ Сумма по всем репликам количества запланированных нераспределённых меток                                  │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasReadAssignedForStealingMarks   │     4 │ Сумма по всем репликам количества запланированных меток, назначенных для кражи по согласованному хешу │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasStealingByHashMicroseconds     │     5 │ Время, затраченное на сбор сегментов, предназначенных для кражи по хешу                                            │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasProcessingPartsMicroseconds    │     5 │ Время, затраченное на обработку частей данных                                                                     │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     3 │ Время, затраченное на сбор сиротских сегментов                                                              │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasUsedCount                      │     2 │ Количество реплик, использованных для выполнения запроса с параллельными репликами                         │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasAvailableCount                 │     6 │ Количество реплик, доступных для выполнения запроса с параллельными репликами                    │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasHandleRequestMicroseconds      │   698 │ Время, затраченное на обработку запросов на метки от реплик                                               │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   644 │ Время, затраченное на обработку объявлений реплик                                                         │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasReadUnassignedMarks            │   190 │ Сумма по всем репликам количества запланированных нераспределённых меток                                  │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasReadAssignedForStealingMarks   │    54 │ Сумма по всем репликам количества запланированных меток, назначенных для кражи по согласованному хешу │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasStealingByHashMicroseconds     │     8 │ Время, затраченное на сбор сегментов, предназначенных для кражи по хешу                                            │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasProcessingPartsMicroseconds    │     4 │ Время, затраченное на обработку частей данных                                                                     │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     2 │ Время, затраченное на сбор сиротских сегментов                                                              │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasUsedCount                      │     2 │ Количество реплик, использованных для выполнения запроса с параллельными репликами                         │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasAvailableCount                 │     6 │ Количество реплик, доступных для выполнения запроса с параллельными репликами                    │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasHandleRequestMicroseconds      │   620 │ Время, затраченное на обработку запросов на метки от реплик                                               │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   656 │ Время, затраченное на обработку объявлений реплик                                                         │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasReadUnassignedMarks            │     1 │ Сумма по всем репликам количества запланированных нераспределённых меток                                  │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasReadAssignedForStealingMarks   │     1 │ Сумма по всем репликам количества запланированных меток, назначенных для кражи по согласованному хешу │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasStealingByHashMicroseconds     │     4 │ Время, затраченное на сбор сегментов, предназначенных для кражи по хешу                                            │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasProcessingPartsMicroseconds    │     3 │ Время, затраченное на обработку частей данных                                                                     │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     1 │ Время, затраченное на сбор сиротских сегментов                                                              │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasUsedCount                      │     2 │ Количество реплик, использованных для выполнения запроса с параллельными репликами                         │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasAvailableCount                 │    12 │ Количество реплик, доступных для выполнения запроса с параллельными репликами                    │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasHandleRequestMicroseconds      │   696 │ Время, затраченное на обработку запросов на метки от реплик                                               │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   717 │ Время, затраченное на обработку объявлений реплик                                                         │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasReadUnassignedMarks            │     2 │ Сумма по всем репликам количества запланированных нераспределённых меток                                  │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasReadAssignedForStealingMarks   │     2 │ Сумма по всем репликам количества запланированных меток, назначенных для кражи по согласованному хешу │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasStealingByHashMicroseconds     │    10 │ Время, затраченное на сбор сегментов, предназначенных для кражи по хешу                                            │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasProcessingPartsMicroseconds    │     6 │ Время, затраченное на обработку частей данных                                                                     │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     2 │ Время, затраченное на сбор сиротских сегментов                                                              │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasUsedCount                      │     2 │ Количество реплик, использованных для выполнения запроса с параллельными репликами                         │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasAvailableCount                 │    12 │ Количество реплик, доступных для выполнения запроса с параллельными репликами                    │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
```
</details>

Таблица [`system.text_log`](/docs/operations/system-tables/text_log) также 
содержит информацию о выполнении запросов с использованием параллельных реплик:

```sql title="Запрос"
SELECT message
FROM clusterAllReplicas('default', system.text_log)
WHERE query_id = 'ad40c712-d25d-45c4-b1a1-a28ba8d4019c'
ORDER BY event_time_microseconds ASC
```

<details>
<summary>Ответ</summary>
```response title="Ответ"
┌─message────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ (from 54.218.178.249:59198) SELECT * FROM session_events WHERE type='type2' LIMIT 10 SETTINGS allow_experimental_parallel_reading_from_replicas=2; (stage: Complete)                                                                                       │
│ Запрос SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 до стадии Complete │
│ Доступ разрешён: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ Запрос SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') до стадии WithMergeableState только для анализа │
│ Доступ разрешён: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ Запрос SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') из стадии FetchColumns до стадии WithMergeableState только для анализа │
│ Запрос SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 до стадии WithMergeableState только для анализа │
│ Доступ разрешён: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ Запрос SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 из стадии FetchColumns до стадии WithMergeableState только для анализа │
│ Запрос SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 из стадии WithMergeableState до стадии Complete │
│ Запрашиваемое количество реплик (100) больше фактического количества доступных в кластере (6). Будет использовано последнее число для выполнения запроса.                                                                                                       │
│ Первый запрос от реплики 4: 2 части: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
Получено от 4 реплики
                                                                                                   │
│ Состояние чтения полностью инициализировано: часть all_0_2_1 с диапазонами [(0, 182)] в репликах [4]; часть all_3_3_0 с диапазонами [(0, 62)] в репликах [4]                                                                                                            │
│ Начатые первоначальные запросы: 1 Количество реплик: 6                                                                                                                                                                                                                 │
│ Первый запрос от реплики 2: 2 части: [part all_0_2_1 с диапазонами [(0, 182)], part all_3_3_0 с диапазонами [(0, 62)]]----------
Получено от 2 реплики
                                                                                                   │
│ Начатые первоначальные запросы: 2 Количество реплик: 6                                                                                                                                                                                                                 │
│ Обработка запроса от реплики 4, минимальный размер меток 240                                                                                                                                                                                                 │
│ Ответ реплике 4 с 1 частью: [part all_0_2_1 с диапазонами [(128, 182)]]. Завершено: false; mine_marks=0, stolen_by_hash=54, stolen_rest=0                                                                                                       │
│ Первый запрос от реплики 1: 2 части: [part all_0_2_1 с диапазонами [(0, 182)], part all_3_3_0 с диапазонами [(0, 62)]]----------
Получено от 1 реплики
                                                                                                   │
│ Начатые первоначальные запросы: 3 Количество реплик: 6                                                                                                                                                                                                                 │
│ Обработка запроса от реплики 4, минимальный размер меток 240                                                                                                                                                                                                 │
│ Ответ реплике 4 с 2 частями: [part all_0_2_1 с диапазонами [(0, 128)], part all_3_3_0 с диапазонами [(0, 62)]]. Завершено: false; mine_marks=0, stolen_by_hash=0, stolen_rest=190                                                                  │
│ Первый запрос от реплики 0: 2 части: [part all_0_2_1 с диапазонами [(0, 182)], part all_3_3_0 с диапазонами [(0, 62)]]----------
Получено от 0 реплики
                                                                                                   │
│ Начатые первоначальные запросы: 4 Количество реплик: 6                                                                                                                                                                                                                 │
│ Первый запрос от реплики 5: 2 части: [part all_0_2_1 с диапазонами [(0, 182)], part all_3_3_0 с диапазонами [(0, 62)]]----------
Получено от 5 реплики
                                                                                                   │
│ Начатые первоначальные запросы: 5 Количество реплик: 6                                                                                                                                                                                                                 │
│ Обработка запроса от реплики 2, минимальный размер меток 240                                                                                                                                                                                                 │
│ Ответ реплике 2 с 0 частями: []. Завершено: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Первый запрос от реплики 3: 2 части: [part all_0_2_1 с диапазонами [(0, 182)], part all_3_3_0 с диапазонами [(0, 62)]]----------
Получено от 3 реплики
                                                                                                   │
│ Начатые первоначальные запросы: 6 Количество реплик: 6                                                                                                                                                                                                                 │
│ Всего строк для чтения: 2000000                                                                                                                                                                                                                                │
│ Обработка запроса от реплики 5, минимальный размер меток 240                                                                                                                                                                                                 │
│ Ответ реплике 5 с 0 частями: []. Завершено: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Обработка запроса от реплики 0, минимальный размер меток 240                                                                                                                                                                                                 │
│ Ответ реплике 0 с 0 частями: []. Завершено: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Обработка запроса от реплики 1, минимальный размер меток 240                                                                                                                                                                                                 │
│ Ответ реплике 1 с 0 частями: []. Завершено: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Обработка запроса от реплики 3, минимальный размер меток 240                                                                                                                                                                                                 │
│ Ответ реплике 3 с 0 частями: []. Завершено: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ (c-crimson-vd-86-server-rdhnsx3-0.c-crimson-vd-86-server-headless.ns-crimson-vd-86.svc.cluster.local:9000) Отмена запроса, так как достаточно данных было считано                                                                                              │
│ Прочитано 81920 строк, 5.16 MiB за 0.013166 сек., 6222087.194288318 строк/сек, 391.63 MiB/сек.                                                                                                                                                                   │
│ Координация завершена: Статистика: реплика 0 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; реплика 1 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; реплика 2 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; реплика 3 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; реплика 4 - {requests: 3 marks: 244 assigned_to_me: 0 stolen_by_hash: 54 stolen_unassigned: 190}; реплика 5 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0} │
│ Пиковое использование памяти (для запроса): 1.81 MiB.                                                                                                                                                                                                                   │
│ Обработано за 0.024095586 сек.                                                                                                                                                                                                                              │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```
</details>

Наконец, вы также можете использовать `EXPLAIN PIPELINE`. Это подчеркивает, как ClickHouse 
будет выполнять запрос и какие ресурсы будут использованы для 
выполнения запроса. Давайте возьмем следующий запрос в качестве примера:

```sql
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) LIMIT 10
```

Давайте посмотрим на конвейер запроса без параллельных реплик:

```sql title="EXPLAIN PIPELINE (без параллельной реплики)"
EXPLAIN PIPELINE graph = 1, compact = 0 
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) 
LIMIT 10 
SETTINGS allow_experimental_parallel_reading_from_replicas=0 
FORMAT TSV;
```

<img src={image_8} alt="EXPLAIN без параллельной реплики"/>

А теперь с параллельной репликой:

```sql title="EXPLAIN PIPELINE (с параллельной репликой)"
EXPLAIN PIPELINE graph = 1, compact = 0 
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) 
LIMIT 10 
SETTINGS allow_experimental_parallel_reading_from_replicas=2 
FORMAT TSV;
```

<img src={image_9} alt="EXPLAIN с параллельной репликой"/>
