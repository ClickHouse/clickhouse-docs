---
description: 'Этот движок обеспечивает интеграцию с экосистемой Amazon S3. Аналогичен
  движку HDFS, но поддерживает функции, специфичные для S3.'
sidebar_label: 'S3'
sidebar_position: 180
slug: /engines/table-engines/integrations/s3
title: 'Табличный движок S3'
doc_type: 'reference'
---



# Движок таблицы S3

Этот движок обеспечивает интеграцию с экосистемой [Amazon S3](https://aws.amazon.com/s3/). Он аналогичен движку [HDFS](/engines/table-engines/integrations/hdfs), но предоставляет возможности, специфичные для S3.



## Пример {#example}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE=S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'gzip')
    SETTINGS input_format_with_names_use_header = 0;

INSERT INTO s3_engine_table VALUES ('one', 1), ('two', 2), ('three', 3);

SELECT * FROM s3_engine_table LIMIT 2;
```

```text
┌─name─┬─value─┐
│ one  │     1 │
│ two  │     2 │
└──────┴───────┘
```


## Создание таблицы {#creating-a-table}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE = S3(path [, NOSIGN | aws_access_key_id, aws_secret_access_key,] format, [compression], [partition_strategy], [partition_columns_in_data_file])
    [PARTITION BY expr]
    [SETTINGS ...]
```

### Параметры движка {#parameters}

- `path` — URL бакета с путём к файлу. Поддерживает следующие подстановочные символы в режиме только для чтения: `*`, `**`, `?`, `{abc,def}` и `{N..M}`, где `N`, `M` — числа, `'abc'`, `'def'` — строки. Подробнее см. [ниже](#wildcards-in-path).
- `NOSIGN` — если это ключевое слово указано вместо учётных данных, все запросы не будут подписываться.
- `format` — [формат](/sql-reference/formats#formats-overview) файла.
- `aws_access_key_id`, `aws_secret_access_key` — долгосрочные учётные данные пользователя аккаунта [AWS](https://aws.amazon.com/). Используются для аутентификации запросов. Параметр необязательный. Если учётные данные не указаны, они берутся из файла конфигурации. Подробнее см. [Использование S3 для хранения данных](../mergetree-family/mergetree.md#table_engine-mergetree-s3).
- `compression` — тип сжатия. Поддерживаемые значения: `none`, `gzip/gz`, `brotli/br`, `xz/LZMA`, `zstd/zst`. Параметр необязательный. По умолчанию тип сжатия определяется автоматически по расширению файла.
- `partition_strategy` — варианты: `WILDCARD` или `HIVE`. `WILDCARD` требует наличия `{_partition_id}` в пути, который заменяется ключом партиции. `HIVE` не допускает подстановочные символы, предполагает, что путь является корнем таблицы, и генерирует партиционированные каталоги в стиле Hive с идентификаторами Snowflake в качестве имён файлов и форматом файла в качестве расширения. По умолчанию `WILDCARD`.
- `partition_columns_in_data_file` — используется только со стратегией партиционирования `HIVE`. Указывает ClickHouse, следует ли ожидать, что столбцы партиций будут записаны в файл данных. По умолчанию `false`.
- `storage_class_name` — варианты: `STANDARD` или `INTELLIGENT_TIERING`, позволяют указать [AWS S3 Intelligent Tiering](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/).

### Кэш данных {#data-cache}

Движок таблиц `S3` поддерживает кэширование данных на локальном диске.
Параметры конфигурации и использование кэша файловой системы см. в этом [разделе](/operations/storing-data.md/#using-local-cache).
Кэширование выполняется в зависимости от пути и ETag объекта хранилища, поэтому ClickHouse не будет читать устаревшую версию кэша.

Чтобы включить кэширование, используйте настройки `filesystem_cache_name = '<name>'` и `enable_filesystem_cache = 1`.

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
SETTINGS filesystem_cache_name = 'cache_for_s3', enable_filesystem_cache = 1;
```

Существует два способа определения кэша в файле конфигурации.

1. Добавьте следующую секцию в файл конфигурации ClickHouse:

```xml
<clickhouse>
    <filesystem_caches>
        <cache_for_s3>
            <path>path to cache directory</path>
            <max_size>10Gi</max_size>
        </cache_for_s3>
    </filesystem_caches>
</clickhouse>
```

2. Повторно используйте конфигурацию кэша (и, следовательно, хранилище кэша) из секции `storage_configuration` ClickHouse, [описанной здесь](/operations/storing-data.md/#using-local-cache)

### PARTITION BY {#partition-by}

`PARTITION BY` — необязательный параметр. В большинстве случаев ключ партиционирования не требуется, а если он нужен, то обычно не требуется ключ партиционирования более детальный, чем по месяцам. Партиционирование не ускоряет запросы (в отличие от выражения ORDER BY). Никогда не используйте слишком детальное партиционирование. Не партиционируйте данные по идентификаторам или именам клиентов (вместо этого сделайте идентификатор или имя клиента первым столбцом в выражении ORDER BY).

Для партиционирования по месяцам используйте выражение `toYYYYMM(date_column)`, где `date_column` — столбец с датой типа [Date](/sql-reference/data-types/date.md). Имена партиций в этом случае имеют формат `"YYYYMM"`.

#### Стратегия партиционирования {#partition-strategy}

`WILDCARD` (по умолчанию): заменяет подстановочный символ `{_partition_id}` в пути к файлу на фактический ключ партиции. Чтение не поддерживается.


`HIVE` реализует партиционирование в стиле Hive для операций чтения и записи. Чтение выполняется с использованием рекурсивного шаблона glob, что эквивалентно `SELECT * FROM s3('table_root/**.parquet')`.
При записи файлы генерируются в следующем формате: `<prefix>/<key1=val1/key2=val2...>/<snowflakeid>.<toLower(file_format)>`.

Примечание: при использовании стратегии партиционирования `HIVE` настройка `use_hive_partitioning` не действует.

Пример стратегии партиционирования `HIVE`:

```sql
arthur :) CREATE TABLE t_03363_parquet (year UInt16, country String, counter UInt8)
ENGINE = S3(s3_conn, filename = 't_03363_parquet', format = Parquet, partition_strategy='hive')
PARTITION BY (year, country);

arthur :) INSERT INTO t_03363_parquet VALUES
    (2022, 'USA', 1),
    (2022, 'Canada', 2),
    (2023, 'USA', 3),
    (2023, 'Mexico', 4),
    (2024, 'France', 5),
    (2024, 'Germany', 6),
    (2024, 'Germany', 7),
    (1999, 'Brazil', 8),
    (2100, 'Japan', 9),
    (2024, 'CN', 10),
    (2025, '', 11);

arthur :) select _path, * from t_03363_parquet;

    ┌─_path──────────────────────────────────────────────────────────────────────┬─year─┬─country─┬─counter─┐
 1. │ test/t_03363_parquet/year=2100/country=Japan/7329604473272971264.parquet   │ 2100 │ Japan   │       9 │
 2. │ test/t_03363_parquet/year=2024/country=France/7329604473323302912.parquet  │ 2024 │ France  │       5 │
 3. │ test/t_03363_parquet/year=2022/country=Canada/7329604473314914304.parquet  │ 2022 │ Canada  │       2 │
 4. │ test/t_03363_parquet/year=1999/country=Brazil/7329604473289748480.parquet  │ 1999 │ Brazil  │       8 │
 5. │ test/t_03363_parquet/year=2023/country=Mexico/7329604473293942784.parquet  │ 2023 │ Mexico  │       4 │
 6. │ test/t_03363_parquet/year=2023/country=USA/7329604473319108608.parquet     │ 2023 │ USA     │       3 │
 7. │ test/t_03363_parquet/year=2025/country=/7329604473327497216.parquet        │ 2025 │         │      11 │
 8. │ test/t_03363_parquet/year=2024/country=CN/7329604473310720000.parquet      │ 2024 │ CN      │      10 │
 9. │ test/t_03363_parquet/year=2022/country=USA/7329604473298137088.parquet     │ 2022 │ USA     │       1 │
10. │ test/t_03363_parquet/year=2024/country=Germany/7329604473306525696.parquet │ 2024 │ Germany │       6 │
11. │ test/t_03363_parquet/year=2024/country=Germany/7329604473306525696.parquet │ 2024 │ Germany │       7 │
    └────────────────────────────────────────────────────────────────────────────┴──────┴─────────┴─────────┘
```

### Запрос партиционированных данных {#querying-partitioned-data}

В этом примере используется [рецепт docker compose](https://github.com/ClickHouse/examples/tree/5fdc6ff72f4e5137e23ea075c88d3f44b0202490/docker-compose-recipes/recipes/ch-and-minio-S3), который интегрирует ClickHouse и MinIO. Вы можете воспроизвести те же запросы с использованием S3, заменив значения конечной точки и параметры аутентификации.

Обратите внимание, что конечная точка S3 в конфигурации `ENGINE` использует токен параметра `{_partition_id}` как часть имени объекта S3 (имени файла), и что запросы SELECT выполняются к этим результирующим именам объектов (например, `test_3.csv`).


:::note
Как показано в примере, запросы к партиционированным таблицам S3 в настоящее время
не поддерживаются напрямую, но могут быть выполнены путём запроса отдельных партиций
с использованием табличной функции S3.

Основной сценарий использования записи
партиционированных данных в S3 — это передача этих данных в другую
систему ClickHouse (например, перенос из локальных систем в ClickHouse
Cloud). Поскольку наборы данных ClickHouse часто имеют очень большой размер, а надёжность сети
иногда оставляет желать лучшего, имеет смысл передавать наборы данных
частями, отсюда и партиционированная запись.
:::

#### Создание таблицы {#create-the-table}

```sql
CREATE TABLE p
(
    `column1` UInt32,
    `column2` UInt32,
    `column3` UInt32
)
ENGINE = S3(
-- highlight-next-line
           'http://minio:10000/clickhouse//test_{_partition_id}.csv',
           'minioadmin',
           'minioadminpassword',
           'CSV')
PARTITION BY column3
```

#### Вставка данных {#insert-data}

```sql
INSERT INTO p VALUES (1, 2, 3), (3, 2, 1), (78, 43, 45)
```

#### Выборка из партиции 3 {#select-from-partition-3}

:::tip
Этот запрос использует табличную функцию s3
:::

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│  1 │  2 │  3 │
└────┴────┴────┘
```

#### Выборка из партиции 1 {#select-from-partition-1}

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_1.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│  3 │  2 │  1 │
└────┴────┴────┘
```

#### Выборка из партиции 45 {#select-from-partition-45}

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_45.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│ 78 │ 43 │ 45 │
└────┴────┴────┘
```

#### Ограничение {#limitation}

Вы можете попытаться выполнить `Select * from p`, но, как отмечено выше, этот запрос завершится ошибкой; используйте предыдущие запросы.

```sql
SELECT * FROM p
```

```response
Received exception from server (version 23.4.1):
Code: 48. DB::Exception: Received from localhost:9000. DB::Exception: Reading from a partitioned S3 storage is not implemented yet. (NOT_IMPLEMENTED)
```


## Вставка данных {#inserting-data}

Обратите внимание, что строки можно вставлять только в новые файлы. Циклы слияния и операции разделения файлов отсутствуют. После записи файла последующие вставки будут завершаться ошибкой. Чтобы этого избежать, используйте настройки `s3_truncate_on_insert` и `s3_create_new_file_on_insert`. Подробнее см. [здесь](/integrations/s3#inserting-data).


## Виртуальные столбцы {#virtual-columns}

- `_path` — Путь к файлу. Тип: `LowCardinality(String)`.
- `_file` — Имя файла. Тип: `LowCardinality(String)`.
- `_size` — Размер файла в байтах. Тип: `Nullable(UInt64)`. Если размер неизвестен, значение — `NULL`.
- `_time` — Время последнего изменения файла. Тип: `Nullable(DateTime)`. Если время неизвестно, значение — `NULL`.
- `_etag` — ETag файла. Тип: `LowCardinality(String)`. Если ETag неизвестен, значение — `NULL`.
- `_tags` — Теги файла. Тип: `Map(String, String)`. Если теги отсутствуют, значение — пустая карта `{}'.

Дополнительную информацию о виртуальных столбцах см. [здесь](../../../engines/table-engines/index.md#table_engines-virtual_columns).


## Детали реализации {#implementation-details}

- Чтение и запись могут выполняться параллельно
- Не поддерживается:
  - Операции `ALTER` и `SELECT...SAMPLE`.
  - Индексы.
  - Репликация [Zero-copy](../../../operations/storing-data.md#zero-copy) возможна, но не поддерживается.

  :::note Репликация Zero-copy не готова для production-среды
  Репликация Zero-copy отключена по умолчанию в ClickHouse версии 22.8 и выше. Эта функция не рекомендуется для использования в production-среде.
  :::


## Подстановочные символы в пути {#wildcards-in-path}

Аргумент `path` может указывать на несколько файлов с использованием подстановочных символов в стиле bash. Для обработки файл должен существовать и соответствовать всему шаблону пути. Список файлов определяется во время выполнения `SELECT` (а не в момент `CREATE`).

- `*` — Заменяет любое количество любых символов, кроме `/`, включая пустую строку.
- `**` — Заменяет любое количество любых символов, включая `/`, включая пустую строку.
- `?` — Заменяет любой одиночный символ.
- `{some_string,another_string,yet_another_one}` — Заменяет любую из строк `'some_string', 'another_string', 'yet_another_one'`.
- `{N..M}` — Заменяет любое число в диапазоне от N до M, включая обе границы. N и M могут иметь ведущие нули, например `000..078`.

Конструкции с `{}` аналогичны табличной функции [remote](../../../sql-reference/table-functions/remote.md).

:::note
Если список файлов содержит числовые диапазоны с ведущими нулями, используйте конструкцию с фигурными скобками для каждой цифры отдельно или используйте `?`.
:::

**Пример с подстановочными символами 1**

Создание таблицы с файлами `file-000.csv`, `file-001.csv`, ... , `file-999.csv`:

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/my_folder/file-{000..999}.csv', 'CSV');
```

**Пример с подстановочными символами 2**

Предположим, у нас есть несколько файлов в формате CSV со следующими URI на S3:

- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_3.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_3.csv'

Существует несколько способов создать таблицу, состоящую из всех шести файлов:

1. Указать диапазон постфиксов файлов:

```sql
CREATE TABLE table_with_range (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_{1..3}', 'CSV');
```

2. Взять все файлы с префиксом `some_file_` (в обеих папках не должно быть лишних файлов с таким префиксом):

```sql
CREATE TABLE table_with_question_mark (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_?', 'CSV');
```

3. Взять все файлы в обеих папках (все файлы должны соответствовать формату и схеме, описанным в запросе):

```sql
CREATE TABLE table_with_asterisk (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/*', 'CSV');
```


## Настройки хранилища {#storage-settings}

- [s3_truncate_on_insert](/operations/settings/settings.md#s3_truncate_on_insert) - позволяет очищать файл перед вставкой в него данных. По умолчанию отключена.
- [s3_create_new_file_on_insert](/operations/settings/settings.md#s3_create_new_file_on_insert) - позволяет создавать новый файл при каждой вставке, если формат имеет суффикс. По умолчанию отключена.
- [s3_skip_empty_files](/operations/settings/settings.md#s3_skip_empty_files) - позволяет пропускать пустые файлы при чтении. По умолчанию включена.


## Настройки, связанные с S3 {#settings}

Следующие настройки можно задать перед выполнением запроса или поместить в конфигурационный файл.

- `s3_max_single_part_upload_size` — Максимальный размер объекта для загрузки в S3 с использованием одночастной загрузки. Значение по умолчанию — `32Mb`.
- `s3_min_upload_part_size` — Минимальный размер части для загрузки при многочастной загрузке в [S3 Multipart upload](https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html). Значение по умолчанию — `16Mb`.
- `s3_max_redirects` — Максимальное количество разрешенных переходов по перенаправлениям S3. Значение по умолчанию — `10`.
- `s3_single_read_retries` — Максимальное количество попыток при одиночном чтении. Значение по умолчанию — `4`.
- `s3_max_put_rps` — Максимальная частота PUT-запросов в секунду до применения троттлинга. Значение по умолчанию — `0` (без ограничений).
- `s3_max_put_burst` — Максимальное количество запросов, которые могут быть выполнены одновременно до достижения лимита запросов в секунду. По умолчанию (значение `0`) равно `s3_max_put_rps`.
- `s3_max_get_rps` — Максимальная частота GET-запросов в секунду до применения троттлинга. Значение по умолчанию — `0` (без ограничений).
- `s3_max_get_burst` — Максимальное количество запросов, которые могут быть выполнены одновременно до достижения лимита запросов в секунду. По умолчанию (значение `0`) равно `s3_max_get_rps`.
- `s3_upload_part_size_multiply_factor` — Умножать `s3_min_upload_part_size` на этот коэффициент каждый раз, когда `s3_multiply_parts_count_threshold` частей загружено из одной операции записи в S3. Значение по умолчанию — `2`.
- `s3_upload_part_size_multiply_parts_count_threshold` — Каждый раз, когда это количество частей загружено в S3, `s3_min_upload_part_size` умножается на `s3_upload_part_size_multiply_factor`. Значение по умолчанию — `500`.
- `s3_max_inflight_parts_for_one_file` — Ограничивает количество PUT-запросов, которые могут выполняться одновременно для одного объекта. Это количество должно быть ограничено. Значение `0` означает отсутствие ограничений. Значение по умолчанию — `20`. Каждая обрабатываемая часть имеет буфер размером `s3_min_upload_part_size` для первых `s3_upload_part_size_multiply_factor` частей и больше, когда файл достаточно большой, см. `upload_part_size_multiply_factor`. При настройках по умолчанию один загружаемый файл потребляет не более `320Mb` для файла размером менее `8G`. Потребление больше для файлов большего размера.

Соображения безопасности: если злоумышленник может указывать произвольные URL-адреса S3, параметр `s3_max_redirects` должен быть установлен в ноль во избежание атак типа [SSRF](https://en.wikipedia.org/wiki/Server-side_request_forgery); или, в качестве альтернативы, в конфигурации сервера должен быть указан параметр `remote_host_filter`.


## Настройки для конечных точек {#endpoint-settings}

Следующие настройки могут быть указаны в файле конфигурации для конкретной конечной точки (которая будет определяться по точному совпадению префикса URL):

- `endpoint` — Указывает префикс конечной точки. Обязательный параметр.
- `access_key_id` и `secret_access_key` — Указывают учетные данные для использования с данной конечной точкой. Необязательный параметр.
- `use_environment_credentials` — Если установлено значение `true`, клиент S3 попытается получить учетные данные из переменных окружения и метаданных [Amazon EC2](https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud) для данной конечной точки. Необязательный параметр, значение по умолчанию — `false`.
- `region` — Указывает имя региона S3. Необязательный параметр.
- `use_insecure_imds_request` — Если установлено значение `true`, клиент S3 будет использовать небезопасный запрос IMDS при получении учетных данных из метаданных Amazon EC2. Необязательный параметр, значение по умолчанию — `false`.
- `expiration_window_seconds` — Период отсрочки для проверки истечения срока действия учетных данных. Необязательный параметр, значение по умолчанию — `120`.
- `no_sign_request` — Игнорировать все учетные данные, чтобы запросы не подписывались. Полезно для доступа к публичным корзинам.
- `header` — Добавляет указанный HTTP-заголовок к запросу к данной конечной точке. Необязательный параметр, может быть указан несколько раз.
- `access_header` — Добавляет указанный HTTP-заголовок к запросу к данной конечной точке в случаях, когда отсутствуют другие учетные данные из других источников.
- `server_side_encryption_customer_key_base64` — Если указано, будут установлены необходимые заголовки для доступа к объектам S3 с шифрованием SSE-C. Необязательный параметр.
- `server_side_encryption_kms_key_id` — Если указано, будут установлены необходимые заголовки для доступа к объектам S3 с [шифрованием SSE-KMS](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html). Если указана пустая строка, будет использоваться управляемый AWS ключ S3. Необязательный параметр.
- `server_side_encryption_kms_encryption_context` — Если указано вместе с `server_side_encryption_kms_key_id`, будет установлен заданный заголовок контекста шифрования для SSE-KMS. Необязательный параметр.
- `server_side_encryption_kms_bucket_key_enabled` — Если указано вместе с `server_side_encryption_kms_key_id`, будет установлен заголовок для включения ключей корзины S3 для SSE-KMS. Необязательный параметр, может быть `true` или `false`, по умолчанию не задано (соответствует настройке на уровне корзины).
- `max_single_read_retries` — Максимальное количество попыток при одиночном чтении. Значение по умолчанию — `4`. Необязательный параметр.
- `max_put_rps`, `max_put_burst`, `max_get_rps` и `max_get_burst` — Настройки регулирования (см. описание выше) для использования с конкретной конечной точкой вместо применения к каждому запросу. Необязательный параметр.

**Пример:**

```xml
<s3>
    <endpoint-name>
        <endpoint>https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/</endpoint>
        <!-- <access_key_id>ACCESS_KEY_ID</access_key_id> -->
        <!-- <secret_access_key>SECRET_ACCESS_KEY</secret_access_key> -->
        <!-- <region>us-west-1</region> -->
        <!-- <use_environment_credentials>false</use_environment_credentials> -->
        <!-- <use_insecure_imds_request>false</use_insecure_imds_request> -->
        <!-- <expiration_window_seconds>120</expiration_window_seconds> -->
        <!-- <no_sign_request>false</no_sign_request> -->
        <!-- <header>Authorization: Bearer SOME-TOKEN</header> -->
        <!-- <server_side_encryption_customer_key_base64>BASE64-ENCODED-KEY</server_side_encryption_customer_key_base64> -->
        <!-- <server_side_encryption_kms_key_id>KMS_KEY_ID</server_side_encryption_kms_key_id> -->
        <!-- <server_side_encryption_kms_encryption_context>KMS_ENCRYPTION_CONTEXT</server_side_encryption_kms_encryption_context> -->
        <!-- <server_side_encryption_kms_bucket_key_enabled>true</server_side_encryption_kms_bucket_key_enabled> -->
        <!-- <max_single_read_retries>4</max_single_read_retries> -->
    </endpoint-name>
</s3>
```


## Работа с архивами {#working-with-archives}

Предположим, что у нас есть несколько архивных файлов со следующими URI в S3:

- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip'

Извлечение данных из этих архивов возможно с помощью оператора `::`. Шаблоны подстановки (globs) можно использовать как в части URL, так и в части после `::` (которая отвечает за имя файла внутри архива).

```sql
SELECT *
FROM s3(
   'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-1{0..2}.csv.zip :: *.csv'
);
```

:::note
ClickHouse поддерживает три формата архивов:
ZIP
TAR
7Z
Архивы ZIP и TAR доступны из любого поддерживаемого хранилища, тогда как архивы 7Z можно читать только из локальной файловой системы, где установлен ClickHouse.
:::


## Доступ к публичным бакетам {#accessing-public-buckets}

ClickHouse пытается получить учетные данные из множества различных источников.
Иногда это может вызывать проблемы при обращении к публичным бакетам, что приводит к возврату клиентом кода ошибки `403`.
Эту проблему можно избежать, используя ключевое слово `NOSIGN`, которое заставляет клиент игнорировать все учетные данные и не подписывать запросы.

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv', NOSIGN, 'CSVWithNames');
```


## Оптимизация производительности {#optimizing-performance}

Подробную информацию об оптимизации производительности функции s3 см. в [подробном руководстве](/integrations/s3/performance).


## См. также {#see-also}

- [Табличная функция s3](../../../sql-reference/table-functions/s3.md)
- [Интеграция S3 с ClickHouse](/integrations/s3)
