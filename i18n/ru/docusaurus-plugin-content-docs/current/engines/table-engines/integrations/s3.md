---
description: 'Этот движок обеспечивает интеграцию с экосистемой Amazon S3. Аналогичен
  движку HDFS, но предоставляет специализированные функции для S3.'
sidebar_label: 'S3'
sidebar_position: 180
slug: /engines/table-engines/integrations/s3
title: 'Табличный движок S3'
doc_type: 'reference'
---



# Табличный движок S3 {#s3-table-engine}

Этот движок обеспечивает интеграцию с экосистемой [Amazon S3](https://aws.amazon.com/s3/). Он похож на движок [HDFS](/engines/table-engines/integrations/hdfs), но реализует ряд возможностей, специфичных для S3.



## Пример {#example}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE=S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'gzip')
    SETTINGS input_format_with_names_use_header = 0;

INSERT INTO s3_engine_table VALUES ('one', 1), ('two', 2), ('three', 3);

SELECT * FROM s3_engine_table LIMIT 2;
```

```text
┌─name─┬─value─┐
│ one  │     1 │
│ two  │     2 │
└──────┴───────┘
```


## Создайте таблицу {#creating-a-table}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE = S3(path [, NOSIGN | aws_access_key_id, aws_secret_access_key,] format, [compression], [partition_strategy], [partition_columns_in_data_file])
    [PARTITION BY expr]
    [SETTINGS ...]
```

### Параметры движка {#parameters}

* `path` — URL бакета с путем к файлу. Поддерживает следующие шаблоны (wildcards) в режиме только для чтения: `*`, `**`, `?`, `{abc,def}` и `{N..M}`, где `N`, `M` — числа, `'abc'`, `'def'` — строки. Дополнительную информацию см. [ниже](#wildcards-in-path).
* `NOSIGN` — если это ключевое слово указано вместо учетных данных, все запросы не подписываются.
* `format` — [формат](/sql-reference/formats#formats-overview) файла.
* `aws_access_key_id`, `aws_secret_access_key` — долгосрочные учетные данные пользователя аккаунта [AWS](https://aws.amazon.com/). Вы можете использовать их для аутентификации своих запросов. Параметр необязательный. Если учетные данные не указаны, они берутся из конфигурационного файла. Дополнительную информацию см. в разделе [Использование S3 для хранения данных](../mergetree-family/mergetree.md#table_engine-mergetree-s3).
* `compression` — тип сжатия. Поддерживаемые значения: `none`, `gzip/gz`, `brotli/br`, `xz/LZMA`, `zstd/zst`. Параметр необязательный. По умолчанию тип сжатия автоматически определяется по расширению файла.
* `partition_strategy` – варианты: `WILDCARD` или `HIVE`. `WILDCARD` требует наличия `{_partition_id}` в пути, который заменяется ключом партиционирования. `HIVE` не допускает шаблонов, предполагает, что путь — это корень таблицы, и создает каталоги партиций в стиле Hive с идентификаторами Snowflake в качестве имен файлов и форматом файла в качестве расширения. По умолчанию используется `WILDCARD`.
* `partition_columns_in_data_file` — используется только со стратегией партиционирования `HIVE`. Сообщает ClickHouse, следует ли ожидать, что столбцы партиции будут записаны в файл данных. По умолчанию `false`.
* `storage_class_name` — варианты: `STANDARD` или `INTELLIGENT_TIERING`, позволяют указать [AWS S3 Intelligent Tiering](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/).

### Кэш данных {#data-cache}

Движок таблиц `S3` поддерживает кэширование данных на локальном диске.
Параметры конфигурации файлового кэша и примеры использования см. в этом [разделе](/operations/storing-data.md/#using-local-cache).
Кэширование выполняется в зависимости от пути и ETag объекта хранилища, поэтому ClickHouse не будет использовать устаревшую версию кэша.

Чтобы включить кэширование, используйте настройки `filesystem_cache_name = '<name>'` и `enable_filesystem_cache = 1`.

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
SETTINGS filesystem_cache_name = 'cache_for_s3', enable_filesystem_cache = 1;
```

Есть два способа задать кэш в конфигурационном файле.

1. Добавьте следующий раздел в конфигурационный файл ClickHouse:

```xml
<clickhouse>
    <filesystem_caches>
        <cache_for_s3>
            <path>path to cache directory</path>
            <max_size>10Gi</max_size>
        </cache_for_s3>
    </filesystem_caches>
</clickhouse>
```

2. повторно использовать конфигурацию кэша (и, соответственно, хранилище кэша) из секции ClickHouse `storage_configuration`, [описанной здесь](/operations/storing-data.md/#using-local-cache)

### PARTITION BY {#partition-by}

`PARTITION BY` — необязательный параметр. В большинстве случаев вам не нужен ключ партиционирования, а если он и нужен, то, как правило, нет необходимости делать его более детализированным, чем помесячный. Партиционирование не ускоряет запросы (в отличие от выражения ORDER BY). Никогда не используйте слишком детализированное партиционирование. Не делите данные на партиции по идентификаторам или именам клиентов (вместо этого сделайте идентификатор или имя клиента первым столбцом в выражении ORDER BY).

Для партиционирования по месяцам используйте выражение `toYYYYMM(date_column)`, где `date_column` — это столбец с датой типа [Date](/sql-reference/data-types/date.md). Имена партиций в этом случае имеют формат `"YYYYMM"`.

#### Стратегия партиционирования {#partition-strategy}

`WILDCARD` (по умолчанию): заменяет шаблон `{_partition_id}` в пути к файлу фактическим ключом партиции. Чтение не поддерживается.


`HIVE` реализует разбиение на партиции в стиле Hive для операций чтения и записи. Чтение выполняется с использованием рекурсивного шаблона glob, что эквивалентно `SELECT * FROM s3('table_root/**.parquet')`.
При записи файлы генерируются в следующем формате: `<prefix>/<key1=val1/key2=val2...>/<snowflakeid>.<toLower(file_format)>`.

Примечание: при использовании стратегии партиционирования `HIVE` настройка `use_hive_partitioning` не влияет на поведение.

Пример стратегии партиционирования `HIVE`:

```sql
arthur :) CREATE TABLE t_03363_parquet (year UInt16, country String, counter UInt8)
ENGINE = S3(s3_conn, filename = 't_03363_parquet', format = Parquet, partition_strategy='hive')
PARTITION BY (year, country);

arthur :) INSERT INTO t_03363_parquet VALUES
    (2022, 'USA', 1),
    (2022, 'Canada', 2),
    (2023, 'USA', 3),
    (2023, 'Mexico', 4),
    (2024, 'France', 5),
    (2024, 'Germany', 6),
    (2024, 'Germany', 7),
    (1999, 'Brazil', 8),
    (2100, 'Japan', 9),
    (2024, 'CN', 10),
    (2025, '', 11);

arthur :) select _path, * from t_03363_parquet;

    ┌─_path──────────────────────────────────────────────────────────────────────┬─year─┬─country─┬─counter─┐
 1. │ test/t_03363_parquet/year=2100/country=Japan/7329604473272971264.parquet   │ 2100 │ Japan   │       9 │
 2. │ test/t_03363_parquet/year=2024/country=France/7329604473323302912.parquet  │ 2024 │ France  │       5 │
 3. │ test/t_03363_parquet/year=2022/country=Canada/7329604473314914304.parquet  │ 2022 │ Canada  │       2 │
 4. │ test/t_03363_parquet/year=1999/country=Brazil/7329604473289748480.parquet  │ 1999 │ Brazil  │       8 │
 5. │ test/t_03363_parquet/year=2023/country=Mexico/7329604473293942784.parquet  │ 2023 │ Mexico  │       4 │
 6. │ test/t_03363_parquet/year=2023/country=USA/7329604473319108608.parquet     │ 2023 │ USA     │       3 │
 7. │ test/t_03363_parquet/year=2025/country=/7329604473327497216.parquet        │ 2025 │         │      11 │
 8. │ test/t_03363_parquet/year=2024/country=CN/7329604473310720000.parquet      │ 2024 │ CN      │      10 │
 9. │ test/t_03363_parquet/year=2022/country=USA/7329604473298137088.parquet     │ 2022 │ USA     │       1 │
10. │ test/t_03363_parquet/year=2024/country=Germany/7329604473306525696.parquet │ 2024 │ Germany │       6 │
11. │ test/t_03363_parquet/year=2024/country=Germany/7329604473306525696.parquet │ 2024 │ Germany │       7 │
    └────────────────────────────────────────────────────────────────────────────┴──────┴─────────┴─────────┘
```

### Выполнение запросов к разбиённым данным {#querying-partitioned-data}

В этом примере используется [docker compose recipe](https://github.com/ClickHouse/examples/tree/5fdc6ff72f4e5137e23ea075c88d3f44b0202490/docker-compose-recipes/recipes/ch-and-minio-S3), который интегрирует ClickHouse и MinIO. Вы можете воспроизвести те же запросы, используя S3, просто заменив endpoint и параметры аутентификации.

Обратите внимание, что endpoint S3 в конфигурации `ENGINE` использует параметр `{_partition_id}` как часть объекта S3 (имени файла) и что запросы SELECT выполняются по этим получившимся именам объектов (например, `test_3.csv`).


:::note
Как показано в примере, выполнение запросов к партиционированным таблицам S3
на данный момент напрямую не поддерживается, но может быть реализовано
путём выполнения запросов к отдельным партициям с использованием `S3` table function.

Основной сценарий записи
партиционированных данных в S3 — последующая передача этих данных в другую
систему ClickHouse (например, при переносе с on‑prem систем в ClickHouse
Cloud). Поскольку наборы данных ClickHouse часто очень большие, а
надёжность сети иногда оставляет желать лучшего, имеет смысл передавать наборы данных
по частям, то есть использовать партиционированную запись.
:::

#### Создайте таблицу {#create-the-table}

```sql
CREATE TABLE p
(
    `column1` UInt32,
    `column2` UInt32,
    `column3` UInt32
)
ENGINE = S3(
-- highlight-next-line
           'http://minio:10000/clickhouse//test_{_partition_id}.csv',
           'minioadmin',
           'minioadminpassword',
           'CSV')
PARTITION BY column3
```

#### Добавление данных {#insert-data}

```sql
INSERT INTO p VALUES (1, 2, 3), (3, 2, 1), (78, 43, 45)
```

#### Выборка из партиции 3 {#select-from-partition-3}

:::tip
Этот запрос использует табличную функцию S3
:::

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│  1 │  2 │  3 │
└────┴────┴────┘
```

#### Выборка данных из партиции 1 {#select-from-partition-1}

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_1.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│  3 │  2 │  1 │
└────┴────┴────┘
```

#### Выборка данных из партиции 45 {#select-from-partition-45}

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_45.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│ 78 │ 43 │ 45 │
└────┴────┴────┘
```

#### Ограничение {#limitation}

Вполне естественно попытаться выполнить `Select * from p`, но, как уже отмечалось выше, этот запрос завершится с ошибкой. Используйте приведённый выше запрос.

```sql
SELECT * FROM p
```

```response
Received exception from server (version 23.4.1):
Code: 48. DB::Exception: Received from localhost:9000. DB::Exception: Reading from a partitioned S3 storage is not implemented yet. (NOT_IMPLEMENTED)
```


## Вставка данных {#inserting-data}

Обратите внимание, что строки можно вставлять только в новые файлы. Операции слияния или разбиения файлов не выполняются. После записи файла последующие вставки будут завершаться с ошибкой. Чтобы этого избежать, вы можете использовать настройки `s3_truncate_on_insert` и `s3_create_new_file_on_insert`. Подробнее см. [здесь](/integrations/s3#inserting-data).



## Виртуальные столбцы {#virtual-columns}

- `_path` — Путь к файлу. Тип: `LowCardinality(String)`.
- `_file` — Имя файла. Тип: `LowCardinality(String)`.
- `_size` — Размер файла в байтах. Тип: `Nullable(UInt64)`. Если размер неизвестен, значение — `NULL`.
- `_time` — Время последнего изменения файла. Тип: `Nullable(DateTime)`. Если время неизвестно, значение — `NULL`.
- `_etag` — ETag файла. Тип: `LowCardinality(String)`. Если ETag неизвестен, значение — `NULL`.
- {_tags} — Теги файла. Тип: `Map(String, String)`. Если теги отсутствуют, значение — пустая карта `{}`.

Для получения дополнительной информации о виртуальных столбцах см. [здесь](../../../engines/table-engines/index.md#table_engines-virtual_columns).



## Подробности реализации {#implementation-details}

- Чтение и запись могут выполняться параллельно
- Не поддерживаются:
  - Операции `ALTER` и `SELECT...SAMPLE`.
  - Индексы.
  - [Zero-copy](../../../operations/storing-data.md#zero-copy) репликация возможна, но не поддерживается.

  :::note Zero-copy репликация не готова для промышленной эксплуатации
  Zero-copy репликация по умолчанию отключена в версиях ClickHouse 22.8 и выше. Эта функция не рекомендуется для использования в производственных средах.
  :::



## Подстановочные символы в `path` {#wildcards-in-path}

Аргумент `path` может указывать на несколько файлов, используя подстановочные символы в стиле bash. Для обработки файл должен существовать и полностью соответствовать шаблону пути. Список файлов определяется во время выполнения `SELECT` (а не в момент `CREATE`).

* `*` — Подставляет любое количество любых символов, кроме `/`, включая пустую строку.
* `**` — Подставляет любое количество любых символов, включая `/`, включая пустую строку.
* `?` — Подставляет любой одиночный символ.
* `{some_string,another_string,yet_another_one}` — Подставляет любую из строк `'some_string', 'another_string', 'yet_another_one'`.
* `{N..M}` — Подставляет любое число в диапазоне от N до M включительно. N и M могут содержать ведущие нули, например `000..078`.

Конструкции с `{}` аналогичны табличной функции [remote](../../../sql-reference/table-functions/remote.md).

:::note
Если список файлов содержит числовые диапазоны с ведущими нулями, используйте конструкцию с фигурными скобками отдельно для каждой цифры или используйте `?`.
:::

**Пример с подстановочными символами 1**

Создайте таблицу, использующую файлы с именами `file-000.csv`, `file-001.csv`, ... , `file-999.csv`:

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/my_folder/file-{000..999}.csv', 'CSV');
```

**Пример с подстановочными символами 2**

Предположим, у нас есть несколько файлов в формате CSV со следующими URI в S3:

* &#39;[https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some&#95;folder/some&#95;file&#95;1.csv](https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_1.csv)&#39;
* &#39;[https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some&#95;folder/some&#95;file&#95;2.csv](https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_2.csv)&#39;
* &#39;[https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some&#95;folder/some&#95;file&#95;3.csv](https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_3.csv)&#39;
* &#39;[https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another&#95;folder/some&#95;file&#95;1.csv](https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_1.csv)&#39;
* &#39;[https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another&#95;folder/some&#95;file&#95;2.csv](https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_2.csv)&#39;
* &#39;[https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another&#95;folder/some&#95;file&#95;3.csv](https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_3.csv)&#39;

Существует несколько способов создать таблицу, включающую все шесть файлов:

1. Указать диапазон суффиксов имён файлов:

```sql
CREATE TABLE table_with_range (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_{1..3}', 'CSV');
```

2. Возьмите все файлы с префиксом `some_file_` (в обоих каталогах не должно быть дополнительных файлов с таким префиксом):

```sql
CREATE TABLE table_with_question_mark (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_?', 'CSV');
```

3. Соберите все файлы из обеих папок (все файлы должны соответствовать формату и схеме, описанным в запросе):

```sql
CREATE TABLE table_with_asterisk (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/*', 'CSV');
```


## Параметры хранения {#storage-settings}

- [s3_truncate_on_insert](/operations/settings/settings.md#s3_truncate_on_insert) — позволяет усекать файл перед вставкой в него. По умолчанию отключено.
- [s3_create_new_file_on_insert](/operations/settings/settings.md#s3_create_new_file_on_insert) — позволяет создавать новый файл при каждой вставке, если формат имеет суффикс. По умолчанию отключено.
- [s3_skip_empty_files](/operations/settings/settings.md#s3_skip_empty_files) — позволяет пропускать пустые файлы при чтении. По умолчанию включено.



## Параметры, связанные с S3 {#settings}

Следующие настройки могут быть заданы перед выполнением запроса или включены в файл конфигурации.

- `s3_max_single_part_upload_size` — максимальный размер объекта для загрузки с использованием однократной (singlepart) загрузки в S3. Значение по умолчанию — `32Mb`.
- `s3_min_upload_part_size` — минимальный размер части для загрузки при многокомпонентной (multipart) загрузке [S3 Multipart upload](https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html). Значение по умолчанию — `16Mb`.
- `s3_max_redirects` — максимальное число допустимых перенаправлений (redirects) S3. Значение по умолчанию — `10`.
- `s3_single_read_retries` — максимальное число попыток при одиночном чтении. Значение по умолчанию — `4`.
- `s3_max_put_rps` — максимальное число запросов PUT в секунду до включения троттлинга. Значение по умолчанию — `0` (без ограничений).
- `s3_max_put_burst` — максимальное количество запросов, которые могут быть выполнены одновременно до достижения лимита запросов в секунду. По умолчанию (значение `0`) равно `s3_max_put_rps`.
- `s3_max_get_rps` — максимальное число запросов GET в секунду до включения троттлинга. Значение по умолчанию — `0` (без ограничений).
- `s3_max_get_burst` — максимальное количество запросов, которые могут быть выполнены одновременно до достижения лимита запросов в секунду. По умолчанию (значение `0`) равно `s3_max_get_rps`.
- `s3_upload_part_size_multiply_factor` — умножать `s3_min_upload_part_size` на этот коэффициент каждый раз, когда `s3_multiply_parts_count_threshold` частей были загружены одним действием записи в S3. Значение по умолчанию — `2`.
- `s3_upload_part_size_multiply_parts_count_threshold` — каждый раз, когда в S3 загружается указанное количество частей, `s3_min_upload_part_size` умножается на `s3_upload_part_size_multiply_factor`. Значение по умолчанию — `500`.
- `s3_max_inflight_parts_for_one_file` — ограничивает количество запросов PUT, которые могут выполняться параллельно для одного объекта. Это значение следует ограничивать. Значение `0` означает отсутствие ограничений. Значение по умолчанию — `20`. Каждая часть «в полёте» имеет буфер размером `s3_min_upload_part_size` для первых `s3_upload_part_size_multiply_factor` частей и больше, когда файл достаточно большой, см. `upload_part_size_multiply_factor`. При настройках по умолчанию один загружаемый файл использует не более `320Mb` для файла размером менее `8G`. Потребление больше для более крупного файла.

Соображения безопасности: если вредоносный пользователь может указывать произвольные S3 URL, параметр `s3_max_redirects` должен быть установлен в ноль, чтобы избежать атак [SSRF](https://en.wikipedia.org/wiki/Server-side_request_forgery), либо в конфигурации сервера должен быть указан `remote_host_filter`.



## Параметры на основе endpoint {#endpoint-settings}

Следующие параметры могут быть указаны в конфигурационном файле для заданного endpoint (который будет сопоставляться по точному префиксу URL):

* `endpoint` — Указывает префикс endpoint. Обязательный параметр.
* `access_key_id` и `secret_access_key` — Указывают учетные данные для использования с данным endpoint. Необязательные параметры.
* `use_environment_credentials` — Если установлено значение `true`, клиент S3 попытается получить учетные данные из переменных окружения и метаданных [Amazon EC2](https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud) для данного endpoint. Необязательный параметр, значение по умолчанию — `false`.
* `region` — Указывает имя региона S3. Необязательный параметр.
* `use_insecure_imds_request` — Если установлено значение `true`, клиент S3 будет использовать небезопасный IMDS-запрос при получении учетных данных из метаданных Amazon EC2. Необязательный параметр, значение по умолчанию — `false`.
* `expiration_window_seconds` — Льготный период при проверке, истекли ли учетные данные с ограниченным сроком действия. Необязательный параметр, значение по умолчанию — `120`.
* `no_sign_request` — Игнорировать все учетные данные, чтобы запросы не подписывались. Полезно для доступа к публичным бакетам.
* `header` — Добавляет указанный HTTP-заголовок к запросу к данному endpoint. Необязательный параметр, может быть указан несколько раз.
* `access_header` — Добавляет указанный HTTP-заголовок к запросу к данному endpoint в случаях, когда нет других учетных данных из другого источника.
* `server_side_encryption_customer_key_base64` — Если указано, будут установлены необходимые заголовки для доступа к объектам S3 с шифрованием SSE-C. Необязательный параметр.
* `server_side_encryption_kms_key_id` — Если указано, будут установлены необходимые заголовки для доступа к объектам S3 с [шифрованием SSE-KMS](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html). Если указана пустая строка, будет использован управляемый AWS ключ S3. Необязательный параметр.
* `server_side_encryption_kms_encryption_context` — Если указано вместе с `server_side_encryption_kms_key_id`, будет установлен соответствующий заголовок контекста шифрования для SSE-KMS. Необязательный параметр.
* `server_side_encryption_kms_bucket_key_enabled` — Если указано вместе с `server_side_encryption_kms_key_id`, будет установлен заголовок для включения ключей бакета S3 для SSE-KMS. Необязательный параметр, может быть `true` или `false`, по умолчанию не задан (соответствует настройке на уровне бакета).
* `max_single_read_retries` — Максимальное количество попыток при одиночном чтении. Значение по умолчанию — `4`. Необязательный параметр.
* `max_put_rps`, `max_put_burst`, `max_get_rps` и `max_get_burst` — Параметры ограничения скорости (см. описание выше), которые используются для конкретного endpoint вместо ограничения на уровне запроса. Необязательные параметры.

**Пример:**

```xml
<s3>
    <endpoint-name>
        <endpoint>https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/</endpoint>
        <!-- <access_key_id>ACCESS_KEY_ID</access_key_id> -->
        <!-- <secret_access_key>SECRET_ACCESS_KEY</secret_access_key> -->
        <!-- <region>us-west-1</region> -->
        <!-- <use_environment_credentials>false</use_environment_credentials> -->
        <!-- <use_insecure_imds_request>false</use_insecure_imds_request> -->
        <!-- <expiration_window_seconds>120</expiration_window_seconds> -->
        <!-- <no_sign_request>false</no_sign_request> -->
        <!-- <header>Authorization: Bearer SOME-TOKEN</header> -->
        <!-- <server_side_encryption_customer_key_base64>BASE64-ENCODED-KEY</server_side_encryption_customer_key_base64> -->
        <!-- <server_side_encryption_kms_key_id>KMS_KEY_ID</server_side_encryption_kms_key_id> -->
        <!-- <server_side_encryption_kms_encryption_context>KMS_ENCRYPTION_CONTEXT</server_side_encryption_kms_encryption_context> -->
        <!-- <server_side_encryption_kms_bucket_key_enabled>true</server_side_encryption_kms_bucket_key_enabled> -->
        <!-- <max_single_read_retries>4</max_single_read_retries> -->
    </endpoint-name>
</s3>
```


## Работа с архивами {#working-with-archives}

Предположим, у нас есть несколько архивных файлов со следующими URI в S3:

* &#39;[https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip](https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip)&#39;
* &#39;[https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip](https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip)&#39;
* &#39;[https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip](https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip)&#39;

Извлекать данные из этих архивов можно с помощью конструкции ::. Шаблоны glob могут использоваться как в части URL, так и в части после ::, которая отвечает за имя файла внутри архива.

```sql
SELECT *
FROM s3(
   'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-1{0..2}.csv.zip :: *.csv'
);
```

:::note
ClickHouse поддерживает три формата архивов:
ZIP
TAR
7Z
Архивы ZIP и TAR могут быть прочитаны из любого поддерживаемого хранилища, а архивы 7Z — только из локальной файловой системы узла, где установлен ClickHouse.
:::


## Доступ к публичным бакетам {#accessing-public-buckets}

ClickHouse пытается получить учетные данные из множества различных источников.
Иногда это может приводить к проблемам при обращении к некоторым публичным бакетам, в результате чего клиент возвращает ошибку `403`.
Этой проблемы можно избежать, используя ключевое слово `NOSIGN`, которое заставляет клиент игнорировать все учетные данные и не подписывать запросы.

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv', NOSIGN, 'CSVWithNames');
```


## Оптимизация производительности {#optimizing-performance}

Подробные сведения об оптимизации производительности функции s3 см. в [нашем подробном руководстве](/integrations/s3/performance).



## См. также {#see-also}

- [табличная функция S3](../../../sql-reference/table-functions/s3.md)
- [Интеграция S3 с ClickHouse](/integrations/s3)
