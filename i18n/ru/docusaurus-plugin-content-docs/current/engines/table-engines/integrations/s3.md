---
slug: /engines/table-engines/integrations/s3
sidebar_position: 180
sidebar_label: S3
title: 'S3 Table Engine'
description: 'Этот движок обеспечивает интеграцию с экосистемой Amazon S3. Аналогично движку HDFS, но предоставляет специфические для S3 функции.'
---


# S3 Table Engine

Этот движок обеспечивает интеграцию с [Amazon S3](https://aws.amazon.com/s3/) экосистемой. Этот движок аналогичен [HDFS](/engines/table-engines/integrations/hdfs) движку, но предоставляет специфические для S3 функции.

## Пример {#example}

``` sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE=S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'gzip')
    SETTINGS input_format_with_names_use_header = 0;

INSERT INTO s3_engine_table VALUES ('one', 1), ('two', 2), ('three', 3);

SELECT * FROM s3_engine_table LIMIT 2;
```

```text
┌─name─┬─value─┐
│ one  │     1 │
│ two  │     2 │
└──────┴───────┘
```

## Создание таблицы {#creating-a-table}

``` sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE = S3(path [, NOSIGN | aws_access_key_id, aws_secret_access_key,] формат, [сжатие])
    [PARTITION BY expr]
    [SETTINGS ...]
```

### Параметры движка {#parameters}

- `path` — URL ведра с путем к файлу. Поддерживает следующие подстановочные знаки в режиме только для чтения: `*`, `**`, `?`, `{abc,def}` и `{N..M}`, где `N`, `M` — числа, `'abc'`, `'def'` — строки. Для получения дополнительной информации смотрите [ниже](#wildcards-in-path).
- `NOSIGN` - Если это ключевое слово указано вместо учетных данных, все запросы не будут подписаны.
- `format` — [формат](/sql-reference/formats#formats-overview) файла.
- `aws_access_key_id`, `aws_secret_access_key` - Долгосрочные учетные данные пользователя учетной записи [AWS](https://aws.amazon.com/). Вы можете использовать их для аутентификации ваших запросов. Параметр является необязательным. Если учетные данные не указаны, используются из конфигурационного файла. Для получения дополнительной информации смотрите [Использование S3 для хранения данных](../mergetree-family/mergetree.md#table_engine-mergetree-s3).
- `compression` — Тип сжатия. Поддерживаемые значения: `none`, `gzip/gz`, `brotli/br`, `xz/LZMA`, `zstd/zst`. Параметр является необязательным. По умолчанию он автоматически определяет сжатие по расширению файла.

### Кэш данных {#data-cache}

`S3` движок таблицы поддерживает кэширование данных на локальном диске. Смотрите параметры конфигурации файловой системы кэширования и использование в этом [разделе](/operations/storing-data.md/#using-local-cache). Кэширование осуществляется в зависимости от пути и ETag объекта хранилища, так что ClickHouse не будет читать устаревшую версию кэша.

Чтобы включить кэширование используйте установку `filesystem_cache_name = '<name>'` и `enable_filesystem_cache = 1`.

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
SETTINGS filesystem_cache_name = 'cache_for_s3', enable_filesystem_cache = 1;
```

Существует два способа определить кэш в конфигурационном файле.

1. Добавьте следующий раздел в файл конфигурации ClickHouse:

``` xml
<clickhouse>
    <filesystem_caches>
        <cache_for_s3>
            <path>путь к директории кэша</path>
            <max_size>10Gi</max_size>
        </cache_for_s3>
    </filesystem_caches>
</clickhouse>
```

2. Используйте конфигурацию кэша (а значит и хранилище кэша) из раздела конфигурации `storage_configuration` ClickHouse, [описанного здесь](/operations/storing-data.md/#using-local-cache)

### PARTITION BY {#partition-by}

`PARTITION BY` — Необязательный. В большинстве случаев вам не нужен ключ партиции, и если он нужен, он обычно не должен быть более детализированным, чем по месяцу. Партиционирование не ускоряет запросы (в отличие от выражения ORDER BY). Никогда не используйте слишком детализированное партиционирование. Не партиционируйте ваши данные по идентификаторам клиентов или именам (вместо этого сделайте идентификатор или имя клиента первой колонкой в выражении ORDER BY).

Для партиционирования по месяцу используйте выражение `toYYYYMM(date_column)`, где `date_column` — колонка с датой типа [Date](/sql-reference/data-types/date.md). Имена партиций имеют формат `"YYYYMM"`.

### Запрос партиционированных данных {#querying-partitioned-data}

Этот пример использует [рецепт docker compose](https://github.com/ClickHouse/examples/tree/5fdc6ff72f4e5137e23ea075c88d3f44b0202490/docker-compose-recipes/recipes/ch-and-minio-S3), который интегрирует ClickHouse и MinIO. Вы должны быть в состоянии воспроизвести те же запросы, используя S3, заменив значения конечной точки и аутентификации.

Обратите внимание, что конечная точка S3 в конфигурации `ENGINE` использует параметр token `{_partition_id}` как часть S3 объекта (имя файла), и что SELECT запросы выбирают по этим именам объектов (например, `test_3.csv`).

:::note
Как показано в примере, запросы из S3 таблиц, которые партиционированы, в данный момент не поддерживаются напрямую, но это можно сделать, запрашивая отдельные партиции, используя функцию таблицы S3.

Основное применение для записи партиционированных данных в S3 — это возможность передачи этих данных в другую систему ClickHouse (например, перемещение из локальных систем в ClickHouse Cloud). Поскольку наборы данных ClickHouse часто очень большие, а надежность сети иногда несовершенна, имеет смысл передавать наборы данных по частям, поэтому партиционированные записи.
:::

#### Создание таблицы {#create-the-table}
```sql
CREATE TABLE p
(
    `column1` UInt32,
    `column2` UInt32,
    `column3` UInt32
)
ENGINE = S3(

# highlight-next-line
           'http://minio:10000/clickhouse//test_{_partition_id}.csv',
           'minioadmin',
           'minioadminpassword',
           'CSV')
PARTITION BY column3
```

#### Вставка данных {#insert-data}
```sql
insert into p values (1, 2, 3), (3, 2, 1), (78, 43, 45)
```

#### Выборка из партиции 3 {#select-from-partition-3}

:::tip
Этот запрос использует функцию таблицы s3
:::

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│  1 │  2 │  3 │
└────┴────┴────┘
```

#### Выборка из партиции 1 {#select-from-partition-1}
```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_1.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│  3 │  2 │  1 │
└────┴────┴────┘
```

#### Выборка из партиции 45 {#select-from-partition-45}
```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_45.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│ 78 │ 43 │ 45 │
└────┴────┴────┘
```

#### Ограничение {#limitation}

Вы, как правило, можете попробовать `Select * from p`, но, как уже упоминалось, этот запрос завершится ошибкой; используйте предыдущий запрос.

```sql
SELECT * FROM p
```
```response
Получено исключение от сервера (версия 23.4.1):
Код: 48. DB::Exception: Получено от localhost:9000. DB::Exception: Чтение из партиционированного S3 хранилища еще не реализовано. (NOT_IMPLEMENTED)
```

## Виртуальные колонки {#virtual-columns}

- `_path` — Путь к файлу. Тип: `LowCardinality(String)`.
- `_file` — Имя файла. Тип: `LowCardinality(String)`.
- `_size` — Размер файла в байтах. Тип: `Nullable(UInt64)`. Если размер неизвестен, значение равно `NULL`.
- `_time` — Время последнего изменения файла. Тип: `Nullable(DateTime)`. Если время неизвестно, значение равно `NULL`.
- `_etag` — ETag файла. Тип: `LowCardinality(String)`. Если etag неизвестен, значение равно `NULL`.

Для получения дополнительной информации о виртуальных колонках смотрите [здесь](../../../engines/table-engines/index.md#table_engines-virtual_columns).

## Детали реализации {#implementation-details}

- Чтения и записи могут быть параллельными.
- Не поддерживается:
    - Операции `ALTER` и `SELECT...SAMPLE`.
    - Индексы.
    - [Zero-copy](../../../operations/storing-data.md#zero-copy) репликация возможна, но не поддерживается.

  :::note Zero-copy репликация не готова для продакшена
  Zero-copy репликация отключена по умолчанию в ClickHouse версии 22.8 и выше. Эта функция не рекомендуется для использования в продакшене.
  :::

## Подстановочные знаки в пути {#wildcards-in-path}

Аргумент `path` может указывать на несколько файлов, используя подстановочные знаки, аналогичные bash. Для обработки файл должен существовать и соответствовать всему шаблону пути. Перечень файлов определяется во время `SELECT` (а не в момент `CREATE`).

- `*` — Заменяет любое количество любых символов, кроме `/`, включая пустую строку.
- `**` — Заменяет любое количество любых символов, включая `/`, включая пустую строку.
- `?` — Заменяет любой один символ.
- `{some_string,another_string,yet_another_one}` — Заменяет любую из строк `'some_string', 'another_string', 'yet_another_one'`.
- `{N..M}` — Заменяет любое число в диапазоне от N до M, включая обе границы. N и M могут иметь ведущие нули, например `000..078`.

Конструкции с `{}` аналогичны функции таблицы [remote](../../../sql-reference/table-functions/remote.md).

:::note
Если перечень файлов содержит диапазоны чисел с ведущими нулями, используйте конструкцию с фигурными скобками для каждой цифры отдельно или используйте `?`.
:::

**Пример с подстановочными знаками 1**

Создайте таблицу с файлами, названными `file-000.csv`, `file-001.csv`, ..., `file-999.csv`:

``` sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/my_folder/file-{000..999}.csv', 'CSV');
```

**Пример с подстановочными знаками 2**

Предположим, у нас есть несколько файлов в формате CSV со следующими URI на S3:

- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_3.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_3.csv'


Есть несколько способов создать таблицу, состоящую из всех шести файлов:

1. Укажите диапазон постфиксов файлов:

``` sql
CREATE TABLE table_with_range (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_{1..3}', 'CSV');
```

2. Возьмите все файлы с префиксом `some_file_` (не должно быть лишних файлов с таким префиксом в обеих папках):

``` sql
CREATE TABLE table_with_question_mark (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_?', 'CSV');
```

3. Возьмите все файлы в обеих папках (все файлы должны соответствовать формату и схеме, описанным в запросе):

``` sql
CREATE TABLE table_with_asterisk (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/*', 'CSV');
```

## Настройки хранения {#storage-settings}

- [s3_truncate_on_insert](/operations/settings/settings.md#s3_truncate_on_insert) - позволяет обрезать файл перед вставкой в него. Отключено по умолчанию.
- [s3_create_new_file_on_insert](/operations/settings/settings.md#s3_create_new_file_on_insert) - позволяет создавать новый файл при каждой вставке, если формат имеет суффикс. Отключено по умолчанию.
- [s3_skip_empty_files](/operations/settings/settings.md#s3_skip_empty_files) - позволяет пропускать пустые файлы при чтении. Включено по умолчанию.

## Настройки, связанные с S3 {#settings}

Следующие настройки могут быть установлены перед выполнением запроса или размещены в конфигурационном файле.

- `s3_max_single_part_upload_size` — Максимальный размер объекта для загрузки с использованием одночастной загрузки в S3. Значение по умолчанию — `32Mb`.
- `s3_min_upload_part_size` — Минимальный размер части для загрузки во время многочастной загрузки в [S3 Multipart upload](https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html). Значение по умолчанию — `16Mb`.
- `s3_max_redirects` — Максимальное количество перенаправлений S3. Значение по умолчанию — `10`.
- `s3_single_read_retries` — Максимальное количество попыток во время одной чтения. Значение по умолчанию — `4`.
- `s3_max_put_rps` — Максимальное количество PUT запросов в секунду, после которого произойдет ограничение. Значение по умолчанию — `0` (неограниченное).
- `s3_max_put_burst` — Максимальное количество запросов, которые могут быть выданы одновременно, прежде чем будет достигнуто ограничение по запросам в секунду. По умолчанию (`0` значение) равно `s3_max_put_rps`.
- `s3_max_get_rps` — Максимальное количество GET запросов в секунду, после которого произойдет ограничение. Значение по умолчанию — `0` (неограниченное).
- `s3_max_get_burst` — Максимальное количество запросов, которые могут быть выданы одновременно, прежде чем будет достигнуто ограничение по запросам в секунду. По умолчанию (`0` значение) равно `s3_max_get_rps`.
- `s3_upload_part_size_multiply_factor` - Умножает `s3_min_upload_part_size` на этот коэффициент каждый раз, когда `s3_upload_part_size_multiply_parts_count_threshold` частей были загружены из одной записи в S3. Значение по умолчанию — `2`.
- `s3_upload_part_size_multiply_parts_count_threshold` - Каждый раз, когда это количество частей было загружено в S3, `s3_min_upload_part_size` умножается на `s3_upload_part_size_multiply_factor`. Значение по умолчанию — `500`.
- `s3_max_inflight_parts_for_one_file` - Ограничивает количество PUT запросов, которые могут выполняться одновременно для одного объекта. Его число должно быть ограничено. Значение `0` означает неограниченное количество. Значение по умолчанию — `20`. Каждая часть в воздухе имеет буфер размером `s3_min_upload_part_size` для первых `s3_upload_part_size_multiply_factor` частей и больше, когда файл достаточно велик, см. `upload_part_size_multiply_factor`. С настройками по умолчанию один загруженный файл потребляет не более `320Mb` для файла, который меньше `8G`. Потребление больше для больших файлов.

Соображения безопасности: если злонамеренный пользователь может указать произвольные S3 URL, `s3_max_redirects` должен быть установлен в ноль, чтобы избежать атак [SSRF](https://en.wikipedia.org/wiki/Server-side_request_forgery); или альтернативно, `remote_host_filter` должен быть указан в конфигурации сервера.

## Настройки на основе конечной точки {#endpoint-settings}

Следующие настройки могут быть указаны в конфигурационном файле для данной конечной точки (которая будет соответствовать точному префиксу URL):

- `endpoint` — Указывает префикс конечной точки. Обязательный.
- `access_key_id` и `secret_access_key` — Указывают учетные данные для использования с данной конечной точкой. Необязательны.
- `use_environment_credentials` — Если установлено в `true`, клиент S3 попытается получить учетные данные из переменных окружения и [метаданных Amazon EC2](https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud) для данной конечной точки. Необязательная, значение по умолчанию — `false`.
- `region` — Указывает название региона S3. Необязательная.
- `use_insecure_imds_request` — Если установлено в `true`, клиент S3 будет использовать небезопасный запрос IMDS при получении учетных данных из метаданных Amazon EC2. Необязательная, значение по умолчанию — `false`.
- `expiration_window_seconds` — Период отсрочки для проверки того, истекли ли учетные данные, основанные на сроке. Необязательная, значение по умолчанию — `120`.
- `no_sign_request` - Игнорирует все учетные данные, поэтому запросы не подписываются. Полезно для доступа к общедоступным ведрам.
- `header` — Добавляет указанный HTTP заголовок к запросу на данную конечную точку. Необязательный, может быть указан несколько раз.
- `access_header` - Добавляет указанный HTTP заголовок к запросу на данную конечную точку, в случаях, когда нет других учетных данных из другого источника.
- `server_side_encryption_customer_key_base64` — Если указано, будут установлены обязательные заголовки для доступа к объектам S3 с шифрованием SSE-C. Необязательная.
- `server_side_encryption_kms_key_id` - Если указано, будут установлены обязательные заголовки для доступа к объектам S3 с [шифрованием SSE-KMS](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html). Если указана пустая строка, будет использован управляемый S3 ключ AWS. Необязательная.
- `server_side_encryption_kms_encryption_context` - Если указано вместе с `server_side_encryption_kms_key_id`, заданный заголовок контекста шифрования для SSE-KMS будет установлен. Необязательная.
- `server_side_encryption_kms_bucket_key_enabled` - Если указано вместе с `server_side_encryption_kms_key_id`, будет установлен заголовок для включения ключей ведра S3 для SSE-KMS. Необязательная, может быть `true` или `false`, по умолчанию ничего (соответствует настройке на уровне ведра).
- `max_single_read_retries` — Максимальное количество попыток во время одного чтения. Значение по умолчанию — `4`. Необязательная.
- `max_put_rps`, `max_put_burst`, `max_get_rps` и `max_get_burst` - Настройки ограничения (см. описание выше) для использования для конкретной конечной точки вместо каждого запроса. Необязательные.

**Пример:**

``` xml
<s3>
    <endpoint-name>
        <endpoint>https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/</endpoint>
        <!-- <access_key_id>ACCESS_KEY_ID</access_key_id> -->
        <!-- <secret_access_key>SECRET_ACCESS_KEY</secret_access_key> -->
        <!-- <region>us-west-1</region> -->
        <!-- <use_environment_credentials>false</use_environment_credentials> -->
        <!-- <use_insecure_imds_request>false</use_insecure_imds_request> -->
        <!-- <expiration_window_seconds>120</expiration_window_seconds> -->
        <!-- <no_sign_request>false</no_sign_request> -->
        <!-- <header>Authorization: Bearer SOME-TOKEN</header> -->
        <!-- <server_side_encryption_customer_key_base64>BASE64-ENCODED-KEY</server_side_encryption_customer_key_base64> -->
        <!-- <server_side_encryption_kms_key_id>KMS_KEY_ID</server_side_encryption_kms_key_id> -->
        <!-- <server_side_encryption_kms_encryption_context>KMS_ENCRYPTION_CONTEXT</server_side_encryption_kms_encryption_context> -->
        <!-- <server_side_encryption_kms_bucket_key_enabled>true</server_side_encryption_kms_bucket_key_enabled> -->
        <!-- <max_single_read_retries>4</max_single_read_retries> -->
    </endpoint-name>
</s3>
```

## Работа с архивами {#working-with-archives}

Предположим, что у нас есть несколько архивных файлов со следующими URI на S3:

- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip'

Извлечение данных из этих архивов возможно, используя ::. Глобусы могут использоваться как в части URL, так и в части после :: (отвечающей за имя файла внутри архива).

``` sql
SELECT *
FROM s3(
   'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-1{0..2}.csv.zip :: *.csv'
);
```

:::note 
ClickHouse поддерживает три формата архивов:
ZIP
TAR
7Z
Хотя ZIP и TAR архивы могут быть доступны из любого поддерживаемого места хранения, архивы 7Z могут быть прочитаны только из локальной файловой системы, где установлен ClickHouse.  
:::


## Доступ к публичным ведрам {#accessing-public-buckets}

ClickHouse пытается получить учетные данные из множества различных типов источников. Иногда это может вызвать проблемы при доступе к некоторым ведрам, которые являются публичными, что приводит к возврату клиентом кода ошибки `403`. Эту проблему можно избежать, используя ключевое слово `NOSIGN`, принуждая клиент игнорировать все учетные данные и не подписывать запросы.

``` sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv', NOSIGN, 'CSVWithNames');
```

## Оптимизация производительности {#optimizing-performance}

Для получения информации о оптимизации производительности функции s3 смотрите [наш подробный гид](/integrations/s3/performance).

## Смотрите также {#see-also}

- [функция таблицы s3](../../../sql-reference/table-functions/s3.md)
