---
description: 'Обзор репликации данных с семейством табличных движков Replicated* в ClickHouse'
sidebar_label: 'Replicated*'
sidebar_position: 20
slug: /engines/table-engines/mergetree-family/replication
title: 'Табличные движки Replicated*'
doc_type: 'reference'
---



# Движки реплицируемых таблиц (Replicated*)

:::note
В ClickHouse Cloud репликация управляется автоматически. Пожалуйста, создавайте таблицы без указания аргументов. Например, в тексте ниже вы бы заменили:

```sql
ENGINE = ReplicatedMergeTree(
    '/clickhouse/tables/{shard}/table_name',
    '{replica}'
)
```

с:

```sql
ENGINE = ReplicatedMergeTree
```

:::

Репликация поддерживается только для таблиц семейства MergeTree:

* ReplicatedMergeTree
* ReplicatedSummingMergeTree
* ReplicatedReplacingMergeTree
* ReplicatedAggregatingMergeTree
* ReplicatedCollapsingMergeTree
* ReplicatedVersionedCollapsingMergeTree
* ReplicatedGraphiteMergeTree

Репликация работает на уровне отдельной таблицы, а не всего сервера. На одном сервере одновременно могут храниться как реплицируемые, так и нереплицируемые таблицы.

Репликация не зависит от шардинга. Каждый шард имеет собственную независимую репликацию.

Сжатые данные для запросов `INSERT` и `ALTER` реплицируются (подробности см. в документации по [ALTER](/sql-reference/statements/alter)).

Запросы `CREATE`, `DROP`, `ATTACH`, `DETACH` и `RENAME` выполняются на одном сервере и не реплицируются:

* Запрос `CREATE TABLE` создаёт на сервере, где выполняется запрос, новую реплицируемую таблицу. Если такая таблица уже существует на других серверах, добавляется новая реплика.
* Запрос `DROP TABLE` удаляет реплику, расположенную на сервере, где выполняется запрос.
* Запрос `RENAME` переименовывает таблицу только на одной из реплик. Другими словами, реплицируемые таблицы могут иметь разные имена на разных репликах.

ClickHouse использует [ClickHouse Keeper](/guides/sre/keeper/index.md) для хранения метаинформации о репликах. Возможно использование ZooKeeper версии 3.4.5 или новее, но рекомендуется ClickHouse Keeper.

Чтобы использовать репликацию, задайте параметры в конфигурации сервера в разделе [zookeeper](/operations/server-configuration-parameters/settings#zookeeper).

:::note
Не пренебрегайте настройками безопасности. ClickHouse поддерживает схему `digest` [ACL](https://zookeeper.apache.org/doc/current/zookeeperProgrammers.html#sc_ZooKeeperAccessControl) подсистемы безопасности ZooKeeper.
:::

Пример настройки адресов кластера ClickHouse Keeper:

```xml
<zookeeper>
    <node>
        <host>example1</host>
        <port>2181</port>
    </node>
    <node>
        <host>example2</host>
        <port>2181</port>
    </node>
    <node>
        <host>example3</host>
        <port>2181</port>
    </node>
</zookeeper>
```

ClickHouse также поддерживает хранение метаинформации о репликах во вспомогательном кластере ZooKeeper. Для этого укажите имя кластера ZooKeeper и путь в параметрах движка.
Другими словами, поддерживается хранение метаданных разных таблиц в разных кластерах ZooKeeper.

Пример задания адресов вспомогательного кластера ZooKeeper:

```xml
<auxiliary_zookeepers>
    <zookeeper2>
        <node>
            <host>example_2_1</host>
            <port>2181</port>
        </node>
        <node>
            <host>example_2_2</host>
            <port>2181</port>
        </node>
        <node>
            <host>example_2_3</host>
            <port>2181</port>
        </node>
    </zookeeper2>
    <zookeeper3>
        <node>
            <host>example_3_1</host>
            <port>2181</port>
        </node>
    </zookeeper3>
</auxiliary_zookeepers>
```

Чтобы хранить метаданные таблицы во вспомогательном кластере ZooKeeper вместо кластера ZooKeeper по умолчанию, можно с помощью SQL создать таблицу с движком ReplicatedMergeTree следующим образом:

```sql
CREATE TABLE table_name ( ... ) ENGINE = ReplicatedMergeTree('имя_zookeeper_настроенное_в_auxiliary_zookeepers:path', 'имя_реплики') ...
```

Вы можете указать любой существующий кластер ZooKeeper, и система будет использовать в нём каталог для своих собственных данных (каталог указывается при создании реплицируемой таблицы).

Если ZooKeeper не задан в конфигурационном файле, вы не сможете создавать реплицируемые таблицы, а все уже существующие реплицируемые таблицы будут доступны только для чтения.


ZooKeeper не используется в запросах `SELECT`, потому что репликация не влияет на производительность `SELECT`, и запросы выполняются так же быстро, как и для нереплицируемых таблиц. При выполнении запросов к распределённым реплицируемым таблицам поведение ClickHouse контролируется настройками [max_replica_delay_for_distributed_queries](/operations/settings/settings.md/#max_replica_delay_for_distributed_queries) и [fallback_to_stale_replicas_for_distributed_queries](/operations/settings/settings.md/#fallback_to_stale_replicas_for_distributed_queries).

Для каждого запроса `INSERT` в ZooKeeper добавляется примерно десять записей через несколько транзакций. (Точнее, это делается для каждого вставляемого блока данных; один запрос `INSERT` содержит один блок или один блок на `max_insert_block_size = 1048576` строк.) Это приводит к немного более высокой задержке для `INSERT` по сравнению с нереплицируемыми таблицами. Но если следовать рекомендациям и вставлять данные пакетами не более одного `INSERT` в секунду, это не создаёт никаких проблем. Весь кластер ClickHouse, использующий один кластер ZooKeeper для координации, в сумме выполняет несколько сотен `INSERT` в секунду. Пропускная способность по вставке данных (число строк в секунду) остаётся такой же высокой, как и для нереплицируемых данных.

Для очень больших кластеров можно использовать разные кластеры ZooKeeper для разных шардов. Однако, исходя из нашего опыта, в этом не возникало необходимости даже в production-кластерах примерно на 300 серверов.

Репликация асинхронная и многомастерная (multi-master). Запросы `INSERT` (так же как и `ALTER`) можно отправлять на любой доступный сервер. Данные вставляются на сервере, где выполняется запрос, а затем копируются на другие серверы. Поскольку репликация асинхронная, недавно вставленные данные появляются на остальных репликах с некоторой задержкой. Если часть реплик недоступна, данные будут записаны, когда они снова станут доступны. Если реплика доступна, задержка равна времени, требуемому для передачи блока сжатых данных по сети. Число потоков, выполняющих фоновые задачи для реплицируемых таблиц, можно задать настройкой [background_schedule_pool_size](/operations/server-configuration-parameters/settings.md/#background_schedule_pool_size).

Движок `ReplicatedMergeTree` использует отдельный пул потоков для реплицируемых загрузок данных (fetches). Размер пула ограничен настройкой [background_fetches_pool_size](/operations/server-configuration-parameters/settings#background_fetches_pool_size), которую можно изменить с перезапуском сервера.

По умолчанию запрос `INSERT` ждёт подтверждения записи данных только от одной реплики. Если данные были успешно записаны только на одну реплику и сервер с этой репликой перестанет существовать, сохранённые данные будут потеряны. Чтобы включить получение подтверждения записи данных от нескольких реплик, используйте опцию `insert_quorum`.

Каждый блок данных записывается атомарно. Запрос `INSERT` разбивается на блоки до `max_insert_block_size = 1048576` строк. Другими словами, если в запросе `INSERT` меньше 1048576 строк, он выполняется атомарно.

Блоки данных дедуплицируются. При многократной записи одного и того же блока данных (блоки данных одного размера, содержащие одни и те же строки в том же порядке) блок записывается только один раз. Это сделано на случай сетевых сбоев, когда клиентскому приложению может быть неизвестно, были ли данные записаны в БД, поэтому запрос `INSERT` можно просто повторить. Не имеет значения, на какие реплики были отправлены `INSERT` с идентичными данными. Запросы `INSERT` идемпотентны. Параметры дедупликации управляются серверными настройками [merge_tree](/operations/server-configuration-parameters/settings.md/#merge_tree).

Во время репликации по сети передаются только исходные данные для вставки. Дальнейшая трансформация данных (слияние) координируется и выполняется на всех репликах одинаковым образом. Это минимизирует использование сети, что означает, что репликация хорошо работает, когда реплики находятся в разных дата-центрах. (Обратите внимание, что дублирование данных в разных дата-центрах — основная цель репликации.)

Вы можете иметь любое количество реплик одних и тех же данных. Согласно нашему опыту, относительно надёжным и удобным решением в продакшене может быть двукратная репликация, когда каждый сервер использует RAID-5 или RAID-6 (а в некоторых случаях RAID-10).

Система контролирует согласованность данных на репликах и способна восстановиться после сбоя. Переключение на резерв (failover) происходит автоматически (при небольших расхождениях в данных) или полуавтоматически (когда данные различаются слишком сильно, что может указывать на ошибку конфигурации).



## Создание реплицируемых таблиц {#creating-replicated-tables}

:::note
В ClickHouse Cloud репликация выполняется автоматически.

Создавайте таблицы с использованием [`MergeTree`](/engines/table-engines/mergetree-family/mergetree) без аргументов репликации. Система автоматически преобразует [`MergeTree`](/engines/table-engines/mergetree-family/mergetree) в [`SharedMergeTree`](/cloud/reference/shared-merge-tree) для репликации и распределения данных.

Избегайте использования `ReplicatedMergeTree` или указания параметров репликации, поскольку репликация управляется платформой.

:::

### Параметры Replicated\*MergeTree {#replicatedmergetree-parameters}

| Параметр           | Описание                                                                                                        |
| ------------------ | --------------------------------------------------------------------------------------------------------------- |
| `zoo_path`         | Путь к таблице в ClickHouse Keeper.                                                                            |
| `replica_name`     | Имя реплики в ClickHouse Keeper.                                                                               |
| `other_parameters` | Параметры движка, используемые для создания реплицируемой версии, например, версия в `ReplacingMergeTree`.     |

Пример:

```sql
CREATE TABLE table_name
(
    EventDate DateTime,
    CounterID UInt32,
    UserID UInt32,
    ver UInt16
ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/{layer}-{shard}/table_name', '{replica}', ver)
PARTITION BY toYYYYMM(EventDate)
ORDER BY (CounterID, EventDate, intHash32(UserID))
SAMPLE BY intHash32(UserID);
```

<details markdown="1">

<summary>Пример в устаревшем синтаксисе</summary>

```sql
CREATE TABLE table_name
(
    EventDate DateTime,
    CounterID UInt32,
    UserID UInt32
) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/table_name', '{replica}', EventDate, intHash32(UserID), (CounterID, EventDate, intHash32(UserID), EventTime), 8192);
```

</details>

Как показано в примере, эти параметры могут содержать подстановки в фигурных скобках. Подставляемые значения берутся из раздела [macros](/operations/server-configuration-parameters/settings.md/#macros) конфигурационного файла.

Пример:

```xml
<macros>
    <shard>02</shard>
    <replica>example05-02-1</replica>
</macros>
```

Путь к таблице в ClickHouse Keeper должен быть уникальным для каждой реплицируемой таблицы. Таблицы на разных шардах должны иметь разные пути.
В данном случае путь состоит из следующих частей:

`/clickhouse/tables/` — общий префикс. Рекомендуется использовать именно его.

`{shard}` будет развернут в идентификатор шарда.

`table_name` — имя узла для таблицы в ClickHouse Keeper. Рекомендуется сделать его таким же, как имя таблицы. Оно определяется явно, поскольку в отличие от имени таблицы, оно не изменяется после запроса RENAME.
_ПОДСКАЗКА_: вы также можете добавить имя базы данных перед `table_name`. Например, `db_name.table_name`

Можно использовать две встроенные подстановки `{database}` и `{table}`, которые разворачиваются в имя базы данных и имя таблицы соответственно (если только эти макросы не определены в разделе `macros`). Таким образом, путь в ClickHouse Keeper можно указать как `'/clickhouse/tables/{shard}/{database}/{table}'`.
Будьте осторожны с переименованием таблиц при использовании этих встроенных подстановок. Путь в ClickHouse Keeper не может быть изменен, и когда таблица переименовывается, макросы развернутся в другой путь, таблица будет ссылаться на несуществующий путь в ClickHouse Keeper и перейдет в режим только для чтения.

Имя реплики идентифицирует различные реплики одной и той же таблицы. Для этого можно использовать имя сервера, как в примере. Имя должно быть уникальным только в пределах каждого шарда.

Вы можете определить параметры явно вместо использования подстановок. Это может быть удобно для тестирования и настройки небольших кластеров. Однако в этом случае вы не сможете использовать распределенные DDL-запросы (`ON CLUSTER`).

При работе с большими кластерами рекомендуется использовать подстановки, поскольку они снижают вероятность ошибок.

Вы можете указать аргументы по умолчанию для движка таблиц `Replicated` в конфигурационном файле сервера. Например:

```xml
<default_replica_path>/clickhouse/tables/{shard}/{database}/{table}</default_replica_path>
<default_replica_name>{replica}</default_replica_name>
```

В этом случае вы можете опустить аргументы при создании таблиц:

```sql
CREATE TABLE table_name (
    x UInt32
) ENGINE = ReplicatedMergeTree
ORDER BY x;
```

Это эквивалентно:


```sql
CREATE TABLE table_name (
    x UInt32
) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/{database}/table_name', '{replica}')
ORDER BY x;
```

Выполните запрос `CREATE TABLE` на каждой реплике. Этот запрос создаёт новую реплицируемую таблицу или добавляет новую реплику к уже существующей.

Если вы добавляете новую реплику после того, как таблица уже содержит некоторые данные на других репликах, после выполнения запроса данные будут скопированы с других реплик на новую. Другими словами, новая реплика синхронизируется с остальными.

Чтобы удалить реплику, выполните `DROP TABLE`. Однако будет удалена только одна реплика — та, которая находится на сервере, где вы выполняете запрос.


## Восстановление после сбоев {#recovery-after-failures}

Если ClickHouse Keeper недоступен при запуске сервера, реплицируемые таблицы переключаются в режим только для чтения. Система периодически пытается подключиться к ClickHouse Keeper.

Если ClickHouse Keeper недоступен во время выполнения `INSERT` или возникает ошибка при взаимодействии с ClickHouse Keeper, генерируется исключение.

После подключения к ClickHouse Keeper система проверяет, соответствует ли набор данных в локальной файловой системе ожидаемому набору данных (ClickHouse Keeper хранит эту информацию). Если обнаружены незначительные несоответствия, система устраняет их путём синхронизации данных с репликами.

Если система обнаруживает повреждённые куски данных (с неправильным размером файлов) или нераспознанные куски (куски, записанные в файловую систему, но не зарегистрированные в ClickHouse Keeper), она перемещает их в подкаталог `detached` (они не удаляются). Все отсутствующие куски копируются с реплик.

Обратите внимание, что ClickHouse не выполняет деструктивных действий, таких как автоматическое удаление большого объёма данных.

При запуске сервера (или установлении новой сессии с ClickHouse Keeper) проверяется только количество и размеры всех файлов. Если размеры файлов совпадают, но байты были изменены где-то в середине, это не обнаруживается сразу, а только при попытке чтения данных для запроса `SELECT`. Запрос генерирует исключение о несовпадении контрольной суммы или размера сжатого блока. В этом случае куски данных добавляются в очередь проверки и при необходимости копируются с реплик.

Если локальный набор данных слишком сильно отличается от ожидаемого, срабатывает механизм безопасности. Сервер записывает это в лог и отказывается запускаться. Причина в том, что этот случай может указывать на ошибку конфигурации, например, если реплика на шарде была случайно настроена как реплика на другом шарде. Однако пороговые значения для этого механизма установлены достаточно низкими, и эта ситуация может возникнуть при обычном восстановлении после сбоя. В этом случае данные восстанавливаются полуавтоматически — путём «нажатия кнопки».

Чтобы начать восстановление, создайте узел `/path_to_table/replica_name/flags/force_restore_data` в ClickHouse Keeper с любым содержимым или выполните команду для восстановления всех реплицируемых таблиц:

```bash
sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data
```

Затем перезапустите сервер. При запуске сервер удаляет эти флаги и начинает восстановление.


## Восстановление после полной потери данных {#recovery-after-complete-data-loss}

Если все данные и метаданные исчезли с одного из серверов, выполните следующие шаги для восстановления:

1.  Установите ClickHouse на сервер. Корректно определите подстановки в конфигурационном файле, содержащем идентификатор шарда и реплик, если вы их используете.
2.  Если у вас были нереплицируемые таблицы, которые необходимо вручную дублировать на серверах, скопируйте их данные с реплики (из каталога `/var/lib/clickhouse/data/db_name/table_name/`).
3.  Скопируйте определения таблиц, расположенные в `/var/lib/clickhouse/metadata/`, с реплики. Если идентификатор шарда или реплики явно определён в определениях таблиц, исправьте его так, чтобы он соответствовал данной реплике. (Альтернативно, запустите сервер и выполните все запросы `ATTACH TABLE`, которые должны были находиться в .sql-файлах в `/var/lib/clickhouse/metadata/`.)
4.  Чтобы начать восстановление, создайте узел ClickHouse Keeper `/path_to_table/replica_name/flags/force_restore_data` с любым содержимым или выполните команду для восстановления всех реплицируемых таблиц: `sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data`

Затем запустите сервер (перезапустите, если он уже работает). Данные будут загружены с реплик.

Альтернативный вариант восстановления — удалить информацию о потерянной реплике из ClickHouse Keeper (`/path_to_table/replica_name`), а затем создать реплику заново, как описано в разделе «[Creating replicated tables](#creating-replicated-tables)».

Во время восстановления отсутствуют ограничения на пропускную способность сети. Учитывайте это при одновременном восстановлении большого числа реплик.


## Преобразование из MergeTree в ReplicatedMergeTree {#converting-from-mergetree-to-replicatedmergetree}

Мы используем термин `MergeTree` для обозначения всех движков таблиц в `семействе MergeTree`, так же как и для `ReplicatedMergeTree`.

Если у вас есть таблица `MergeTree`, которая реплицировалась вручную, вы можете преобразовать её в реплицируемую таблицу. Это может потребоваться, если вы уже накопили большой объём данных в таблице `MergeTree` и теперь хотите включить репликацию.

Инструкция [ATTACH TABLE ... AS REPLICATED](/sql-reference/statements/attach.md#attach-mergetree-table-as-replicatedmergetree) позволяет присоединить отключённую таблицу `MergeTree` как `ReplicatedMergeTree`.

Таблица `MergeTree` может быть автоматически преобразована при перезапуске сервера, если флаг `convert_to_replicated` установлен в каталоге данных таблицы (`/store/xxx/xxxyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy/` для базы данных `Atomic`).
Создайте пустой файл `convert_to_replicated`, и таблица будет загружена как реплицируемая при следующем перезапуске сервера.

Этот запрос можно использовать для получения пути к данным таблицы. Если у таблицы несколько путей к данным, необходимо использовать первый.

```sql
SELECT data_paths FROM system.tables WHERE table = 'table_name' AND database = 'database_name';
```

Обратите внимание, что таблица ReplicatedMergeTree будет создана со значениями настроек `default_replica_path` и `default_replica_name`.
Чтобы создать преобразованную таблицу на других репликах, необходимо явно указать её путь в первом аргументе движка `ReplicatedMergeTree`. Следующий запрос можно использовать для получения пути.

```sql
SELECT zookeeper_path FROM system.replicas WHERE table = 'table_name';
```

Также существует ручной способ выполнить это преобразование.

Если данные различаются на разных репликах, сначала синхронизируйте их или удалите данные на всех репликах, кроме одной.

Переименуйте существующую таблицу MergeTree, затем создайте таблицу `ReplicatedMergeTree` со старым именем.
Переместите данные из старой таблицы в подкаталог `detached` внутри каталога с данными новой таблицы (`/var/lib/clickhouse/data/db_name/table_name/`).
Затем выполните `ALTER TABLE ATTACH PARTITION` на одной из реплик, чтобы добавить эти куски данных в рабочий набор.


## Преобразование из ReplicatedMergeTree в MergeTree {#converting-from-replicatedmergetree-to-mergetree}

Используйте оператор [ATTACH TABLE ... AS NOT REPLICATED](/sql-reference/statements/attach.md#attach-mergetree-table-as-replicatedmergetree) для подключения отсоединённой таблицы `ReplicatedMergeTree` в качестве `MergeTree` на одном сервере.

Другой способ предполагает перезапуск сервера. Создайте таблицу MergeTree с другим именем. Переместите все данные из каталога с данными таблицы `ReplicatedMergeTree` в каталог данных новой таблицы. Затем удалите таблицу `ReplicatedMergeTree` и перезапустите сервер.

Если необходимо удалить таблицу `ReplicatedMergeTree` без запуска сервера:

- Удалите соответствующий файл `.sql` в каталоге метаданных (`/var/lib/clickhouse/metadata/`).
- Удалите соответствующий путь в ClickHouse Keeper (`/path_to_table/replica_name`).

После этого можно запустить сервер, создать таблицу `MergeTree`, переместить данные в её каталог и перезапустить сервер.


## Восстановление при потере или повреждении метаданных в кластере ClickHouse Keeper {#recovery-when-metadata-in-the-zookeeper-cluster-is-lost-or-damaged}

Если данные в ClickHouse Keeper были утрачены или повреждены, вы можете сохранить данные, переместив их в нереплицируемую таблицу, как описано выше.

**См. также**

- [background_schedule_pool_size](/operations/server-configuration-parameters/settings.md/#background_schedule_pool_size)
- [background_fetches_pool_size](/operations/server-configuration-parameters/settings.md/#background_fetches_pool_size)
- [execute_merges_on_single_replica_time_threshold](/operations/settings/merge-tree-settings#execute_merges_on_single_replica_time_threshold)
- [max_replicated_fetches_network_bandwidth](/operations/settings/merge-tree-settings.md/#max_replicated_fetches_network_bandwidth)
- [max_replicated_sends_network_bandwidth](/operations/settings/merge-tree-settings.md/#max_replicated_sends_network_bandwidth)
