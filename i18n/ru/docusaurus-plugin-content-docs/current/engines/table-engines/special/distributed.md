---
description: 'Таблицы с движком Distributed не хранят собственные данные, а
  позволяют выполнять распределённую обработку запросов по нескольким серверам. Операции чтения автоматически
  выполняются параллельно. При чтении используются индексы таблиц на удалённых серверах,
  если они есть.'
sidebar_label: 'Distributed'
sidebar_position: 10
slug: /engines/table-engines/special/distributed
title: 'Движок таблиц Distributed'
doc_type: 'reference'
---



# Движок распределённой таблицы

:::warning Distributed engine in Cloud
Чтобы создать распределённую таблицу в ClickHouse Cloud, вы можете использовать табличные функции [`remote` и `remoteSecure`](../../../sql-reference/table-functions/remote). 
Синтаксис `Distributed(...)` в ClickHouse Cloud использовать нельзя.
:::

Таблицы с движком Distributed сами по себе не хранят данные, но позволяют выполнять распределённую обработку запросов на нескольких серверах. 
Чтение автоматически параллелизуется. При чтении, если на удалённых серверах существуют индексы таблиц, они используются.



## Создание таблицы {#distributed-creating-a-table}

```sql
CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
(
    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
    ...
) ENGINE = Distributed(cluster, database, table[, sharding_key[, policy_name]])
[SETTINGS name=value, ...]
```

### Из таблицы {#distributed-from-a-table}

Если таблица `Distributed` указывает на таблицу на текущем сервере, можно использовать схему этой таблицы:

```sql
CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] AS [db2.]name2 ENGINE = Distributed(cluster, database, table[, sharding_key[, policy_name]]) [SETTINGS name=value, ...]
```

### Параметры Distributed {#distributed-parameters}

| Параметр                  | Описание                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `cluster`                 | Имя кластера в конфигурационном файле сервера                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| `database`                | Имя удалённой базы данных                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| `table`                   | Имя удалённой таблицы                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| `sharding_key` (необязательно) | Ключ шардирования. <br/> Указание `sharding_key` необходимо в следующих случаях: <ul><li>Для операций `INSERT` в распределённую таблицу (поскольку движку таблицы требуется `sharding_key` для определения способа разделения данных). Однако если включена настройка `insert_distributed_one_random_shard`, то для операций `INSERT` ключ шардирования не требуется.</li><li>Для использования с `optimize_skip_unused_shards`, поскольку `sharding_key` необходим для определения того, какие шарды должны быть опрошены</li></ul> |
| `policy_name` (необязательно)  | Имя политики, которая будет использоваться для хранения временных файлов при фоновой отправке                                                                                                                                                                                                                                                                                                                                                                                         |

**См. также**

- Настройка [distributed_foreground_insert](../../../operations/settings/settings.md#distributed_foreground_insert)
- [MergeTree](../../../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-multiple-volumes) — примеры использования

### Настройки Distributed {#distributed-settings}


| Setting                                    | Description                                                                                                                                                                                                               | Default value |
| ------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------- |
| `fsync_after_insert`                       | Выполнять `fsync` для файловых данных после фоновой вставки в Distributed. Гарантирует, что ОС сбросила все вставленные данные в файл **на диске узла-инициатора**.                                                       | `false`       |
| `fsync_directories`                        | Выполнять `fsync` для каталогов. Гарантирует, что ОС обновила метаданные каталога после операций, связанных с фоновыми вставками в таблицу Distributed (после вставки, после отправки данных на шард и т. п.).            | `false`       |
| `skip_unavailable_shards`                  | Если `true`, ClickHouse безошибочно пропускает недоступные шарды. Шард помечается как недоступный, когда: 1) к нему нет доступа из‑за ошибки соединения; 2) шард неразрешим через DNS; 3) таблица не существует на шарде. | `false`       |
| `bytes_to_throw_insert`                    | Если количество сжатых байт, ожидающих фонового `INSERT`, превышает это значение, генерируется исключение. `0` — не генерировать.                                                                                         | `0`           |
| `bytes_to_delay_insert`                    | Если количество сжатых байт, ожидающих фонового `INSERT`, превышает это значение, запрос будет задержан. `0` — не задерживать.                                                                                            | `0`           |
| `max_delay_to_insert`                      | Максимальная задержка вставки данных в таблицу Distributed в секундах, если имеется много байт, ожидающих фоновой отправки.                                                                                               | `60`          |
| `background_insert_batch`                  | То же, что и [`distributed_background_insert_batch`](../../../operations/settings/settings.md#distributed_background_insert_batch)                                                                                        | `0`           |
| `background_insert_split_batch_on_failure` | То же, что и [`distributed_background_insert_split_batch_on_failure`](../../../operations/settings/settings.md#distributed_background_insert_split_batch_on_failure)                                                      | `0`           |
| `background_insert_sleep_time_ms`          | То же, что и [`distributed_background_insert_sleep_time_ms`](../../../operations/settings/settings.md#distributed_background_insert_sleep_time_ms)                                                                        | `0`           |
| `background_insert_max_sleep_time_ms`      | То же, что и [`distributed_background_insert_max_sleep_time_ms`](../../../operations/settings/settings.md#distributed_background_insert_max_sleep_time_ms)                                                                | `0`           |
| `flush_on_detach`                          | Сбрасывать данные на удалённые узлы при `DETACH`/`DROP`/остановке сервера.                                                                                                                                                | `true`        |

:::note
**Параметры надёжности** (`fsync_...`):

* Влияют только на фоновые `INSERT` (т. е. при `distributed_foreground_insert=false`), когда данные сначала сохраняются на диск узла-инициатора и позже, в фоновом режиме, отправляются на шарды.
* Могут значительно снизить производительность `INSERT`.
* Влияют на запись данных, хранящихся в каталоге распределённой таблицы, на **узел, который принял вашу вставку**. Если вам нужны гарантии записи данных в базовые таблицы MergeTree, см. параметры надёжности (`...fsync...`) в `system.merge_tree_settings`.

Для **параметров ограничений вставки** (`..._insert`) см. также:

* параметр [`distributed_foreground_insert`](../../../operations/settings/settings.md#distributed_foreground_insert)
* параметр [`prefer_localhost_replica`](/operations/settings/settings#prefer_localhost_replica)
* `bytes_to_throw_insert` обрабатывается раньше, чем `bytes_to_delay_insert`, поэтому не следует задавать для него значение меньше, чем для `bytes_to_delay_insert`.
  :::

**Пример**

```sql
CREATE TABLE hits_all AS hits
ENGINE = Distributed(logs, default, hits[, sharding_key[, policy_name]])
SETTINGS
    fsync_after_insert=0,
    fsync_directories=0;
```

Данные будут читаться со всех серверов кластера `logs` из таблицы `default.hits`, расположенной на каждом сервере кластера. Данные не только считываются, но и частично обрабатываются на удалённых серверах (насколько это возможно). Например, для запроса с `GROUP BY` данные будут агрегироваться на удалённых серверах, а промежуточные состояния агрегатных функций будут отправляться на сервер, выполняющий запрос. Затем данные будут дополнительно агрегированы.

Вместо имени базы данных можно использовать константное выражение, возвращающее строку. Например: `currentDatabase()`.


## Кластеры {#distributed-clusters}

Кластеры настраиваются в [конфигурационном файле сервера](../../../operations/configuration-files.md):

```xml
<remote_servers>
    <logs>
        <!-- Межсерверный секрет для распределённых запросов на уровне кластера
             по умолчанию: секрет отсутствует (аутентификация не выполняется)

             Если установлен, распределённые запросы будут проверяться на шардах, поэтому как минимум:
             - такой кластер должен существовать на шарде,
             - такой кластер должен иметь тот же секрет.

             Кроме того (и это более важно), initial_user будет
             использоваться в качестве текущего пользователя для запроса.
        -->
        <!-- <secret></secret> -->

        <!-- Необязательно. Разрешены ли распределённые DDL-запросы (конструкция ON CLUSTER) для этого кластера. По умолчанию: true (разрешены). -->
        <!-- <allow_distributed_ddl_queries>true</allow_distributed_ddl_queries> -->

        <shard>
            <!-- Необязательно. Вес шарда при записи данных. По умолчанию: 1. -->
            <weight>1</weight>
            <!-- Необязательно. Имя шарда. Должно быть непустым и уникальным среди шардов в кластере. Если не указано, будет пустым. -->
            <name>shard_01</name>
            <!-- Необязательно. Записывать ли данные только на одну из реплик. По умолчанию: false (записывать данные на все реплики). -->
            <internal_replication>false</internal_replication>
            <replica>
                <!-- Необязательно. Приоритет реплики для балансировки нагрузки (см. также настройку load_balancing). По умолчанию: 1 (меньшее значение означает более высокий приоритет). -->
                <priority>1</priority>
                <host>example01-01-1</host>
                <port>9000</port>
            </replica>
            <replica>
                <host>example01-01-2</host>
                <port>9000</port>
            </replica>
        </shard>
        <shard>
            <weight>2</weight>
            <name>shard_02</name>
            <internal_replication>false</internal_replication>
            <replica>
                <host>example01-02-1</host>
                <port>9000</port>
            </replica>
            <replica>
                <host>example01-02-2</host>
                <secure>1</secure>
                <port>9440</port>
            </replica>
        </shard>
    </logs>
</remote_servers>
```

Здесь определён кластер с именем `logs`, который состоит из двух шардов, каждый из которых содержит две реплики. Шарды — это серверы, содержащие различные части данных (для чтения всех данных необходимо обратиться ко всем шардам). Реплики — это дублирующие серверы (для чтения всех данных можно обратиться к данным на любой из реплик).

Имена кластеров не должны содержать точки.

Для каждого сервера указываются параметры `host`, `port` и, опционально, `user`, `password`, `secure`, `compression`, `bind_host`:


| Parameter     | Description                                                                                                                                                                                                                                                                                                                              | Default Value        |
|---------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------|
| `host`        | Адрес удалённого сервера. Можно использовать доменное имя или IPv4/IPv6-адрес. Если указано доменное имя, сервер выполняет DNS-запрос при запуске и сохраняет результат на всё время работы. Если DNS-запрос завершается неудачно, сервер не запускается. При изменении DNS-записи перезапустите сервер.                                | -                    |
| `port`        | TCP-порт для работы (`tcp_port` в конфигурации, обычно 9000). Не путать с `http_port`.                                                                                                                                                                                                                                                   | -                    |
| `user`        | Имя пользователя для подключения к удалённому серверу. Этот пользователь должен иметь доступ для подключения к указанному серверу. Доступ настраивается в файле `users.xml`. Дополнительные сведения см. в разделе [Права доступа](../../../guides/sre/user-management/index.md).                                                        | `default`            |
| `password`    | Пароль для подключения к удалённому серверу (не маскируется).                                                                                                                                                                                                                                                                            | ''                   |
| `secure`      | Использовать ли защищённое SSL/TLS-соединение. Обычно также требуется указать порт (порт по умолчанию для защищённого соединения — `9440`). Сервер должен прослушивать порт `<tcp_port_secure>9440</tcp_port_secure>` и быть настроен с корректными сертификатами.                                                                       | `false`              |
| `compression` | Использовать сжатие данных.                                                                                                                                                                                                                                                                                                              | `true`               |
| `bind_host`   | Исходный адрес, используемый при подключении к удалённому серверу с этого узла. Поддерживается только IPv4-адрес. Предназначен для продвинутых сценариев развёртывания, когда требуется задать исходный IP-адрес, используемый ClickHouse для распределённых запросов.                                                                 | -                    |

При указании реплик для каждого шарда при чтении будет выбрана одна из доступных реплик. Можно настроить алгоритм балансировки нагрузки (предпочтительную реплику для доступа) — см. настройку [load_balancing](../../../operations/settings/settings.md#load_balancing). Если подключение к серверу не установлено, предпринимается попытка подключения с коротким тайм-аутом ожидания. Если подключение не удалось, выбирается следующая реплика и так далее для всех реплик. Если попытка подключения не удалась для всех реплик, попытка будет повторена тем же образом несколько раз. Это повышает устойчивость, но не обеспечивает полной отказоустойчивости: удалённый сервер может принять подключение, но при этом не работать или работать некорректно.

Можно указать только один шард (в этом случае обработку запроса корректнее называть удалённой, а не распределённой) или любое количество шардов. В каждом шарде можно указать от одной до любого количества реплик. Для каждого шарда можно задать разное количество реплик.

В конфигурации можно указать произвольное количество кластеров.

Для просмотра кластеров используйте таблицу `system.clusters`.

Движок `Distributed` позволяет работать с кластером как с локальным сервером. Однако конфигурацию кластера нельзя задавать динамически, её нужно указывать в конфигурационном файле сервера. Обычно все серверы в кластере имеют одинаковую конфигурацию кластера (хотя это и не является обязательным). Кластеры из конфигурационного файла обновляются на лету, без перезапуска сервера.

Если вам нужно каждый раз отправлять запрос на неизвестный набор шардов и реплик, нет необходимости создавать таблицу `Distributed` — вместо этого используйте табличную функцию `remote`. См. раздел [Табличные функции](../../../sql-reference/table-functions/index.md).



## Запись данных {#distributed-writing-data}

Существует два метода записи данных в кластер:

Первый метод: вы можете определить, на какие серверы записывать какие данные, и выполнять запись непосредственно на каждом шарде. Другими словами, выполнять прямые запросы `INSERT` в удалённые таблицы кластера, на которые ссылается таблица `Distributed`. Это наиболее гибкое решение, поскольку вы можете использовать любую схему шардирования, даже нетривиальную, обусловленную требованиями предметной области. Это также наиболее оптимальное решение, так как данные могут записываться на различные шарды полностью независимо друг от друга.

Второй метод: вы можете выполнять запросы `INSERT` в таблицу `Distributed`. В этом случае таблица сама распределит вставляемые данные по серверам. Для записи в таблицу `Distributed` необходимо настроить параметр `sharding_key` (за исключением случая, когда имеется только один шард).

Для каждого шарда в конфигурационном файле может быть определён параметр `<weight>`. По умолчанию вес равен `1`. Данные распределяются по шардам в количестве, пропорциональном весу шарда. Все веса шардов суммируются, затем вес каждого шарда делится на общую сумму для определения доли каждого шарда. Например, если имеется два шарда, причём первый имеет вес 1, а второй — вес 2, то первому будет отправлена одна треть (1 / 3) вставляемых строк, а второму — две трети (2 / 3).

Для каждого шарда в конфигурационном файле может быть определён параметр `internal_replication`. Если этот параметр установлен в `true`, операция записи выбирает первую работоспособную реплику и записывает данные в неё. Используйте этот параметр, если таблицы, лежащие в основе таблицы `Distributed`, являются реплицируемыми таблицами (например, любой из движков таблиц `Replicated*MergeTree`). Одна из реплик таблицы получит запись, и она будет автоматически реплицирована на другие реплики.

Если `internal_replication` установлен в `false` (по умолчанию), данные записываются во все реплики. В этом случае таблица `Distributed` сама реплицирует данные. Это хуже, чем использование реплицируемых таблиц, поскольку согласованность реплик не проверяется, и со временем они будут содержать немного различающиеся данные.

Для выбора шарда, на который отправляется строка данных, вычисляется выражение шардирования, и берётся остаток от его деления на общий вес шардов. Строка отправляется на шард, который соответствует полуинтервалу остатков от `prev_weights` до `prev_weights + weight`, где `prev_weights` — это общий вес шардов с меньшими номерами, а `weight` — вес данного шарда. Например, если имеется два шарда, причём первый имеет вес 9, а второй — вес 10, то строка будет отправлена на первый шард для остатков из диапазона \[0, 9), а на второй — для остатков из диапазона \[9, 19).

Выражение шардирования может быть любым выражением из констант и столбцов таблицы, которое возвращает целое число. Например, вы можете использовать выражение `rand()` для случайного распределения данных или `UserID` для распределения по остатку от деления идентификатора пользователя (в этом случае данные одного пользователя будут находиться на одном шарде, что упрощает выполнение запросов `IN` и `JOIN` по пользователям). Если один из столбцов распределён недостаточно равномерно, вы можете обернуть его в хеш-функцию, например `intHash64(UserID)`.

Простой остаток от деления является ограниченным решением для шардирования и не всегда подходит. Он работает для средних и больших объёмов данных (десятки серверов), но не для очень больших объёмов данных (сотни серверов и более). В последнем случае используйте схему шардирования, требуемую предметной областью, а не записи в таблицах `Distributed`.

Вам следует уделить внимание схеме шардирования в следующих случаях:


- Используются запросы, которым требуется объединение данных (`IN` или `JOIN`) по определённому ключу. Если данные шардированы по этому ключу, можно использовать локальные `IN` или `JOIN` вместо `GLOBAL IN` или `GLOBAL JOIN`, что гораздо эффективнее.
- Используется большое количество серверов (сотни и более) с большим числом небольших запросов, например, запросов к данным отдельных клиентов (например, веб-сайтов, рекламодателей или партнёров). Чтобы небольшие запросы не влияли на весь кластер, имеет смысл размещать данные одного клиента на одном шарде. В качестве альтернативы можно настроить двухуровневое шардирование: разделить весь кластер на «слои», где слой может состоять из нескольких шардов. Данные одного клиента располагаются в одном слое, но в слой можно добавлять шарды по мере необходимости, а данные внутри них распределяются случайным образом. Для каждого слоя создаются таблицы `Distributed`, а для глобальных запросов создаётся одна общая распределённая таблица.

Данные записываются в фоновом режиме. При вставке в таблицу блок данных просто записывается в локальную файловую систему. Данные отправляются на удалённые серверы в фоновом режиме как можно скорее. Периодичность отправки данных управляется настройками [distributed_background_insert_sleep_time_ms](../../../operations/settings/settings.md#distributed_background_insert_sleep_time_ms) и [distributed_background_insert_max_sleep_time_ms](../../../operations/settings/settings.md#distributed_background_insert_max_sleep_time_ms). Движок `Distributed` отправляет каждый файл с вставленными данными отдельно, но можно включить пакетную отправку файлов с помощью настройки [distributed_background_insert_batch](../../../operations/settings/settings.md#distributed_background_insert_batch). Эта настройка повышает производительность кластера за счёт более эффективного использования ресурсов локального сервера и сети. Следует проверять, успешно ли отправляются данные, просматривая список файлов (данных, ожидающих отправки) в каталоге таблицы: `/var/lib/clickhouse/data/database/table/`. Количество потоков, выполняющих фоновые задачи, можно задать настройкой [background_distributed_schedule_pool_size](/operations/server-configuration-parameters/settings#background_distributed_schedule_pool_size).

Если сервер перестал существовать или был аварийно перезагружен (например, из-за аппаратного сбоя) после `INSERT` в таблицу `Distributed`, вставленные данные могут быть потеряны. Если в каталоге таблицы обнаруживается повреждённая часть данных, она переносится во вложенный каталог `broken` и больше не используется.



## Чтение данных {#distributed-reading-data}

При выполнении запросов к таблице типа `Distributed` запросы `SELECT` отправляются на все шарды и выполняются независимо от того, как данные распределены по шардам (они могут быть распределены совершенно произвольно). При добавлении нового шарда не требуется переносить в него старые данные. Вместо этого можно записывать новые данные, используя больший вес — данные будут распределены немного неравномерно, но запросы будут выполняться корректно и эффективно.

При включении параметра `max_parallel_replicas` обработка запросов распараллеливается по всем репликам внутри одного шарда. Подробнее см. раздел [max_parallel_replicas](../../../operations/settings/settings.md#max_parallel_replicas).

Подробнее о том, как обрабатываются распределённые запросы `in` и `global in`, см. в [этой](/sql-reference/operators/in#distributed-subqueries) документации.


## Виртуальные столбцы {#virtual-columns}

#### \_Shard_num {#\_shard_num}

`_shard_num` — содержит значение `shard_num` из таблицы `system.clusters`. Тип: [UInt32](../../../sql-reference/data-types/int-uint.md).

:::note
Поскольку табличные функции [`remote`](../../../sql-reference/table-functions/remote.md) и [`cluster`](../../../sql-reference/table-functions/cluster.md) внутренне создают временную распределённую таблицу (Distributed), `_shard_num` также доступен и в них.
:::

**См. также**

- Описание [виртуальных столбцов](../../../engines/table-engines/index.md#table_engines-virtual_columns)
- Настройка [`background_distributed_schedule_pool_size`](/operations/server-configuration-parameters/settings#background_distributed_schedule_pool_size)
- Функции [`shardNum()`](../../../sql-reference/functions/other-functions.md#shardNum) и [`shardCount()`](../../../sql-reference/functions/other-functions.md#shardCount)
