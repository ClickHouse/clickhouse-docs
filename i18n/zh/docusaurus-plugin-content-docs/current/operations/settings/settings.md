---
'title': '会话设置'
'sidebar_label': '会话设置'
'slug': '/operations/settings/settings'
'toc_max_heading_level': 2
'description': '在 ``system.settings`` 表中找到的设置。'
'doc_type': 'reference'
---

import ExperimentalBadge from '@theme/badges/ExperimentalBadge';
import BetaBadge from '@theme/badges/BetaBadge';
import CloudOnlyBadge from '@theme/badges/CloudOnlyBadge';
import SettingsInfoBlock from '@theme/SettingsInfoBlock/SettingsInfoBlock';
import VersionHistory from '@theme/VersionHistory/VersionHistory';

<!-- Autogenerated -->
以下所有设置也可在表 [system.settings](/docs/operations/system-tables/settings) 中找到。这些设置是自动生成的，来自 [source](https://github.com/ClickHouse/ClickHouse/blob/master/src/Core/Settings.cpp)。
## add_http_cors_header {#add_http_cors_header} 



<SettingsInfoBlock type="Bool" default_value="0" />

写入添加 HTTP CORS 头。
## additional_result_filter {#additional_result_filter} 

应用于 `SELECT` 查询结果的附加过滤表达式。
此设置不应用于任何子查询。

**示例**

```sql
INSERT INTO table_1 VALUES (1, 'a'), (2, 'bb'), (3, 'ccc'), (4, 'dddd');
SElECT * FROM table_1;
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 2 │ bb   │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
```sql
SELECT *
FROM table_1
SETTINGS additional_result_filter = 'x != 2'
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
## additional_table_filters {#additional_table_filters} 



<SettingsInfoBlock type="Map" default_value="{}" />

在从指定表读取后应用的附加过滤表达式。

**示例**

```sql
INSERT INTO table_1 VALUES (1, 'a'), (2, 'bb'), (3, 'ccc'), (4, 'dddd');
SELECT * FROM table_1;
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 2 │ bb   │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
```sql
SELECT *
FROM table_1
SETTINGS additional_table_filters = {'table_1': 'x != 2'}
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
## aggregate_functions_null_for_empty {#aggregate_functions_null_for_empty} 



<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在查询中重写所有聚合函数，为它们添加 [-OrNull](/sql-reference/aggregate-functions/combinators#-ornull) 后缀。为了兼容 SQL 标准，请启用此选项。
它是通过查询重写实现的（类似于 [count_distinct_implementation](#count_distinct_implementation) 设置），以便为分布式查询获取一致的结果。

可能值：

- 0 — 禁用。
- 1 — 启用。

**示例**

考虑以下带有聚合函数的查询：
```sql
SELECT SUM(-1), MAX(0) FROM system.one WHERE 0;
```

在 `aggregate_functions_null_for_empty = 0` 时，它会产生：
```text
┌─SUM(-1)─┬─MAX(0)─┐
│       0 │      0 │
└─────────┴────────┘
```

在 `aggregate_functions_null_for_empty = 1` 时，结果将是：
```text
┌─SUMOrNull(-1)─┬─MAXOrNull(0)─┐
│          NULL │         NULL │
└───────────────┴──────────────┘
```
## aggregation_in_order_max_block_bytes {#aggregation_in_order_max_block_bytes} 



<SettingsInfoBlock type="UInt64" default_value="50000000" />

按照主键顺序聚合时块的最大字节大小。较小的块大小可以在聚合的最终合并阶段实现更大的并行性。
## aggregation_memory_efficient_merge_threads {#aggregation_memory_efficient_merge_threads} 



<SettingsInfoBlock type="UInt64" default_value="0" />

用于以内存高效模式合并中间聚合结果的线程数量。更大时，消耗更多内存。0 表示 - 同 'max_threads'。
## allow_aggregate_partitions_independently {#allow_aggregate_partitions_independently} 



<SettingsInfoBlock type="Bool" default_value="0" />

当分区键适合分组键时，启用在单独线程上独立聚合分区。当分区数量接近核心数量且分区大小大致相等时，这是有益的。
## allow_archive_path_syntax {#allow_archive_path_syntax} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.8"},{"label": "1"},{"label": "Added new setting to allow disabling archive path syntax."}]}, {"id": "row-2","items": [{"label": "24.5"},{"label": "1"},{"label": "Added new setting to allow disabling archive path syntax."}]}]}/>

文件/S3 引擎/表函数将解析路径为 '::' 作为 `<archive> :: <file>`，前提是归档具有正确的扩展名。
## allow_asynchronous_read_from_io_pool_for_merge_tree {#allow_asynchronous_read_from_io_pool_for_merge_tree} 



<SettingsInfoBlock type="Bool" default_value="0" />

使用后台 I/O 池从 MergeTree 表中读取。此设置可以提高 I/O 绑定查询的性能。
## allow_changing_replica_until_first_data_packet {#allow_changing_replica_until_first_data_packet} 



<SettingsInfoBlock type="Bool" default_value="0" />

如果启用，在加速请求中，我们可以在接收第一个数据包之前建立新的连接，即使我们已经取得了一些进展
（但进展没有更新到 `receive_data_timeout` 超时），否则在第一次取得进展后我们禁用更改副本。
## allow_create_index_without_type {#allow_create_index_without_type} 



<SettingsInfoBlock type="Bool" default_value="0" />

允许没有 TYPE 的 CREATE INDEX 查询。查询将被忽略。用于 SQL 兼容性测试。
## allow_custom_error_code_in_throwif {#allow_custom_error_code_in_throwif} 



<SettingsInfoBlock type="Bool" default_value="0" />

在函数 throwIf() 中启用自定义错误代码。如果为真，抛出的异常可能具有意外的错误代码。
## allow_ddl {#allow_ddl} 



<SettingsInfoBlock type="Bool" default_value="1" />

如果设置为 true，则用户可以执行 DDL 查询。
## allow_deprecated_database_ordinary {#allow_deprecated_database_ordinary} 



<SettingsInfoBlock type="Bool" default_value="0" />

允许创建具有已废弃普通引擎的数据库。
## allow_deprecated_error_prone_window_functions {#allow_deprecated_error_prone_window_functions} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.5"},{"label": "0"},{"label": "Allow usage of deprecated error prone window functions (neighbor, runningAccumulate, runningDifferenceStartingWithFirstValue, runningDifference)"}]}]}/>

允许使用已废弃的易出错窗口函数（邻近、runningAccumulate、runningDifferenceStartingWithFirstValue、runningDifference）。
## allow_deprecated_snowflake_conversion_functions {#allow_deprecated_snowflake_conversion_functions} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "0"},{"label": "Disabled deprecated functions snowflakeToDateTime[64] and dateTime[64]ToSnowflake."}]}]}/>

函数 `snowflakeToDateTime`、`snowflakeToDateTime64`、`dateTimeToSnowflake` 和 `dateTime64ToSnowflake` 已被废弃并默认禁用。
请改用函数 `snowflakeIDToDateTime`、`snowflakeIDToDateTime64`、`dateTimeToSnowflakeID` 和 `dateTime64ToSnowflakeID`。

要重新启用已废弃的函数（例如，在过渡期间），请将此设置设置为 `true`。
## allow_deprecated_syntax_for_merge_tree {#allow_deprecated_syntax_for_merge_tree} 



<SettingsInfoBlock type="Bool" default_value="0" />

允许使用已废弃引擎定义语法创建 *MergeTree 表。
## allow_distributed_ddl {#allow_distributed_ddl} 



<SettingsInfoBlock type="Bool" default_value="1" />

如果设置为 true，则用户可以执行分布式 DDL 查询。
## allow_drop_detached {#allow_drop_detached} 



<SettingsInfoBlock type="Bool" default_value="0" />

允许执行 ALTER TABLE ... DROP DETACHED PART[ITION] ... 查询。
## allow_dynamic_type_in_join_keys {#allow_dynamic_type_in_join_keys} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.10"},{"label": "0"},{"label": "Disallow using Dynamic type in JOIN keys by default"}]}]}/>

允许在 JOIN 键中使用动态类型。添加兼容性。由于与其他类型的比较可能导致意外结果，不推荐在 JOIN 键中使用动态类型。
## allow_execute_multiif_columnar {#allow_execute_multiif_columnar} 



<SettingsInfoBlock type="Bool" default_value="1" />

允许以列式模式执行 multiIf 函数。
## allow_experimental_analyzer {#allow_experimental_analyzer} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "1"},{"label": "Enable analyzer and planner by default."}]}]}/>

允许新查询分析器。
## allow_experimental_codecs {#allow_experimental_codecs} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

如果设置为 true，允许指定实验性压缩编解码器（但我们还没有这些，且此选项无效）。
## allow_experimental_correlated_subqueries {#allow_experimental_correlated_subqueries} 

<BetaBadge/>



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "1"},{"label": "Mark correlated subqueries support as Beta."}]}, {"id": "row-2","items": [{"label": "25.4"},{"label": "0"},{"label": "Added new setting to allow correlated subqueries execution."}]}]}/>

允许执行相关子查询。
## allow_experimental_database_glue_catalog {#allow_experimental_database_glue_catalog} 

<BetaBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.3"},{"label": "0"},{"label": "Allow experimental database engine DataLakeCatalog with catalog_type = 'glue'"}]}]}/>

允许实验性数据库引擎 DataLakeCatalog，catalog_type = 'glue'。
## allow_experimental_database_hms_catalog {#allow_experimental_database_hms_catalog} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "0"},{"label": "Allow experimental database engine DataLakeCatalog with catalog_type = 'hive'"}]}]}/>

允许实验性数据库引擎 DataLakeCatalog，catalog_type = 'hms'。
## allow_experimental_database_iceberg {#allow_experimental_database_iceberg} 

<BetaBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.12"},{"label": "0"},{"label": "New setting."}]}]}/>

允许实验性数据库引擎 DataLakeCatalog，catalog_type = 'iceberg'。
## allow_experimental_database_materialized_postgresql {#allow_experimental_database_materialized_postgresql} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

允许创建数据库，使用 Engine=MaterializedPostgreSQL(...)。
## allow_experimental_database_unity_catalog {#allow_experimental_database_unity_catalog} 

<BetaBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.3"},{"label": "0"},{"label": "Allow experimental database engine DataLakeCatalog with catalog_type = 'unity'"}]}]}/>

允许实验性数据库引擎 DataLakeCatalog，catalog_type = 'unity'。
## allow_experimental_delta_kernel_rs {#allow_experimental_delta_kernel_rs} 

<BetaBadge/>



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "1"},{"label": "New setting"}]}]}/>

允许实验性 delta-kernel-rs 实现。
## allow_experimental_delta_lake_writes {#allow_experimental_delta_lake_writes} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.9"},{"label": "0"},{"label": "New setting."}]}]}/>

启用 delta-kernel 写入功能。
## allow_experimental_full_text_index {#allow_experimental_full_text_index} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "0"},{"label": "Enable experimental text index"}]}]}/>

如果设置为 true，允许使用实验性文本索引。
## allow_experimental_funnel_functions {#allow_experimental_funnel_functions} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

启用实验性漏斗分析函数。
## allow_experimental_hash_functions {#allow_experimental_hash_functions} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

启用实验性哈希函数。
## allow_experimental_iceberg_compaction {#allow_experimental_iceberg_compaction} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "0"},{"label": "New setting "}]}]}/>

允许明确使用 'OPTIMIZE' 对于 iceberg 表。
## allow_experimental_insert_into_iceberg {#allow_experimental_insert_into_iceberg} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.7"},{"label": "0"},{"label": "New setting."}]}]}/>

允许对 iceberg 执行 `insert` 查询。
## allow_experimental_join_right_table_sorting {#allow_experimental_join_right_table_sorting} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.9"},{"label": "0"},{"label": "If it is set to true, and the conditions of `join_to_sort_minimum_perkey_rows` and `join_to_sort_maximum_table_rows` are met, rerange the right table by key to improve the performance in left or inner hash join"}]}]}/>

如果设置为 true，并且满足 `join_to_sort_minimum_perkey_rows` 和 `join_to_sort_maximum_table_rows` 的条件，那么按键对右边表重新排序以提高左连接或内连接的性能。
## allow_experimental_kafka_offsets_storage_in_keeper {#allow_experimental_kafka_offsets_storage_in_keeper} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.8"},{"label": "0"},{"label": "Allow the usage of experimental Kafka storage engine that stores the committed offsets in ClickHouse Keeper"}]}]}/>

允许实验性功能将与 Kafka 相关的偏移存储在 ClickHouse Keeper 中。当启用时，可以将 ClickHouse Keeper 路径和副本名称指定给 Kafka 表引擎。因此，不使用常规的 Kafka 引擎，而是使用一种新类型的存储引擎，主要在 ClickHouse Keeper 中存储已提交的偏移量。
## allow_experimental_kusto_dialect {#allow_experimental_kusto_dialect} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0"},{"label": "A new setting"}]}]}/>

启用 Kusto 查询语言 (KQL) - SQL 的替代品。
## allow_experimental_live_view {#allow_experimental_live_view} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

允许创建已废弃的 LIVE VIEW。

可能的值：

- 0 — 禁用对实时视图的工作。
- 1 — 启用对实时视图的工作。
## allow_experimental_materialized_postgresql_table {#allow_experimental_materialized_postgresql_table} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

允许使用 MaterializedPostgreSQL 表引擎。默认禁用，因为此功能是实验性的。
## allow_experimental_nlp_functions {#allow_experimental_nlp_functions} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

启用实验性自然语言处理函数。
## allow_experimental_object_type {#allow_experimental_object_type} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

允许使用过时的对象数据类型。
## allow_experimental_parallel_reading_from_replicas {#allow_experimental_parallel_reading_from_replicas} 

<BetaBadge/>



<SettingsInfoBlock type="UInt64" default_value="0" />

允许从每个分片中使用最多 `max_parallel_replicas` 个副本执行 SELECT 查询。读取是并行化和动态协调的。0 - 禁用，1 - 启用，在失败时无声禁用，2 - 启用，在失败时抛出异常。
## allow_experimental_prql_dialect {#allow_experimental_prql_dialect} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0"},{"label": "A new setting"}]}]}/>

启用 PRQL - SQL 的替代品。
## allow_experimental_qbit_type {#allow_experimental_qbit_type} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.10"},{"label": "0"},{"label": "New experimental setting"}]}]}/>

允许创建 [QBit](../../sql-reference/data-types/qbit.md) 数据类型。
## allow_experimental_query_deduplication {#allow_experimental_query_deduplication} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

实验性数据去重，用于基于部分 UUID 的 SELECT 查询。
## allow_experimental_statistics {#allow_experimental_statistics} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "0"},{"label": "The setting was renamed. The previous name is `allow_experimental_statistic`."}]}]}/>

允许定义具有 [统计信息](../../engines/table-engines/mergetree-family/mergetree.md/#table_engine-mergetree-creating-a-table) 的列，并且能够 [操作统计信息](../../engines/table-engines/mergetree-family/mergetree.md/#column-statistics)。
## allow_experimental_time_series_aggregate_functions {#allow_experimental_time_series_aggregate_functions} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.6"},{"label": "0"},{"label": "New setting to enable experimental timeSeries* aggregate functions."}]}]}/>

实验性 timeSeries* 聚合函数，用于 Prometheus 类似的时间序列重采样、速率和差异计算。
## allow_experimental_time_series_table {#allow_experimental_time_series_table} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.8"},{"label": "0"},{"label": "Added new setting to allow the TimeSeries table engine"}]}]}/>

允许创建具有 [TimeSeries](../../engines/table-engines/integrations/time-series.md) 表引擎的表。可能的值：
- 0 — [TimeSeries](../../engines/table-engines/integrations/time-series.md) 表引擎被禁用。
- 1 — [TimeSeries](../../engines/table-engines/integrations/time-series.md) 表引擎被启用。
## allow_experimental_time_time64_type {#allow_experimental_time_time64_type} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.6"},{"label": "0"},{"label": "New settings. Allows to use a new experimental Time and Time64 data types."}]}]}/>

允许创建 [Time](../../sql-reference/data-types/time.md) 和 [Time64](../../sql-reference/data-types/time64.md) 数据类型。
## allow_experimental_window_view {#allow_experimental_window_view} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

启用窗口视图。尚不成熟。
## allow_experimental_ytsaurus_dictionary_source {#allow_experimental_ytsaurus_dictionary_source} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "0"},{"label": "New setting."}]}]}/>

实验性字典来源，用于与 YTsaurus 集成。
## allow_experimental_ytsaurus_table_engine {#allow_experimental_ytsaurus_table_engine} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "0"},{"label": "New setting."}]}]}/>

实验性表引擎，用于与 YTsaurus 集成。
## allow_experimental_ytsaurus_table_function {#allow_experimental_ytsaurus_table_function} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "0"},{"label": "New setting."}]}]}/>

实验性表引擎，用于与 YTsaurus 集成。
## allow_general_join_planning {#allow_general_join_planning} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "1"},{"label": "Allow more general join planning algorithm when hash join algorithm is enabled."}]}]}/>

允许使用更通用的连接规划算法，可以处理更复杂的条件，但仅适用于哈希连接。如果未启用哈希连接，则将使用传统的连接规划算法，而不考虑此设置的值。
## allow_get_client_http_header {#allow_get_client_http_header} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "0"},{"label": "Introduced a new function."}]}]}/>

允许使用 `getClientHTTPHeader` 函数来获取当前 HTTP 请求标题的值。出于安全原因，默认未启用此功能，因为某些标题（例如 `Cookie`）可能包含敏感信息。请注意，`X-ClickHouse-*` 和 `Authentication` 标头始终受到限制，无法通过此函数获取。
## allow_hyperscan {#allow_hyperscan} 



<SettingsInfoBlock type="Bool" default_value="1" />

允许使用 Hyperscan 库的函数。禁用以避免可能较长的编译时间和过多的资源占用。
## allow_introspection_functions {#allow_introspection_functions} 



<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用用于查询剖析的 [内省函数](../../sql-reference/functions/introspection.md)。

可能值：

- 1 — 启用内省函数。
- 0 — 禁用内省函数。

**另见**

- [采样查询分析器](../../operations/optimizing-performance/sampling-query-profiler.md)
- 系统表 [trace_log](/operations/system-tables/trace_log)
## allow_materialized_view_with_bad_select {#allow_materialized_view_with_bad_select} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "0"},{"label": "Don't allow creating MVs referencing nonexistent columns or tables"}]}, {"id": "row-2","items": [{"label": "24.9"},{"label": "1"},{"label": "Support (but not enable yet) stricter validation in CREATE MATERIALIZED VIEW"}]}]}/>

允许使用引用不存在表或列的 SELECT 查询创建物化视图（MATERIALIZED VIEW）。它仍然必须在语法上有效。该选项不适用于可刷新的 MV。如果 MV 架构需要从 SELECT 查询推断（即 CREATE 缺少列列表且没有 TO 表），则不会使用。可以在其源表之前创建 MV。
## allow_named_collection_override_by_default {#allow_named_collection_override_by_default} 



<SettingsInfoBlock type="Bool" default_value="1" />

允许默认命名集合字段的重写。
## allow_non_metadata_alters {#allow_non_metadata_alters} 



<SettingsInfoBlock type="Bool" default_value="1" />

允许执行影响的不仅仅是表元数据的调整，而是还影响磁盘上的数据。
## allow_nonconst_timezone_arguments {#allow_nonconst_timezone_arguments} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.4"},{"label": "0"},{"label": "Allow non-const timezone arguments in certain time-related functions like toTimeZone(), fromUnixTimestamp*(), snowflakeToDateTime*()."}]}]}/>

允许某些时间相关函数（如 toTimeZone()、fromUnixTimestamp*()、snowflakeToDateTime*()）中的非常量时区参数。
此设置仅存在于兼容性原因。在 ClickHouse 中，时区是数据类型的属性，因此是列的属性。
启用此设置容易给人错觉，认为列中的不同值可以具有不同的时区。
因此，请勿启用此设置。
## allow_nondeterministic_mutations {#allow_nondeterministic_mutations} 



<SettingsInfoBlock type="Bool" default_value="0" />

用户级设置，允许复制表上的变更使用非确定性函数如 `dictGet`。

考虑到例如字典可以在节点之间不同步，默认情况下不允许从中提取值的变更在复制表上进行。启用此设置允许这种行为，由用户负责确保所使用的数据在所有节点之间保持同步。

**示例**

```xml
<profiles>
    <default>
        <allow_nondeterministic_mutations>1</allow_nondeterministic_mutations>

        <!-- ... -->
    </default>

    <!-- ... -->

</profiles>
```
## allow_nondeterministic_optimize_skip_unused_shards {#allow_nondeterministic_optimize_skip_unused_shards} 



<SettingsInfoBlock type="Bool" default_value="0" />

允许在分片键中使用非确定性函数（如 `rand` 或 `dictGet`，因为后者在更新上有一些陷阱）。

可能值：

- 0 — 不允许。
- 1 — 允许。
## allow_not_comparable_types_in_comparison_functions {#allow_not_comparable_types_in_comparison_functions} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0"},{"label": "Don't allow not comparable types in comparison functions by default"}]}]}/>

允许或限制在比较函数 `equal/less/greater/etc` 中使用不可比较的类型（如 JSON/对象/聚合函数）。
## allow_not_comparable_types_in_order_by {#allow_not_comparable_types_in_order_by} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0"},{"label": "Don't allow not comparable types in order by by default"}]}]}/>

允许或限制在 ORDER BY 键中使用不可比较的类型（如 JSON/对象/聚合函数）。
## allow_prefetched_read_pool_for_local_filesystem {#allow_prefetched_read_pool_for_local_filesystem} 



<SettingsInfoBlock type="Bool" default_value="0" />

如果所有部分都位于本地文件系统，则优先考虑预取线程池。
## allow_prefetched_read_pool_for_remote_filesystem {#allow_prefetched_read_pool_for_remote_filesystem} 



<SettingsInfoBlock type="Bool" default_value="1" />

如果所有部分均位于远程文件系统，则优先考虑预取线程池。
## allow_push_predicate_ast_for_distributed_subqueries {#allow_push_predicate_ast_for_distributed_subqueries} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "1"},{"label": "A new setting"}]}]}/>

允许在启用分析器的分布式子查询的 AST 级别推出谓词。
## allow_push_predicate_when_subquery_contains_with {#allow_push_predicate_when_subquery_contains_with} 



<SettingsInfoBlock type="Bool" default_value="1" />

允许在子查询包含 WITH 子句时推出谓词。
## allow_reorder_prewhere_conditions {#allow_reorder_prewhere_conditions} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "1"},{"label": "New setting"}]}]}/>

移动 WHERE 的条件到 PREWHERE 时，允许重新排序以优化过滤。
## allow_settings_after_format_in_insert {#allow_settings_after_format_in_insert} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "22.4"},{"label": "0"},{"label": "Do not allow SETTINGS after FORMAT for INSERT queries because ClickHouse interpret SETTINGS as some values, which is misleading"}]}]}/>

控制在 `INSERT` 查询中是否允许在 `FORMAT` 后面使用 `SETTINGS`。不建议使用此选项，因为这可能将部分 `SETTINGS` 解释为值。

示例：

```sql
INSERT INTO FUNCTION null('foo String') SETTINGS max_threads=1 VALUES ('bar');
```

但以下查询将仅在 `allow_settings_after_format_in_insert` 为真时有效：

```sql
SET allow_settings_after_format_in_insert=1;
INSERT INTO FUNCTION null('foo String') VALUES ('bar') SETTINGS max_threads=1;
```

可能值：

- 0 — 禁止。
- 1 — 允许。

:::note
仅在如果您的用例依赖于旧语法的情况下使用此设置以保持向后兼容性。
:::
## allow_simdjson {#allow_simdjson} 



<SettingsInfoBlock type="Bool" default_value="1" />

允许在 'JSON*' 函数中使用 simdjson 库，如果 AVX2 指令可用。禁用后将使用 rapidjson。
## allow_statistics_optimize {#allow_statistics_optimize} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "0"},{"label": "The setting was renamed. The previous name is `allow_statistic_optimize`."}]}]}/>

允许使用统计信息来优化查询。
## allow_suspicious_codecs {#allow_suspicious_codecs} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "20.5"},{"label": "0"},{"label": "Don't allow to specify meaningless compression codecs"}]}]}/>

如果设置为 true，允许指定无意义的压缩编解码器。
## allow_suspicious_fixed_string_types {#allow_suspicious_fixed_string_types} 



<SettingsInfoBlock type="Bool" default_value="0" />

在 CREATE TABLE 语句中，允许创建 FixedString(n) 类型的列，其中 n > 256。长度 >= 256 的 FixedString 是可疑的，并且很可能表示误用。
## allow_suspicious_indices {#allow_suspicious_indices} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.4"},{"label": "0"},{"label": "If true, index can defined with identical expressions"}]}]}/>

拒绝具有相同表达式的主键/辅助索引和排序键。
## allow_suspicious_low_cardinality_types {#allow_suspicious_low_cardinality_types} 



<SettingsInfoBlock type="Bool" default_value="0" />

允许或限制使用 [LowCardinality](../../sql-reference/data-types/lowcardinality.md) 与大小固定为 8 字节或更小的数据类型：数值数据类型和 `FixedString(8_bytes_or_less)`。

对于小的固定值，使用 `LowCardinality` 通常效率不高，因为 ClickHouse 为每一行存储一个数值索引。结果：

- 磁盘空间使用可能增加。
- RAM 消耗可能更高，取决于字典的大小。
- 某些函数由于额外的编码/解码操作可能会运行得更慢。

MergeTree 引擎表的合并时间可能因上述所有原因而增加。

可能值：

- 1 — 不限制使用 `LowCardinality`。
- 0 — 限制使用 `LowCardinality`。
## allow_suspicious_primary_key {#allow_suspicious_primary_key} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "0"},{"label": "Forbid suspicious PRIMARY KEY/ORDER BY for MergeTree (i.e. SimpleAggregateFunction)"}]}]}/>

允许可疑的 `PRIMARY KEY`/`ORDER BY` 用于 MergeTree（例如 SimpleAggregateFunction）。
## allow_suspicious_ttl_expressions {#allow_suspicious_ttl_expressions} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.12"},{"label": "0"},{"label": "It is a new setting, and in previous versions the behavior was equivalent to allowing."}]}]}/>

拒绝不依赖于任何表列的 TTL 表达式。这通常表明用户错误。
## allow_suspicious_types_in_group_by {#allow_suspicious_types_in_group_by} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "0"},{"label": "Don't allow Variant/Dynamic types in GROUP BY by default"}]}]}/>

允许或限制在 GROUP BY 键中使用 [Variant](../../sql-reference/data-types/variant.md) 和 [Dynamic](../../sql-reference/data-types/dynamic.md) 类型。
## allow_suspicious_types_in_order_by {#allow_suspicious_types_in_order_by} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "0"},{"label": "Don't allow Variant/Dynamic types in ORDER BY by default"}]}]}/>

允许或限制在 ORDER BY 键中使用 [Variant](../../sql-reference/data-types/variant.md) 和 [Dynamic](../../sql-reference/data-types/dynamic.md) 类型。
## allow_suspicious_variant_types {#allow_suspicious_variant_types} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "0"},{"label": "Don't allow creating Variant type with suspicious variants by default"}]}]}/>

在 CREATE TABLE 语句中，允许指定具有相似变体类型（例如，不同的数值或日期类型）的 Variant 类型。启用此设置可能在处理相似类型值时引入一些模糊性。
## allow_unrestricted_reads_from_keeper {#allow_unrestricted_reads_from_keeper} 



<SettingsInfoBlock type="Bool" default_value="0" />

允许无条件地从 system.zookeeper 表中读取，这可能很方便，但对 zookeeper 来说并不安全。
## alter_move_to_space_execute_async {#alter_move_to_space_execute_async} 



<SettingsInfoBlock type="Bool" default_value="0" />

异步执行 ALTER TABLE MOVE ... TO [DISK|VOLUME]。
## alter_partition_verbose_result {#alter_partition_verbose_result} 



<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用成功应用于分区和部分的操作的信息显示。
适用于 [ATTACH PARTITION|PART](/sql-reference/statements/alter/partition#attach-partitionpart) 和 [FREEZE PARTITION](/sql-reference/statements/alter/partition#freeze-partition)。

可能值：

- 0 — 禁用详细信息。
- 1 — 启用详细信息。

**示例**

```sql
CREATE TABLE test(a Int64, d Date, s String) ENGINE = MergeTree PARTITION BY toYYYYMDECLARE(d) ORDER BY a;
INSERT INTO test VALUES(1, '2021-01-01', '');
INSERT INTO test VALUES(1, '2021-01-01', '');
ALTER TABLE test DETACH PARTITION ID '202101';

ALTER TABLE test ATTACH PARTITION ID '202101' SETTINGS alter_partition_verbose_result = 1;

┌─command_type─────┬─partition_id─┬─part_name────┬─old_part_name─┐
│ ATTACH PARTITION │ 202101       │ 202101_7_7_0 │ 202101_5_5_0  │
│ ATTACH PARTITION │ 202101       │ 202101_8_8_0 │ 202101_6_6_0  │
└──────────────────┴──────────────┴──────────────┴───────────────┘

ALTER TABLE test FREEZE SETTINGS alter_partition_verbose_result = 1;

┌─command_type─┬─partition_id─┬─part_name────┬─backup_name─┬─backup_path───────────────────┬─part_backup_path────────────────────────────────────────────┐
│ FREEZE ALL   │ 202101       │ 202101_7_7_0 │ 8           │ /var/lib/clickhouse/shadow/8/ │ /var/lib/clickhouse/shadow/8/data/default/test/202101_7_7_0 │
│ FREEZE ALL   │ 202101       │ 202101_8_8_0 │ 8           │ /var/lib/clickhouse/shadow/8/ │ /var/lib/clickhouse/shadow/8/data/default/test/202101_8_8_0 │
└──────────────┴──────────────┴──────────────┴─────────────┴───────────────────────────────┴─────────────────────────────────────────────────────────────┘
```
## alter_sync {#alter_sync} 



<SettingsInfoBlock type="UInt64" default_value="1" />

允许设置在 [ALTER](../../sql-reference/statements/alter/index.md)、[OPTIMIZE](../../sql-reference/statements/optimize.md) 或 [TRUNCATE](../../sql-reference/statements/truncate.md) 查询上在副本上等待执行的操作。

可能值：

- `0` — 不等待。
- `1` — 等待自身执行。
- `2` — 等待所有人。

云默认值：`1`。

:::note
`alter_sync` 仅适用于 `Replicated` 表，对非 `Replicated` 表的调整无效。
:::
## alter_update_mode {#alter_update_mode} 



<SettingsInfoBlock type="AlterUpdateMode" default_value="heavy" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "heavy"},{"label": "A new setting"}]}]}/>

用于具有 `UPDATE` 命令的 `ALTER` 查询的模式。

可能值：
- `heavy` - 运行常规变更。
- `lightweight` - 如果可能，运行轻量级更新，否则运行常规变更。
- `lightweight_force` - 如果可能，运行轻量级更新，否则抛出异常。
## analyze_index_with_space_filling_curves {#analyze_index_with_space_filling_curves} 



<SettingsInfoBlock type="Bool" default_value="1" />

如果表的索引中具有空间填充曲线，例如 `ORDER BY mortonEncode(x, y)` 或 `ORDER BY hilbertEncode(x, y)`，并且查询对其参数有条件，例如 `x >= 10 AND x <= 20 AND y >= 20 AND y <= 30`，则使用空间填充曲线进行索引分析。
## analyzer_compatibility_allow_compound_identifiers_in_unflatten_nested {#analyzer_compatibility_allow_compound_identifiers_in_unflatten_nested} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "1"},{"label": "New setting."}]}]}/>

允许将复合标识符添加到嵌套。这是一个兼容性设置，因为它会改变查询结果。当禁用时，`SELECT a.b.c FROM table ARRAY JOIN a` 不起作用，且 `SELECT a FROM table` 不包括 `a.b.c` 列到 `Nested a` 结果中。
## analyzer_compatibility_join_using_top_level_identifier {#analyzer_compatibility_join_using_top_level_identifier} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "0"},{"label": "Force to resolve identifier in JOIN USING from projection"}]}]}/>

强制从投影中解析 JOIN USING 中的标识符（例如，在 `SELECT a + 1 AS b FROM t1 JOIN t2 USING (b)` 中，将通过 `t1.a + 1 = t2.b` 进行连接，而不是 `t1.b = t2.b`）。
## any_join_distinct_right_table_keys {#any_join_distinct_right_table_keys} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "19.14"},{"label": "0"},{"label": "Disable ANY RIGHT and ANY FULL JOINs by default to avoid inconsistency"}]}]}/>

在 `ANY INNER|LEFT JOIN` 操作中启用传统的 ClickHouse 服务器行为。

:::note
仅在您的用例依赖于旧 `JOIN` 行为的情况下使用此设置以保持向后兼容性。
:::

启用传统行为时：

- `t1 ANY LEFT JOIN t2` 和 `t2 ANY RIGHT JOIN t1` 操作的结果不相等，因为 ClickHouse 使用许多对一个左到右表键映射的逻辑。
- `ANY INNER JOIN` 操作的结果包含来自左表的所有行，就像 `SEMI LEFT JOIN` 操作一样。

禁用传统行为时：

- `t1 ANY LEFT JOIN t2` 和 `t2 ANY RIGHT JOIN t1` 操作的结果相等，因为 ClickHouse 使用提供多个键映射的逻辑。
- `ANY INNER JOIN` 操作的结果从左表和右表各包含一行每个键。

可能值：

- 0 — 禁用传统行为。
- 1 — 启用传统行为。

另见：

- [JOIN 严格性](/sql-reference/statements/select/join#settings)
## apply_deleted_mask {#apply_deleted_mask} 



<SettingsInfoBlock type="Bool" default_value="1" />

启用过滤掉通过轻量级 DELETE 删除的行。如果禁用，查询将能够读取那些行。这对于调试和“取消删除”场景很有用。
## apply_mutations_on_fly {#apply_mutations_on_fly} 



<SettingsInfoBlock type="Bool" default_value="0" />

如果为真，则在 SELECT 时应用尚未在数据部分中物化的变更（UPDATE 和 DELETE）。
## apply_patch_parts {#apply_patch_parts} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "1"},{"label": "A new setting"}]}]}/>

如果为真，则在 SELECT 时应用补丁部分（表示轻量级更新）。
## apply_patch_parts_join_cache_buckets {#apply_patch_parts_join_cache_buckets} 



<SettingsInfoBlock type="NonZeroUInt64" default_value="8" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "8"},{"label": "New setting"}]}]}/>

加入模式中应用补丁部分的临时缓存中的桶数。
## apply_settings_from_server {#apply_settings_from_server} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.2"},{"label": "1"},{"label": "Client-side code (e.g. INSERT input parsing and query output formatting) will use the same settings as the server, including settings from server config."}]}]}/>

客户端是否应该接受来自服务器的设置。

这仅影响在客户端中执行的操作，特别是解析 INSERT 输入数据和格式化查询结果。大多数查询执行发生在服务器上，并不受此设置的影响。

通常应该在用户配置文件（users.xml 或 `ALTER USER` 等查询）中设置此设置，而不是通过客户端（客户端命令行参数、`SET` 查询或 `SELECT` 查询的 `SETTINGS` 部分）。通过客户端可以将其更改为 false，但无法更改为 true（因为如果用户配置文件中具有 `apply_settings_from_server = false`，则服务器不会发送设置）。

请注意，最初（24.12）有一个服务器设置（`send_settings_to_client`），但后来被此客户端设置取代，以改善可用性。
## asterisk_include_alias_columns {#asterisk_include_alias_columns} 



<SettingsInfoBlock type="Bool" default_value="0" />

为通配符查询（`SELECT *`）包括 [ALIAS](../../sql-reference/statements/create/table.md/#alias) 列。

可能值：

- 0 - 禁用
- 1 - 启用
## asterisk_include_materialized_columns {#asterisk_include_materialized_columns} 



<SettingsInfoBlock type="Bool" default_value="0" />

为通配符查询 (`SELECT *`) 包括 [MATERIALIZED](/sql-reference/statements/create/view#materialized-view) 列。

可能值：

- 0 - 禁用
- 1 - 启用
## async_insert {#async_insert} 



<SettingsInfoBlock type="Bool" default_value="0" />

如果为真，来自 INSERT 查询的数据将存储在队列中，并在后台刷新到表中。如果 wait_for_async_insert 为假，INSERT 查询几乎会立即被处理，否则客户端将等待直到数据被刷新到表中。
## async_insert_busy_timeout_decrease_rate {#async_insert_busy_timeout_decrease_rate} 



<SettingsInfoBlock type="Double" default_value="0.2" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "0.2"},{"label": "The exponential growth rate at which the adaptive asynchronous insert timeout decreases"}]}]}/>

自适应异步插入超时降低的指数增长率。
## async_insert_busy_timeout_increase_rate {#async_insert_busy_timeout_increase_rate} 



<SettingsInfoBlock type="Double" default_value="0.2" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "0.2"},{"label": "The exponential growth rate at which the adaptive asynchronous insert timeout increases"}]}]}/>

自适应异步插入超时增加的指数增长率。
## async_insert_busy_timeout_max_ms {#async_insert_busy_timeout_max_ms} 



<SettingsInfoBlock type="Milliseconds" default_value="200" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "200"},{"label": "The minimum value of the asynchronous insert timeout in milliseconds; async_insert_busy_timeout_ms is aliased to async_insert_busy_timeout_max_ms"}]}]}/>

自第一次数据出现以来每个查询的最大等待时间。
## async_insert_busy_timeout_min_ms {#async_insert_busy_timeout_min_ms}

<SettingsInfoBlock type="Milliseconds" default_value="50" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "50"},{"label": "The minimum value of the asynchronous insert timeout in milliseconds; it also serves as the initial value, which may be increased later by the adaptive algorithm"}]}]}/>

如果通过 async_insert_use_adaptive_busy_timeout 启用自动调整，则在第一个数据出现后，每次查询收集数据之前要等待的最短时间。它也作为自适应算法的初始值。

## async_insert_deduplicate {#async_insert_deduplicate}

<SettingsInfoBlock type="Bool" default_value="0" />

对于在复制表中的异步 INSERT 查询，指定应执行插入块的去重。

## async_insert_max_data_size {#async_insert_max_data_size}

<SettingsInfoBlock type="UInt64" default_value="10485760" />

每个查询收集的未经解析的数据的最大字节大小，在插入之前。

## async_insert_max_query_number {#async_insert_max_query_number}

<SettingsInfoBlock type="UInt64" default_value="450" />

在插入之前的最大插入查询数。仅在设置 [`async_insert_deduplicate`](#async_insert_deduplicate) 为 1 时生效。

## async_insert_poll_timeout_ms {#async_insert_poll_timeout_ms}

<SettingsInfoBlock type="Milliseconds" default_value="10" />

从异步插入队列轮询数据的超时。

## async_insert_use_adaptive_busy_timeout {#async_insert_use_adaptive_busy_timeout}

<SettingsInfoBlock type="Bool" default_value="1" />

如果设置为 true，则对异步插入使用自适应繁忙超时。

## async_query_sending_for_remote {#async_query_sending_for_remote}

<SettingsInfoBlock type="Bool" default_value="1" />

在执行远程查询时，启用异步连接创建和查询发送。

默认启用。

## async_socket_for_remote {#async_socket_for_remote}

<SettingsInfoBlock type="Bool" default_value="1" />

在执行远程查询时，启用从套接字异步读取。

默认启用。

## azure_allow_parallel_part_upload {#azure_allow_parallel_part_upload}

<SettingsInfoBlock type="Bool" default_value="1" />

使用多个线程进行 Azure 多部分上传。

## azure_check_objects_after_upload {#azure_check_objects_after_upload}

<SettingsInfoBlock type="Bool" default_value="0" />

检查每个上传的对象在 Azure Blob 存储中确保上传成功。

## azure_connect_timeout_ms {#azure_connect_timeout_ms}

<SettingsInfoBlock type="UInt64" default_value="1000" />

Azure 磁盘主机的连接超时。

## azure_create_new_file_on_insert {#azure_create_new_file_on_insert}

在每次插入 Azure 引擎表时启用或禁用创建新文件。

## azure_ignore_file_doesnt_exist {#azure_ignore_file_doesnt_exist}

<SettingsInfoBlock type="Bool" default_value="0" />

在读取某些键时，如果文件不存在则忽略文件的缺失。

可能的值：
- 1 — `SELECT` 返回空结果。
- 0 — `SELECT` 抛出异常。

## azure_list_object_keys_size {#azure_list_object_keys_size}

批量返回的最大文件数量，由 ListObject 请求返回。

## azure_max_blocks_in_multipart_upload {#azure_max_blocks_in_multipart_upload}

<SettingsInfoBlock type="UInt64" default_value="50000" />

Azure 多部分上传的最大块数。

## azure_max_get_burst {#azure_max_get_burst}

<SettingsInfoBlock type="UInt64" default_value="0" />

在触及每秒请求限制之前，可以同时发出的最大请求数。默认（0）等于 `azure_max_get_rps`。

## azure_max_get_rps {#azure_max_get_rps}

<SettingsInfoBlock type="UInt64" default_value="0" />

在输出限流之前，Azure GET 请求每秒的限制。零表示无限制。

## azure_max_inflight_parts_for_one_file {#azure_max_inflight_parts_for_one_file}

<SettingsInfoBlock type="UInt64" default_value="20" />

在多部分上传请求中，单个文件的最大并发加载部分数。0 表示无限制。

## azure_max_put_burst {#azure_max_put_burst}

<SettingsInfoBlock type="UInt64" default_value="0" />

在触及每秒请求限制之前，可以同时发出的最大请求数。默认（0）等于 `azure_max_put_rps`。

## azure_max_put_rps {#azure_max_put_rps}

<SettingsInfoBlock type="UInt64" default_value="0" />

在输出限流之前，Azure PUT 请求每秒的限制。零表示无限制。

## azure_max_redirects {#azure_max_redirects}

允许的 Azure 重定向跳数的最大数量。

## azure_max_single_part_copy_size {#azure_max_single_part_copy_size}

使用单部分复制到 Azure Blob 存储的最大对象大小。

## azure_max_single_part_upload_size {#azure_max_single_part_upload_size}

使用单部分上传到 Azure Blob 存储的最大对象大小。

## azure_max_single_read_retries {#azure_max_single_read_retries}

单个 Azure Blob 存储读取的最大重试次数。

## azure_max_unexpected_write_error_retries {#azure_max_unexpected_write_error_retries}

<SettingsInfoBlock type="UInt64" default_value="4" />

在 Azure Blob 存储写入期间遇到意外错误时的最大重试次数。

## azure_max_upload_part_size {#azure_max_upload_part_size}

在向 Azure Blob 存储进行多部分上传期间的最大部分大小。

## azure_min_upload_part_size {#azure_min_upload_part_size}

在向 Azure Blob 存储进行多部分上传期间的最小部分大小。

## azure_request_timeout_ms {#azure_request_timeout_ms}

向/从 Azure 发送和接收数据的空闲超时。如果单个 TCP 读取或写入调用阻塞超过此时间，则失败。

## azure_sdk_max_retries {#azure_sdk_max_retries}

Azure SDK 中的最大重试次数。

## azure_sdk_retry_initial_backoff_ms {#azure_sdk_retry_initial_backoff_ms}

Azure SDK 中重试之间的最小退避时间（毫秒）。

## azure_sdk_retry_max_backoff_ms {#azure_sdk_retry_max_backoff_ms}

Azure SDK 中重试之间的最大退避时间（毫秒）。

## azure_sdk_use_native_client {#azure_sdk_use_native_client}

对 Azure SDK 使用 ClickHouse 原生 HTTP 客户端。

## azure_skip_empty_files {#azure_skip_empty_files}

启用或禁用在 S3 引擎中跳过空文件。

可能的值：
- 0 — 如果空文件与请求格式不兼容，`SELECT` 抛出异常。
- 1 — `SELECT` 返回空结果。

## azure_strict_upload_part_size {#azure_strict_upload_part_size}

在向 Azure Blob 存储进行多部分上传期间上传部分的确切大小。

## azure_throw_on_zero_files_match {#azure_throw_on_zero_files_match}

如果根据通配符扩展规则匹配零文件则抛出错误。

可能的值：
- 1 — `SELECT` 抛出异常。
- 0 — `SELECT` 返回空结果。

## azure_truncate_on_insert {#azure_truncate_on_insert}

启用或禁用在 Azure 引擎表中插入之前进行截断。

## azure_upload_part_size_multiply_factor {#azure_upload_part_size_multiply_factor}

每当从单次写入到 Azure Blob 存储上传 azure_multiply_parts_count_threshold 部分时，将 azure_min_upload_part_size 乘以此因子。

## azure_upload_part_size_multiply_parts_count_threshold {#azure_upload_part_size_multiply_parts_count_threshold}

每当此数字的部分上传到 Azure Blob 存储时，azure_min_upload_part_size 会被 azure_upload_part_size_multiply_factor 乘以。

## azure_use_adaptive_timeouts {#azure_use_adaptive_timeouts}

当设置为 `true` 时，所有 Azure 请求的前两次尝试在较低的发送和接收超时下进行。设置为 `false` 时，所有尝试均在相同的超时下进行。

## backup_restore_batch_size_for_keeper_multi {#backup_restore_batch_size_for_keeper_multi}

在备份或恢复期间，向 [Zoo]Keeper 发送的多请求的最大批大小。

## backup_restore_batch_size_for_keeper_multiread {#backup_restore_batch_size_for_keeper_multiread}

在备份或恢复期间多读请求向 [Zoo]Keeper 发送的最大批大小。

## backup_restore_failure_after_host_disconnected_for_seconds {#backup_restore_failure_after_host_disconnected_for_seconds}

如果在 BACKUP ON CLUSTER 或 RESTORE ON CLUSTER 操作期间，主机在此时间段内未能在 ZooKeeper 中重建其短暂的“存活”节点，则整个备份或恢复将被视为失败。此值应大于主机在故障后重新连接到 ZooKeeper 的任何合理时间。零表示无限制。

## backup_restore_finish_timeout_after_error_sec {#backup_restore_finish_timeout_after_error_sec}

发起者应等待多长时间，以便其他主机对“错误”节点做出反应并停止其在当前 BACKUP ON CLUSTER 或 RESTORE ON CLUSTER 操作中的工作。

## backup_restore_keeper_fault_injection_probability {#backup_restore_keeper_fault_injection_probability}

在备份或恢复期间，对 keeper 请求的故障注入的近似概率。有效值在 [0.0f, 1.0f] 的区间内。

## backup_restore_keeper_fault_injection_seed {#backup_restore_keeper_fault_injection_seed}

0 - 随机种子，其他情况为设置值。

## backup_restore_keeper_max_retries {#backup_restore_keeper_max_retries}

在进行 BACKUP 或 RESTORE 操作期间，ZooKeeper 操作的最大重试次数。应足够大，以便整个操作不会因为暂时的 ZooKeeper 故障而失败。

## backup_restore_keeper_max_retries_while_handling_error {#backup_restore_keeper_max_retries_while_handling_error}

在处理 BACKUP ON CLUSTER 或 RESTORE ON CLUSTER 操作错误时，ZooKeeper 操作的最大重试次数。

## backup_restore_keeper_max_retries_while_initializing {#backup_restore_keeper_max_retries_while_initializing}

在初始化 BACKUP ON CLUSTER 或 RESTORE ON CLUSTER 操作期间，ZooKeeper 操作的最大重试次数。

## backup_restore_keeper_retry_initial_backoff_ms {#backup_restore_keeper_retry_initial_backoff_ms}

在备份或恢复期间，ZooKeeper 操作的初始退避超时。

## backup_restore_keeper_retry_max_backoff_ms {#backup_restore_keeper_retry_max_backoff_ms}

在备份或恢复期间，ZooKeeper 操作的最大退避超时。

## backup_restore_keeper_value_max_size {#backup_restore_keeper_value_max_size}

在备份期间，ZooKeeper 节点的最大数据大小。

## backup_restore_s3_retry_attempts {#backup_restore_s3_retry_attempts}

覆盖 Aws::Client::RetryStrategy 的设置，Aws::Client 自行进行重试，0 意味着没有重试。仅适用于备份/恢复。

## backup_restore_s3_retry_initial_backoff_ms {#backup_restore_s3_retry_initial_backoff_ms}

在备份和恢复期间，第一次重试尝试之前的初始退避延迟（毫秒）。每个后续重试将延迟指数增加，直到达到 `backup_restore_s3_retry_max_backoff_ms` 指定的最大值。

## backup_restore_s3_retry_jitter_factor {#backup_restore_s3_retry_jitter_factor}

在备份和恢复操作期间，应用于 Aws::Client::RetryStrategy 的重试退避延迟的抖动因子。计算出的退避延迟乘以一个随机因子，范围在 [1.0, 1.0 + jitter] 之间，最大为 `backup_restore_s3_retry_max_backoff_ms`。必须在 [0.0, 1.0] 的区间内。

## backup_restore_s3_retry_max_backoff_ms {#backup_restore_s3_retry_max_backoff_ms}

在备份和恢复操作期间，重试之间的最大延迟（毫秒）。

## cache_warmer_threads {#cache_warmer_threads}

<CloudOnlyBadge/>

仅在 ClickHouse Cloud 中有效。在启用 [cache_populated_by_fetch](merge-tree-settings.md/#cache_populated_by_fetch) 时，后台线程的数量用于投机性下载新数据部分到文件缓存。零表示禁用。

## calculate_text_stack_trace {#calculate_text_stack_trace}

在查询执行期间发生异常时计算文本堆栈跟踪。这是默认值。它需要符号查找，可能会在执行大量错误查询时减慢模糊测试。在正常情况下，您不应该禁用此选项。

## cancel_http_readonly_queries_on_client_close {#cancel_http_readonly_queries_on_client_close}

在客户端关闭连接而不等待响应时，取消 HTTP 只读查询（例如 SELECT）。

云默认值：`0`。

## cast_ipv4_ipv6_default_on_conversion_error {#cast_ipv4_ipv6_default_on_conversion_error}

CAST 操作符转向 IPv4、CAST 操作符转向 IPV6 类型，toIPv4、toIPv6 函数将在转换错误时返回默认值，而不是抛出异常。

## cast_keep_nullable {#cast_keep_nullable}

启用或禁用在 [CAST](/sql-reference/functions/type-conversion-functions#cast) 操作中保留 `Nullable` 数据类型。

当设置为启用并且 `CAST` 函数的参数为 `Nullable` 时，结果也转化为 `Nullable` 类型。当设置为禁用时，结果始终具有确切的目标类型。

可能的值：
- 0 — `CAST` 结果有确切的目标类型。
- 1 — 如果参数类型为 `Nullable`，则 `CAST` 结果转化为 `Nullable(DestinationDataType)`。

**示例**

以下查询结果具有确切的目标数据类型：

```sql
SET cast_keep_nullable = 0;
SELECT CAST(toNullable(toInt32(0)) AS Int32) as x, toTypeName(x);
```

结果：

```text
┌─x─┬─toTypeName(CAST(toNullable(toInt32(0)), 'Int32'))─┐
│ 0 │ Int32                                             │
└───┴───────────────────────────────────────────────────┘
```

以下查询结果在目标数据类型上有 `Nullable` 修改：

```sql
SET cast_keep_nullable = 1;
SELECT CAST(toNullable(toInt32(0)) AS Int32) as x, toTypeName(x);
```

结果：

```text
┌─x─┬─toTypeName(CAST(toNullable(toInt32(0)), 'Int32'))─┐
│ 0 │ Nullable(Int32)                                   │
└───┴───────────────────────────────────────────────────┘
```

**另见**

- [CAST](/sql-reference/functions/type-conversion-functions#cast) 函数

## cast_string_to_date_time_mode {#cast_string_to_date_time_mode}

<SettingsInfoBlock type="DateTimeInputFormat" default_value="basic" />

允许在从字符串转换时选择日期和时间的文本表示解析器。

可能的值：
- `'best_effort'` — 启用扩展解析。

    ClickHouse 可以解析基本的 `YYYY-MM-DD HH:MM:SS` 格式以及所有 [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) 日期和时间格式。例如，`'2018-06-08T01:02:03.000Z'`。

- `'best_effort_us'` — 类似于 `best_effort`（请查看 [parseDateTimeBestEffortUS](../../sql-reference/functions/type-conversion-functions#parsedatetimebesteffortus) 中的区别）。

- `'basic'` — 使用基本解析器。

    ClickHouse 只能够解析基本的 `YYYY-MM-DD HH:MM:SS` 或 `YYYY-MM-DD` 格式。例如，`2019-08-20 10:18:56` 或 `2019-08-20`。

另见：
- [DateTime 数据类型。](../../sql-reference/data-types/datetime.md)
- [用于处理日期和时间的函数。](../../sql-reference/functions/date-time-functions.md)

## cast_string_to_dynamic_use_inference {#cast_string_to_dynamic_use_inference}

<SettingsInfoBlock type="Bool" default_value="0" />

在字符串到动态转换期间使用类型推断。

## cast_string_to_variant_use_inference {#cast_string_to_variant_use_inference}

<SettingsInfoBlock type="Bool" default_value="1" />

在字符串到变体转换期间使用类型推断。

## check_query_single_value_result {#check_query_single_value_result}

定义 `MergeTree` 系列引擎的 [CHECK TABLE](/sql-reference/statements/check-table) 查询结果的详细程度。

可能的值：
- 0 — 查询显示每个数据部分的检查状态。
- 1 — 查询显示一般的表检查状态。

## check_referential_table_dependencies {#check_referential_table_dependencies}

检查 DDL 查询（如 DROP TABLE 或 RENAME）不会破坏引用依赖关系。

## check_table_dependencies {#check_table_dependencies}

检查 DDL 查询（如 DROP TABLE 或 RENAME）不会破坏依赖关系。

## checksum_on_read {#checksum_on_read}

在读取时验证校验和。默认启用，并且在生产中应始终启用。请不要期望禁用此设置会有任何好处。它仅可用于实验和基准测试。该设置仅适用于 MergeTree 系列的表。对于其他表引擎和通过网络接收数据时，始终验证校验和。

## cloud_mode {#cloud_mode}

云模式。

## cloud_mode_database_engine {#cloud_mode_database_engine}

<SettingsInfoBlock type="UInt64" default_value="1" />

云中允许的数据库引擎。 1 - 重写 DDL 使用复制数据库，2 - 重写 DDL 使用共享数据库。

## cloud_mode_engine {#cloud_mode_engine}

云中允许的引擎系列。

- 0 - 允许一切。
- 1 - 重写 DDL 使用 *ReplicatedMergeTree。
- 2 - 重写 DDL 使用 SharedMergeTree。
- 3 - 重写 DDL 使用 SharedMergeTree，除非明确传递了指定的远程磁盘。

UInt64以最小化公共部分。

## cluster_for_parallel_replicas {#cluster_for_parallel_replicas}

<BetaBadge/>

当前服务器所在分片的集群。

## cluster_function_process_archive_on_multiple_nodes {#cluster_function_process_archive_on_multiple_nodes}

<SettingsInfoBlock type="Bool" default_value="1" />

如果设置为 `true`，则提高在集群函数中处理档案的性能。若在早期版本中使用带档案的集群函数，则应将其设置为 `false` 以确保兼容性并避免在升级到 25.7+ 时的错误。

## collect_hash_table_stats_during_aggregation {#collect_hash_table_stats_during_aggregation}

启用收集哈希表统计数据以优化内存分配。

## collect_hash_table_stats_during_joins {#collect_hash_table_stats_during_joins}

<SettingsInfoBlock type="Bool" default_value="1" />

启用收集哈希表统计数据以优化内存分配。

## compatibility {#compatibility}

`compatibility` 设置使 ClickHouse 使用之前版本 ClickHouse 的默认设置，之前版本作为设置提供。

如果设置为非默认值，则会遵循这些设置（仅未修改的设置会受到 `compatibility` 设置的影响）。

该设置接受字符串格式的 ClickHouse 版本号，如 `22.3`、`22.8`。空值表示禁用此设置。

默认禁用。

:::note 
在 ClickHouse Cloud 中，兼容性设置必须由 ClickHouse Cloud 支持设置。请 [开立工单](https://clickhouse.cloud/support) 进行设置。
:::

## compatibility_ignore_auto_increment_in_create_table {#compatibility_ignore_auto_increment_in_create_table}

如果为 true，则在列声明中忽略 AUTO_INCREMENT 关键字，否则返回错误。它简化了从 MySQL 的迁移。

## compatibility_ignore_collation_in_create_table {#compatibility_ignore_collation_in_create_table}

在创建表时的兼容性忽略排序规则。

## compile_aggregate_expressions {#compile_aggregate_expressions}

启用或禁用将聚合函数 JIT 编译为本地代码。启用此设置可以提高性能。

可能的值：
- 0 — 无需 JIT 编译即可完成聚合。
- 1 — 使用 JIT 编译完成聚合。

**另见**
- [min_count_to_compile_aggregate_expression](#min_count_to_compile_aggregate_expression)

## compile_expressions {#compile_expressions}

<SettingsInfoBlock type="Bool" default_value="1" />

将某些标量函数和运算符编译为本地代码。

## compile_sort_description {#compile_sort_description}

将排序描述编译为本地代码。

## connect_timeout {#connect_timeout}

如果没有副本，则连接超时。

## connect_timeout_with_failover_ms {#connect_timeout_with_failover_ms}

<SettingsInfoBlock type="Milliseconds" default_value="1000" />

在集群定义中使用“分片”和“副本”部分时，连接到远程服务器的超时时间（毫秒）。如果连接不成功，将尝试多次连接到不同的副本。

## connect_timeout_with_failover_secure_ms {#connect_timeout_with_failover_secure_ms}

<SettingsInfoBlock type="Milliseconds" default_value="1000" />

选择第一个健康副本的连接超时（安全连接）。

## connection_pool_max_wait_ms {#connection_pool_max_wait_ms}

连接池满时等待获取连接的时间（毫秒）。

可能的值：
- 正整数。
- 0 — 无限超时。

## connections_with_failover_max_tries {#connections_with_failover_max_tries}

用于分布式表引擎的每个副本的最大连接尝试次数。

## convert_query_to_cnf {#convert_query_to_cnf}

当设置为 `true` 时，`SELECT` 查询将转换为合取范式（CNF）。在某些情况下，将查询重写为 CNF 可能执行得更快（可以查看这个 [Github 问题](https://github.com/ClickHouse/ClickHouse/issues/11749) 进行解释）。

例如，注意以下 `SELECT` 查询未被修改（默认行为）：

```sql
EXPLAIN SYNTAX
SELECT *
FROM
(
    SELECT number AS x
    FROM numbers(20)
) AS a
WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))
SETTINGS convert_query_to_cnf = false;
```

结果是：

```response
┌─explain────────────────────────────────────────────────────────┐
│ SELECT x                                                       │
│ FROM                                                           │
│ (                                                              │
│     SELECT number AS x                                         │
│     FROM numbers(20)                                           │
│     WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15)) │
│ ) AS a                                                         │
│ WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))     │
│ SETTINGS convert_query_to_cnf = 0                              │
└────────────────────────────────────────────────────────────────┘
```

让我们将 `convert_query_to_cnf` 设置为 `true` 看看有什么变化：

```sql
EXPLAIN SYNTAX
SELECT *
FROM
(
    SELECT number AS x
    FROM numbers(20)
) AS a
WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))
SETTINGS convert_query_to_cnf = true;
```

注意 `WHERE` 子句被重写为 CNF，但结果集保持不变 - 布尔逻辑未改变：

```response
┌─explain───────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ SELECT x                                                                                                              │
│ FROM                                                                                                                  │
│ (                                                                                                                     │
│     SELECT number AS x                                                                                                │
│     FROM numbers(20)                                                                                                  │
│     WHERE ((x <= 15) OR (x <= 5)) AND ((x <= 15) OR (x >= 1)) AND ((x >= 10) OR (x <= 5)) AND ((x >= 10) OR (x >= 1)) │
│ ) AS a                                                                                                                │
│ WHERE ((x >= 10) OR (x >= 1)) AND ((x >= 10) OR (x <= 5)) AND ((x <= 15) OR (x >= 1)) AND ((x <= 15) OR (x <= 5))     │
│ SETTINGS convert_query_to_cnf = 1                                                                                     │
└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

可能的值：true, false。

## correlated_subqueries_substitute_equivalent_expressions {#correlated_subqueries_substitute_equivalent_expressions}

使用过滤表达式推断等效表达式，并在创建 CROSS JOIN 时替换这些表达式。

## count_distinct_implementation {#count_distinct_implementation}

指定应使用哪一个 `uniq*` 函数来执行 [COUNT(DISTINCT ...)](/sql-reference/aggregate-functions/reference/count) 结构。

可能的值：
- [uniq](/sql-reference/aggregate-functions/reference/uniq)
- [uniqCombined](/sql-reference/aggregate-functions/reference/uniqcombined)
- [uniqCombined64](/sql-reference/aggregate-functions/reference/uniqcombined64)
- [uniqHLL12](/sql-reference/aggregate-functions/reference/uniqhll12)
- [uniqExact](/sql-reference/aggregate-functions/reference/uniqexact)

## count_distinct_optimization {#count_distinct_optimization}

将计数不同重写为分组的子查询。

## count_matches_stop_at_empty_match {#count_matches_stop_at_empty_match}

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.6"},{"label": "0"},{"label": "New setting."}]}]}/>

一旦 `countMatches` 函数中的模式匹配零长度时停止计数。

## create_if_not_exists {#create_if_not_exists}

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.9"},{"label": "0"},{"label": "New setting."}]}]}/>

默认启用 `CREATE` 语句的 `IF NOT EXISTS`。如果此设置或 `IF NOT EXISTS` 被指定且提供名称的表已存在，则不会抛出异常。

## create_index_ignore_unique {#create_index_ignore_unique}

在 CREATE UNIQUE INDEX 中忽略 UNIQUE 关键字。用于 SQL 兼容性测试。

## create_replicated_merge_tree_fault_injection_probability {#create_replicated_merge_tree_fault_injection_probability}

在创建表格时，在 ZooKeeper 中创建元数据之后故障注入的概率。

## create_table_empty_primary_key_by_default {#create_table_empty_primary_key_by_default}

允许在未指定 ORDER BY 和 PRIMARY KEY 的情况下创建 *MergeTree 表，并且主键为空。

## cross_join_min_bytes_to_compress {#cross_join_min_bytes_to_compress}

在 CROSS JOIN 中压缩的最小块大小。零值表示禁用此阈值。当达到行或字节中的任何两个阈值时，将压缩此块。

## cross_join_min_rows_to_compress {#cross_join_min_rows_to_compress}

在 CROSS JOIN 中压缩块的最小行数。零值表示禁用此阈值。当达到行或字节中的任何两个阈值时，将压缩此块。

## data_type_default_nullable {#data_type_default_nullable}

允许没有显式修饰符 [NULL 或 NOT NULL](/sql-reference/statements/create/table#null-or-not-null-modifiers) 的数据类型在列定义中为 [Nullable](/sql-reference/data-types/nullable)。

可能的值：
- 1 — 列定义中的数据类型默认设置为 `Nullable`。
- 0 — 列定义中的数据类型默认设置为非 `Nullable`。

## database_atomic_wait_for_drop_and_detach_synchronously {#database_atomic_wait_for_drop_and_detach_synchronously}

为所有 `DROP` 和 `DETACH` 查询添加修饰符 `SYNC`。

可能的值：
- 0 — 查询将在延迟下执行。
- 1 — 查询将立即执行。

## database_replicated_allow_explicit_uuid {#database_replicated_allow_explicit_uuid}

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.9"},{"label": "0"},{"label": "Added a new setting to disallow explicitly specifying table UUID"}]}]}/>

0 - 不允许在复制数据库的表中显式指定 UUID。1 - 允许。2 - 允许，但忽略指定的 UUID 并生成一个随机的 UUID。

## database_replicated_allow_heavy_create {#database_replicated_allow_heavy_create}

允许在复制数据库引擎中运行长时间的 DDL 查询（CREATE AS SELECT 和 POPULATE）。请注意，它可能会长时间阻塞 DDL 队列。

## database_replicated_allow_only_replicated_engine {#database_replicated_allow_only_replicated_engine}

在复制的数据库中仅允许创建 Replicated 表。

## database_replicated_allow_replicated_engine_arguments {#database_replicated_allow_replicated_engine_arguments}

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.9"},{"label": "0"},{"label": "Don't allow explicit arguments by default"}]}]}/>

0 - 不允许在复制数据库中为 *MergeTree 表显式指定 ZooKeeper 路径和副本名称。1 - 允许。2 - 允许，但忽略指定的路径，并使用默认路径。3 - 允许，并且不记录警告。

## database_replicated_always_detach_permanently {#database_replicated_always_detach_permanently}

如果数据库引擎是 Replicated，则将 DETACH TABLE 执行为 DETACH TABLE PERMANENTLY。

## database_replicated_enforce_synchronous_settings {#database_replicated_enforce_synchronous_settings}

强制某些查询（另请参见 database_atomic_wait_for_drop_and_detach_synchronously、mutations_sync、alter_sync）同步等待。建议不要启用这些设置。

## database_replicated_initial_query_timeout_sec {#database_replicated_initial_query_timeout_sec}

设置初始 DDL 查询在复制数据库处理之前的 DDL 队列条目时的等待时间（秒）。

可能的值：
- 正整数。
- 0 — 无限期。

## decimal_check_overflow {#decimal_check_overflow}

检查十进制运算/比较操作的溢出。

## deduplicate_blocks_in_dependent_materialized_views {#deduplicate_blocks_in_dependent_materialized_views}

启用或禁用对接收来自 Replicated\* 表数据的物化视图的去重检查。

可能的值：
0 — 禁用。
1 — 启用。

启用时，ClickHouse 会在依赖于 Replicated\* 表的物化视图中执行块的去重。此设置对于确保当由于故障重试插入操作时，物化视图不包含重复数据非常有用。

**另见**
- [在重试中处理 NULL 的处理](/guides/developer/deduplicating-inserts-on-retries#insert-deduplication-with-materialized-views)

## default_materialized_view_sql_security {#default_materialized_view_sql_security}

允许在创建物化视图时设置 SQL SECURITY 选项的默认值。 [有关 SQL 安全性的更多信息](../../sql-reference/statements/create/view.md/#sql_security)。

默认值为 `DEFINER`。

## default_max_bytes_in_join {#default_max_bytes_in_join}

如果需要限制而未设置 `max_bytes_in_join`，则右侧表的最大大小。

## default_normal_view_sql_security {#default_normal_view_sql_security}

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "INVOKER"},{"label": "Allows to set default `SQL SECURITY` option while creating a normal view"}]}]}/>

允许在创建普通视图时设置默认 `SQL SECURITY` 选项。 [有关 SQL 安全性的更多信息](../../sql-reference/statements/create/view.md/#sql_security)。

默认值为 `INVOKER`。

## default_table_engine {#default_table_engine}

在 `CREATE` 语句中未设置 `ENGINE` 时使用的默认表引擎。

可能的值：
- 代表任何有效表引擎名称的字符串。

云默认值：`SharedMergeTree`。

**示例**

查询：

```sql
SET default_table_engine = 'Log';

SELECT name, value, changed FROM system.settings WHERE name = 'default_table_engine';
```

结果：

```response
┌─name─────────────────┬─value─┬─changed─┐
│ default_table_engine │ Log   │       1 │
└──────────────────────┴───────┴─────────┘
```

在此示例中，任何未指定 `Engine` 的新表将使用 `Log` 表引擎：

查询：

```sql
CREATE TABLE my_table (
    x UInt32,
    y UInt32
);

SHOW CREATE TABLE my_table;
```

结果：

```response
┌─statement────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.my_table
(
    `x` UInt32,
    `y` UInt32
)
ENGINE = Log
└──────────────────────────────────────────────────────────────────────────┘
```

## default_temporary_table_engine {#default_temporary_table_engine}

与 [default_table_engine](#default_table_engine) 相同，但用于临时表。

在此示例中，任何未指定 `Engine` 的新临时表将使用 `Log` 表引擎：

查询：

```sql
SET default_temporary_table_engine = 'Log';

CREATE TEMPORARY TABLE my_table (
    x UInt32,
    y UInt32
);

SHOW CREATE TEMPORARY TABLE my_table;
```

结果：

```response
┌─statement────────────────────────────────────────────────────────────────┐
│ CREATE TEMPORARY TABLE default.my_table
(
    `x` UInt32,
    `y` UInt32
)
ENGINE = Log
└──────────────────────────────────────────────────────────────────────────┘
```
## default_view_definer {#default_view_definer} 



<SettingsInfoBlock type="String" default_value="CURRENT_USER" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "CURRENT_USER"},{"label": "Allows to set default `DEFINER` option while creating a view"}]}]}/>

允许在创建视图时设置默认的 `DEFINER` 选项。 [关于 SQL 安全性更多信息](../../sql-reference/statements/create/view.md/#sql_security)。

默认值为 `CURRENT_USER`。
## delta_lake_enable_engine_predicate {#delta_lake_enable_engine_predicate} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "1"},{"label": "New setting"}]}]}/>

启用 delta-kernel 内部数据修剪。
## delta_lake_enable_expression_visitor_logging {#delta_lake_enable_expression_visitor_logging} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "0"},{"label": "New setting"}]}]}/>

启用 DeltaLake 表达式访问者的测试级别日志。这些日志可能会过于冗长，即使对于测试日志也是如此。
## delta_lake_insert_max_bytes_in_data_file {#delta_lake_insert_max_bytes_in_data_file} 



<SettingsInfoBlock type="NonZeroUInt64" default_value="1073741824" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.9"},{"label": "1073741824"},{"label": "New setting."}]}]}/>

定义 delta lake 中单个插入数据文件的字节限制。
## delta_lake_insert_max_rows_in_data_file {#delta_lake_insert_max_rows_in_data_file} 



<SettingsInfoBlock type="NonZeroUInt64" default_value="1000000" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.9"},{"label": "100000"},{"label": "New setting."}]}, {"id": "row-2","items": [{"label": "25.10"},{"label": "1000000"},{"label": "New setting."}]}]}/>

定义 delta lake 中单个插入数据文件的行限制。
## delta_lake_log_metadata {#delta_lake_log_metadata} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.10"},{"label": "0"},{"label": "New setting."}]}]}/>

启用将 delta lake 元数据文件记录到系统表中。
## delta_lake_snapshot_version {#delta_lake_snapshot_version} 



<SettingsInfoBlock type="Int64" default_value="-1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "-1"},{"label": "New setting"}]}]}/>

要读取的 delta lake 快照版本。值 -1 表示读取最新版本（值 0 是有效的快照版本）。
## delta_lake_throw_on_engine_predicate_error {#delta_lake_throw_on_engine_predicate_error} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "0"},{"label": "New setting"}]}]}/>

如果在 delta-kernel 中分析扫描谓词时发生错误，则启用抛出异常。
## describe_compact_output {#describe_compact_output} 



<SettingsInfoBlock type="Bool" default_value="0" />

如果为 true，则只包括列名和类型到 DESCRIBE 查询的结果中。
## describe_extend_object_types {#describe_extend_object_types} 



<SettingsInfoBlock type="Bool" default_value="0" />

推断 DESCRIBE 查询中 Object 类型列的具体类型。
## describe_include_subcolumns {#describe_include_subcolumns} 



<SettingsInfoBlock type="Bool" default_value="0" />

启用在 [DESCRIBE](../../sql-reference/statements/describe-table.md) 查询中描述子列。例如，元组中的成员或字典、Nullable 或数组数据类型的子列。

可能的值：

- 0 — 不包括子列在 `DESCRIBE` 查询中。
- 1 — 包括子列在 `DESCRIBE` 查询中。

**示例**

查看 [DESCRIBE](../../sql-reference/statements/describe-table.md) 语句的示例。
## describe_include_virtual_columns {#describe_include_virtual_columns} 



<SettingsInfoBlock type="Bool" default_value="0" />

如果为 true，表的虚拟列将包括在 DESCRIBE 查询的结果中。
## dialect {#dialect} 



<SettingsInfoBlock type="Dialect" default_value="clickhouse" />

将用于解析查询的方言。
## dictionary_validate_primary_key_type {#dictionary_validate_primary_key_type} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.7"},{"label": "0"},{"label": "Validate primary key type for dictionaries. By default id type for simple layouts will be implicitly converted to UInt64."}]}]}/>

验证字典的主键类型。对于简单布局，默认 id 类型将隐式转换为 UInt64。
## distinct_overflow_mode {#distinct_overflow_mode} 



<SettingsInfoBlock type="OverflowMode" default_value="throw" />

设置当数据量超过其中一个限制时发生的事情。

可能的值：
- `throw`：抛出异常（默认）。
- `break`：停止执行查询并返回部分结果，就像源数据用尽了一样。
## distributed_aggregation_memory_efficient {#distributed_aggregation_memory_efficient} 



<SettingsInfoBlock type="Bool" default_value="1" />

是否启用分布式聚合的节省内存模式。
## distributed_background_insert_batch {#distributed_background_insert_batch} 



<SettingsInfoBlock type="Bool" default_value="0" />

启用/禁用插入数据以批量发送。

当启用批量发送时，[Distributed](../../engines/table-engines/special/distributed.md) 表引擎尝试在一次操作中发送多个插入数据的文件，而不是单独发送。批量发送通过更好地利用服务器和网络资源提高集群性能。

可能的值：

- 1 — 启用。
- 0 — 禁用。
## distributed_background_insert_max_sleep_time_ms {#distributed_background_insert_max_sleep_time_ms} 



<SettingsInfoBlock type="Milliseconds" default_value="30000" />

[Distributed](../../engines/table-engines/special/distributed.md) 表引擎发送数据的最大间隔。限制 [distributed_background_insert_sleep_time_ms](#distributed_background_insert_sleep_time_ms) 设置中设定的间隔的指数增长。

可能的值：

- 正整数毫秒数。
## distributed_background_insert_sleep_time_ms {#distributed_background_insert_sleep_time_ms} 



<SettingsInfoBlock type="Milliseconds" default_value="100" />

[Distributed](../../engines/table-engines/special/distributed.md) 表引擎发送数据的基数间隔。实际的间隔在发生错误时会指数增长。

可能的值：

- 正整数毫秒数。
## distributed_background_insert_split_batch_on_failure {#distributed_background_insert_split_batch_on_failure} 



<SettingsInfoBlock type="Bool" default_value="0" />

启用/禁用在失败时拆分批次。

有时将特定批次发送到远程分片可能会失败，因为在其后的某些复杂管道（例如带有 `GROUP BY` 的 `MATERIALIZED VIEW`）中可能由于 `Memory limit exceeded` 等类似错误而导致。在这种情况下，重试将无济于事（这将阻止分布式发送），但逐个发送该批次中的文件可能会成功进行 INSERT。

因此，将该设置安装为 `1` 将禁用这些批次的批量处理（即临时禁用失败批次的 `distributed_background_insert_batch`）。

可能的值：

- 1 — 启用。
- 0 — 禁用。

:::note
此设置也会影响由于服务器异常终止（机器）而导致的损坏批次（并且对于 [Distributed](../../engines/table-engines/special/distributed.md) 表引擎没有 `fsync_after_insert`/`fsync_directories`）。
:::

:::note
您不应依赖自动批量拆分，因为这可能会损害性能。
:::
## distributed_background_insert_timeout {#distributed_background_insert_timeout} 



<SettingsInfoBlock type="UInt64" default_value="0" />

分布式插入查询的超时。该设置仅在启用 insert_distributed_sync 时使用。零值表示没有超时。
## distributed_cache_alignment {#distributed_cache_alignment} 

<CloudOnlyBadge/>



<SettingsInfoBlock type="UInt64" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.7"},{"label": "0"},{"label": "Rename of distributed_cache_read_alignment"}]}]}/>

仅在 ClickHouse Cloud 中有效。用于测试目的的设置，请勿更改。
## distributed_cache_bypass_connection_pool {#distributed_cache_bypass_connection_pool} 

<CloudOnlyBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "0"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。允许绕过分布式缓存连接池。
## distributed_cache_connect_backoff_max_ms {#distributed_cache_connect_backoff_max_ms} 

<CloudOnlyBadge/>



<SettingsInfoBlock type="UInt64" default_value="50" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "50"},{"label": "New setting"}]}]}/>

仅在 ClickHouse Cloud 中有效。创建分布式缓存连接的最大回退毫秒数。
## distributed_cache_connect_backoff_min_ms {#distributed_cache_connect_backoff_min_ms} 

<CloudOnlyBadge/>



<SettingsInfoBlock type="UInt64" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "0"},{"label": "New setting"}]}]}/>

仅在 ClickHouse Cloud 中有效。创建分布式缓存连接的最小回退毫秒数。
## distributed_cache_connect_max_tries {#distributed_cache_connect_max_tries} 

<CloudOnlyBadge/>



<SettingsInfoBlock type="UInt64" default_value="5" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "5"},{"label": "Changed setting value"}]}, {"id": "row-2","items": [{"label": "25.1"},{"label": "20"},{"label": "Cloud only"}]}, {"id": "row-3","items": [{"label": "24.10"},{"label": "20"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。如果连接不成功，尝试连接到分布式缓存的次数。
## distributed_cache_credentials_refresh_period_seconds {#distributed_cache_credentials_refresh_period_seconds} 

<CloudOnlyBadge/>



<SettingsInfoBlock type="UInt64" default_value="5" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.6"},{"label": "5"},{"label": "New private setting"}]}]}/>

仅在 ClickHouse Cloud 中有效。凭据刷新的周期。
## distributed_cache_data_packet_ack_window {#distributed_cache_data_packet_ack_window} 

<CloudOnlyBadge/>



<SettingsInfoBlock type="UInt64" default_value="5" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "5"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。在单个分布式缓存读取请求中发送数据包 ACK 的窗口。
## distributed_cache_discard_connection_if_unread_data {#distributed_cache_discard_connection_if_unread_data} 

<CloudOnlyBadge/>



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "1"},{"label": "New setting"}]}, {"id": "row-2","items": [{"label": "24.10"},{"label": "1"},{"label": "New setting"}]}]}/>

仅在 ClickHouse Cloud 中有效。如果有未读数据则丢弃连接。
## distributed_cache_fetch_metrics_only_from_current_az {#distributed_cache_fetch_metrics_only_from_current_az} 

<CloudOnlyBadge/>



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "1"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。仅从当前可用区域提取 system.distributed_cache_metrics 和 system.distributed_cache_events 的指标。
## distributed_cache_log_mode {#distributed_cache_log_mode} 

<CloudOnlyBadge/>



<SettingsInfoBlock type="DistributedCacheLogMode" default_value="on_error" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "on_error"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。对于系统.logs 记录的写入模式。
## distributed_cache_max_unacked_inflight_packets {#distributed_cache_max_unacked_inflight_packets} 

<CloudOnlyBadge/>



<SettingsInfoBlock type="UInt64" default_value="10" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "10"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。单个分布式缓存读取请求中的最大未确认的在途数据包数。
## distributed_cache_min_bytes_for_seek {#distributed_cache_min_bytes_for_seek} 

<CloudOnlyBadge/>



<SettingsInfoBlock type="UInt64" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0"},{"label": "New private setting."}]}]}/>

仅在 ClickHouse Cloud 中有效。进行分布式缓存查找的最小字节数。
## distributed_cache_pool_behaviour_on_limit {#distributed_cache_pool_behaviour_on_limit} 

<CloudOnlyBadge/>



<SettingsInfoBlock type="DistributedCachePoolBehaviourOnLimit" default_value="wait" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "wait"},{"label": "Cloud only"}]}, {"id": "row-2","items": [{"label": "24.10"},{"label": "allocate_bypassing_pool"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。确定在达到连接池限制时分布式缓存连接的行为。
## distributed_cache_prefer_bigger_buffer_size {#distributed_cache_prefer_bigger_buffer_size} 

<CloudOnlyBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.10"},{"label": "0"},{"label": "New setting."}]}]}/>

仅在 ClickHouse Cloud 中有效。与 filesystem_cache_prefer_bigger_buffer_size 相同，适用于分布式缓存。
## distributed_cache_read_only_from_current_az {#distributed_cache_read_only_from_current_az} 

<CloudOnlyBadge/>



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "1"},{"label": "New setting"}]}]}/>

仅在 ClickHouse Cloud 中有效。仅允许从当前可用区域读取。如果禁用，将从所有可用区域的所有缓存服务器读取。
## distributed_cache_read_request_max_tries {#distributed_cache_read_request_max_tries} 

<CloudOnlyBadge/>



<SettingsInfoBlock type="UInt64" default_value="10" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "10"},{"label": "Changed setting value"}]}, {"id": "row-2","items": [{"label": "25.4"},{"label": "20"},{"label": "New setting"}]}]}/>

仅在 ClickHouse Cloud 中有效。如果分布式缓存请求不成功，尝试的次数。
## distributed_cache_receive_response_wait_milliseconds {#distributed_cache_receive_response_wait_milliseconds} 

<CloudOnlyBadge/>



<SettingsInfoBlock type="UInt64" default_value="60000" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "60000"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。等待从分布式缓存请求接收数据的毫秒数。
## distributed_cache_receive_timeout_milliseconds {#distributed_cache_receive_timeout_milliseconds} 

<CloudOnlyBadge/>



<SettingsInfoBlock type="UInt64" default_value="10000" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "10000"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。等待接收任何类型响应的毫秒数。
## distributed_cache_throw_on_error {#distributed_cache_throw_on_error} 

<CloudOnlyBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "0"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。在与分布式缓存通信时重新抛出发生的异常或从分布式缓存接收到的异常。否则，退回到错误时跳过分布式缓存。
## distributed_cache_wait_connection_from_pool_milliseconds {#distributed_cache_wait_connection_from_pool_milliseconds} 

<CloudOnlyBadge/>



<SettingsInfoBlock type="UInt64" default_value="100" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "100"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。如果 distributed_cache_pool_behaviour_on_limit 设置为 wait，等待从连接池获得连接的毫秒数。
## distributed_connections_pool_size {#distributed_connections_pool_size} 



<SettingsInfoBlock type="UInt64" default_value="1024" />

用于与远程服务器进行分布式处理的所有查询的最大并发连接数。我们建议设置的值不少于集群中服务器的数量。
## distributed_ddl_entry_format_version {#distributed_ddl_entry_format_version} 



<SettingsInfoBlock type="UInt64" default_value="5" />

分布式 DDL（ON CLUSTER）查询的兼容版本。
## distributed_ddl_output_mode {#distributed_ddl_output_mode} 



<SettingsInfoBlock type="DistributedDDLOutputMode" default_value="throw" />

设置分布式 DDL 查询结果的格式。

可能的值：

- `throw` — 返回执行状态的结果集，适用于查询已完成的所有主机。如果某些主机上的查询失败，则将重新抛出第一次异常。如果某些主机上的查询尚未完成并且超过了 [distributed_ddl_task_timeout](#distributed_ddl_task_timeout)，则将抛出 `TIMEOUT_EXCEEDED` 异常。
- `none` — 与抛出类似，但分布式 DDL 查询返回结果集为空。
- `null_status_on_timeout` — 在结果集中的某些行返回 `NULL` 作为执行状态，而不是在相应主机没有完成查询时抛出 `TIMEOUT_EXCEEDED`。
- `never_throw` — 如果某些主机上的查询失败则不抛出 `TIMEOUT_EXCEEDED`，也不重新抛出异常。
- `none_only_active` - 类似于 `none`，但不等待 `Replicated` 数据库的非活动副本。注意：在这种模式下无法确定查询没有在某些副本上执行，并且将在后台执行。
- `null_status_on_timeout_only_active` — 类似于 `null_status_on_timeout`，但不等待 `Replicated` 数据库的非活动副本。
- `throw_only_active` — 类似于 `throw`，但不等待 `Replicated` 数据库的非活动副本。

云端默认值：`throw`。
## distributed_ddl_task_timeout {#distributed_ddl_task_timeout} 



<SettingsInfoBlock type="Int64" default_value="180" />

设置来自集群中所有主机的 DDL 查询响应超时。如果在所有主机上没有执行 DDL 请求，则响应将包含超时错误，请求将以异步模式执行。负值表示无限制。

可能的值：

- 正整数。
- 0 — 异步模式。
- 负整数 — 无限超时。
## distributed_foreground_insert {#distributed_foreground_insert} 



<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用对 [Distributed](/engines/table-engines/special/distributed) 表的同步数据插入。

默认情况下，当将数据插入到 `Distributed` 表时，ClickHouse 服务器会在后台模式下将数据发送到集群节点。当 `distributed_foreground_insert=1` 时，数据以同步方式处理，只有在所有分片上的所有数据都保存后（如果 `internal_replication` 为 true，则每个分片至少一个副本）`INSERT` 操作才会成功。

可能的值：

- `0` — 数据在后台模式下插入。
- `1` — 数据以同步模式插入。

云端默认值：`0`。

**另见**

- [分布式表引擎](/engines/table-engines/special/distributed)
- [管理分布式表](/sql-reference/statements/system#managing-distributed-tables)
## distributed_group_by_no_merge {#distributed_group_by_no_merge} 



<SettingsInfoBlock type="UInt64" default_value="0" />

在分布式查询处理中，未能合并来自不同服务器的聚合状态，如果可以确认在不同的分片上有不同的键，可以使用此设置。

可能的值：

- `0` — 禁用（最终查询处理在发起节点上完成）。
- `1` - 不合并来自不同服务器的聚合状态进行分布式查询处理（查询在分片上完全处理，发起节点仅代理数据），如果确认在不同分片上有不同的键，则可以使用此设置。
- `2` - 与 `1` 相同，但在发起节点上应用 `ORDER BY` 和 `LIMIT`（当查询在远程节点完全处理时，如 `distributed_group_by_no_merge=1`，则无法执行）。

**示例**

```sql
SELECT *
FROM remote('127.0.0.{2,3}', system.one)
GROUP BY dummy
LIMIT 1
SETTINGS distributed_group_by_no_merge = 1
FORMAT PrettyCompactMonoBlock

┌─dummy─┐
│     0 │
│     0 │
└───────┘
```

```sql
SELECT *
FROM remote('127.0.0.{2,3}', system.one)
GROUP BY dummy
LIMIT 1
SETTINGS distributed_group_by_no_merge = 2
FORMAT PrettyCompactMonoBlock

┌─dummy─┐
│     0 │
└───────┘
```
## distributed_insert_skip_read_only_replicas {#distributed_insert_skip_read_only_replicas} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "0"},{"label": "If true, INSERT into Distributed will skip read-only replicas"}]}]}/>

启用跳过只读副本的 INSERT 查询到 Distributed。

可能的值：

- 0 — INSERT 行为如常，如果将转发到只读副本，则会失败。
- 1 — 发起者在将数据发送到分片之前会跳过只读副本。
## distributed_plan_default_reader_bucket_count {#distributed_plan_default_reader_bucket_count} 

<ExperimentalBadge/>



<SettingsInfoBlock type="UInt64" default_value="8" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "8"},{"label": "New experimental setting."}]}]}/>

分布式查询的默认并行读取任务数量。任务在副本之间分配。
## distributed_plan_default_shuffle_join_bucket_count {#distributed_plan_default_shuffle_join_bucket_count} 

<ExperimentalBadge/>



<SettingsInfoBlock type="UInt64" default_value="8" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "8"},{"label": "New experimental setting."}]}]}/>

分布式 shuffle-hash-join 的默认桶数量。
## distributed_plan_execute_locally {#distributed_plan_execute_locally} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "0"},{"label": "New experimental setting."}]}]}/>

在本地运行分布式查询计划的所有任务。用于测试和调试。
## distributed_plan_force_exchange_kind {#distributed_plan_force_exchange_kind} 

<ExperimentalBadge/>



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": ""},{"label": "New experimental setting."}]}]}/>

强制分布式查询阶段之间的指定类型交换操作符。

可能的值：

 - '' - 不强制任何类型的交换操作符，允许优化器选择，
 - 'Persisted' - 使用对象存储中的临时文件，
 - 'Streaming' - 在网络上传输交换数据。
## distributed_plan_force_shuffle_aggregation {#distributed_plan_force_shuffle_aggregation} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.7"},{"label": "0"},{"label": "New experimental setting"}]}]}/>

在分布式查询计划中使用 Shuffle 聚合策略，而不是 PartialAggregation + Merge。
## distributed_plan_max_rows_to_broadcast {#distributed_plan_max_rows_to_broadcast} 

<ExperimentalBadge/>



<SettingsInfoBlock type="UInt64" default_value="20000" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.7"},{"label": "20000"},{"label": "New experimental setting."}]}]}/>

在分布式查询计划中，广播连接所使用的最大行数，而不是 shuffle 连接。
## distributed_plan_optimize_exchanges {#distributed_plan_optimize_exchanges} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "1"},{"label": "New experimental setting."}]}]}/>

在分布式查询计划中移除不必要的交换。对于调试，请禁用它。
## distributed_product_mode {#distributed_product_mode} 



<SettingsInfoBlock type="DistributedProductMode" default_value="deny" />

更改 [分布式子查询](../../sql-reference/operators/in.md) 的行为。

ClickHouse 在查询包含分布式表的乘积时应用此设置，即当分布式表的查询包含对该分布式表的非全局子查询时。

限制：

- 仅适用于 IN 和 JOIN 子查询。
- 仅在 FROM 部分使用包含多个分片的分布式表时适用。
- 如果子查询涉及包含多个分片的分布式表。
- 不用于表值 [remote](../../sql-reference/table-functions/remote.md) 函数。

可能的值：

- `deny` — 默认值。禁止使用这些类型的子查询（返回“禁止双重分布的 in/JOIN 子查询”异常）。
- `local` — 在子查询中将数据库和表替换为目标服务器（分片）的本地表，保留正常的`IN`/`JOIN`。
- `global` — 用 `GLOBAL IN`/`GLOBAL JOIN` 替换 `IN`/`JOIN` 查询。
- `allow` — 允许使用这些类型的子查询。
## distributed_push_down_limit {#distributed_push_down_limit} 



<SettingsInfoBlock type="UInt64" default_value="1" />

启用或禁用在每个分片上单独应用 [LIMIT](#limit)。

这将避免：
- 通过网络发送额外的行；
- 在发起节点处理超出限制的行。

从 21.9 版本开始，您不再会获得不准确的结果，因为 `distributed_push_down_limit` 仅在满足至少一个条件时才会改变查询执行：
- [distributed_group_by_no_merge](#distributed_group_by_no_merge) > 0。
- 查询 **没有** `GROUP BY`/`DISTINCT`/`LIMIT BY`，但它有 `ORDER BY`/`LIMIT`。
- 查询 **有** `GROUP BY`/`DISTINCT`/`LIMIT BY` 带有 `ORDER BY`/`LIMIT` 以及：
    - [optimize_skip_unused_shards](#optimize_skip_unused_shards) 被启用。
    - [optimize_distributed_group_by_sharding_key](#optimize_distributed_group_by_sharding_key) 被启用。

可能的值：

- 0 — 禁用。
- 1 — 启用。

另见：

- [distributed_group_by_no_merge](#distributed_group_by_no_merge)
- [optimize_skip_unused_shards](#optimize_skip_unused_shards)
- [optimize_distributed_group_by_sharding_key](#optimize_distributed_group_by_sharding_key)
## distributed_replica_error_cap {#distributed_replica_error_cap} 



<SettingsInfoBlock type="UInt64" default_value="1000" />

- 类型：无符号整型
- 默认值：1000

每个副本的错误计数限制在此值，不允许单个副本累积过多错误。

另见：

- [load_balancing](#load_balancing-round_robin)
- [表引擎 Distributed](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_half_life](#distributed_replica_error_half_life)
- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)
## distributed_replica_error_half_life {#distributed_replica_error_half_life} 



<SettingsInfoBlock type="Seconds" default_value="60" />

- 类型：秒
- 默认值：60秒

控制分布式表中错误归零的速度。如果某个副本在一段时间内不可用，累积 5 个错误，并且 distributed_replica_error_half_life 设置为 1 秒，则在最后一个错误后的 3 秒内，该副本被视为正常。

另见：

- [load_balancing](#load_balancing-round_robin)
- [表引擎 Distributed](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_cap](#distributed_replica_error_cap)
- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)
## distributed_replica_max_ignored_errors {#distributed_replica_max_ignored_errors} 



<SettingsInfoBlock type="UInt64" default_value="0" />

- 类型：无符号整型
- 默认值：0

在选择副本时将忽略的错误数（根据 `load_balancing` 算法）。

另见：

- [load_balancing](#load_balancing-round_robin)
- [表引擎 Distributed](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_cap](#distributed_replica_error_cap)
- [distributed_replica_error_half_life](#distributed_replica_error_half_life)
## do_not_merge_across_partitions_select_final {#do_not_merge_across_partitions_select_final} 



<SettingsInfoBlock type="Bool" default_value="0" />

在 select final 中仅在一个分区合并部分。
## empty_result_for_aggregation_by_constant_keys_on_empty_set {#empty_result_for_aggregation_by_constant_keys_on_empty_set} 



<SettingsInfoBlock type="Bool" default_value="1" />

在空集合上按常量键进行聚合时返回空结果。
## empty_result_for_aggregation_by_empty_set {#empty_result_for_aggregation_by_empty_set} 



<SettingsInfoBlock type="Bool" default_value="0" />

在空集合上无键进行聚合时返回空结果。
## enable_adaptive_memory_spill_scheduler {#enable_adaptive_memory_spill_scheduler} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.2"},{"label": "0"},{"label": "New setting. Enable spill memory data into external storage adaptively."}]}]}/>

在适应性条件下触发处理程序将数据溢出到外部存储。目前支持 grace join。
## enable_add_distinct_to_in_subqueries {#enable_add_distinct_to_in_subqueries} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "0"},{"label": "New setting to reduce the size of temporary tables transferred for distributed IN subqueries."}]}]}/>

在 `IN` 子查询中启用 `DISTINCT`。这是一个权衡设置：启用它可以大大减少传输给分布式 IN 子查询的临时表的大小，并显著加快分片间的数据传输，因为只确保唯一值被发送。
然而，启用此设置在每个节点上增加了额外的合并工作，因为必须执行去重（DISTINCT）。当网络传输成为瓶颈且额外的合并成本可接受时使用此设置。
## enable_blob_storage_log {#enable_blob_storage_log} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "1"},{"label": "Write information about blob storage operations to system.blob_storage_log table"}]}]}/>

将有关 blob 存储操作的信息写入 system.blob_storage_log 表。
## enable_deflate_qpl_codec {#enable_deflate_qpl_codec} 



<SettingsInfoBlock type="Bool" default_value="0" />

如果启用，可以使用 DEFLATE_QPL 编解码器压缩列。
## enable_early_constant_folding {#enable_early_constant_folding} 



<SettingsInfoBlock type="Bool" default_value="1" />

启用查询优化，其中我们分析函数和子查询结果，如果存在常量则重写查询。
## enable_extended_results_for_datetime_functions {#enable_extended_results_for_datetime_functions} 



<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用返回扩展范围的 `Date32` 类型（与 `Date` 类型相比）或扩展范围的 `DateTime64`（与 `DateTime` 类型相比）的结果。

可能的值：

- `0` — 对所有类型的参数，函数返回 `Date` 或 `DateTime`。
- `1` — 函数对 `Date32` 或 `DateTime64` 参数返回 `Date32` 或 `DateTime64`，而其他情况返回 `Date` 或 `DateTime`。

下表展示了该设置对各种日期时间函数的行为。

| 函数 | `enable_extended_results_for_datetime_functions = 0` | `enable_extended_results_for_datetime_functions = 1` |
|----------|---------------------------------------------------|---------------------------------------------------|
| `toStartOfYear` | 返回 `Date` 或 `DateTime` | 返回 `Date`/`DateTime` 对于 `Date`/`DateTime` 输入<br/>返回 `Date32`/`DateTime64` 对于 `Date32`/`DateTime64` 输入 |
| `toStartOfISOYear` | 返回 `Date` 或 `DateTime` | 返回 `Date`/`DateTime` 对于 `Date`/`DateTime` 输入<br/>返回 `Date32`/`DateTime64` 对于 `Date32`/`DateTime64` 输入 |
| `toStartOfQuarter` | 返回 `Date` 或 `DateTime` | 返回 `Date`/`DateTime` 对于 `Date`/`DateTime` 输入<br/>返回 `Date32`/`DateTime64` 对于 `Date32`/`DateTime64` 输入 |
| `toStartOfMonth` | 返回 `Date` 或 `DateTime` | 返回 `Date`/`DateTime` 对于 `Date`/`DateTime` 输入<br/>返回 `Date32`/`DateTime64` 对于 `Date32`/`DateTime64` 输入 |
| `toStartOfWeek` | 返回 `Date` 或 `DateTime` | 返回 `Date`/`DateTime` 对于 `Date`/`DateTime` 输入<br/>返回 `Date32`/`DateTime64` 对于 `Date32`/`DateTime64` 输入 |
| `toLastDayOfWeek` | 返回 `Date` 或 `DateTime` | 返回 `Date`/`DateTime` 对于 `Date`/`DateTime` 输入<br/>返回 `Date32`/`DateTime64` 对于 `Date32`/`DateTime64` 输入 |
| `toLastDayOfMonth` | 返回 `Date` 或 `DateTime` | 返回 `Date`/`DateTime` 对于 `Date`/`DateTime` 输入<br/>返回 `Date32`/`DateTime64` 对于 `Date32`/`DateTime64` 输入 |
| `toMonday` | 返回 `Date` 或 `DateTime` | 返回 `Date`/`DateTime` 对于 `Date`/`DateTime` 输入<br/>返回 `Date32`/`DateTime64` 对于 `Date32`/`DateTime64` 输入 |
| `toStartOfDay` | 返回 `DateTime`<br/>*注意：对于 1970-2149 范围以外的值，结果错误* | 返回 `DateTime` 对于 `Date`/`DateTime` 输入<br/>返回 `DateTime64` 对于 `Date32`/`DateTime64` 输入 |
| `toStartOfHour` | 返回 `DateTime`<br/>*注意：对于 1970-2149 范围以外的值，结果错误* | 返回 `DateTime` 对于 `Date`/`DateTime` 输入<br/>返回 `DateTime64` 对于 `Date32`/`DateTime64` 输入 |
| `toStartOfFifteenMinutes` | 返回 `DateTime`<br/>*注意：对于 1970-2149 范围以外的值，结果错误* | 返回 `DateTime` 对于 `Date`/`DateTime` 输入<br/>返回 `DateTime64` 对于 `Date32`/`DateTime64` 输入 |
| `toStartOfTenMinutes` | 返回 `DateTime`<br/>*注意：对于 1970-2149 范围以外的值，结果错误* | 返回 `DateTime` 对于 `Date`/`DateTime` 输入<br/>返回 `DateTime64` 对于 `Date32`/`DateTime64` 输入 |
| `toStartOfFiveMinutes` | 返回 `DateTime`<br/>*注意：对于 1970-2149 范围以外的值，结果错误* | 返回 `DateTime` 对于 `Date`/`DateTime` 输入<br/>返回 `DateTime64` 对于 `Date32`/`DateTime64` 输入 |
| `toStartOfMinute` | 返回 `DateTime`<br/>*注意：对于 1970-2149 范围以外的值，结果错误* | 返回 `DateTime` 对于 `Date`/`DateTime` 输入<br/>返回 `DateTime64` 对于 `Date32`/`DateTime64` 输入 |
| `timeSlot` | 返回 `DateTime`<br/>*注意：对于 1970-2149 范围以外的值，结果错误* | 返回 `DateTime` 对于 `Date`/`DateTime` 输入<br/>返回 `DateTime64` 对于 `Date32`/`DateTime64` 输入 |
## enable_filesystem_cache {#enable_filesystem_cache} 



<SettingsInfoBlock type="Bool" default_value="1" />

为远程文件系统使用缓存。此设置不会打开/关闭磁盘的缓存（必须通过磁盘配置完成），但允许在某些查询中绕过缓存（如果有意）。
## enable_filesystem_cache_log {#enable_filesystem_cache_log} 



<SettingsInfoBlock type="Bool" default_value="0" />

允许记录每个查询的文件系统缓存日志。
## enable_filesystem_cache_on_write_operations {#enable_filesystem_cache_on_write_operations} 



<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用 `write-through` 缓存。如果设置为 `false`，则在写操作中禁用 `write-through` 缓存。如果设置为 `true`，则只有在服务器配置的缓存磁盘配置部分中开启 `cache_on_write_operations` 时，才启用 `write-through` 缓存。
有关更多详细信息，请参阅["使用本地缓存"](/operations/storing-data#using-local-cache)。
## enable_filesystem_read_prefetches_log {#enable_filesystem_read_prefetches_log} 



<SettingsInfoBlock type="Bool" default_value="0" />

在查询期间记录系统.filesystem prefetch_log。仅应在测试或调试中使用，不建议默认开启。
## enable_global_with_statement {#enable_global_with_statement} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "21.2"},{"label": "1"},{"label": "Propagate WITH statements to UNION queries and all subqueries by default"}]}]}/>

将 WITH 语句传播到 UNION 查询和所有子查询。
## enable_hdfs_pread {#enable_hdfs_pread} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "1"},{"label": "New setting."}]}]}/>

启用或禁用 HDFS 文件的 pread。默认情况下使用 `hdfsPread`。如果禁用，将使用 `hdfsRead` 和 `hdfsSeek` 来读取 hdfs 文件。
## enable_http_compression {#enable_http_compression} 



<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用对 HTTP 请求的响应进行数据压缩。

有关更多信息，请阅读 [HTTP 接口描述](../../interfaces/http.md)。

可能的值：

- 0 — 禁用。
- 1 — 启用。
## enable_job_stack_trace {#enable_job_stack_trace} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.6"},{"label": "0"},{"label": "The setting was disabled by default to avoid performance overhead."}]}, {"id": "row-2","items": [{"label": "24.11"},{"label": "0"},{"label": "Enables collecting stack traces from job's scheduling. Disabled by default to avoid performance overhead."}]}]}/>

当作业结果发生异常时输出作业创建者的堆栈跟踪。默认情况下禁用，以避免性能开销。
## enable_join_runtime_filters {#enable_join_runtime_filters} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.10"},{"label": "0"},{"label": "New setting"}]}]}/>

通过在运行时从右侧收集的 JOIN 键集过滤左侧。
## enable_lightweight_delete {#enable_lightweight_delete} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用轻量级 DELETE 变更以用于 mergetree 表。
## enable_lightweight_update {#enable_lightweight_update} 

<BetaBadge/>

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "1"},{"label": "Lightweight updates were moved to Beta. Added an alias for setting 'allow_experimental_lightweight_update'."}]}]}/>

允许使用轻量级更新。
## enable_memory_bound_merging_of_aggregation_results {#enable_memory_bound_merging_of_aggregation_results} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用内存限制的聚合合并策略。
## enable_multiple_prewhere_read_steps {#enable_multiple_prewhere_read_steps} 

<SettingsInfoBlock type="Bool" default_value="1" />

将更多条件从 WHERE 移动到 PREWHERE，并在存在多个 AND 条件时以多个步骤从磁盘读取和过滤。
## enable_named_columns_in_function_tuple {#enable_named_columns_in_function_tuple} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.7"},{"label": "0"},{"label": "Generate named tuples in function tuple() when all names are unique and can be treated as unquoted identifiers."}]}, {"id": "row-2","items": [{"label": "24.10"},{"label": "0"},{"label": "Disabled pending usability improvements"}]}]}/>

在 function tuple() 中生成命名元组，当所有名称都是唯一的并且可以被视为未引用的标识符时。
## enable_optimize_predicate_expression {#enable_optimize_predicate_expression} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "18.12.17"},{"label": "1"},{"label": "Optimize predicates to subqueries by default"}]}]}/>

在 `SELECT` 查询中启用谓词下推。

谓词下推可以显著减少分布式查询的网络流量。

可能的值：

- 0 — 禁用。
- 1 — 启用。

用法

考虑以下查询：

1.  `SELECT count() FROM test_table WHERE date = '2018-10-10'`
2.  `SELECT count() FROM (SELECT * FROM test_table) WHERE date = '2018-10-10'`

如果 `enable_optimize_predicate_expression = 1`，那么这两个查询的执行时间是相等的，因为 ClickHouse 在处理子查询时对其应用了 `WHERE`。

如果 `enable_optimize_predicate_expression = 0`，则第二个查询的执行时间会明显更长，因为 `WHERE` 子句在子查询完成后应用于所有数据。
## enable_optimize_predicate_expression_to_final_subquery {#enable_optimize_predicate_expression_to_final_subquery} 

<SettingsInfoBlock type="Bool" default_value="1" />

允许将谓词推送到最终子查询。
## enable_order_by_all {#enable_order_by_all} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用或禁用带有 `ORDER BY ALL` 语法的排序，详见 [ORDER BY](../../sql-reference/statements/select/order-by.md)。

可能的值：

- 0 — 禁用 ORDER BY ALL。
- 1 — 启用 ORDER BY ALL。

**示例**

查询：

```sql
CREATE TABLE TAB(C1 Int, C2 Int, ALL Int) ENGINE=Memory();

INSERT INTO TAB VALUES (10, 20, 30), (20, 20, 10), (30, 10, 20);

SELECT * FROM TAB ORDER BY ALL; -- returns an error that ALL is ambiguous

SELECT * FROM TAB ORDER BY ALL SETTINGS enable_order_by_all = 0;
```

结果：

```text
┌─C1─┬─C2─┬─ALL─┐
│ 20 │ 20 │  10 │
│ 30 │ 10 │  20 │
│ 10 │ 20 │  30 │
└────┴────┴─────┘
```
## enable_parallel_blocks_marshalling {#enable_parallel_blocks_marshalling} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.6"},{"label": "true"},{"label": "A new setting"}]}]}/>

仅影响分布式查询。如果启用，块将在管道线程上进行（反）序列化和（解）压缩（即在默认并行度以上）在发送给发起者之前/之后。
## enable_parsing_to_custom_serialization {#enable_parsing_to_custom_serialization} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "1"},{"label": "New setting"}]}]}/>

如果为真，则数据可以直接解析为具有自定义序列化（例如 Sparse）的列，按照从表中获得的序列化提示。
## enable_positional_arguments {#enable_positional_arguments} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "22.7"},{"label": "1"},{"label": "Enable positional arguments feature by default"}]}]}/>

启用或禁用支持位置参数用于 [GROUP BY](/sql-reference/statements/select/group-by)、[LIMIT BY](../../sql-reference/statements/select/limit-by.md)、[ORDER BY](../../sql-reference/statements/select/order-by.md) 语句。

可能的值：

- 0 — 不支持位置参数。
- 1 — 支持位置参数：可以使用列编号代替列名。

**示例**

查询：

```sql
CREATE TABLE positional_arguments(one Int, two Int, three Int) ENGINE=Memory();

INSERT INTO positional_arguments VALUES (10, 20, 30), (20, 20, 10), (30, 10, 20);

SELECT * FROM positional_arguments ORDER BY 2,3;
```

结果：

```text
┌─one─┬─two─┬─three─┐
│  30 │  10 │   20  │
│  20 │  20 │   10  │
│  10 │  20 │   30  │
└─────┴─────┴───────┘
```
## enable_producing_buckets_out_of_order_in_aggregation {#enable_producing_buckets_out_of_order_in_aggregation} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.9"},{"label": "1"},{"label": "New setting"}]}]}/>

允许内存高效聚合（见 `distributed_aggregation_memory_efficient`）生成无序的桶。
这可能在聚合桶大小不均衡时提高性能，让副本在处理一些较小的桶时仍然可以将具有更大 id 的桶发送给发起者。
缺点是可能会增加内存使用。
## enable_reads_from_query_cache {#enable_reads_from_query_cache} 

<SettingsInfoBlock type="Bool" default_value="1" />

如果启用，`SELECT` 查询的结果将从 [查询缓存](../query-cache.md) 中获取。

可能的值：

- 0 - 禁用
- 1 - 启用
## enable_s3_requests_logging {#enable_s3_requests_logging} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用 S3 请求的详细日志记录。仅用于调试。
## enable_scalar_subquery_optimization {#enable_scalar_subquery_optimization} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "19.18"},{"label": "1"},{"label": "Prevent scalar subqueries from (de)serializing large scalar values and possibly avoid running the same subquery more than once"}]}]}/>

如果设为真，防止标量子查询（反）序列化大型标量值，有可能避免多次运行相同的子查询。
## enable_scopes_for_with_statement {#enable_scopes_for_with_statement} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.7"},{"label": "1"},{"label": "New setting for backward compatibility with the old analyzer."}]}, {"id": "row-2","items": [{"label": "25.6"},{"label": "1"},{"label": "New setting for backward compatibility with the old analyzer."}]}, {"id": "row-3","items": [{"label": "25.5"},{"label": "1"},{"label": "New setting for backward compatibility with the old analyzer."}]}, {"id": "row-4","items": [{"label": "25.4"},{"label": "1"},{"label": "New setting for backward compatibility with the old analyzer."}]}]}/>

如果禁用，父级 WITH 子句中的声明将在当前作用域中表现出相同的作用域。

请注意，这是一个兼容性设置，旨在允许运行一些旧分析器能够执行的无效查询。
## enable_shared_storage_snapshot_in_query {#enable_shared_storage_snapshot_in_query} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.6"},{"label": "0"},{"label": "A new setting to share storage snapshot in query"}]}]}/>

如果启用，单个查询内的所有子查询将共享每个表的相同 StorageSnapshot。
这确保了在整个查询中对数据的一致视图，即使是同一表被访问多次。

对于数据部分的内部一致性很重要的查询，这是必需的。示例：

```sql
SELECT
    count()
FROM events
WHERE (_part, _part_offset) IN (
    SELECT _part, _part_offset
    FROM events
    WHERE user_id = 42
)
```

没有这个设置，外部和内部查询可能会在不同的数据快照上操作，导致结果不正确。

:::note
启用该设置会禁用在规划阶段完成后从快照中移除不必要的数据部分的优化。
因此，长时间运行的查询可能会在整个过程中持有过时的部分，延迟部分清理并增加存储压力。

该设置当前仅适用于 MergeTree 家族的表。
:::

可能的值：

- 0 - 禁用
- 1 - 启用
## enable_sharing_sets_for_mutations {#enable_sharing_sets_for_mutations} 

<SettingsInfoBlock type="Bool" default_value="1" />

允许在相同变更的不同任务之间共享为 IN 子查询构建的集合对象。这样可以减少内存使用和 CPU 消耗。
## enable_software_prefetch_in_aggregation {#enable_software_prefetch_in_aggregation} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用在聚合中的软件预取。
## enable_unaligned_array_join {#enable_unaligned_array_join} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许与大小不同的多个数组进行 ARRAY JOIN。当启用此设置时，数组将调整为最长的数组。
## enable_url_encoding {#enable_url_encoding} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "0"},{"label": "Changed existing setting's default value"}]}]}/>

允许在 [URL](../../engines/table-engines/special/url.md) 引擎表中启用/禁用对 uri 路径的解码/编码。

默认情况下禁用。
## enable_vertical_final {#enable_vertical_final} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "1"},{"label": "Enable vertical final by default again after fixing bug"}]}, {"id": "row-2","items": [{"label": "24.1"},{"label": "1"},{"label": "Use vertical final by default"}]}]}/>

如果启用，在 FINAL 期间通过将行标记为删除并稍后过滤它们来移除重复行，而不是合并行。
## enable_writes_to_query_cache {#enable_writes_to_query_cache} 

<SettingsInfoBlock type="Bool" default_value="1" />

如果启用，`SELECT` 查询的结果将存储在 [查询缓存](../query-cache.md) 中。

可能的值：

- 0 - 禁用
- 1 - 启用
## enable_zstd_qat_codec {#enable_zstd_qat_codec} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.1"},{"label": "0"},{"label": "Add new ZSTD_QAT codec"}]}]}/>

如果启用，可以使用 ZSTD_QAT 编解码器来压缩列。
## enforce_strict_identifier_format {#enforce_strict_identifier_format} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "0"},{"label": "New setting."}]}]}/>

如果启用，仅允许包含字母数字字符和下划线的标识符。
## engine_file_allow_create_multiple_files {#engine_file_allow_create_multiple_files} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在文件引擎表中每次插入时创建新文件，如果格式有后缀（`JSON`、`ORC`、`Parquet` 等）。如果启用，每次插入将创建一个新文件，名称遵循以下模式：

`data.Parquet` -> `data.1.Parquet` -> `data.2.Parquet`，以此类推。

可能的值：
- 0 — `INSERT` 查询将新数据附加到文件末尾。
- 1 — `INSERT` 查询将创建一个新文件。
## engine_file_empty_if_not_exists {#engine_file_empty_if_not_exists} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许从没有文件的文件引擎表中选择数据。

可能的值：
- 0 — `SELECT` 抛出异常。
- 1 — `SELECT` 返回空结果。
## engine_file_skip_empty_files {#engine_file_skip_empty_files} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在 [File](../../engines/table-engines/special/file.md) 引擎表中跳过空文件。

可能的值：
- 0 — 如果空文件与请求格式不兼容，`SELECT` 抛出异常。
- 1 — 对于空文件，`SELECT` 返回空结果。
## engine_file_truncate_on_insert {#engine_file_truncate_on_insert} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在 [File](../../engines/table-engines/special/file.md) 引擎表中插入时截断。

可能的值：
- 0 — `INSERT` 查询将新数据附加到文件末尾。
- 1 — `INSERT` 查询将用新数据替换文件的现有内容。
## engine_url_skip_empty_files {#engine_url_skip_empty_files} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在 [URL](../../engines/table-engines/special/url.md) 引擎表中跳过空文件。

可能的值：
- 0 — 如果空文件与请求格式不兼容，`SELECT` 抛出异常。
- 1 — 对于空文件，`SELECT` 返回空结果。
## except_default_mode {#except_default_mode} 

<SettingsInfoBlock type="SetOperationMode" default_value="ALL" />

设置 EXCEPT 查询中的默认模式。可能的值：空字符串、'ALL'、'DISTINCT'。如果为空，则没有模式的查询将抛出异常。
## exclude_materialize_skip_indexes_on_insert {#exclude_materialize_skip_indexes_on_insert} 

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.10"},{"label": ""},{"label": "New setting."}]}]}/>

在 INSERT 期间排除指定的跳过索引不被构建和存储。被排除的跳过索引仍将在 [合并期间](merge-tree-settings.md/#materialize_skip_indexes_on_merge)被构建和存储，或者通过显式的 [MATERIALIZE INDEX](/sql-reference/statements/alter/skipping-index.md/#materialize-index) 查询来构建。

如果 [materialize_skip_indexes_on_insert](#materialize_skip_indexes_on_insert) 为 false，则无效。

示例：

```sql
CREATE TABLE tab
(
    a UInt64,
    b UInt64,
    INDEX idx_a a TYPE minmax,
    INDEX idx_b b TYPE set(3)
)
ENGINE = MergeTree ORDER BY tuple();

SET exclude_materialize_skip_indexes_on_insert='idx_a'; -- idx_a will be not be updated upon insert
--SET exclude_materialize_skip_indexes_on_insert='idx_a, idx_b'; -- neither index would be updated on insert

INSERT INTO tab SELECT number, number / 50 FROM numbers(100); -- only idx_b is updated

-- since it is a session setting it can be set on a per-query level
INSERT INTO tab SELECT number, number / 50 FROM numbers(100, 100) SETTINGS exclude_materialize_skip_indexes_on_insert='idx_b';

ALTER TABLE tab MATERIALIZE INDEX idx_a; -- this query can be used to explicitly materialize the index

SET exclude_materialize_skip_indexes_on_insert = DEFAULT; -- reset setting to default
```
## execute_exists_as_scalar_subquery {#execute_exists_as_scalar_subquery} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "1"},{"label": "New setting"}]}]}/>

将非相关的 EXISTS 子查询作为标量子查询执行。对于标量子查询，使用缓存，并且常量折叠适用于结果。
## external_storage_connect_timeout_sec {#external_storage_connect_timeout_sec} 

<SettingsInfoBlock type="UInt64" default_value="10" />

连接超时（秒）。目前仅支持 MySQL。
## external_storage_max_read_bytes {#external_storage_max_read_bytes} 

限制外部引擎表在刷新历史数据时的最大字节数。当前仅支持 MySQL 表引擎、数据库引擎和字典。如果等于 0，则该设置无效。
## external_storage_max_read_rows {#external_storage_max_read_rows} 

限制外部引擎表在刷新历史数据时的最大行数。当前仅支持 MySQL 表引擎、数据库引擎和字典。如果等于 0，则该设置无效。
## external_storage_rw_timeout_sec {#external_storage_rw_timeout_sec} 

读取/写入超时（秒）。目前仅支持 MySQL。
## external_table_functions_use_nulls {#external_table_functions_use_nulls} 

定义 [mysql](../../sql-reference/table-functions/mysql.md)、[postgresql](../../sql-reference/table-functions/postgresql.md) 和 [odbc](../../sql-reference/table-functions/odbc.md) 表函数如何使用 Nullable 列。

可能的值：

- 0 — 表函数明确使用 Nullable 列。
- 1 — 表函数隐式使用 Nullable 列。

**用法**

如果设置为 `0`，表函数不会生成 Nullable 列，并在 NULL 处插入默认值。这也适用于数组中的 NULL 值。
## external_table_strict_query {#external_table_strict_query} 

如果设置为 true，则禁止将表达式转化为外部表查询的本地过滤器。
## extract_key_value_pairs_max_pairs_per_row {#extract_key_value_pairs_max_pairs_per_row} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "0"},{"label": "Max number of pairs that can be produced by the `extractKeyValuePairs` function. Used as a safeguard against consuming too much memory."}]}]}/>

通过 `extractKeyValuePairs` 函数生成的最大对数。作为防止过多内存消耗的保障措施。
## extremes {#extremes} 

<SettingsInfoBlock type="Bool" default_value="0" />

是否计算极值（查询结果中列的最小值和最大值）。接受 0 或 1。默认值为 0（禁用）。更多信息，请参见“极值”部分。
## fallback_to_stale_replicas_for_distributed_queries {#fallback_to_stale_replicas_for_distributed_queries} 

如果更新的数据不可用，则强制查询切换到过期副本。请参见 [复制](../../engines/table-engines/mergetree-family/replication.md)。

ClickHouse 会从过期的表副本中选择最相关的。

在进行 `SELECT` 从指向复制表的分布式表时使用。

默认为 1（启用）。
## filesystem_cache_boundary_alignment {#filesystem_cache_boundary_alignment} 

<SettingsInfoBlock type="UInt64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "0"},{"label": "New setting"}]}]}/>

文件系统缓存边界对齐。此设置仅应用于非磁盘读取（例如，远程表引擎/表函数的缓存，不适用于 MergeTree 表的存储配置）。值 0 表示无对齐。
## filesystem_cache_enable_background_download_during_fetch {#filesystem_cache_enable_background_download_during_fetch} 

<CloudOnlyBadge/>

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "1"},{"label": "New setting"}]}]}/>

仅在 ClickHouse Cloud 中有效。锁定缓存以进行空间保留的等待时间。
## filesystem_cache_enable_background_download_for_metadata_files_in_packed_storage {#filesystem_cache_enable_background_download_for_metadata_files_in_packed_storage} 

<CloudOnlyBadge/>

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "1"},{"label": "New setting"}]}]}/>

仅在 ClickHouse Cloud 中有效。锁定缓存以进行空间保留的等待时间。
## filesystem_cache_max_download_size {#filesystem_cache_max_download_size} 

单个查询可以下载的最大远程文件系统缓存大小。
## filesystem_cache_name {#filesystem_cache_name} 

用于无状态表引擎或数据湖的文件系统缓存名称。
## filesystem_cache_prefer_bigger_buffer_size {#filesystem_cache_prefer_bigger_buffer_size} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "1"},{"label": "New setting"}]}]}/>

如果启用文件系统缓存，则优先选择更大的缓冲区大小，以避免写入小文件段，这会降低缓存性能。另一方面，启用此设置可能会增加内存使用。
## filesystem_cache_reserve_space_wait_lock_timeout_milliseconds {#filesystem_cache_reserve_space_wait_lock_timeout_milliseconds} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "1000"},{"label": "Wait time to lock cache for space reservation in filesystem cache"}]}]}/>

锁定缓存以进行空间保留的等待时间。
## filesystem_cache_segments_batch_size {#filesystem_cache_segments_batch_size} 

<SettingsInfoBlock type="UInt64" default_value="20" />

限制读取缓冲区可以从缓存请求的单个批次的文件段的大小。过低的值会导致对缓存的请求过多，过大的值可能会减慢从缓存中逐出。
## filesystem_cache_skip_download_if_exceeds_per_query_cache_write_limit {#filesystem_cache_skip_download_if_exceeds_per_query_cache_write_limit} 

<SettingsInfoBlock type="Bool" default_value="1" />

如果超过查询缓存大小，则跳过从远程文件系统的下载。
## filesystem_prefetch_max_memory_usage {#filesystem_prefetch_max_memory_usage} 

预取的最大内存使用量。
## filesystem_prefetch_step_bytes {#filesystem_prefetch_step_bytes} 

预取步骤（字节）。零表示 `auto` - 预取步骤的最佳值将自动推断，但可能不是 100% 最佳。实际值可能会有所不同，因为设置了 filesystem_prefetch_min_bytes_for_single_read_task。
## filesystem_prefetch_step_marks {#filesystem_prefetch_step_marks} 

预取步骤（标记）。零表示 `auto` - 大约最佳预取步骤将被自动推断，但可能不是 100% 最佳。实际值可能会有所不同，因为设置了 filesystem_prefetch_min_bytes_for_single_read_task。
## filesystem_prefetches_limit {#filesystem_prefetches_limit} 

最大预取次数。零表示无限。如果想要限制预取次数，建议设置 `filesystem_prefetches_max_memory_usage`。
## final {#final} 

<SettingsInfoBlock type="Bool" default_value="0" />

自动对查询中所有表应用 [FINAL](../../sql-reference/statements/select/from.md/#final-modifier) 修饰符，对适用的表（包括连接表和子查询中的表）以及分布式表。

可能的值：

- 0 - 禁用
- 1 - 启用

示例：

```sql
CREATE TABLE test
(
    key Int64,
    some String
)
ENGINE = ReplacingMergeTree
ORDER BY key;

INSERT INTO test FORMAT Values (1, 'first');
INSERT INTO test FORMAT Values (1, 'second');

SELECT * FROM test;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘
┌─key─┬─some──┐
│   1 │ first │
└─────┴───────┘

SELECT * FROM test SETTINGS final = 1;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘

SET final = 1;
SELECT * FROM test;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘
```
## flatten_nested {#flatten_nested} 

<SettingsInfoBlock type="Bool" default_value="1" />

设置 [nested](../../sql-reference/data-types/nested-data-structures/index.md) 列的数据格式。

可能的值：

- 1 — 嵌套列被展平为单独的数组。
- 0 — 嵌套列保留为单一的元组数组。

**用法**

如果设置为 `0`，则可以使用任意级别的嵌套。

**示例**

查询：

```sql
SET flatten_nested = 1;
CREATE TABLE t_nest (`n` Nested(a UInt32, b UInt32)) ENGINE = MergeTree ORDER BY tuple();

SHOW CREATE TABLE t_nest;
```

结果：

```text
┌─statement───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.t_nest
(
    `n.a` Array(UInt32),
    `n.b` Array(UInt32)
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS index_granularity = 8192 │
└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

查询：

```sql
SET flatten_nested = 0;

CREATE TABLE t_nest (`n` Nested(a UInt32, b UInt32)) ENGINE = MergeTree ORDER BY tuple();

SHOW CREATE TABLE t_nest;
```

结果：

```text
┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.t_nest
(
    `n` Nested(a UInt32, b UInt32)
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS index_granularity = 8192 │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```
## force_aggregate_partitions_independently {#force_aggregate_partitions_independently} 

<SettingsInfoBlock type="Bool" default_value="0" />

强制在适用时使用优化，但启发式决定不使用。
## force_aggregation_in_order {#force_aggregation_in_order} 

<SettingsInfoBlock type="Bool" default_value="0" />

该设置由服务器本身使用以支持分布式查询。请勿手动更改，因为这会破坏正常操作。（强制在分布式聚合期间在远程节点上按顺序使用聚合）。
## force_data_skipping_indices {#force_data_skipping_indices} 

如果未使用传递的数据跳过索引，则禁用查询执行。

考虑以下示例：

```sql
CREATE TABLE data
(
    key Int,
    d1 Int,
    d1_null Nullable(Int),
    INDEX d1_idx d1 TYPE minmax GRANULARITY 1,
    INDEX d1_null_idx assumeNotNull(d1_null) TYPE minmax GRANULARITY 1
)
Engine=MergeTree()
ORDER BY key;

SELECT * FROM data_01515;
SELECT * FROM data_01515 SETTINGS force_data_skipping_indices=''; -- query will produce CANNOT_PARSE_TEXT error.
SELECT * FROM data_01515 SETTINGS force_data_skipping_indices='d1_idx'; -- query will produce INDEX_NOT_USED error.
SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='d1_idx'; -- Ok.
SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='`d1_idx`'; -- Ok (example of full featured parser).
SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='`d1_idx`, d1_null_idx'; -- query will produce INDEX_NOT_USED error, since d1_null_idx is not used.
SELECT * FROM data_01515 WHERE d1 = 0 AND assumeNotNull(d1_null) = 0 SETTINGS force_data_skipping_indices='`d1_idx`, d1_null_idx'; -- Ok.
```
## force_grouping_standard_compatibility {#force_grouping_standard_compatibility} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "22.9"},{"label": "1"},{"label": "Make GROUPING function output the same as in SQL standard and other DBMS"}]}]}/>

使 GROUPING 函数在参数未用作聚合键时返回 1。
## force_index_by_date {#force_index_by_date} 

<SettingsInfoBlock type="Bool" default_value="0" />

如果无法按日期索引，则禁用查询执行。

适用于 MergeTree 家族的表。

如果 `force_index_by_date=1`，ClickHouse 将检查查询是否具有可以用来限制数据范围的日期键条件。如果没有合适的条件，它将抛出异常。然而，它不会检查条件是否减少了要读取的数据量。例如，条件 `Date != ' 2000-01-01 '` 是可接受的，即使它匹配了表中的所有数据（即，运行查询要求进行完全扫描）。有关 MergeTree 表中的数据范围的更多信息，请参见 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)。
## force_optimize_projection {#force_optimize_projection} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在启用投影优化时 `SELECT` 查询强制使用 [投影](../../engines/table-engines/mergetree-family/mergetree.md/#projections)。

可能的值：

- 0 — 投影优化不是强制性的。
- 1 — 投影优化是强制性的。
## force_optimize_projection_name {#force_optimize_projection_name} 

如果设置为非空字符串，则检查此投影是否至少在查询中使用一次。

可能的值：

- 字符串：在查询中使用的投影名称。
## force_optimize_skip_unused_shards {#force_optimize_skip_unused_shards} 

<SettingsInfoBlock type="UInt64" default_value="0" />

启用或禁用查询执行，如果 [optimize_skip_unused_shards](#optimize_skip_unused_shards) 启用且无法跳过未使用的分片。如果无法跳过且启用了该设置，将抛出异常。

可能的值：

- 0 — 禁用。ClickHouse 不会抛出异常。
- 1 — 启用。仅当表具有分片键时，禁用查询执行。
- 2 — 启用。无论表是否已定义分片键，均禁用查询执行。
## force_optimize_skip_unused_shards_nesting {#force_optimize_skip_unused_shards_nesting} 

<SettingsInfoBlock type="UInt64" default_value="0" />

控制 [`force_optimize_skip_unused_shards`](#force_optimize_skip_unused_shards)（因此仍然需要 [`force_optimize_skip_unused_shards`](#force_optimize_skip_unused_shards））取决于分布式查询的嵌套级别（例如，当你有 `Distributed` 表指向另一个 `Distributed` 表时）。

可能的值：

- 0 - 禁用，`force_optimize_skip_unused_shards` 始终有效。
- 1 — 仅对第一层启用 `force_optimize_skip_unused_shards`。
- 2 — 对第二层启用 `force_optimize_skip_unused_shards`。
## force_primary_key {#force_primary_key} 

<SettingsInfoBlock type="Bool" default_value="0" />

如果无法按主键索引，则禁用查询执行。

适用于 MergeTree 家族的表。

如果 `force_primary_key=1`，ClickHouse 将检查查询是否具有可以用来限制数据范围的主键条件。如果没有合适的条件，它将抛出异常。然而，它不会检查条件是否减少了要读取的数据量。有关 MergeTree 表中的数据范围的更多信息，请参见 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)。
## force_remove_data_recursively_on_drop {#force_remove_data_recursively_on_drop} 

<SettingsInfoBlock type="Bool" default_value="0" />

在 DROP 查询中递归地删除数据。避免“目录不空”错误，但可能会默默删除已分离的数据。
## formatdatetime_e_with_space_padding {#formatdatetime_e_with_space_padding} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "0"},{"label": "Improved compatibility with MySQL DATE_FORMAT/STR_TO_DATE"}]}]}/>

格式化函数 'formatDateTime' 中的格式器 '%e' 输出一位数的日期时，前面带有空格，例如 ' 2' 而不是 '2'。
## formatdatetime_f_prints_scale_number_of_digits {#formatdatetime_f_prints_scale_number_of_digits} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0"},{"label": "New setting."}]}]}/>

格式化函数 'formatDateTime' 中的格式器 '%f' 仅根据 DateTime64 的规模打印数字，而不是固定的 6 位数字。
## formatdatetime_f_prints_single_zero {#formatdatetime_f_prints_single_zero} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.4"},{"label": "0"},{"label": "Improved compatibility with MySQL DATE_FORMAT()/STR_TO_DATE()"}]}]}/>

格式化函数 'formatDateTime' 中的格式器 '%f' 如果格式化值没有小数秒，打印单个零而不是六个零。
## formatdatetime_format_without_leading_zeros {#formatdatetime_format_without_leading_zeros} 

<SettingsInfoBlock type="Bool" default_value="0" />

格式化函数 'formatDateTime' 中的格式器 '%c'、'%l' 和 '%k' 以无前导零的形式打印月份和小时。
## formatdatetime_parsedatetime_m_is_month_name {#formatdatetime_parsedatetime_m_is_month_name} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.4"},{"label": "1"},{"label": "Improved compatibility with MySQL DATE_FORMAT/STR_TO_DATE"}]}]}/>

格式化函数 'formatDateTime' 和 'parseDateTime' 中的格式器 '%M' 打印/解析月份名称而不是分钟。
## fsync_metadata {#fsync_metadata} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用或禁用在写入 `.sql` 文件时的 [fsync](http://pubs.opengroup.org/onlinepubs/9699919799/functions/fsync.html)。默认为启用。

如果服务器有数百万个持续创建和销毁的小表，禁用它是合理的。
## function_date_trunc_return_type_behavior {#function_date_trunc_return_type_behavior} 

<SettingsInfoBlock type="UInt64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.7"},{"label": "0"},{"label": "Add new setting to preserve old behaviour of dateTrunc function"}]}, {"id": "row-2","items": [{"label": "25.4"},{"label": "0"},{"label": "Change the result type for dateTrunc function for DateTime64/Date32 arguments to DateTime64/Date32 regardless of time unit to get correct result for negative values"}]}]}/>

允许更改 `dateTrunc` 函数结果类型的行为。

可能的值：

- 0 - 当第二个参数为 `DateTime64/Date32` 时，返回类型为 `DateTime64/Date32`，与第一个参数中的时间单位无关。
- 1 - 对于 `Date32`，结果始终为 `Date`。对于 `DateTime64`，如果时间单位为 `second` 及更高，结果为 `DateTime`。
## function_implementation {#function_implementation} 

选择特定目标或变体（实验性）的函数实现。如果为空则启用所有实现。
## function_json_value_return_type_allow_complex {#function_json_value_return_type_allow_complex} 

<SettingsInfoBlock type="Bool" default_value="0" />

控制 json_value 函数是否允许返回复杂类型（例如：结构、数组、映射）。

```sql
SELECT JSON_VALUE('{"hello":{"world":"!"}}', '$.hello') settings function_json_value_return_type_allow_complex=true

┌─JSON_VALUE('{"hello":{"world":"!"}}', '$.hello')─┐
│ {"world":"!"}                                    │
└──────────────────────────────────────────────────┘

1 row in set. Elapsed: 0.001 sec.
```

可能的值：

- true — 允许。
- false — 不允许。
## function_json_value_return_type_allow_nullable {#function_json_value_return_type_allow_nullable} 

<SettingsInfoBlock type="Bool" default_value="0" />

控制 JSON_VALUE 函数在值不存在时是否允许返回 `NULL`。

```sql
SELECT JSON_VALUE('{"hello":"world"}', '$.b') settings function_json_value_return_type_allow_nullable=true;

┌─JSON_VALUE('{"hello":"world"}', '$.b')─┐
│ ᴺᵁᴸᴸ                                   │
└────────────────────────────────────────┘

1 row in set. Elapsed: 0.001 sec.
```

可能的值：

- true — 允许。
- false — 不允许。
## function_locate_has_mysql_compatible_argument_order {#function_locate_has_mysql_compatible_argument_order} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "1"},{"label": "Increase compatibility with MySQL's locate function."}]}]}/>

控制函数 [locate](../../sql-reference/functions/string-search-functions.md/#locate) 中参数的顺序。

可能的值：

- 0 — 函数 `locate` 接受参数 `(haystack, needle[, start_pos])`。
- 1 — 函数 `locate` 接受参数 `(needle, haystack, [, start_pos])`（与 MySQL 兼容的行为）。
## function_range_max_elements_in_block {#function_range_max_elements_in_block} 

<SettingsInfoBlock type="UInt64" default_value="500000000" />

设置函数 [range](/sql-reference/functions/array-functions#range) 生成的数据量的安全阈值。定义每个数据块生成的最大值数量（每块每行数组大小的总和）。

可能的值：

- 正整数。

**另见**

- [`max_block_size`](#max_block_size)
- [`min_insert_block_size_rows`](#min_insert_block_size_rows)
## function_sleep_max_microseconds_per_block {#function_sleep_max_microseconds_per_block} 

<SettingsInfoBlock type="UInt64" default_value="3000000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.7"},{"label": "3000000"},{"label": "In previous versions, the maximum sleep time of 3 seconds was applied only for `sleep`, but not for `sleepEachRow` function. In the new version, we introduce this setting. If you set compatibility with the previous versions, we will disable the limit altogether."}]}]}/>

函数 `sleep` 允许每块暂停的最大微秒数。如果用户使用更大的值调用它，抛出异常。这是一个安全阈值。
## function_visible_width_behavior {#function_visible_width_behavior} 

<SettingsInfoBlock type="UInt64" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.1"},{"label": "1"},{"label": "We changed the default behavior of `visibleWidth` to be more precise"}]}]}/>

`visibleWidth` 行为的版本。0 - 仅计算代码点数量；1 - 正确计算零宽字符和组合字符，将全宽字符计为两个，估算制表符宽度，计算删除字符。
## geo_distance_returns_float64_on_float64_arguments {#geo_distance_returns_float64_on_float64_arguments} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "1"},{"label": "Increase the default precision."}]}]}/>

如果传递给 `geoDistance`、`greatCircleDistance`、`greatCircleAngle` 函数的四个参数都是 Float64，则返回 Float64，并以双精度进行内部计算。在之前的 ClickHouse 版本中，这些函数始终返回 Float32。
## geotoh3_argument_order {#geotoh3_argument_order} 

<BetaBadge/>

<SettingsInfoBlock type="GeoToH3ArgumentOrder" default_value="lat_lon" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "lat_lon"},{"label": "A new setting for legacy behaviour to set lon and lat argument order"}]}]}/>

函数 'geoToH3' 如果设置为 'lon_lat' 则接受 (lon, lat)，如果设置为 'lat_lon' 则接受 (lat, lon)。
## glob_expansion_max_elements {#glob_expansion_max_elements} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

允许的最大地址数（对于外部存储、表函数等）。
## grace_hash_join_initial_buckets {#grace_hash_join_initial_buckets} 

<ExperimentalBadge/>

<SettingsInfoBlock type="NonZeroUInt64" default_value="1" />

初始的宽松哈希连接桶数。
## grace_hash_join_max_buckets {#grace_hash_join_max_buckets} 

<ExperimentalBadge/>

<SettingsInfoBlock type="NonZeroUInt64" default_value="1024" />

宽松哈希连接桶的数量限制。
## group_by_overflow_mode {#group_by_overflow_mode} 

<SettingsInfoBlock type="OverflowModeGroupBy" default_value="throw" />

设置当聚合的唯一键数量超过限制时会发生的情况：
- `throw`: 抛出异常
- `break`: 停止执行查询并返回部分结果
- `any`: 继续聚合已进入集合的键，但不向集合中添加新键。

使用 'any' 值让您运行 GROUP BY 的近似。此近似的质量依赖于数据的统计特性。
## group_by_two_level_threshold {#group_by_two_level_threshold} 

<SettingsInfoBlock type="UInt64" default_value="100000" />

从多少个键开始，两级聚合开始。0 - 未设置阈值。
## group_by_two_level_threshold_bytes {#group_by_two_level_threshold_bytes} 

<SettingsInfoBlock type="UInt64" default_value="50000000" />

从多少字节的聚合状态大小开始，开始使用两级聚合。0 - 不设定阈值。触发至少一个阈值时使用两级聚合。
## group_by_use_nulls {#group_by_use_nulls} 

<SettingsInfoBlock type="Bool" default_value="0" />

更改 [GROUP BY 子句](/sql-reference/statements/select/group-by) 如何对聚合键的类型进行处理。
当使用 `ROLLUP`、`CUBE` 或 `GROUPING SETS` 指定器时，一些聚合键可能不会被用于生成某些结果行。
这些键对应的列根据此设置用默认值或 `NULL` 填充。

可能的值：

- 0 — 使用聚合键类型的默认值来生成缺失值。
- 1 — ClickHouse 按照 SQL 标准的方式执行 `GROUP BY`。聚合键的类型被转换为 [Nullable](/sql-reference/data-types/nullable)。对于未使用的行，对应聚合键的列填充 [NULL](/sql-reference/syntax#null)。

另见：

- [GROUP BY 子句](/sql-reference/statements/select/group-by)
## h3togeo_lon_lat_result_order {#h3togeo_lon_lat_result_order} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0"},{"label": "A new setting"}]}]}/>

函数 'h3ToGeo' 返回 (lon, lat) 如果为 true，否则返回 (lat, lon)。
## handshake_timeout_ms {#handshake_timeout_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="10000" />

握手期间接收 Hello 数据包的超时时间（毫秒）。
## hdfs_create_new_file_on_insert {#hdfs_create_new_file_on_insert} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在 HDFS 引擎表中的每次插入时创建新文件。如果启用，在每次插入时将创建一个新 HDFS 文件，其名称类似于以下模式：

初始：`data.Parquet.gz` -> `data.1.Parquet.gz` -> `data.2.Parquet.gz` 等等。

可能的值：
- 0 — `INSERT` 查询将新数据附加到文件末尾。
- 1 — `INSERT` 查询创建一个新文件。

## hdfs_ignore_file_doesnt_exist {#hdfs_ignore_file_doesnt_exist} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "0"},{"label": "Allow to return 0 rows when the requested files don't exist instead of throwing an exception in HDFS table engine"}]}]}/>

在读取某些键时，如果文件不存在，则忽略文件不存在的情况。

可能的值：
- 1 — `SELECT` 返回空结果。
- 0 — `SELECT` 抛出异常。

## hdfs_replication {#hdfs_replication} 

<SettingsInfoBlock type="UInt64" default_value="0" />

创建 hdfs 文件时可以指定实际的复制数量。

## hdfs_skip_empty_files {#hdfs_skip_empty_files} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在 [HDFS](../../engines/table-engines/integrations/hdfs.md) 引擎表中跳过空文件。

可能的值：
- 0 — 如果空文件与请求的格式不兼容，`SELECT` 抛出异常。
- 1 — 空文件的 `SELECT` 返回空结果。

## hdfs_throw_on_zero_files_match {#hdfs_throw_on_zero_files_match} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "0"},{"label": "Allow to throw an error when ListObjects request cannot match any files in HDFS engine instead of empty query result"}]}]}/>

如果根据 glob 扩展规则匹配到零个文件则抛出错误。

可能的值：
- 1 — `SELECT` 抛出异常。
- 0 — `SELECT` 返回空结果。

## hdfs_truncate_on_insert {#hdfs_truncate_on_insert} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在 hdfs 引擎表中插入之前进行截断。如果禁用，如果 HDFS 中已存在文件，则在尝试插入时将抛出异常。

可能的值：
- 0 — `INSERT` 查询将新数据附加到文件末尾。
- 1 — `INSERT` 查询用新数据替换文件的现有内容。

## hedged_connection_timeout_ms {#hedged_connection_timeout_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="50" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.4"},{"label": "50"},{"label": "Start new connection in hedged requests after 50 ms instead of 100 to correspond with previous connect timeout"}]}]}/>

为 Hedged 请求建立与副本的连接的超时。

## hnsw_candidate_list_size_for_search {#hnsw_candidate_list_size_for_search} 

<SettingsInfoBlock type="UInt64" default_value="256" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "256"},{"label": "New setting. Previously, the value was optionally specified in CREATE INDEX and 64 by default."}]}]}/>

搜索向量相似度索引时动态候选列表的大小，也称为 'ef_search'。

## hsts_max_age {#hsts_max_age} 

HSTS 的过期时间。 0 表示禁用 HSTS。

## http_connection_timeout {#http_connection_timeout} 

HTTP 连接超时（以秒为单位）。

可能的值：

- 任何正整数。
- 0 - 禁用（无限超时）。

## http_headers_progress_interval_ms {#http_headers_progress_interval_ms} 

在每个指定间隔内不要更频繁地发送 HTTP 头 X-ClickHouse-Progress。

## http_make_head_request {#http_make_head_request} 

`http_make_head_request` 设置允许在从 HTTP 读取数据时执行 `HEAD` 请求，以检索要读取的文件的信息，例如其大小。由于默认启用，因此在服务器不支持 `HEAD` 请求的情况下，可能希望禁用此设置。

## http_max_field_name_size {#http_max_field_name_size} 

HTTP 头中字段名称的最大长度。

## http_max_field_value_size {#http_max_field_value_size} 

HTTP 头中字段值的最大长度。

## http_max_fields {#http_max_fields} 

HTTP 头中的最大字段数量。

## http_max_multipart_form_data_size {#http_max_multipart_form_data_size} 

对 multipart/form-data 内容大小的限制。此设置不能从 URL 参数解析，应该在用户配置文件中设置。请注意，在查询执行开始之前，会解析内容并在内存中创建外部表。这是唯一对该阶段有效的限制（最大内存使用量和最大执行时间的限制在读取 HTTP 表单数据时没有效果）。

## http_max_request_param_data_size {#http_max_request_param_data_size} 

对用作预定义 HTTP 请求中查询参数的请求数据的大小的限制。

## http_max_tries {#http_max_tries} 

通过 http 读取的最大尝试次数。

## http_max_uri_size {#http_max_uri_size} 

设置 HTTP 请求的最大 URI 长度。

可能的值：

- 正整数。

## http_native_compression_disable_checksumming_on_decompress {#http_native_compression_disable_checksumming_on_decompress} 

启用或禁用在从客户端解压 HTTP POST 数据时的校验和验证。仅用于 ClickHouse 原生压缩格式（不使用 `gzip` 或 `deflate`）。

有关更多信息，请阅读 [HTTP 接口说明](../../interfaces/http.md)。

可能的值：

- 0 — 禁用。
- 1 — 启用。

## http_receive_timeout {#http_receive_timeout} 

<SettingsInfoBlock type="Seconds" default_value="30" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.6"},{"label": "30"},{"label": "See http_send_timeout."}]}]}/>

HTTP 接收超时（以秒为单位）。

可能的值：

- 任何正整数。
- 0 - 禁用（无限超时）。

## http_response_buffer_size {#http_response_buffer_size} 

在向客户端发送 HTTP 响应或刷新到磁盘（当 http_wait_end_of_query 启用时）之前要缓冲的字节数。

## http_response_headers {#http_response_headers} 

<SettingsInfoBlock type="Map" default_value="{}" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.12"},{"label": ""},{"label": "New setting."}]}]}/>

允许添加或覆盖服务器在成功查询结果的响应中返回的 HTTP 头。这只影响 HTTP 接口。

如果头已经默认设置，提供的值将覆盖它。如果头没有默认设置，它将被添加到头列表中。由服务器默认设置而未被此设置覆盖的头将保留。

该设置允许你将头设置为常量值。目前还没有办法将头设置为动态计算的值。

名称和值都不能包含 ASCII 控制字符。

如果你实现了允许用户修改设置的 UI 应用程序，同时根据返回的头做出决策，建议将此设置限制为只读。

示例：`SET http_response_headers = '{"Content-Type": "image/png"}'`

## http_retry_initial_backoff_ms {#http_retry_initial_backoff_ms} 

重试通过 http 读取时的最小回退毫秒数。

## http_retry_max_backoff_ms {#http_retry_max_backoff_ms} 

重试通过 http 读取时的最大回退毫秒数。

## http_send_timeout {#http_send_timeout} 

<SettingsInfoBlock type="Seconds" default_value="30" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.6"},{"label": "30"},{"label": "3 minutes seems crazy long. Note that this is timeout for a single network write call, not for the whole upload operation."}]}]}/>

HTTP 发送超时（以秒为单位）。

可能的值：

- 任何正整数。
- 0 - 禁用（无限超时）。

:::note
仅适用于默认配置文件。需要重新启动服务器才能使更改生效。
:::

## http_skip_not_found_url_for_globs {#http_skip_not_found_url_for_globs} 

对具有 HTTP_NOT_FOUND 错误的 glob 跳过 URL。

## http_wait_end_of_query {#http_wait_end_of_query} 

在服务器端启用 HTTP 响应缓冲。

## http_write_exception_in_output_format {#http_write_exception_in_output_format} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.9"},{"label": "1"},{"label": "Output valid JSON/XML on exception in HTTP streaming."}]}]}/>

在输出格式中写入异常以生成有效输出。适用于 JSON 和 XML 格式。

## http_zlib_compression_level {#http_zlib_compression_level} 

设置对 HTTP 请求的响应中数据压缩的级别，如果 [enable_http_compression = 1](#enable_http_compression)。

可能的值：数字 1 到 9。

## iceberg_delete_data_on_drop {#iceberg_delete_data_on_drop} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.9"},{"label": "0"},{"label": "New setting"}]}]}/>

在删除时是否删除所有 iceberg 文件。

## iceberg_insert_max_bytes_in_data_file {#iceberg_insert_max_bytes_in_data_file} 

<SettingsInfoBlock type="UInt64" default_value="1073741824" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.9"},{"label": "100000000"},{"label": "New setting."}]}, {"id": "row-2","items": [{"label": "25.10"},{"label": "100000000"},{"label": "New setting."}]}]}/>

插入操作中 iceberg parquet 数据文件的最大字节数。

## iceberg_insert_max_rows_in_data_file {#iceberg_insert_max_rows_in_data_file} 

<SettingsInfoBlock type="UInt64" default_value="1000000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.9"},{"label": "100000"},{"label": "New setting."}]}, {"id": "row-2","items": [{"label": "25.10"},{"label": "1000000"},{"label": "New setting."}]}]}/>

插入操作中 iceberg parquet 数据文件的最大行数。

## iceberg_metadata_compression_method {#iceberg_metadata_compression_method} 

<ExperimentalBadge/>

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": ""},{"label": "New setting"}]}]}/>

压缩 `.metadata.json` 文件的方法。

## iceberg_metadata_log_level {#iceberg_metadata_log_level} 

<SettingsInfoBlock type="IcebergMetadataLogLevel" default_value="none" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.9"},{"label": "none"},{"label": "New setting."}]}]}/>

控制 Iceberg 表的元数据日志记录级别到 system.iceberg_metadata_log。通常此设置可以为调试目的进行修改。

可能的值：
- none - 无元数据日志。
- metadata - 根 metadata.json 文件。
- manifest_list_metadata - 上述所有 + 来自与快照对应的 avro 清单列表的元数据。
- manifest_list_entry - 上述所有 + avro 清单列表条目。
- manifest_file_metadata - 上述所有 + 来源于已遍历的 avro 清单文件的元数据。
- manifest_file_entry - 上述所有 + 已遍历的 avro 清单文件条目。

## iceberg_snapshot_id {#iceberg_snapshot_id} 

<SettingsInfoBlock type="Int64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "0"},{"label": "New setting."}]}]}/>

使用特定快照 ID 查询 Iceberg 表。

## iceberg_timestamp_ms {#iceberg_timestamp_ms} 

<SettingsInfoBlock type="Int64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "0"},{"label": "New setting."}]}]}/>

使用在特定时间戳时的快照查询 Iceberg 表。

## idle_connection_timeout {#idle_connection_timeout} 

在指定时间（以秒为单位）后关闭空闲 TCP 连接的超时。

可能的值：

- 正整数（0 - 立即关闭，0 秒后）。

## ignore_cold_parts_seconds {#ignore_cold_parts_seconds} 

<CloudOnlyBadge/>

<SettingsInfoBlock type="Int64" default_value="0" />

仅在 ClickHouse Cloud 中生效。将新数据片排除在 SELECT 查询之外，直到它们被预热（请参见 [cache_populated_by_fetch](merge-tree-settings.md/#cache_populated_by_fetch)）或此多秒钟旧。仅适用于 Replicated-/SharedMergeTree。

## ignore_data_skipping_indices {#ignore_data_skipping_indices} 

如果查询中使用指定的数据跳过索引，则忽略这些索引。

考虑以下示例：

```sql
CREATE TABLE data
(
    key Int,
    x Int,
    y Int,
    INDEX x_idx x TYPE minmax GRANULARITY 1,
    INDEX y_idx y TYPE minmax GRANULARITY 1,
    INDEX xy_idx (x,y) TYPE minmax GRANULARITY 1
)
Engine=MergeTree()
ORDER BY key;

INSERT INTO data VALUES (1, 2, 3);

SELECT * FROM data;
SELECT * FROM data SETTINGS ignore_data_skipping_indices=''; -- query will produce CANNOT_PARSE_TEXT error.
SELECT * FROM data SETTINGS ignore_data_skipping_indices='x_idx'; -- Ok.
SELECT * FROM data SETTINGS ignore_data_skipping_indices='na_idx'; -- Ok.

SELECT * FROM data WHERE x = 1 AND y = 1 SETTINGS ignore_data_skipping_indices='xy_idx',force_data_skipping_indices='xy_idx' ; -- query will produce INDEX_NOT_USED error, since xy_idx is explicitly ignored.
SELECT * FROM data WHERE x = 1 AND y = 2 SETTINGS ignore_data_skipping_indices='xy_idx';
```

查询未忽略任何索引：
```sql
EXPLAIN indexes = 1 SELECT * FROM data WHERE x = 1 AND y = 2;

Expression ((Projection + Before ORDER BY))
  Filter (WHERE)
    ReadFromMergeTree (default.data)
    Indexes:
      PrimaryKey
        Condition: true
        Parts: 1/1
        Granules: 1/1
      Skip
        Name: x_idx
        Description: minmax GRANULARITY 1
        Parts: 0/1
        Granules: 0/1
      Skip
        Name: y_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
      Skip
        Name: xy_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
```

忽略 `xy_idx` 索引：
```sql
EXPLAIN indexes = 1 SELECT * FROM data WHERE x = 1 AND y = 2 SETTINGS ignore_data_skipping_indices='xy_idx';

Expression ((Projection + Before ORDER BY))
  Filter (WHERE)
    ReadFromMergeTree (default.data)
    Indexes:
      PrimaryKey
        Condition: true
        Parts: 1/1
        Granules: 1/1
      Skip
        Name: x_idx
        Description: minmax GRANULARITY 1
        Parts: 0/1
        Granules: 0/1
      Skip
        Name: y_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
```

适用于 MergeTree 系列的表。

## ignore_drop_queries_probability {#ignore_drop_queries_probability} 

<SettingsInfoBlock type="Float" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.4"},{"label": "0"},{"label": "Allow to ignore drop queries in server with specified probability for testing purposes"}]}]}/>

如果启用，服务器将忽略具有指定概率的所有 DROP 表查询（对于 Memory 和 JOIN 引擎，将 DROP 替换为 TRUNCATE）。用于测试目的。

## ignore_materialized_views_with_dropped_target_table {#ignore_materialized_views_with_dropped_target_table} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.1"},{"label": "0"},{"label": "Add new setting to allow to ignore materialized views with dropped target table"}]}]}/>

在推送到视图期间忽略有目标表被删除的物化视图。

## ignore_on_cluster_for_replicated_access_entities_queries {#ignore_on_cluster_for_replicated_access_entities_queries} 

忽略复制访问实体管理查询的 ON CLUSTER 子句。

## ignore_on_cluster_for_replicated_named_collections_queries {#ignore_on_cluster_for_replicated_named_collections_queries} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.7"},{"label": "0"},{"label": "Ignore ON CLUSTER clause for replicated named collections management queries."}]}]}/>

忽略复制命名集合管理查询的 ON CLUSTER 子句。

## ignore_on_cluster_for_replicated_udf_queries {#ignore_on_cluster_for_replicated_udf_queries} 

忽略复制 UDF 管理查询的 ON CLUSTER 子句。

## implicit_select {#implicit_select} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "0"},{"label": "A new setting."}]}]}/>

允许在不带前导 SELECT 关键字的情况下编写简单的 SELECT 查询，这使得计算器风格的用法变得简单，例如 `1 + 2` 成为有效查询。

在 `clickhouse-local` 中默认启用，并可以明确禁用。

## implicit_table_at_top_level {#implicit_table_at_top_level} 

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": ""},{"label": "A new setting, used in clickhouse-local"}]}]}/>

如果不为空，顶层没有 FROM 的查询将从此表读取，而不是 system.one。

这在 clickhouse-local 用于输入数据处理。该设置可以由用户明确设置，但不打算用于此类用途。

子查询不受此设置的影响（无论是标量、FROM 还是 IN 子查询）。UNION、INTERSECT、EXCEPT 链的顶层 SELECT 会统一处理并受此设置影响，无论其在括号中分组如何。此设置对视图和分布式查询的影响未指定。

该设置接受表名（然后从当前数据库解析表）或以 'database.table' 形式的限定名称。数据库和表名都必须未加引号 - 仅允许简单标识符。

## implicit_transaction {#implicit_transaction} 

<ExperimentalBadge/>

<SettingsInfoBlock type="Bool" default_value="0" />

如果启用且尚未在事务中，则将查询包装在完整事务中（开始 + 提交或回滚）。

## input_format_parallel_parsing {#input_format_parallel_parsing} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用或禁用数据格式的顺序保持并行解析。仅支持 [TSV](../../interfaces/formats.md/#tabseparated)、[TSKV](../../interfaces/formats.md/#tskv)、[CSV](../../interfaces/formats.md/#csv) 和 [JSONEachRow](../../interfaces/formats.md/#jsoneachrow) 格式。

可能的值：

- 1 — 启用。
- 0 — 禁用。

## insert_allow_materialized_columns {#insert_allow_materialized_columns} 

如果设置启用，允许在 INSERT 中使用物化列。

## insert_deduplicate {#insert_deduplicate} 

启用或禁用 `INSERT` 的块去重（对于 Replicated* 表）。

可能的值：

- 0 — 禁用。
- 1 — 启用。

默认情况下，通过 `INSERT` 语句插入到复制表中的块会被去重（请参阅 [数据复制](../../engines/table-engines/mergetree-family/replication.md)）。对于复制表，默认情况下，每个分区仅去重最近的 100 个块（请参阅 [replicated_deduplication_window](merge-tree-settings.md/#replicated_deduplication_window)、[replicated_deduplication_window_seconds](merge-tree-settings.md/#replicated_deduplication_window_seconds)）。对于未复制的表，请参见 [non_replicated_deduplication_window](merge-tree-settings.md/#non_replicated_deduplication_window)。

## insert_deduplication_token {#insert_deduplication_token} 

该设置允许用户在 MergeTree/ReplicatedMergeTree 中提供自己的去重语义。例如，通过在每个 INSERT 语句中提供一个唯一值，用户可以避免重复插入的数据被去重。

可能的值：

- 任何字符串

`insert_deduplication_token` 仅在非空时用于去重。

对于复制表，默认情况下，每个分区仅去重最近的 100 个插入（请参阅 [replicated_deduplication_window](merge-tree-settings.md/#replicated_deduplication_window)、[replicated_deduplication_window_seconds](merge-tree-settings.md/#replicated_deduplication_window_seconds)）。对于未复制的表，请参见 [non_replicated_deduplication_window](merge-tree-settings.md/#non_replicated_deduplication_window)。

:::note
`insert_deduplication_token` 在分区级别上工作（与 `insert_deduplication` 校验和相同）。多个分区可以具有相同的 `insert_deduplication_token`。
:::

示例：

```sql
CREATE TABLE test_table
( A Int64 )
ENGINE = MergeTree
ORDER BY A
SETTINGS non_replicated_deduplication_window = 100;

INSERT INTO test_table SETTINGS insert_deduplication_token = 'test' VALUES (1);

-- the next insert won't be deduplicated because insert_deduplication_token is different
INSERT INTO test_table SETTINGS insert_deduplication_token = 'test1' VALUES (1);

-- the next insert will be deduplicated because insert_deduplication_token
-- is the same as one of the previous
INSERT INTO test_table SETTINGS insert_deduplication_token = 'test' VALUES (2);

SELECT * FROM test_table

┌─A─┐
│ 1 │
└───┘
┌─A─┐
│ 1 │
└───┘
```

## insert_keeper_fault_injection_probability {#insert_keeper_fault_injection_probability} 

<SettingsInfoBlock type="Float" default_value="0" />

插入时 keeper 请求的故障概率。有效值在区间 [0.0f, 1.0f] 之间。

## insert_keeper_fault_injection_seed {#insert_keeper_fault_injection_seed} 

0 - 随机种子，否则为设置值。

## insert_keeper_max_retries {#insert_keeper_max_retries} 

<SettingsInfoBlock type="UInt64" default_value="20" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.2"},{"label": "20"},{"label": "Enable reconnections to Keeper on INSERT, improve reliability"}]}]}/>

该设置设置在插入到复制 MergeTree 时 ClickHouse Keeper（或 ZooKeeper）请求的最大重试次数。仅考虑因网络错误、Keeper 会话超时或请求超时而失败的 Keeper 请求。

可能的值：

- 正整数。
- 0 — 禁用重试。

云默认值：`20`。

Keeper 请求的重试在一定超时后进行。超时由以下设置控制：`insert_keeper_retry_initial_backoff_ms`、`insert_keeper_retry_max_backoff_ms`。第一次重试在 `insert_keeper_retry_initial_backoff_ms` 超时后进行。后续超时将按照以下方式计算：
```
timeout = min(insert_keeper_retry_max_backoff_ms, latest_timeout * 2)
```

例如，如果 `insert_keeper_retry_initial_backoff_ms=100`，`insert_keeper_retry_max_backoff_ms=10000`，`insert_keeper_max_retries=8`，则超时将为 `100, 200, 400, 800, 1600, 3200, 6400, 10000`。

除了容错，重试旨在提供更好的用户体验 - 例如，允许在 INSERT 执行期间避免返回错误，如果 Keeper 由于升级而重新启动。

## insert_keeper_retry_initial_backoff_ms {#insert_keeper_retry_initial_backoff_ms} 

执行 INSERT 查询时重试失败的 Keeper 请求的初始超时（以毫秒为单位）。

可能的值：

- 正整数。
- 0 — 无超时。

## insert_keeper_retry_max_backoff_ms {#insert_keeper_retry_max_backoff_ms} 

执行 INSERT 查询时重试失败的 Keeper 请求的最大超时（以毫秒为单位）。

可能的值：

- 正整数。
- 0 — 最大超时没有限制。

## insert_null_as_default {#insert_null_as_default} 

启用或禁用在非 [nullable](/sql-reference/data-types/nullable) 数据类型的列中插入 [默认值](/sql-reference/statements/create/table#default_values) 而不是 [NULL](/sql-reference/syntax#null)。如果列类型非空且此设置禁用，则插入 `NULL` 会导致异常。如果列类型可为空，则无论此设置如何，`NULL` 值都是原样插入的。

此设置适用于 [INSERT ... SELECT](../../sql-reference/statements/insert-into.md/#inserting-the-results-of-select) 查询。请注意，`SELECT` 子查询可以与 `UNION ALL` 子句连接。

可能的值：

- 0 — 在非空列中插入 `NULL` 导致异常。
- 1 — 插入默认列值而不是 `NULL`。

## insert_quorum {#insert_quorum} 

:::note
此设置不适用于 SharedMergeTree，更多信息请参见 [SharedMergeTree 一致性](/cloud/reference/shared-merge-tree#consistency)。
:::

启用 quorum 写入。

- 如果 `insert_quorum < 2`，则禁用 quorum 写入。
- 如果 `insert_quorum >= 2`，则启用 quorum 写入。
- 如果 `insert_quorum = 'auto'`，则使用多数数（`number_of_replicas / 2 + 1`）作为 quorum 数。

quorum 写入

`INSERT` 仅在 ClickHouse 成功地将数据写入到 `insert_quorum` 的副本中，并在 `insert_quorum_timeout` 内成功完成时才成功。如果由于任何原因，成功写入的副本数量未达到 `insert_quorum`，则视为写入失败，ClickHouse 将从所有已写入数据的副本中删除插入的块。

当 `insert_quorum_parallel` 被禁用时，quorum 中的所有副本都保持一致，即它们包含所有先前 `INSERT` 查询的数据（`INSERT` 序列是线性化的）。在读取使用 `insert_quorum` 和 `insert_quorum_parallel` 禁用时写入的数据时，可以通过使用 [select_sequential_consistency](#select_sequential_consistency) 开启 `SELECT` 查询的顺序一致性。

ClickHouse 会生成异常：

- 如果查询时可用副本的数量少于 `insert_quorum`。
- 禁用 `insert_quorum_parallel` 时，如果尝试在上一个块尚未在 `insert_quorum` 的副本中插入时写入数据。这种情况可能发生在用户尝试在上一个带有 `insert_quorum` 的查询完成之前对同一表执行另一个 `INSERT` 查询时。

另请参见：

- [insert_quorum_timeout](#insert_quorum_timeout)
- [insert_quorum_parallel](#insert_quorum_parallel)
- [select_sequential_consistency](#select_sequential_consistency)

## insert_quorum_parallel {#insert_quorum_parallel} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "21.1"},{"label": "1"},{"label": "Use parallel quorum inserts by default. It is significantly more convenient to use than sequential quorum inserts"}]}]}/>

:::note
此设置不适用于 SharedMergeTree，更多信息请参见 [SharedMergeTree 一致性](/cloud/reference/shared-merge-tree#consistency)。
:::

启用或禁用 quorum `INSERT` 查询的并行性。如果启用，可以在先前查询尚未完成的情况下发送额外的 `INSERT` 查询。如果禁用，将拒绝对同一表的额外写入。

可能的值：

- 0 — 禁用。
- 1 — 启用。

另请参见：

- [insert_quorum](#insert_quorum)
- [insert_quorum_timeout](#insert_quorum_timeout)
- [select_sequential_consistency](#select_sequential_consistency)

## insert_quorum_timeout {#insert_quorum_timeout} 

以毫秒为单位的对 quorum 写入的超时。如果超时已过而尚未进行任何写入，ClickHouse 将生成异常，客户必须重复查询以将同一块写入同一或其他副本。

另请参见：

- [insert_quorum](#insert_quorum)
- [insert_quorum_parallel](#insert_quorum_parallel)
- [select_sequential_consistency](#select_sequential_consistency)

## insert_shard_id {#insert_shard_id} 

如果不为 `0`，则指定将数据同步插入的 [Distributed](/engines/table-engines/special/distributed) 表的分片。

如果 `insert_shard_id` 值不正确，服务器将抛出异常。

要获取 `requested_cluster` 上的分片数量，您可以检查服务器配置或使用以下查询：

```sql
SELECT uniq(shard_num) FROM system.clusters WHERE cluster = 'requested_cluster';
```

可能的值：

- 0 — 禁用。
- 从 `1` 到相应的 [Distributed](/engines/table-engines/special/distributed) 表的 `shards_num` 的任何数字。

**示例**

查询：

```sql
CREATE TABLE x AS system.numbers ENGINE = MergeTree ORDER BY number;
CREATE TABLE x_dist AS x ENGINE = Distributed('test_cluster_two_shards_localhost', currentDatabase(), x);
INSERT INTO x_dist SELECT * FROM numbers(5) SETTINGS insert_shard_id = 1;
SELECT * FROM x_dist ORDER BY number ASC;
```

结果：

```text
┌─number─┐
│      0 │
│      0 │
│      1 │
│      1 │
│      2 │
│      2 │
│      3 │
│      3 │
│      4 │
│      4 │
└────────┘
```

## interactive_delay {#interactive_delay} 

检查请求执行是否已被取消并发送进度的间隔（以微秒为单位）。

## intersect_default_mode {#intersect_default_mode} 

设置 INTERSECT 查询的默认模式。可能的值：空字符串、'ALL'、'DISTINCT'。如果为空，未指定模式的查询将抛出异常。

## jemalloc_collect_profile_samples_in_trace_log {#jemalloc_collect_profile_samples_in_trace_log} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.9"},{"label": "0"},{"label": "New setting"}]}]}/>

在追踪日志中收集 jemalloc 分配和释放样本。

## jemalloc_enable_profiler {#jemalloc_enable_profiler} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.9"},{"label": "0"},{"label": "New setting"}]}]}/>

为查询启用 jemalloc 个人分析器。jemalloc 将采样分配及所有已采样分配的释放。可以使用 SYSTEM JEMALLOC FLUSH PROFILE 刷新配置，这可以用于分配分析。样本也可以使用配置 jemalloc_collect_global_profile_samples_in_trace_log 存储在 system.trace_log 中，或使用查询设置 jemalloc_collect_profile_samples_in_trace_log。有关更多信息，请参阅 [分配分析](/operations/allocation-profiling)。

## join_algorithm {#join_algorithm} 

<SettingsInfoBlock type="JoinAlgorithm" default_value="direct,parallel_hash,hash" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.12"},{"label": "direct,parallel_hash,hash"},{"label": "'default' was deprecated in favor of explicitly specified join algorithms, also parallel_hash is now preferred over hash"}]}]}/>

指定使用哪个 [JOIN](../../sql-reference/statements/select/join.md) 算法。

可以指定多个算法，并且会根据类型/严格性和表引擎为特定查询选择可用的算法。

可能的值：

- grace_hash

 使用 [Grace hash join](https://en.wikipedia.org/wiki/Hash_join#Grace_hash_join)。Grace hash 提供了一种算法选项，能够在限制内存使用时提高复杂连接的性能。

 配置哈希桶的个数（N），根据关键列的哈希值将右表分割成 N 个桶。每个桶可以独立处理。第一阶段会读取右表并分割，若哈希表超出内存限制（例如，由 [`max_bytes_in_join`](/operations/settings/settings#max_bytes_in_join) 限制），则增大桶的个数并重新分配每行。

 支持 `INNER/LEFT/RIGHT/FULL ALL/ANY JOIN`。

- hash

 使用 [Hash join algorithm](https://en.wikipedia.org/wiki/Hash_join)。最通用的实现，支持所有的类型和严格性、多个用 `OR` 结合的连接键的组合。

 使用 `hash` 算法时，连接的右侧部分会上传到内存。

- parallel_hash

 `hash` 连接的一种变体，将数据分割成多个桶并同时构建多个哈希表，以加快此过程。

 使用 `parallel_hash` 算法时，连接的右侧部分会上传到内存。

- partial_merge

 [sort-merge algorithm](https://en.wikipedia.org/wiki/Sort-merge_join) 的变体，只有右表是完全排序的。

 仅支持 `RIGHT JOIN` 和 `FULL JOIN`，且必须采用 `ALL` 的严格性（不支持 `SEMI`、`ANTI`、`ANY` 和 `ASOF`）。

 ClickHouse 会按连接键的顺序对右表进行排序，然后将其转储到磁盘，以保证 order 和 min-max 索引的块排序。左侧的数据会被分块排序后，将其与右侧表进行 join。在此操作的过程中，也利用 min-max 索引以跳过右边无需的块。

- direct

 此算法可以在右表存储支持键值请求时应用。

 此 `direct` 算法会使用来自左表的行作为键，在右表中进行查找。仅支持特殊存储，如 [Dictionary](/engines/table-engines/special/dictionary) 或 [EmbeddedRocksDB](../../engines/table-engines/integrations/embedded-rocksdb.md)，并仅支持 `LEFT` 和 `INNER` JOIN。

- auto

 设置为 `auto` 时，首先尝试使用 `hash` 进行连接，并在内存限制被违反时动态切换到其他算法。

- full_sorting_merge

 [Sort-merge algorithm](https://en.wikipedia.org/wiki/Sort-merge_join) 在加入之前对连接的表进行完全排序。

- prefer_partial_merge

 ClickHouse 始终尝试使用 `partial_merge` 连接，如果不可能，则使用 `hash`。*不推荐使用，与 `partial_merge,hash` 相同。

- default (deprecated)

 遗留值，请勿再使用。与 `direct,hash` 相同，也就是说，尝试使用 direct join 和 hash join（按此顺序）。

## join_any_take_last_row {#join_any_take_last_row} 

更改 `ANY` 严格性的连接操作行为。

:::note
此设置仅适用于与 [Join](../../engines/table-engines/special/join.md) 引擎表的 `JOIN` 操作。
:::

可能的值：

- 0 — 如果右表有多个匹配行，则仅连接找到的第一条。
- 1 — 如果右表有多个匹配行，则连接找到的最后一条。

另请参见：

- [JOIN 子句](/sql-reference/statements/select/join)
- [Join 表引擎](../../engines/table-engines/special/join.md)
- [join_default_strictness](#join_default_strictness)

## join_default_strictness {#join_default_strictness} 

为 [JOIN 子句](/sql-reference/statements/select/join) 设置默认严格性。

可能的值：

- `ALL` — 如果右表有多个匹配行，ClickHouse 会从匹配行中创建一个 [笛卡尔积](https://en.wikipedia.org/wiki/Cartesian_product)。这是标准 SQL 的正常 `JOIN` 行为。
- `ANY` — 如果右表有多个匹配行，则仅连接找到的第一条。如果右表仅有一行匹配，则 `ANY` 和 `ALL` 的结果是相同的。
- `ASOF` — 用于连接具有不确定匹配的序列。
- 空字符串 — 如果查询中未指定 `ALL` 或 `ANY`，ClickHouse 将抛出异常。

## join_on_disk_max_files_to_merge {#join_on_disk_max_files_to_merge} 

限制在 MergeJoin 操作时允许在磁盘上并行排序的文件数量。

设置的值越大，使用的 RAM 越多，磁盘 I/O 需求越少。

可能的值：

- 任何正整数，起始值为 2。

## join_output_by_rowlist_perkey_rows_threshold {#join_output_by_rowlist_perkey_rows_threshold} 

<SettingsInfoBlock type="UInt64" default_value="5" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.9"},{"label": "5"},{"label": "The lower limit of per-key average rows in the right table to determine whether to output by row list in hash join."}]}]}/>

右表中每个键的平均行数的下限，以确定在哈希连接中是否按行列表输出。

## join_overflow_mode {#join_overflow_mode} 

定义 ClickHouse 在达到以下任意连接限制时执行的操作：

- [max_bytes_in_join](/operations/settings/settings#max_bytes_in_join)
- [max_rows_in_join](/operations/settings/settings#max_rows_in_join)

可能的值：

- `THROW` — ClickHouse 抛出异常并中断操作。
- `BREAK` — ClickHouse 中断操作而不抛出异常。

默认值：`THROW`。

**另请参见**

- [JOIN 子句](/sql-reference/statements/select/join)
- [Join 表引擎](/engines/table-engines/special/join)

## join_runtime_bloom_filter_bytes {#join_runtime_bloom_filter_bytes} 

<ExperimentalBadge/>

<SettingsInfoBlock type="UInt64" default_value="524288" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.10"},{"label": "524288"},{"label": "New setting"}]}]}/>

作为 JOIN 运行时过滤器使用的布隆过滤器的大小（以字节为单位）（请参见 enable_join_runtime_filters 设置）。

## join_runtime_bloom_filter_hash_functions {#join_runtime_bloom_filter_hash_functions} 

<ExperimentalBadge/>

<SettingsInfoBlock type="UInt64" default_value="3" />

作为 JOIN 运行时过滤器使用的布隆过滤器中的哈希函数数量（请参见 enable_join_runtime_filters 设置）。
## join_to_sort_maximum_table_rows {#join_to_sort_maximum_table_rows} 

<ExperimentalBadge/>

<SettingsInfoBlock type="UInt64" default_value="10000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.9"},{"label": "10000"},{"label": "The maximum number of rows in the right table to determine whether to rerange the right table by key in left or inner join"}]}]}/>

右表中的最大行数，用于确定在左连接或内连接中是否按键重新排列右表。
## join_to_sort_minimum_perkey_rows {#join_to_sort_minimum_perkey_rows} 

<ExperimentalBadge/>

<SettingsInfoBlock type="UInt64" default_value="40" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.9"},{"label": "40"},{"label": "The lower limit of per-key average rows in the right table to determine whether to rerange the right table by key in left or inner join. This setting ensures that the optimization is not applied for sparse table keys"}]}]}/>

右表中每个键的平均行数的下限，用于确定在左连接或内连接中是否按键重新排列右表。此设置确保优化不应用于稀疏表键。
## join_use_nulls {#join_use_nulls} 

<SettingsInfoBlock type="Bool" default_value="0" />

设置[JOIN](../../sql-reference/statements/select/join.md)行为的类型。在合并表时，可能会出现空单元格。ClickHouse根据此设置以不同的方式填充它们。

可能的值：

- 0 — 空单元格用相应字段类型的默认值填充。
- 1 — `JOIN`的行为与标准SQL相同。相应字段的类型被转换为[Nullable](/sql-reference/data-types/nullable)，并且空单元格用[NULL](/sql-reference/syntax)填充。
## joined_subquery_requires_alias {#joined_subquery_requires_alias} 

<SettingsInfoBlock type="Bool" default_value="1" />

强制连接子查询和表函数具有别名以进行正确的名称限定。
## kafka_disable_num_consumers_limit {#kafka_disable_num_consumers_limit} 

<SettingsInfoBlock type="Bool" default_value="0" />

禁用依赖于可用CPU核心数量的kafka_num_consumers限制。
## kafka_max_wait_ms {#kafka_max_wait_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="5000" />

从[Kafka](/engines/table-engines/integrations/kafka)读取消息前的等待时间（以毫秒为单位）。

可能的值：

- 正整数。
- 0 — 无限超时。

另见：

- [Apache Kafka](https://kafka.apache.org/)
## keeper_map_strict_mode {#keeper_map_strict_mode} 

<SettingsInfoBlock type="Bool" default_value="0" />

在KeeperMap操作中强制执行额外检查。例如：在插入已存在键时抛出异常。
## keeper_max_retries {#keeper_max_retries} 

<SettingsInfoBlock type="UInt64" default_value="10" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "10"},{"label": "Max retries for general keeper operations"}]}]}/>

通用keeper操作的最大重试次数。
## keeper_retry_initial_backoff_ms {#keeper_retry_initial_backoff_ms} 

<SettingsInfoBlock type="UInt64" default_value="100" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "100"},{"label": "Initial backoff timeout for general keeper operations"}]}]}/>

通用keeper操作的初始退避超时时间。
## keeper_retry_max_backoff_ms {#keeper_retry_max_backoff_ms} 

<SettingsInfoBlock type="UInt64" default_value="5000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "5000"},{"label": "Max backoff timeout for general keeper operations"}]}]}/>

通用keeper操作的最大退避超时时间。
## least_greatest_legacy_null_behavior {#least_greatest_legacy_null_behavior} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.12"},{"label": "0"},{"label": "New setting"}]}]}/>

如果启用，函数'least'和'greatest'在其参数之一为NULL时返回NULL。
## legacy_column_name_of_tuple_literal {#legacy_column_name_of_tuple_literal} 

<SettingsInfoBlock type="Bool" default_value="0" />

将大型元组文字的所有元素名称列在它们的列名称中，而不是哈希。此设置仅出于兼容性原因存在。在将集群从低于21.7版本逐步更新到更高版本时，将其设置为'true'是有意义的。
## lightweight_delete_mode {#lightweight_delete_mode} 

<SettingsInfoBlock type="LightweightDeleteMode" default_value="alter_update" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "alter_update"},{"label": "A new setting"}]}]}/>

轻量级删除的一种内部更新查询模式。

可能的值：
- `alter_update` - 运行创建较重突变的`ALTER UPDATE`查询。
- `lightweight_update` - 在可能的情况下运行轻量级更新，否则运行`ALTER UPDATE`。
- `lightweight_update_force` - 若可能则运行轻量级更新，否则抛出异常。
## lightweight_deletes_sync {#lightweight_deletes_sync} 

<SettingsInfoBlock type="UInt64" default_value="2" />

与[`mutations_sync`](#mutations_sync)相同，但仅控制轻量级删除的执行。

可能的值：

| 值   | 描述                                                                                                                                             |
|------|--------------------------------------------------------------------------------------------------------------------------------------------------|
| `0`  | 突变异步执行。                                                                                                                                |
| `1`  | 查询等待当前服务器上的轻量级删除完成。                                                                                                              |
| `2`  | 查询等待所有副本（如果存在）的轻量级删除完成。                                                                                                              |
| `3`  | 查询仅等待活动副本。仅支持`SharedMergeTree`。对于`ReplicatedMergeTree`，它的行为与`mutations_sync = 2`相同。                          |

**另见**

- [ALTER 查询的同步性](../../sql-reference/statements/alter/index.md/#synchronicity-of-alter-queries)
- [突变](../../sql-reference/statements/alter/index.md/#mutations)
## limit {#limit} 

<SettingsInfoBlock type="UInt64" default_value="0" />

设置从查询结果中获取的最大行数。它会调整由[LIMIT](/sql-reference/statements/select/limit)子句设置的值，以确保查询中指定的限制不能超过该设置所设定的限制。

可能的值：

- 0 — 行数没有限制。
- 正整数。
## live_view_heartbeat_interval {#live_view_heartbeat_interval} 

<ExperimentalBadge/>

<SettingsInfoBlock type="Seconds" default_value="15" />

心跳间隔（以秒为单位），以指示实时查询存活。
## load_balancing {#load_balancing} 

<SettingsInfoBlock type="LoadBalancing" default_value="random" />

指定用于分布式查询处理的副本选择算法。

ClickHouse支持以下副本选择算法：

- [随机](#load_balancing-random)（默认）
- [最近主机名](#load_balancing-nearest_hostname)
- [主机名的levenshtein距离](#load_balancing-hostname_levenshtein_distance)
- [按顺序](#load_balancing-in_order)
- [第一个或随机](#load_balancing-first_or_random)
- [轮询](#load_balancing-round_robin)

另见：

- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)
### 随机（默认） {#load_balancing-random}

```sql
load_balancing = random
```

为每个副本计算错误数量。查询发送到错误最少的副本，如果有多个这样的副本，则发送到任意一个。
缺点：不考虑服务器的接近性；如果副本具有不同的数据，您将获得不同的数据。
### 最近主机名 {#load_balancing-nearest_hostname}

```sql
load_balancing = nearest_hostname
```

为每个副本计算错误数量。每5分钟，错误数量会被整除以2。因此，错误数量是根据近期时间计算的，带有指数平滑。如果只有一个副本具有最少的错误（即最近在其他副本上发生了错误），查询会发送到该副本。如果有多个副本具有相同数量的最少错误，则查询发送到与配置文件中服务器的主机名最相似的副本（在相同位置不同字符的数量，直到两个主机名的最小长度）。

例如，example01-01-1和example01-01-2在一个位置不同，而example01-01-1和example01-02-2在两个位置不同。
这种方法可能看起来原始，但它不需要网络拓扑的外部数据，并且不比较IP地址，这对于我们的IPv6地址来说是复杂的。

因此，如果存在等效副本，按名称最接近的副本会被优先选择。
我们还可以假设，当向同一个服务器发送查询时，在没有失败的情况下，分布式查询也会发到相同的服务器上。因此，即使副本上放置了不同的数据，查询也会返回大致相同的结果。
### 主机名的levenshtein距离 {#load_balancing-hostname_levenshtein_distance}

```sql
load_balancing = hostname_levenshtein_distance
```

与`nearest_hostname`相似，但以[levenshtein距离](https://en.wikipedia.org/wiki/Levenshtein_distance)的方式比较主机名。例如：

```text
example-clickhouse-0-0 ample-clickhouse-0-0
1

example-clickhouse-0-0 example-clickhouse-1-10
2

example-clickhouse-0-0 example-clickhouse-12-0
3
```
### 按顺序 {#load_balancing-in_order}

```sql
load_balancing = in_order
```

访问具有相同错误数的副本的顺序与它们在配置中的说明顺序相同。
此方法适用于确切知道哪个副本更可取的情况。
### 第一个或随机 {#load_balancing-first_or_random}

```sql
load_balancing = first_or_random
```

此算法选择集合中的第一个副本，如果第一个不可用，则选择随机副本。在交叉复制拓扑设置中有效，但在其他配置中无用。

`first_or_random`算法解决了`in_order`算法的问题。使用`in_order`时，如果一个副本宕机，接下来的副本会负担双重负载，而其余副本处理正常的流量。使用`first_or_random`算法时，负载在仍然可用的副本之间均匀分配。

可以通过使用设置`load_balancing_first_offset`显式定义第一个副本。这使得在副本之间重新平衡查询负载有了更多的控制。
### 轮询 {#load_balancing-round_robin}

```sql
load_balancing = round_robin
```

此算法采用轮询策略在具有相同错误数的副本间进行分配（只有使用`round_robin`策略的查询才被计算）。
## load_balancing_first_offset {#load_balancing_first_offset} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在使用FIRST_OR_RANDOM负载均衡策略时，优先发送查询的副本。
## load_marks_asynchronously {#load_marks_asynchronously} 

<SettingsInfoBlock type="Bool" default_value="0" />

异步加载MergeTree标记。
## local_filesystem_read_method {#local_filesystem_read_method} 

<SettingsInfoBlock type="String" default_value="pread_threadpool" />

从本地文件系统读取数据的方法：read, pread, mmap, io_uring, pread_threadpool。

'io_uring'方法为实验性，且在存在并发读取和写入的情况下，不能用于Log, TinyLog, StripeLog, File, Set和Join等表。
如果您在互联网上查看关于'io_uring'的各种文章，请不要被其所蒙蔽。它并不是读取文件的更好方法，除非在大量小IO请求的情况下，而这在ClickHouse中并不适用。没有理由启用'io_uring'。
## local_filesystem_read_prefetch {#local_filesystem_read_prefetch} 

在从本地文件系统读取数据时应使用预读取。
## lock_acquire_timeout {#lock_acquire_timeout} 

定义锁定请求在失败之前等待的秒数。

锁定超时旨在在执行读/写表操作时防止死锁。当超时到期并且锁定请求失败时，ClickHouse服务器抛出异常“锁定尝试超时！可能已避免死锁。客户端应重试。”错误代码为`DEADLOCK_AVOIDED`。

可能的值：

- 正整数（以秒为单位）。
- 0 — 没有锁定超时。
## log_comment {#log_comment} 

指定[system.query_log](../system-tables/query_log.md)表中`log_comment`字段的值以及服务器日志的注释文本。

它可以用来提高服务器日志的可读性。此外，它有助于选择与测试相关的查询，从而可以在运行[clickhouse-test](../../development/tests.md)后从`system.query_log`中筛选。

可能的值：

- 任何不超过[max_query_size](#max_query_size)的字符串。如果超出max_query_size，服务器将抛出异常。

**示例**

查询：

```sql
SET log_comment = 'log_comment test', log_queries = 1;
SELECT 1;
SYSTEM FLUSH LOGS;
SELECT type, query FROM system.query_log WHERE log_comment = 'log_comment test' AND event_date >= yesterday() ORDER BY event_time DESC LIMIT 2;
```

结果：

```text
┌─type────────┬─query─────┐
│ QueryStart  │ SELECT 1; │
│ QueryFinish │ SELECT 1; │
└─────────────┴───────────┘
```
## log_formatted_queries {#log_formatted_queries} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许将格式化查询记录到[system.query_log](../../operations/system-tables/query_log.md)系统表中（填充[`formatted_query`](../../operations/system-tables/query_log.md)列）。

可能的值：

- 0 — 系统表中不记录格式化查询。
- 1 — 系统表中记录格式化查询。
## log_processors_profiles {#log_processors_profiles} 

<SettingsInfoBlock type="Bool" default_value="1" />

记录处理器在执行/等待数据期间花费的时间到`system.processors_profile_log`表。

另见：

- [`system.processors_profile_log`](../../operations/system-tables/processors_profile_log.md)
- [`EXPLAIN PIPELINE`](../../sql-reference/statements/explain.md/#explain-pipeline)
## log_profile_events {#log_profile_events} 

将查询性能统计信息记录到query_log、query_thread_log和query_views_log中。
## log_queries {#log_queries} 

设置查询日志记录。

根据[query_log](../../operations/server-configuration-parameters/settings.md/#query_log)服务器配置参数中的规则，借助此设置发送到ClickHouse的查询将被记录。

示例：

```text
log_queries=1
```
## log_queries_cut_to_length {#log_queries_cut_to_length} 

<SettingsInfoBlock type="UInt64" default_value="100000" />

如果查询长度超过指定阈值（以字节为单位），则在写入查询日志时截断查询。同时限制普通文本日志中打印查询的长度。
## log_queries_min_query_duration_ms {#log_queries_min_query_duration_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="0" />

如果启用（非零），则比该设置值快的查询将不会被记录（可以将其视为[MySQL慢查询日志](https://dev.mysql.com/doc/refman/5.7/slow-query-log.html)的`long_query_time`），这基本上意味着您在以下表中找不到它们：

- `system.query_log`
- `system.query_thread_log`

只有以下类型的查询将进入日志：

- `QUERY_FINISH`
- `EXCEPTION_WHILE_PROCESSING`

- 类型：毫秒
- 默认值：0（任何查询）
## log_queries_min_type {#log_queries_min_type} 

<SettingsInfoBlock type="LogQueriesType" default_value="QUERY_START" />

`query_log`记录的最小类型。

可能的值：
- `QUERY_START` (`=1`)
- `QUERY_FINISH` (`=2`)
- `EXCEPTION_BEFORE_START` (`=3`)
- `EXCEPTION_WHILE_PROCESSING` (`=4`)

可以用来限制哪些实体将进入`query_log`，比如如果您只对错误感兴趣，可以使用`EXCEPTION_WHILE_PROCESSING`：

```text
log_queries_min_type='EXCEPTION_WHILE_PROCESSING'
```
## log_queries_probability {#log_queries_probability} 

<SettingsInfoBlock type="Float" default_value="1" />

允许用户将查询写入[query_log](../../operations/system-tables/query_log.md)、[query_thread_log](../../operations/system-tables/query_thread_log.md)和[query_views_log](../../operations/system-tables/query_views_log.md)系统表中，仅根据指定概率随机选择一部分查询。这有助于减少每秒查询量大的负载。

可能的值：

- 0 — 查询不记录在系统表中。
- 正浮点数，范围在[0..1]内。例如，如果设置值为`0.5`，则大约一半的查询将记录在系统表中。
- 1 — 所有查询记录在系统表中。
## log_query_settings {#log_query_settings} 

将查询设置记录到query_log和OpenTelemetry跨度日志中。
## log_query_threads {#log_query_threads} 

设置查询线程日志记录。

查询线程记录到[system.query_thread_log](../../operations/system-tables/query_thread_log.md)表中。当[log_queries](#log_queries)为true时，此设置有效。通过ClickHouse运行的查询线程在此设置下将根据[query_thread_log](/operations/server-configuration-parameters/settings#query_thread_log)服务器配置参数中的规则被记录。

可能的值：

- 0 — 禁用。
- 1 — 启用。

**示例**

```text
log_query_threads=1
```
## log_query_views {#log_query_views} 

<SettingsInfoBlock type="Bool" default_value="1" />

设置查询视图日志记录。

当ClickHouse以此设置启用的方式运行查询时，如果有关联的视图（物化或实时视图），则会在[query_views_log](/operations/server-configuration-parameters/settings#query_views_log)服务器配置参数中记录。

示例：

```text
log_query_views=1
```
## low_cardinality_allow_in_native_format {#low_cardinality_allow_in_native_format} 

<SettingsInfoBlock type="Bool" default_value="1" />

允许或限制在[NATIVE](../../interfaces/formats.md/#native)格式中使用[LowCardinality](../../sql-reference/data-types/lowcardinality.md)数据类型。

如果限制使用`LowCardinality`，ClickHouse服务器会在`SELECT`查询中将`LowCardinality`列转换为普通列，并在`INSERT`查询中将普通列转换为`LowCardinality`列。

此设置主要针对不支持`LowCardinality`数据类型的第三方客户端。

可能的值：

- 1 — 不限制使用`LowCardinality`。
- 0 — 限制使用`LowCardinality`。
## low_cardinality_max_dictionary_size {#low_cardinality_max_dictionary_size} 

<SettingsInfoBlock type="UInt64" default_value="8192" />

设置可写入存储文件系统的[LowCardinality](../../sql-reference/data-types/lowcardinality.md)数据类型的共享全局字典的最大行数。此设置防止在字典无限增长的情况下出现内存问题。所有由于最大字典大小限制而无法编码的数据，ClickHouse将以普通方式写入。

可能的值：

- 任何正整数。
## low_cardinality_use_single_dictionary_for_part {#low_cardinality_use_single_dictionary_for_part} 

<SettingsInfoBlock type="Bool" default_value="0" />

打开或关闭为数据部分使用单个字典。

默认情况下，ClickHouse服务器监视字典的大小，如果字典溢出，则服务器开始写入下一个字典。要禁止为数据部分创建多个字典，请设置`low_cardinality_use_single_dictionary_for_part = 1`。

可能的值：

- 1 — 禁止为数据部分创建多个字典。
- 0 — 不禁止为数据部分创建多个字典。
## low_priority_query_wait_time_ms {#low_priority_query_wait_time_ms} 

<BetaBadge/>

<SettingsInfoBlock type="Milliseconds" default_value="1000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "1000"},{"label": "New setting."}]}]}/>

当采用查询优先级机制时（参见设置`priority`），低优先级查询在高优先级查询完成之前等待。此设置指定了等待的持续时间。
## make_distributed_plan {#make_distributed_plan} 

<ExperimentalBadge/>

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "0"},{"label": "New experimental setting."}]}]}/>

生成分布式查询计划。
## materialize_skip_indexes_on_insert {#materialize_skip_indexes_on_insert} 

<SettingsInfoBlock type="Bool" default_value="1" />

如果INSERT构建并存储跳过索引。如果禁用，则跳过索引仅在[合并](merge-tree-settings.md/#materialize_skip_indexes_on_merge)或通过显式[MATERIALIZE INDEX](/sql-reference/statements/alter/skipping-index.md/#materialize-index)构建和存储。

另见[exclude_materialize_skip_indexes_on_insert](#exclude_materialize_skip_indexes_on_insert)。
## materialize_statistics_on_insert {#materialize_statistics_on_insert} 

<SettingsInfoBlock type="Bool" default_value="1" />

如果INSERT构建并插入统计信息。如果禁用，则统计信息将在合并期间或通过显式MATERIALIZE STATISTICS构建和存储。
## materialize_ttl_after_modify {#materialize_ttl_after_modify} 

应用旧数据的TTL，后跟ALTER MODIFY TTL查询。
## materialized_views_ignore_errors {#materialized_views_ignore_errors} 

允许忽略MATERIALIZED VIEW的错误，无论MV仍将原始块传递给表。
## materialized_views_squash_parallel_inserts {#materialized_views_squash_parallel_inserts} 

<SettingsInfoBlock type="Bool" default_value="1" />

压缩来自平行插入的单个INSERT查询到物化视图目标表的插入，减少生成的部分数量。
如果设置为false并且`parallel_view_processing`启用，INSERT查询将为每个`max_insert_thread`在目标表中生成部分。
## max_analyze_depth {#max_analyze_depth} 

解释器执行的最大分析次数。
## max_ast_depth {#max_ast_depth} 

查询语法树的最大嵌套深度。如果超过，则抛出异常。

:::note
目前，它在解析期间未进行检查，而是在解析完查询后进行检查。
这意味着在解析过程中可能会创建一个过深的语法树，但查询会失败。
:::
## max_ast_elements {#max_ast_elements} 

查询语法树中元素的最大数量。如果超过，则抛出异常。

:::note
目前，它在解析期间未进行检查，而是在解析完查询后进行检查。
这意味着在解析过程中可能会创建一个过深的语法树，但查询会失败。
:::
## max_autoincrement_series {#max_autoincrement_series} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "1000"},{"label": "A new setting"}]}]}/>

由`generateSeriesID`函数创建的系列数的限制。

由于每个系列表示Keeper中的一个节点，建议不超过几百万个系列。
## max_backup_bandwidth {#max_backup_bandwidth} 

特定备份在服务器上每秒的最大读取速度（以字节为单位）。零表示无限制。
## max_block_size {#max_block_size} 

在ClickHouse中，数据是通过块处理的，块是列部分的集合。单个块的内部处理周期高效，但处理每个块时会产生明显的开销。

`max_block_size`设置指示从表加载数据时单个块中包含的推荐最大行数。大小为`max_block_size`的块不会始终从表中加载：如果ClickHouse确定需要检索的数据较少，则会处理较小的块。

块大小不应太小，以避免在处理每个块时产生明显的开销。它也不应太大，以确保在处理第一个块后，带有LIMIT子句的查询可以快速执行。在设置`max_block_size`时，目标应是避免在多线程中提取大量列时消耗过多内存，并保持至少一些缓存局部性。
## max_bytes_before_external_group_by {#max_bytes_before_external_group_by} 

云的默认值：每个副本内存量的一半。

启用或禁用在外部内存中执行`GROUP BY`子句。
（参见[在外部内存中的GROUP BY](/sql-reference/statements/select/group-by#group-by-in-external-memory)）

可能的值：

- 单个[GROUP BY](/sql-reference/statements/select/group-by)操作可以使用的最大RAM量（以字节为单位）。
- `0` — 在外部内存中禁用`GROUP BY`。

:::note
如果GROUP BY操作期间的内存使用超过该阈值（以字节为单位），则激活“外部聚合”模式（将数据溢出到磁盘）。

推荐值为可用系统内存的一半。
:::
## max_bytes_before_external_sort {#max_bytes_before_external_sort} 

云的默认值：每个副本内存量的一半。

启用或禁用在外部内存中执行`ORDER BY`子句。参见[ORDER BY实现细节](../../sql-reference/statements/select/order-by.md#implementation-details)
如果ORDER BY操作期间的内存使用超过该阈值（以字节为单位），则激活“外部排序”模式（将数据溢出到磁盘）。

可能的值：

- 单个[ORDER BY](../../sql-reference/statements/select/order-by.md)操作可以使用的最大RAM量（以字节为单位）。
  推荐值为可用系统内存的一半
- `0` — 在外部内存中禁用`ORDER BY`。
## max_bytes_before_remerge_sort {#max_bytes_before_remerge_sort} 

在ORDER BY与LIMIT的情况下，当内存使用高于指定阈值时，在最终合并之前执行额外的合并步骤，以仅保持顶级LIMIT行。
## max_bytes_in_distinct {#max_bytes_in_distinct} 

用于DISTINCT时，状态在内存中最大使用的字节数（以未压缩字节为单位），通过哈希表使用。
## max_bytes_in_join {#max_bytes_in_join} 

表连接时使用的哈希表的最大字节数。

此设置适用于[SELECT ... JOIN](/sql-reference/statements/select/join)操作和[Join表引擎](/engines/table-engines/special/join)。

如果查询包含连接，ClickHouse会检查每个中间结果的此设置。

当达到限制时，ClickHouse可以执行不同的一步。使用[join_overflow_mode](/operations/settings/settings#join_overflow_mode)设置来选择行动方案。

可能的值：

- 正整数。
- 0 — 禁用内存控制。
## max_bytes_in_set {#max_bytes_in_set} 

由子查询创建的IN子句中的集合使用的最大字节数（未压缩数据）。
## max_bytes_ratio_before_external_group_by {#max_bytes_ratio_before_external_group_by} 

<SettingsInfoBlock type="Double" default_value="0.5" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0.5"},{"label": "Enable automatic spilling to disk by default."}]}, {"id": "row-2","items": [{"label": "24.12"},{"label": "0"},{"label": "New setting."}]}]}/>

允许用于`GROUP BY`的可用内存的比率。一旦达到，将使用外部内存进行聚合。

例如，设置为`0.6`，`GROUP BY`将允许在执行开始时使用60%的可用内存（用于服务器/用户/合并），之后将开始使用外部聚合。
## max_bytes_ratio_before_external_sort {#max_bytes_ratio_before_external_sort} 

<SettingsInfoBlock type="Double" default_value="0.5" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0.5"},{"label": "Enable automatic spilling to disk by default."}]}, {"id": "row-2","items": [{"label": "24.12"},{"label": "0"},{"label": "New setting."}]}]}/>

允许用于`ORDER BY`的可用内存的比率。一旦达到，将使用外部排序。

例如，设置为`0.6`，`ORDER BY`将允许在执行开始时使用60%的可用内存（用于服务器/用户/合并），之后将开始使用外部排序。

注意，`max_bytes_before_external_sort`仍然会被尊重，只有当排序块大于`max_bytes_before_external_sort`时，才会进行溢出到磁盘。
## max_bytes_to_read {#max_bytes_to_read} 

执行查询时从表中可以读取的最大字节数（未压缩数据）。
对每个处理的数据块进行限制，仅适用于最深的表表达式，并且在从远程服务器读取时，仅对远程服务器进行检查。
## max_bytes_to_read_leaf {#max_bytes_to_read_leaf} 

在运行分布式查询时，从叶节点的本地表中可以读取的最大字节数（未压缩数据）。虽然分布式查询可以向每个分片（叶）发出多个子查询 - 但此限制仅在叶节点的读取阶段进行检查，并将在根节点的结果合并阶段被忽略。

例如，集群由两个分片组成，每个分片包含100字节数据。一个计划从两个表中读取所有数据的分布式查询，如果设置为`max_bytes_to_read=150`将会失败，因为总共会有200字节的数据。设置为`max_bytes_to_read_leaf=150`的查询将会成功，因为叶节点最多将读取100字节。

限制针对每个处理的数据块进行检查。

:::note
此设置在`prefer_localhost_replica=1`时不稳定。
:::
## max_bytes_to_sort {#max_bytes_to_sort} 

在排序之前的最大字节数。如果需要处理的未压缩字节数超过指定数量，则`ORDER BY`操作的行为将由`sort_overflow_mode`确定，默认设置为`throw`。
## max_bytes_to_transfer {#max_bytes_to_transfer} 

当执行GLOBAL IN/JOIN部分时，可以传递给远程服务器或保存在临时表中的最大字节数（未压缩数据）。
## max_columns_to_read {#max_columns_to_read} 

可以在单个查询中从表中读取的最大列数。
如果一个查询需要读取超过指定数量的列，则会抛出异常。

:::tip
此设置有助于防止过于复杂的查询。
:::

`0`值表示无限制。
## max_compress_block_size {#max_compress_block_size} 

为了写入表，压缩前未压缩数据块的最大大小。默认值为1,048,576（1 MiB）。指定较小的块大小通常会导致压缩比略有降低，压缩和解压缩速度略有提高，因为缓存局部性较好，并且内存消耗降低。

:::note
这是一个专家级设置，如果您刚开始使用ClickHouse，不要更改它。
:::

不要将用于压缩的块（一个由字节组成的内存块）与查询处理块（表中的一组行）混淆。
## max_concurrent_queries_for_all_users {#max_concurrent_queries_for_all_users} 

如果此设置的值小于或等于当前正在处理的查询数量，则抛出异常。

示例：可以将`max_concurrent_queries_for_all_users`设置为99，所有用户的数据库管理员可以将其设置为100，以便在服务器过载时也能够运行查询进行调查。

对单个查询或用户的设置修改不会影响其他查询。

可能的值：

- 正整数。
- 0 — 没有限制。

**示例**

```xml
<max_concurrent_queries_for_all_users>99</max_concurrent_queries_for_all_users>
```

**另见**

- [max_concurrent_queries](/operations/server-configuration-parameters/settings#max_concurrent_queries)
## max_concurrent_queries_for_user {#max_concurrent_queries_for_user} 

每个用户同时处理的查询的最大数量。

可能的值：

- 正整数。
- 0 — 没有限制。

**示例**

```xml
<max_concurrent_queries_for_user>5</max_concurrent_queries_for_user>
```
## max_distributed_connections {#max_distributed_connections} 

用于分布式处理单个查询到单个Distributed表的远程服务器的最大同时连接数。我们建议将值设置为不低于集群中服务器的数量。
## max_distributed_depth {#max_distributed_depth} 

<SettingsInfoBlock type="UInt64" default_value="5" />

限制递归查询的最大深度，适用于 [Distributed](../../engines/table-engines/special/distributed.md) 表。

如果超过该值，服务器将抛出异常。

可能的值：

- 正整数。
- 0 — 无限深度。
## max_download_buffer_size {#max_download_buffer_size} 

<SettingsInfoBlock type="UInt64" default_value="10485760" />

每个线程用于并行下载的最大缓冲区大小（例如，用于 URL 引擎）。
## max_download_threads {#max_download_threads} 

<SettingsInfoBlock type="MaxThreads" default_value="4" />

下载数据的最大线程数（例如，用于 URL 引擎）。
## max_estimated_execution_time {#max_estimated_execution_time} 

<SettingsInfoBlock type="Seconds" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.1"},{"label": "0"},{"label": "Separate max_execution_time and max_estimated_execution_time"}]}]}/>

查询估算的最大执行时间（以秒为单位）。在 [`timeout_before_checking_execution_speed`](/operations/settings/settings#timeout_before_checking_execution_speed) 过期时检查每个数据块。
## max_execution_speed {#max_execution_speed} 

<SettingsInfoBlock type="UInt64" default_value="0" />

每秒执行的最大行数。在 [`timeout_before_checking_execution_speed`](/operations/settings/settings#timeout_before_checking_execution_speed) 过期时检查每个数据块。如果执行速度过快，将降低执行速度。
## max_execution_speed_bytes {#max_execution_speed_bytes} 

<SettingsInfoBlock type="UInt64" default_value="0" />

每秒执行的最大字节数。在 [`timeout_before_checking_execution_speed`](/operations/settings/settings#timeout_before_checking_execution_speed) 过期时检查每个数据块。如果执行速度过快，将降低执行速度。
## max_execution_time {#max_execution_time} 

<SettingsInfoBlock type="Seconds" default_value="0" />

查询的最大执行时间（以秒为单位）。

`max_execution_time` 参数可能有点难以理解。
它根据当前查询的执行速度进行插值操作（这种行为由 [`timeout_before_checking_execution_speed`](/operations/settings/settings#timeout_before_checking_execution_speed) 控制）。

如果预计执行时间超过指定的 `max_execution_time`，ClickHouse 将中断查询。默认情况下，`timeout_before_checking_execution_speed` 设置为 10 秒。这意味着在查询执行了 10 秒后，ClickHouse 将开始估计总执行时间。如果例如 `max_execution_time` 设置为 3600 秒（1 小时），如果预计时间超过这个 3600 秒的限制，ClickHouse 将终止该查询。如果您将 `timeout_before_checking_execution_speed` 设置为 0，ClickHouse 将以时钟时间作为 `max_execution_time` 的基础。

如果查询运行时间超过指定秒数，则行为将由 `timeout_overflow_mode` 确定，默认设置为 `throw`。

:::note
超时检查仅在数据处理的指定位置可以停止查询。
目前无法在聚合状态合并或查询分析期间停止，实际运行时间将高于此设置的值。
:::
## max_execution_time_leaf {#max_execution_time_leaf} 

<SettingsInfoBlock type="Seconds" default_value="0" />

在语义上与 [`max_execution_time`](#max_execution_time) 相似，但仅适用于分布式或远程查询的叶节点。

例如，如果我们想将叶节点上的执行时间限制为 `10s`，但不对初始节点限制，而不在嵌套子查询设置中设置 `max_execution_time`：

```sql
SELECT count()
FROM cluster(cluster, view(SELECT * FROM t SETTINGS max_execution_time = 10));
```

我们可以将 `max_execution_time_leaf` 用作查询设置：

```sql
SELECT count()
FROM cluster(cluster, view(SELECT * FROM t)) SETTINGS max_execution_time_leaf = 10;
```
## max_expanded_ast_elements {#max_expanded_ast_elements} 

<SettingsInfoBlock type="UInt64" default_value="500000" />

查询语法树扩展后节点的最大数量。
## max_fetch_partition_retries_count {#max_fetch_partition_retries_count} 

<SettingsInfoBlock type="UInt64" default_value="5" />

从另一个主机获取分区时的重试次数。
## max_final_threads {#max_final_threads} 

<SettingsInfoBlock type="MaxThreads" default_value="'auto(N)'" />

设置 `SELECT` 查询数据读取阶段的最大并行线程数，适用于 [FINAL](/sql-reference/statements/select/from#final-modifier) 修饰符。

可能的值：

- 正整数。
- 0 或 1 — 禁用。`SELECT` 查询在单线程中执行。
## max_http_get_redirects {#max_http_get_redirects} 

<SettingsInfoBlock type="UInt64" default_value="0" />

允许的最大 HTTP GET 重定向跳转次数。确保采取额外的安全措施，以防恶意服务器将您的请求重定向到意外服务。\n\n在外部服务器重定向到另一个地址时，即使该地址看似是公司基础设施内部的，通过向内部服务器发送 HTTP 请求，您可能会请求来自内部网络的内部 API，绕过身份验证，甚至查询其他服务，如 Redis 或 Memcached。当您没有内部基础设施（包括在本地主机上运行的东西），或者您信任服务器时，允许重定向是安全的。不过请记住，如果 URL 使用 HTTP 而不是 HTTPS，您不仅需要信任远程服务器，还需要信任您的 ISP 以及中间的每一个网络。
## max_hyperscan_regexp_length {#max_hyperscan_regexp_length} 

<SettingsInfoBlock type="UInt64" default_value="0" />

定义 [hyperscan 多匹配函数](/sql-reference/functions/string-search-functions#multimatchany) 中每个正则表达式的最大长度。

可能的值：

- 正整数。
- 0 - 长度不受限制。

**示例**

查询：

```sql
SELECT multiMatchAny('abcd', ['ab','bcd','c','d']) SETTINGS max_hyperscan_regexp_length = 3;
```

结果：

```text
┌─multiMatchAny('abcd', ['ab', 'bcd', 'c', 'd'])─┐
│                                              1 │
└────────────────────────────────────────────────┘
```

查询：

```sql
SELECT multiMatchAny('abcd', ['ab','bcd','c','d']) SETTINGS max_hyperscan_regexp_length = 2;
```

结果：

```text
Exception: Regexp length too large.
```

**另见**

- [max_hyperscan_regexp_total_length](#max_hyperscan_regexp_total_length)
## max_hyperscan_regexp_total_length {#max_hyperscan_regexp_total_length} 

<SettingsInfoBlock type="UInt64" default_value="0" />

设置每个 [hyperscan 多匹配函数](/sql-reference/functions/string-search-functions#multimatchany) 中所有正则表达式的最大总长度。

可能的值：

- 正整数。
- 0 - 长度不受限制。

**示例**

查询：

```sql
SELECT multiMatchAny('abcd', ['a','b','c','d']) SETTINGS max_hyperscan_regexp_total_length = 5;
```

结果：

```text
┌─multiMatchAny('abcd', ['a', 'b', 'c', 'd'])─┐
│                                           1 │
└─────────────────────────────────────────────┘
```

查询：

```sql
SELECT multiMatchAny('abcd', ['ab','bc','c','d']) SETTINGS max_hyperscan_regexp_total_length = 5;
```

结果：

```text
Exception: Total regexp lengths too large.
```

**另见**

- [max_hyperscan_regexp_length](#max_hyperscan_regexp_length)
## max_insert_block_size {#max_insert_block_size} 

<SettingsInfoBlock type="NonZeroUInt64" default_value="1048449" />

用于插入表的块大小（以行数为单位）。
此设置仅适用于服务器形成块的情况。
例如，通过 HTTP 接口进行的 INSERT，服务器解析数据格式并形成指定大小的块。
但使用 clickhouse-client 时，客户端自己解析数据，服务器上的 `max_insert_block_size` 设置不影响插入块的大小。
在使用 INSERT SELECT 时，该设置也没有作用，因为数据是使用在 SELECT 后形成的相同块插入的。

默认值略大于 `max_block_size`。原因是某些表引擎（`*MergeTree`）对每个插入的块在磁盘上形成一个数据部分，这是一个相当大的实体。同样，`*MergeTree` 表在插入时对数据进行排序，足够大的块大小允许在内存中排序更多数据。
## max_insert_delayed_streams_for_parallel_write {#max_insert_delayed_streams_for_parallel_write} 

<SettingsInfoBlock type="UInt64" default_value="0" />

延迟最终部分刷新操作的最大流数（列）。默认值 - 自动（如果基础存储支持并行写入，则为 100，其他情况禁用）。
## max_insert_threads {#max_insert_threads} 

<SettingsInfoBlock type="UInt64" default_value="0" />

执行 `INSERT SELECT` 查询的最大线程数。

可能的值：

- 0（或 1） — `INSERT SELECT` 不支持并行执行。
- 正整数，大于 1。

云默认值：
- 8 GiB 内存的节点为 `1`
- 16 GiB 内存的节点为 `2`
- 更大的节点为 `4`

并行 `INSERT SELECT` 仅在 `SELECT` 部分并行执行时有效，请参见 [`max_threads`](#max_threads) 设置。
更高的值将导致更高的内存使用。
## max_joined_block_size_bytes {#max_joined_block_size_bytes} 

<SettingsInfoBlock type="UInt64" default_value="4194304" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "4194304"},{"label": "New setting"}]}]}/>

JOIN 结果的最大块大小（以字节为单位）（如果连接算法支持）。0 表示无限制。
## max_joined_block_size_rows {#max_joined_block_size_rows} 

<SettingsInfoBlock type="UInt64" default_value="65409" />

JOIN 结果的最大块大小（以行数为单位）（如果连接算法支持）。0 表示无限制。
## max_limit_for_vector_search_queries {#max_limit_for_vector_search_queries} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

对于 LIMIT 大于此设置的 SELECT 查询，无法使用向量相似度索引。帮助防止向量相似度索引中的内存溢出。
## max_live_view_insert_blocks_before_refresh {#max_live_view_insert_blocks_before_refresh} 

<ExperimentalBadge/>

<SettingsInfoBlock type="UInt64" default_value="64" />

限制插入块的最大数量，在此之后可合并的块被丢弃并重新执行查询。
## max_local_read_bandwidth {#max_local_read_bandwidth} 

<SettingsInfoBlock type="UInt64" default_value="0" />

本地读取的最大速度（以字节为单位）。
## max_local_write_bandwidth {#max_local_write_bandwidth} 

<SettingsInfoBlock type="UInt64" default_value="0" />

本地写入的最大速度（以字节为单位）。
## max_memory_usage {#max_memory_usage} 

<SettingsInfoBlock type="UInt64" default_value="0" />

云默认值：取决于副本的 RAM 数量。

在单个服务器上运行查询时可以使用的最大 RAM 量。
值为 `0` 表示无限制。

此设置并不考虑可用内存的容量或机器上的总内存容量。限制适用于单个服务器上的单个查询。

您可以使用 `SHOW PROCESSLIST` 查看每个查询的当前内存消耗。
每个查询的峰值内存消耗会被跟踪并记录到日志中。

以下聚合函数的状态的内存使用未完全跟踪：
- `min`
- `max`
- `any`
- `anyLast`
- `argMin`
- `argMax`

内存消耗还受以下参数 [`max_memory_usage_for_user`](/operations/settings/settings#max_memory_usage_for_user) 和 [`max_server_memory_usage`](/operations/server-configuration-parameters/settings#max_server_memory_usage) 的限制。
## max_memory_usage_for_user {#max_memory_usage_for_user} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在单个服务器上运行用户查询时可以使用的最大 RAM 量。值为 0 表示无限制。

默认情况下，数量不受限制（`max_memory_usage_for_user = 0`）。

另见 [`max_memory_usage`](/operations/settings/settings#max_memory_usage) 的描述。

例如，如果您想将 `max_memory_usage_for_user` 设置为 1000 字节给名为 `clickhouse_read` 的用户，您可以使用以下语句：

```sql
ALTER USER clickhouse_read SETTINGS max_memory_usage_for_user = 1000;
```

您可以通过注销客户端，再次登录，然后使用 `getSetting` 函数验证其是否生效：

```sql
SELECT getSetting('max_memory_usage_for_user');
```
## max_network_bandwidth {#max_network_bandwidth} 

<SettingsInfoBlock type="UInt64" default_value="0" />

限制网络数据交换的速度（以字节为单位）。此设置适用于每个查询。

可能的值：

- 正整数。
- 0 — 禁用带宽控制。
## max_network_bandwidth_for_all_users {#max_network_bandwidth_for_all_users} 

<SettingsInfoBlock type="UInt64" default_value="0" />

限制网络数据交换的速度（以字节为单位）。此设置适用于所有并发运行的查询。

可能的值：

- 正整数。
- 0 — 禁用数据速率控制。
## max_network_bandwidth_for_user {#max_network_bandwidth_for_user} 

<SettingsInfoBlock type="UInt64" default_value="0" />

限制网络数据交换速度（以字节为单位）。此设置适用于所有单个用户执行的并发查询。

可能的值：

- 正整数。
- 0 — 禁用数据速率控制。
## max_network_bytes {#max_network_bytes} 

<SettingsInfoBlock type="UInt64" default_value="0" />

限制执行查询时通过网络接收或传输的数据量（以字节为单位）。此设置适用于每个单独查询。

可能的值：

- 正整数。
- 0 — 禁用数据量控制。
## max_number_of_partitions_for_independent_aggregation {#max_number_of_partitions_for_independent_aggregation} 

<SettingsInfoBlock type="UInt64" default_value="128" />

在表中可应用优化的最大分区数。
## max_os_cpu_wait_time_ratio_to_throw {#max_os_cpu_wait_time_ratio_to_throw} 

<SettingsInfoBlock type="Float" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "0"},{"label": "Setting values were changed and backported to 25.4"}]}, {"id": "row-2","items": [{"label": "25.4"},{"label": "0"},{"label": "New setting"}]}]}/>

在 OSCPUWaitMicroseconds 指标和 OSCPUVirtualTimeMicroseconds 指标之间考虑拒绝查询的最大比率。使用最小和最大比率之间的线性插值计算概率，在这一点上概率为 1。
## max_parallel_replicas {#max_parallel_replicas} 

<SettingsInfoBlock type="NonZeroUInt64" default_value="1000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "1000"},{"label": "Use up to 1000 parallel replicas by default."}]}]}/>

执行查询时每个分片的最大副本数。

可能的值：

- 正整数。

**附加信息**

此选项将根据使用的设置产生不同的结果。

:::note
当涉及连接或子查询时，此设置将产生不正确的结果，并且所有表不满足某些要求。有关详细信息，请参阅 [Distributed Subqueries and max_parallel_replicas](/operations/settings/settings#max_parallel_replicas)。
:::
### 使用 `SAMPLE` 键进行并行处理

如果在多个服务器上并行执行查询，查询可能会更快处理。但是，在以下情况下查询性能可能会下降：

- 采样键在分区键中的位置不允许有效的范围扫描。
- 将采样键添加到表会使通过其他列进行过滤的效率降低。
- 采样键是一个计算开销大的表达式。
- 集群延迟分布具有较长尾巴，因此查询更多服务器会增加整体延迟。
### 使用 [parallel_replicas_custom_key](#parallel_replicas_custom_key) 进行并行处理

此设置适用于任何复制表。
## max_parser_backtracks {#max_parser_backtracks} 

<SettingsInfoBlock type="UInt64" default_value="1000000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "1000000"},{"label": "Limiting the complexity of parsing"}]}]}/>

最大解析器回溯次数（在递归下降解析过程中尝试不同选择的次数）。
## max_parser_depth {#max_parser_depth} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

限制递归下降解析器的最大递归深度。允许控制堆栈大小。

可能的值：

- 正整数。
- 0 — 递归深度不限。
## max_parsing_threads {#max_parsing_threads} 

<SettingsInfoBlock type="MaxThreads" default_value="'auto(N)'" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.4"},{"label": "0"},{"label": "Add a separate setting to control number of threads in parallel parsing from files"}]}]}/>

解析支持并行解析的输入格式数据的最大线程数。默认情况下，自动确定。
## max_partition_size_to_drop {#max_partition_size_to_drop} 

<SettingsInfoBlock type="UInt64" default_value="50000000000" />

查询时删除分区的限制。值 `0` 表示可以删除分区而没有任何限制。

云默认值：1 TB。

:::note
此查询设置会覆盖其服务器设置等效项，请参见 [max_partition_size_to_drop](/operations/server-configuration-parameters/settings#max_partition_size_to_drop)
:::
## max_partitions_per_insert_block {#max_partitions_per_insert_block} 

<SettingsInfoBlock type="UInt64" default_value="100" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "19.5"},{"label": "100"},{"label": "Add a limit for the number of partitions in one block"}]}]}/>

限制单个插入块中的最大分区数，如果块包含过多的分区，将抛出异常。

- 正整数。
- `0` — 分区数量无限制。

**详细信息**

插入数据时，ClickHouse 会计算插入块中的分区数。如果分区数大于 `max_partitions_per_insert_block`，ClickHouse 将根据 `throw_on_max_partitions_per_insert_block` 记录警告或抛出异常。异常的文本如下：

> "单个 INSERT 块的分区数过多（`partitions_count` 分区，限制为 " + toString(max_partitions) + "）。
  该限制由 'max_partitions_per_insert_block' 设置控制。
  大量分区是一个常见的误解。它将导致严重的负性能影响，包括服务器启动缓慢、INSERT 查询缓慢和 SELECT 查询缓慢。建议的表分区总数应少于 1000 至 10000。请注意，分区并不是为了加速 SELECT 查询（ORDER BY 键足以使范围查询快速）。
  分区用于数据操作（DROP PARTITION 等）。"

:::note
该设置是一个安全阈值，因为使用大量分区是一种常见的误解。
:::
## max_partitions_to_read {#max_partitions_to_read} 

<SettingsInfoBlock type="Int64" default_value="-1" />

限制在单个查询中可以访问的最大分区数。

在创建表时指定的设置值可以通过查询级别设置覆盖。

可能的值：

- 正整数
- `-1` - 无限制（默认）

:::note
您还可以在表的设置中指定 MergeTree 设置 [`max_partitions_to_read`](/operations/settings/settings#max_partitions_to_read)。
:::
## max_parts_to_move {#max_parts_to_move} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "1000"},{"label": "New setting"}]}]}/>

限制在一个查询中可以移动的部分的数量。零表示无限制。
## max_query_size {#max_query_size} 

<SettingsInfoBlock type="UInt64" default_value="262144" />

SQL 解析器解析的查询字符串的最大字节数。
INSERT 查询中的 VALUES 子句中的数据由单独的流解析器处理（消耗 O(1) 的 RAM），不受此限制的影响。

:::note
`max_query_size` 不能在 SQL 查询中设置（例如，`SELECT now() SETTINGS max_query_size=10000`），因为 ClickHouse 需要分配一个缓冲区来解析查询，而此缓冲区的大小由 `max_query_size` 设置决定，该设置必须在执行查询之前配置。
:::
## max_read_buffer_size {#max_read_buffer_size} 

<SettingsInfoBlock type="NonZeroUInt64" default_value="1048576" />

从文件系统读取的缓冲区的最大大小。
## max_read_buffer_size_local_fs {#max_read_buffer_size_local_fs} 

<SettingsInfoBlock type="UInt64" default_value="131072" />

从本地文件系统读取的缓冲区的最大大小。如果设置为 0，则将使用 max_read_buffer_size。
## max_read_buffer_size_remote_fs {#max_read_buffer_size_remote_fs} 

<SettingsInfoBlock type="UInt64" default_value="0" />

从远程文件系统读取的缓冲区的最大大小。如果设置为 0，则将使用 max_read_buffer_size。
## max_recursive_cte_evaluation_depth {#max_recursive_cte_evaluation_depth} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

递归 CTE 评估深度的最大限制。
## max_remote_read_network_bandwidth {#max_remote_read_network_bandwidth} 

<SettingsInfoBlock type="UInt64" default_value="0" />

读取时网络数据交换的最大速度（以字节为单位）。
## max_remote_write_network_bandwidth {#max_remote_write_network_bandwidth} 

<SettingsInfoBlock type="UInt64" default_value="0" />

写入时网络数据交换的最大速度（以字节为单位）。
## max_replica_delay_for_distributed_queries {#max_replica_delay_for_distributed_queries} 

<SettingsInfoBlock type="UInt64" default_value="300" />

对于分布式查询，禁用滞后副本。请参见 [Replication](../../engines/table-engines/mergetree-family/replication.md)。

设置以秒为单位。如果副本的滞后时间大于或等于设定值，则不使用该副本。

可能的值：

- 正整数。
- 0 — 不检查副本的滞后时间。

要防止使用任何滞后时间不为零的副本，请将此参数设置为 1。

在执行从指向复制表的分布式表的 `SELECT` 时使用。
## max_result_bytes {#max_result_bytes} 

<SettingsInfoBlock type="UInt64" default_value="0" />

限制结果大小（以字节为单位，未经压缩）。如果处理数据块时达到阈值，查询将停止，但不会截断结果的最后一个块，因此结果大小可能超过阈值。

**注意事项**

内存中的结果大小被纳入此阈值。
即使结果大小很小，它仍可以引用内存中的更大数据结构，代表低基数列的字典和聚合函数列的区域，因此尽管结果大小较小，阈值仍可能被超出。

:::warning
该设置相对低级，应谨慎使用。
:::
## max_result_rows {#max_result_rows} 

<SettingsInfoBlock type="UInt64" default_value="0" />

云默认值：`0`。

限制结果中的行数。对于子查询和在远程服务器上运行分布式查询的一部分时也进行检查。
当值为 `0` 时不应用限制。

如果处理数据块时达到阈值，查询将停止，但不会截断结果的最后一个块，因此结果大小可能大于阈值。
## max_rows_in_distinct {#max_rows_in_distinct} 

<SettingsInfoBlock type="UInt64" default_value="0" />

使用 DISTINCT 时不同的最大行数。
## max_rows_in_join {#max_rows_in_join} 

<SettingsInfoBlock type="UInt64" default_value="0" />

限制连接表时使用的哈希表中的行数。

此设置适用于 [SELECT ... JOIN](/sql-reference/statements/select/join) 操作和 [Join](/engines/table-engines/special/join) 表引擎。

如果查询包含多个连接，ClickHouse 将对每个中间结果检查此设置。

当达到限制时，ClickHouse 可以执行不同的操作。使用 [`join_overflow_mode`](/operations/settings/settings#join_overflow_mode) 设置选择操作。

可能的值：

- 正整数。
- `0` — 行数无限制。
## max_rows_in_set {#max_rows_in_set} 

<SettingsInfoBlock type="UInt64" default_value="0" />

从子查询创建的 IN 子句中数据集的最大行数。
## max_rows_in_set_to_optimize_join {#max_rows_in_set_to_optimize_join} 

<SettingsInfoBlock type="UInt64" default_value="0" />

用于在连接之前基于彼此行集过滤连接表的最大集的大小。

可能的值：

- 0 — 禁用。
- 任何正整数。
## max_rows_to_group_by {#max_rows_to_group_by} 

<SettingsInfoBlock type="UInt64" default_value="0" />

从聚合中接收到的唯一键的最大数量。此设置允许您限制聚合时的内存消耗。

如果在 GROUP BY 期间生成的行（唯一 GROUP BY 键）超过指定数量，则行为将由 'group_by_overflow_mode' 确定，默认设置为 `throw`，但也可以切换到近似 GROUP BY 模式。
## max_rows_to_read {#max_rows_to_read} 

<SettingsInfoBlock type="UInt64" default_value="0" />

运行查询时从表中可以读取的最大行数。
对每个处理的数据块都进行检查的限制，仅适用于最深的表表达式，并在从远程服务器读取时，仅在远程服务器上进行检查。
## max_rows_to_read_leaf {#max_rows_to_read_leaf} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在运行分布式查询时，从叶节点的本地表中可以读取的最大行数。虽然分布式查询可以对每个分片（叶）发出多个子查询，但此限制仅在叶节点的读取阶段进行检查，并在根节点合并结果阶段被忽略。

例如，一个集群由 2 个分片组成，每个分片包含一个 100 行的表。预期从两个表中读取所有数据的分布式查询，如果设置为 `max_rows_to_read=150`，将会失败，因为总共将有 200 行。查询 `max_rows_to_read_leaf=150` 将会成功，因为叶节点最多将读取 100 行。

对每个处理的数据块进行检查。

:::note
此设置与 `prefer_localhost_replica=1` 一起使用时不稳定。
:::
## max_rows_to_sort {#max_rows_to_sort} 

<SettingsInfoBlock type="UInt64" default_value="0" />

排序前的最大行数。这允许您在排序时限制内存消耗。
如果ORDER BY操作需要处理的记录数超过指定数量，则行为将由 `sort_overflow_mode` 确定，默认设置为 `throw`。
## max_rows_to_transfer {#max_rows_to_transfer} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在执行 GLOBAL IN/JOIN 部分时，传递给远程服务器或保存到临时表的最大大小（以行数为单位）。
## max_sessions_for_user {#max_sessions_for_user} 

<SettingsInfoBlock type="UInt64" default_value="0" />

每个已认证用户与 ClickHouse 服务器的最大同时会话数。

示例：

```xml
<profiles>
    <single_session_profile>
        <max_sessions_for_user>1</max_sessions_for_user>
    </single_session_profile>
    <two_sessions_profile>
        <max_sessions_for_user>2</max_sessions_for_user>
    </two_sessions_profile>
    <unlimited_sessions_profile>
        <max_sessions_for_user>0</max_sessions_for_user>
    </unlimited_sessions_profile>
</profiles>
<users>
    <!-- User Alice can connect to a ClickHouse server no more than once at a time. -->
    <Alice>
        <profile>single_session_user</profile>
    </Alice>
    <!-- User Bob can use 2 simultaneous sessions. -->
    <Bob>
        <profile>two_sessions_profile</profile>
    </Bob>
    <!-- User Charles can use arbitrarily many of simultaneous sessions. -->
    <Charles>
        <profile>unlimited_sessions_profile</profile>
    </Charles>
</users>
```

可能的值：
- 正整数
- `0` - 无限数量的同时会话（默认）
## max_size_to_preallocate_for_aggregation {#max_size_to_preallocate_for_aggregation} 

<SettingsInfoBlock type="UInt64" default_value="1000000000000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.12"},{"label": "1000000000000"},{"label": "Enable optimisation for bigger tables."}]}, {"id": "row-2","items": [{"label": "22.12"},{"label": "100000000"},{"label": "This optimizes performance"}]}]}/>

在聚合之前，所有哈希表中允许预分配的元素的总数。
## max_size_to_preallocate_for_joins {#max_size_to_preallocate_for_joins} 

<SettingsInfoBlock type="UInt64" default_value="1000000000000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.7"},{"label": "100000000"},{"label": "New setting."}]}, {"id": "row-2","items": [{"label": "24.12"},{"label": "1000000000000"},{"label": "Enable optimisation for bigger tables."}]}]}/>

在连接之前，所有哈希表中允许预分配的元素的总数。
## max_streams_for_merge_tree_reading {#max_streams_for_merge_tree_reading} 

<SettingsInfoBlock type="UInt64" default_value="0" />

如果不为零，限制 MergeTree 表的读取流数量。
## max_streams_multiplier_for_merge_tables {#max_streams_multiplier_for_merge_tables} 

<SettingsInfoBlock type="Float" default_value="5" />

在从合并表读取时请求更多流。流将分布在合并表将使用的表上。这允许更均匀地分配线程间的工作，并在合并表之间尺寸不同的情况下特别有帮助。
## max_streams_to_max_threads_ratio {#max_streams_to_max_threads_ratio} 

<SettingsInfoBlock type="Float" default_value="1" />

允许您使用的来源数超过线程数——以更均匀地分配线程之间的工作。假设这是一个临时解决方案，因为将来将能够使源的数量等于线程数量，但对于每个源动态选择可用的工作。
## max_subquery_depth {#max_subquery_depth} 

<SettingsInfoBlock type="UInt64" default_value="100" />

如果查询的嵌套子查询数超过指定数量，则抛出异常。

:::tip
这可以帮助您进行合理检查，以保护集群中的用户，以免编写过于复杂的查询。
:::
## max_table_size_to_drop {#max_table_size_to_drop} 

<SettingsInfoBlock type="UInt64" default_value="50000000000" />

查询时删除表的限制。值 `0` 表示可以删除所有表而没有任何限制。

云默认值：1 TB。

:::note
此查询设置会覆盖其服务器设置等效项，请参见 [max_table_size_to_drop](/operations/server-configuration-parameters/settings#max_table_size_to_drop)。
:::
## max_temporary_columns {#max_temporary_columns} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在运行查询时需要同时保留在内存中的临时列的最大数量，包括常量列。如果查询由于中间计算生成的临时列超过指定数量，则将抛出异常。

:::tip
此设置有助于防止过于复杂的查询。
:::

`0` 值表示无限制。
## max_temporary_data_on_disk_size_for_query {#max_temporary_data_on_disk_size_for_query} 

<SettingsInfoBlock type="UInt64" default_value="0" />

所有并发运行查询的临时文件在磁盘上消耗的数据的最大字节数。

可能的值：

- 正整数。
- `0` — 无限制（默认）。
## max_temporary_data_on_disk_size_for_user {#max_temporary_data_on_disk_size_for_user} 

<SettingsInfoBlock type="UInt64" default_value="0" />

所有并发运行的用户查询的临时文件在磁盘上消耗的数据的最大字节数。

可能的值：

- 正整数。
- `0` — 无限制（默认）。
## max_temporary_non_const_columns {#max_temporary_non_const_columns} 

<SettingsInfoBlock type="UInt64" default_value="0" />

与 `max_temporary_columns` 类似，在运行查询时需要同时保留在内存中的临时列的最大数量，但不计算常量列。

:::note
在运行查询时，常量列会相对频繁地产生，但它们几乎不需要计算资源。
:::
## max_threads {#max_threads} 

<SettingsInfoBlock type="MaxThreads" default_value="'auto(N)'" />

查询处理线程的最大数量，不包括从远程服务器检索数据的线程（请参见 'max_distributed_connections' 参数）。

此参数适用于在查询处理管道中并行执行相同阶段的线程。
例如，在从表中读取时，如果可以使用至少 'max_threads' 数量的线程并行评估函数的表达式、使用 WHERE 进行筛选和为 GROUP BY 进行预聚合，则将使用 'max_threads'。

对于因 LIMIT 快速完成的查询，可以设置较低的 'max_threads'。例如，如果必要数量的条目位于每个块中，且 max_threads = 8，则将检索 8 个块，尽管只需读取一个就够了。

`max_threads` 值越小，消耗的内存越少。

云默认值：`auto(3)`。
## max_threads_for_indexes {#max_threads_for_indexes} 

<SettingsInfoBlock type="UInt64" default_value="0" />

处理索引的最大线程数。
## max_untracked_memory {#max_untracked_memory} 

<SettingsInfoBlock type="UInt64" default_value="4194304" />

小的分配和解除分配被归为线程局部变量，仅在其绝对值超过指定值时才进行跟踪或分析。如果该值高于 'memory_profiler_step'，它将有效地降低到 'memory_profiler_step'。
## memory_overcommit_ratio_denominator {#memory_overcommit_ratio_denominator} 

<SettingsInfoBlock type="UInt64" default_value="1073741824" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "22.5"},{"label": "1073741824"},{"label": "Enable memory overcommit feature by default"}]}]}/>

它表示当全局级别的硬限制达到时的软内存限制。
此值用于计算查询的过量使用比率。
零表示跳过查询。
有关 [内存过量使用](memory-overcommit.md) 的更多信息。
## memory_overcommit_ratio_denominator_for_user {#memory_overcommit_ratio_denominator_for_user} 

<SettingsInfoBlock type="UInt64" default_value="1073741824" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "22.5"},{"label": "1073741824"},{"label": "Enable memory overcommit feature by default"}]}]}/>

它表示当用户级别的硬限制达到时的软内存限制。
此值用于计算查询的过量使用比率。
零表示跳过查询。
有关 [内存过量使用](memory-overcommit.md) 的更多信息。

## memory_profiler_sample_max_allocation_size {#memory_profiler_sample_max_allocation_size} 

<SettingsInfoBlock type="UInt64" default_value="0" />

收集大小小于或等于指定值的随机分配，其概率等于 `memory_profiler_sample_probability`。0 表示禁用。您可能希望将 'max_untracked_memory' 设置为 0，以使此阈值按预期工作。
## memory_profiler_sample_min_allocation_size {#memory_profiler_sample_min_allocation_size} 

<SettingsInfoBlock type="UInt64" default_value="0" />

收集大小大于或等于指定值的随机分配，其概率等于 `memory_profiler_sample_probability`。0 表示禁用。您可能希望将 'max_untracked_memory' 设置为 0，以使此阈值按预期工作。
## memory_profiler_sample_probability {#memory_profiler_sample_probability} 

<SettingsInfoBlock type="Float" default_value="0" />

收集随机分配和释放，并将其写入 system.trace_log，跟踪类型为 'MemorySample'。该概率适用于每个分配/释放，无论分配的大小如何（可以通过 `memory_profiler_sample_min_allocation_size` 和 `memory_profiler_sample_max_allocation_size` 进行更改）。请注意，只有当未跟踪内存的数量超过 'max_untracked_memory' 时，才会进行采样。您可能希望将 'max_untracked_memory' 设置为 0 以实现更细粒度的采样。
## memory_profiler_step {#memory_profiler_step} 

<SettingsInfoBlock type="UInt64" default_value="4194304" />

设置内存分析器的步骤。每当查询内存使用量超过以字节为单位的每个后续步骤时，内存分析器将收集分配堆栈跟踪并将其写入 [trace_log](/operations/system-tables/trace_log)。

可能值：

- 一个正整数的字节数。

- 0 表示关闭内存分析器。
## memory_tracker_fault_probability {#memory_tracker_fault_probability} 

<SettingsInfoBlock type="Float" default_value="0" />

用于测试 `exception safety` - 每次分配内存时按指定概率抛出异常。
## memory_usage_overcommit_max_wait_microseconds {#memory_usage_overcommit_max_wait_microseconds} 

<SettingsInfoBlock type="UInt64" default_value="5000000" />

在用户级别内存过度分配的情况下，线程等待内存释放的最长时间。如果达到超时而内存未释放，将抛出异常。了解更多关于 [memory overcommit](memory-overcommit.md) 的信息。
## merge_table_max_tables_to_look_for_schema_inference {#merge_table_max_tables_to_look_for_schema_inference} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "1000"},{"label": "A new setting"}]}]}/>

在创建 `Merge` 表时没有显式架构或者使用 `merge` 表函数时，推导出不超过指定数量匹配表的联合架构。如果表的数量更大，则只从前面指定数量的表中推导架构。
## merge_tree_coarse_index_granularity {#merge_tree_coarse_index_granularity} 

<SettingsInfoBlock type="UInt64" default_value="8" />

在搜索数据时，ClickHouse 会检查索引文件中的数据标记。如果 ClickHouse 发现所需的键在某个范围内，它会将该范围划分为 `merge_tree_coarse_index_granularity` 个子范围，并在其中递归搜索所需的键。

可能值：

- 任何正偶数。
## merge_tree_compact_parts_min_granules_to_multibuffer_read {#merge_tree_compact_parts_min_granules_to_multibuffer_read} 

<CloudOnlyBadge/>

<SettingsInfoBlock type="UInt64" default_value="16" />

仅在 ClickHouse Cloud 中生效。MergeTree 表的紧凑部分条带中使用多缓冲读取器的最小粒度数，该读取器支持并行读取和预取。在从远程文件系统读取时，使用多缓冲读取器会增加读取请求的数量。
## merge_tree_determine_task_size_by_prewhere_columns {#merge_tree_determine_task_size_by_prewhere_columns} 

<SettingsInfoBlock type="Bool" default_value="1" />

是否仅使用 prewhere 列的大小来确定读取任务的大小。
## merge_tree_max_bytes_to_use_cache {#merge_tree_max_bytes_to_use_cache} 

<SettingsInfoBlock type="UInt64" default_value="2013265920" />

如果 ClickHouse 在一个查询中应读取超过 `merge_tree_max_bytes_to_use_cache` 字节，则不使用未压缩块的缓存。

未压缩块缓存存储为查询提取的数据。ClickHouse 使用此缓存以加快对重复小查询的响应。此设置保护缓存免受读取大量数据的查询破坏。[uncompressed_cache_size](/operations/server-configuration-parameters/settings#uncompressed_cache_size) 服务器设置定义未压缩块的缓存大小。

可能值：

- 任何正整数。
## merge_tree_max_rows_to_use_cache {#merge_tree_max_rows_to_use_cache} 

<SettingsInfoBlock type="UInt64" default_value="1048576" />

如果 ClickHouse 在一个查询中应读取超过 `merge_tree_max_rows_to_use_cache` 行，则不使用未压缩块的缓存。

未压缩块缓存存储为查询提取的数据。ClickHouse 使用此缓存以加快对重复小查询的响应。此设置保护缓存免受读取大量数据的查询破坏。[uncompressed_cache_size](/operations/server-configuration-parameters/settings#uncompressed_cache_size) 服务器设置定义未压缩块的缓存大小。

可能值：

- 任何正整数。
## merge_tree_min_bytes_for_concurrent_read {#merge_tree_min_bytes_for_concurrent_read} 

<SettingsInfoBlock type="UInt64" default_value="251658240" />

如果从一个 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 引擎表中读取的字节数超过 `merge_tree_min_bytes_for_concurrent_read`，则 ClickHouse 尝试在多个线程中并发读取该文件。

可能值：

- 正整数。
## merge_tree_min_bytes_for_concurrent_read_for_remote_filesystem {#merge_tree_min_bytes_for_concurrent_read_for_remote_filesystem} 

<SettingsInfoBlock type="UInt64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "0"},{"label": "Setting is deprecated"}]}]}/>

从一个文件中读取的最小字节数，才能在读取远程文件系统时， [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 引擎可以并行化读取。我们不建议使用此设置。

可能值：

- 正整数。
## merge_tree_min_bytes_for_seek {#merge_tree_min_bytes_for_seek} 

<SettingsInfoBlock type="UInt64" default_value="0" />

如果在一个文件中要读取的两个数据块之间的距离小于 `merge_tree_min_bytes_for_seek` 字节，则 ClickHouse 顺序读取包含这两个块的文件范围，从而避免额外的寻道。

可能值：

- 任何正整数。
## merge_tree_min_bytes_per_task_for_remote_reading {#merge_tree_min_bytes_per_task_for_remote_reading} 

<SettingsInfoBlock type="UInt64" default_value="2097152" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.8"},{"label": "2097152"},{"label": "Value is unified with `filesystem_prefetch_min_bytes_for_single_read_task`"}]}]}/>

每个任务要读取的最小字节数。
## merge_tree_min_read_task_size {#merge_tree_min_read_task_size} 

<SettingsInfoBlock type="NonZeroUInt64" default_value="8" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "8"},{"label": "New setting"}]}]}/>

任务大小的硬下限（即使粒度数量较低，且可用线程数量较高，我们也不会分配较小的任务）。
## merge_tree_min_rows_for_concurrent_read {#merge_tree_min_rows_for_concurrent_read} 

<SettingsInfoBlock type="UInt64" default_value="163840" />

如果要从一个 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 表的文件中读取超过 `merge_tree_min_rows_for_concurrent_read` 行，则 ClickHouse 尝试在多个线程中对该文件进行并发读取。

可能值：

- 正整数。
## merge_tree_min_rows_for_concurrent_read_for_remote_filesystem {#merge_tree_min_rows_for_concurrent_read_for_remote_filesystem} 

<SettingsInfoBlock type="UInt64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "0"},{"label": "Setting is deprecated"}]}]}/>

从一个文件读取的最小行数，才能在读取远程文件系统时，[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 引擎可以并行化读取。我们不建议使用此设置。

可能值：

- 正整数。
## merge_tree_min_rows_for_seek {#merge_tree_min_rows_for_seek} 

<SettingsInfoBlock type="UInt64" default_value="0" />

如果在一个文件中要读取的两个数据块之间的距离小于 `merge_tree_min_rows_for_seek` 行，则 ClickHouse 不会在文件中寻道，而是顺序读取数据。

可能值：

- 任何正整数。
## merge_tree_read_split_ranges_into_intersecting_and_non_intersecting_injection_probability {#merge_tree_read_split_ranges_into_intersecting_and_non_intersecting_injection_probability} 

<SettingsInfoBlock type="Float" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "0"},{"label": "For testing of `PartsSplitter` - split read ranges into intersecting and non intersecting every time you read from MergeTree with the specified probability."}]}]}/>

用于测试 `PartsSplitter` - 每次从 MergeTree 读取时，将读取范围分成重叠和不重叠，在指定概率下进行。
## merge_tree_storage_snapshot_sleep_ms {#merge_tree_storage_snapshot_sleep_ms} 

<SettingsInfoBlock type="UInt64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.6"},{"label": "0"},{"label": "A new setting to debug storage snapshot consistency in query"}]}]}/>

在创建 MergeTree 表的存储快照时，注入人工延迟（以毫秒为单位）。仅用于测试和调试目的。

可能值：
- 0 - 无延迟（默认）
- N - 延迟（以毫秒为单位）
## merge_tree_use_const_size_tasks_for_remote_reading {#merge_tree_use_const_size_tasks_for_remote_reading} 

<SettingsInfoBlock type="Bool" default_value="1" />

是否使用固定大小的任务来从远程表读取。
## merge_tree_use_deserialization_prefixes_cache {#merge_tree_use_deserialization_prefixes_cache} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用从远程磁盘读取 MergeTree 时，文件前缀的列元数据缓存。
## merge_tree_use_prefixes_deserialization_thread_pool {#merge_tree_use_prefixes_deserialization_thread_pool} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用对 MergeTree 中宽部分的前缀读取的线程池进行并行化。该线程池的大小由服务器设置 `max_prefixes_deserialization_thread_pool_size` 控制。
## merge_tree_use_v1_object_and_dynamic_serialization {#merge_tree_use_v1_object_and_dynamic_serialization} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用时，将在 MergeTree 中使用 JSON 和动态类型的 V1 序列化版本，而不是 V2。更改此设置只在服务器重启后生效。
## metrics_perf_events_enabled {#metrics_perf_events_enabled} 

<SettingsInfoBlock type="Bool" default_value="0" />

如果启用，则在查询执行过程中将测量某些性能事件。
## metrics_perf_events_list {#metrics_perf_events_list} 

以逗号分隔的性能指标列表，将在查询执行期间进行测量。为空表示所有事件。有关可用事件的信息，请参阅源中的 PerfEventInfo。
## min_bytes_to_use_direct_io {#min_bytes_to_use_direct_io} 

<SettingsInfoBlock type="UInt64" default_value="0" />

使用直接 I/O 访问存储磁盘所需的最小数据量。

ClickHouse 在从表中读取数据时使用此设置。如果要读取的所有数据的总存储量超过 `min_bytes_to_use_direct_io` 字节，则 ClickHouse 将使用 `O_DIRECT` 选项从存储磁盘读取数据。

可能值：

- 0 — 禁用直接 I/O。
- 正整数。
## min_bytes_to_use_mmap_io {#min_bytes_to_use_mmap_io} 

<SettingsInfoBlock type="UInt64" default_value="0" />

这是一个实验性设置。设置读取大文件而不从内核复制数据到用户空间所需的最小内存量。推荐阈值约为 64 MB，因为 [mmap/munmap](https://en.wikipedia.org/wiki/Mmap) 速度较慢。它仅在大文件时有意义，并且仅在数据位于页面缓存中时才有效。

可能值：

- 正整数。
- 0 — 大文件仅通过从内核复制数据到用户空间读取。
## min_chunk_bytes_for_parallel_parsing {#min_chunk_bytes_for_parallel_parsing} 

<SettingsInfoBlock type="NonZeroUInt64" default_value="10485760" />

- 类型：无符号整数
- 默认值：1 MiB

每个线程并行解析的最小块大小（以字节为单位）。
## min_compress_block_size {#min_compress_block_size} 

<SettingsInfoBlock type="UInt64" default_value="65536" />

对于 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 表。为了减少处理查询的延迟，当写入下一个标记时，如果其大小至少为 `min_compress_block_size`，则会压缩块。默认值为 65,536。

如果未压缩数据的实际大小小于 `max_compress_block_size`，则块的实际大小不小于该值，并且至少不小于一个标记的数据量。

让我们看一个例子。假设在创建表时将 `index_granularity` 设置为 8192。

我们正在写入 UInt32 类型的列（每个值 4 字节）。写入 8192 行时，总共 32 KB 的数据。由于 min_compress_block_size = 65,536，每两个标记将形成一个压缩块。

我们正在写入一个字符串类型的 URL 列（平均每值 60 字节）。写入 8192 行时，平均将略低于 500 KB 的数据。由于这大于 65,536，将为每个标记形成一个压缩块。在这种情况下，从磁盘读取单个标记范围时，不会解压缩额外的数据。

:::note
这是一个专家级设置，如果您刚开始使用 ClickHouse，则不应更改它。
:::
## min_count_to_compile_aggregate_expression {#min_count_to_compile_aggregate_expression} 

<SettingsInfoBlock type="UInt64" default_value="3" />

开始 JIT 编译的最小相同聚合表达式数量。仅在启用 [compile_aggregate_expressions](#compile_aggregate_expressions) 设置时有效。

可能值：

- 正整数。
- 0 — 相同的聚合表达式始终 JIT 编译。
## min_count_to_compile_expression {#min_count_to_compile_expression} 

<SettingsInfoBlock type="UInt64" default_value="3" />

执行同一表达式之前的最小计数，以便进行编译。
## min_count_to_compile_sort_description {#min_count_to_compile_sort_description} 

<SettingsInfoBlock type="UInt64" default_value="3" />

在 JIT 编译之前需要的相同排序描述的数量。
## min_execution_speed {#min_execution_speed} 

<SettingsInfoBlock type="UInt64" default_value="0" />

每秒最低执行速度（以行计）。在 [`timeout_before_checking_execution_speed`](/operations/settings/settings#timeout_before_checking_execution_speed) 到期时检查每个数据块。如果执行速度较低，则会抛出异常。
## min_execution_speed_bytes {#min_execution_speed_bytes} 

<SettingsInfoBlock type="UInt64" default_value="0" />

每秒最低执行字节数（以字节计）。在 [`timeout_before_checking_execution_speed`](/operations/settings/settings#timeout_before_checking_execution_speed) 到期时检查每个数据块。如果执行速度较低，则会抛出异常。
## min_external_table_block_size_bytes {#min_external_table_block_size_bytes} 

<SettingsInfoBlock type="UInt64" default_value="268402944" />

如果块不够大，则将传递给外部表的块压缩到指定的字节大小。
## min_external_table_block_size_rows {#min_external_table_block_size_rows} 

<SettingsInfoBlock type="UInt64" default_value="1048449" />

如果块不够大，则将传递给外部表的块压缩到指定的行数。
## min_free_disk_bytes_to_perform_insert {#min_free_disk_bytes_to_perform_insert} 

<SettingsInfoBlock type="UInt64" default_value="0" />

执行插入操作所需的最小可用磁盘空间字节数。
## min_free_disk_ratio_to_perform_insert {#min_free_disk_ratio_to_perform_insert} 

<SettingsInfoBlock type="Float" default_value="0" />

执行插入操作所需的最小可用磁盘空间比例。
## min_free_disk_space_for_temporary_data {#min_free_disk_space_for_temporary_data} 

写入用于外部排序和聚合的临时数据时所需的最小磁盘空间。
## min_hit_rate_to_use_consecutive_keys_optimization {#min_hit_rate_to_use_consecutive_keys_optimization} 

在聚合中用于连续键优化的缓存的最小命中率，以保持其启用。
## min_insert_block_size_bytes {#min_insert_block_size_bytes} 

设置可以通过 `INSERT` 查询插入到表中的块的最小字节数。较小的块会被压缩成较大的块。

可能值：

- 正整数。
- 0 — 禁用压缩。
## min_insert_block_size_bytes_for_materialized_views {#min_insert_block_size_bytes_for_materialized_views} 

设置可以通过 `INSERT` 查询插入到表中的块的最小字节数。较小的块会被压缩成较大的块。此设置仅适用于插入到 [物化视图](../../sql-reference/statements/create/view.md) 的块。通过调整此设置，您可以控制推送到物化视图时的块压缩，并避免过度使用内存。

可能值：

- 任何正整数。
- 0 — 禁用压缩。

**另见**

- [min_insert_block_size_bytes](#min_insert_block_size_bytes)
## min_insert_block_size_rows {#min_insert_block_size_rows} 

设置可以通过 `INSERT` 查询插入到表中的块的最小行数。较小的块会被压缩成较大的块。

可能值：

- 正整数。
- 0 — 禁用压缩。
## min_insert_block_size_rows_for_materialized_views {#min_insert_block_size_rows_for_materialized_views} 

设置可以通过 `INSERT` 查询插入到表中的块的最小行数。较小的块会被压缩成较大的块。此设置仅适用于插入到 [物化视图](../../sql-reference/statements/create/view.md) 的块。通过调整此设置，您可以控制推送到物化视图时的块压缩，并避免过度使用内存。

可能值：

- 任何正整数。
- 0 — 禁用压缩。

**另见**

- [min_insert_block_size_rows](#min_insert_block_size_rows)
## min_joined_block_size_bytes {#min_joined_block_size_bytes} 

<SettingsInfoBlock type="UInt64" default_value="524288" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "524288"},{"label": "New setting."}]}]}/>

JOIN 输入和输出块的最小块大小（以字节为单位）（如果连接算法支持）。小块将被压缩。0 表示无限制。
## min_joined_block_size_rows {#min_joined_block_size_rows} 

<SettingsInfoBlock type="UInt64" default_value="65409" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.7"},{"label": "65409"},{"label": "New setting."}]}]}/>

JOIN 输入和输出块的最小行数（如果连接算法支持）。小块将被压缩。0 表示无限制。
## min_os_cpu_wait_time_ratio_to_throw {#min_os_cpu_wait_time_ratio_to_throw} 

<SettingsInfoBlock type="Float" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "0"},{"label": "Setting values were changed and backported to 25.4"}]}, {"id": "row-2","items": [{"label": "25.4"},{"label": "0"},{"label": "New setting"}]}]}/>

考虑拒绝查询的操作系统 CPU 等待（OSCPUWaitMicroseconds 指标）和繁忙（OSCPUVirtualTimeMicroseconds 指标）时间的最小比例。使用线性插值在最小和最大比率之间计算概率，该点时概率为 0。
## min_outstreams_per_resize_after_split {#min_outstreams_per_resize_after_split} 

<SettingsInfoBlock type="UInt64" default_value="24" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.6"},{"label": "24"},{"label": "New setting."}]}]}/>

指定在生成管道过程中执行拆分时，`Resize` 或 `StrictResize` 处理器的最小输出流数量。如果生成的流数少于此值，拆分操作将不会发生。
### 什么是 Resize 节点
`Resize` 节点是查询管道中的处理器，负责调整流经管道的数据流数量。它可以增加或减少流的数量，以平衡多个线程或处理器之间的工作负载。例如，如果查询需要更多并行性，则 `Resize` 节点可以将单个流拆分为多个流。相反，可以将多个流合并为更少的流，以整合数据处理。

`Resize` 节点确保数据在流之间均匀分配，保持数据块的结构。这有助于优化资源利用率并提高查询性能。
### 为什么需要拆分 Resize 节点
在管道执行过程中，中央集线的 `Resize` 节点的 ExecutingGraph::Node::status_mutex 受到高度争用，尤其是在高核心数环境中，这种争用导致：
1. ExecutingGraph::updateNode 的延迟增加，直接影响查询性能。
2. 在自旋锁争用中，因浪费了过多的 CPU 周期 (native_queued_spin_lock_slowpath)，降低了效率。
3. CPU 利用率降低，限制并行性和吞吐量。
### 如何拆分 Resize 节点
1. 检查输出流的数量，以确保可以执行拆分：每个拆分处理器的输出流数量满足或超过 `min_outstreams_per_resize_after_split` 阈值。
2. 将 `Resize` 节点划分为具有相同端口数量的小型 `Resize` 节点，每个节点处理一部分输入和输出流。
3. 每组独立处理，减少锁争用。
### 拆分具有任意输入/输出的 Resize 节点
在某些情况下，当输入/输出无法被拆分的 `Resize` 节点数量整除时，一些输入被连接到 `NullSource`，而一些输出连接到 `NullSink`。这允许拆分发生，而不影响整体数据流。
### 设置的目的
`min_outstreams_per_resize_after_split` 设置确保 `Resize` 节点的拆分是有意义的，避免了生成太少的流，这可能导致低效的并行处理。通过强制实施输出流的最小数量，此设置有助于在涉及流拆分和合并的场景中优化查询执行，在并行性和开销之间保持平衡。
### 禁用此设置
要禁用 `Resize` 节点的拆分，请将此设置设置为 0。这将防止在生成管道时拆分 `Resize` 节点，允许它们保留原始结构，而不会拆分成更小的节点。
## mongodb_throw_on_unsupported_query {#mongodb_throw_on_unsupported_query} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.9"},{"label": "1"},{"label": "New setting."}]}, {"id": "row-2","items": [{"label": "24.10"},{"label": "1"},{"label": "New setting."}]}]}/>

如果启用，当无法构建 MongoDB 查询时，MongoDB 表将返回错误。否则，ClickHouse 将读取整个表并在本地处理它。当 'allow_experimental_analyzer=0' 时，此选项不适用。
## move_all_conditions_to_prewhere {#move_all_conditions_to_prewhere} 

将所有可行条件从 WHERE 移动到 PREWHERE。
## move_primary_key_columns_to_end_of_prewhere {#move_primary_key_columns_to_end_of_prewhere} 

将包含主键列的 PREWHERE 条件移动到 AND 链的末尾。这些条件在主键分析中可能会被考虑，因此不会对 PREWHERE 过滤产生很大贡献。
## multiple_joins_try_to_keep_original_names {#multiple_joins_try_to_keep_original_names} 

在多个连接重写时，不在顶层表达式列表中添加别名。
## mutations_execute_nondeterministic_on_initiator {#mutations_execute_nondeterministic_on_initiator} 

如果为真常量非确定性函数（例如函数 `now()`）在发起者上执行并在 `UPDATE` 和 `DELETE` 查询中替换为文字。这有助于在执行常量非确定性函数的突变时保持副本中的数据同步。默认值：`false`。
## mutations_execute_subqueries_on_initiator {#mutations_execute_subqueries_on_initiator} 

如果为真标量子查询在发起者上执行，并在 `UPDATE` 和 `DELETE` 查询中替换为文字。默认值：`false`。
## mutations_max_literal_size_to_replace {#mutations_max_literal_size_to_replace} 

在 `UPDATE` 和 `DELETE` 查询中要替换的序列化文字的最大字节大小。仅在启用上述两个设置中的至少一个时生效。默认值：16384（16 KiB）。
## mutations_sync {#mutations_sync} 

允许以同步方式执行 `ALTER TABLE ... UPDATE|DELETE|MATERIALIZE INDEX|MATERIALIZE PROJECTION|MATERIALIZE COLUMN|MATERIALIZE STATISTICS` 查询（[突变](../../sql-reference/statements/alter/index.md/#mutations)）。

可能值：

| 值 | 描述                                                                                                                                             |
|-------|-----------------------------------------------------------------------------------------------------------------------------------------------------|
| `0`   | 突变异步执行。                                                                                                                                     |
| `1`   | 查询等待当前服务器上的所有突变完成。                                                                                                                |
| `2`   | 查询等待所有副本上的所有突变完成（如果存在）。                                                                                                         |
| `3`   | 查询只等待活动副本。仅支持 `SharedMergeTree`。对于 `ReplicatedMergeTree`，它的行为与 `mutations_sync = 2` 相同。|
## mysql_datatypes_support_level {#mysql_datatypes_support_level} 

定义 MySQL 类型如何转换为相应的 ClickHouse 类型。以逗号分隔的列表，可以为 `decimal`、`datetime64`、`date2Date32` 或 `date2String` 的任意组合。
- `decimal`：在精度允许时，将 `NUMERIC` 和 `DECIMAL` 类型转换为 `Decimal`。
- `datetime64`：在精度不为 `0` 时，将 `DATETIME` 和 `TIMESTAMP` 类型转换为 `DateTime64` 而不是 `DateTime`。
- `date2Date32`：将 `DATE` 转换为 `Date32` 而不是 `Date`。优先于 `date2String`。
- `date2String`：将 `DATE` 转换为 `String` 而不是 `Date`。被 `datetime64` 重写。
## mysql_map_fixed_string_to_text_in_show_columns {#mysql_map_fixed_string_to_text_in_show_columns} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "1"},{"label": "Reduce the configuration effort to connect ClickHouse with BI tools."}]}]}/>

如果启用，[FixedString](../../sql-reference/data-types/fixedstring.md) ClickHouse 数据类型将显示为 [SHOW COLUMNS](../../sql-reference/statements/show.md/#show_columns)中的 `TEXT`。

仅在通过 MySQL 线协议建立连接时有效。

- 0 - 使用 `BLOB`。
- 1 - 使用 `TEXT`。
## mysql_map_string_to_text_in_show_columns {#mysql_map_string_to_text_in_show_columns} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "1"},{"label": "Reduce the configuration effort to connect ClickHouse with BI tools."}]}]}/>

如果启用，[String](../../sql-reference/data-types/string.md) ClickHouse 数据类型将显示为 [SHOW COLUMNS](../../sql-reference/statements/show.md/#show_columns)中的 `TEXT`。

仅在通过 MySQL 线协议建立连接时有效。

- 0 - 使用 `BLOB`。
- 1 - 使用 `TEXT`。
## mysql_max_rows_to_insert {#mysql_max_rows_to_insert} 

<SettingsInfoBlock type="UInt64" default_value="65536" />

MySQL 存储引擎的 MySQL 批量插入中最大行数。
## network_compression_method {#network_compression_method} 

客户端/服务器和服务器/服务器通信的压缩编解码器。

可能值：

- `NONE` — 无压缩。
- `LZ4` — 使用 LZ4 编解码器。
- `LZ4HC` — 使用 LZ4HC 编解码器。
- `ZSTD` — 使用 ZSTD 编解码器。

**另见**

- [network_zstd_compression_level](#network_zstd_compression_level)
## network_zstd_compression_level {#network_zstd_compression_level} 

调整 ZSTD 压缩级别。仅在将 [network_compression_method](#network_compression_method) 设置为 `ZSTD` 时使用。

可能值：

- 从 1 到 15 的正整数。
## normalize_function_names {#normalize_function_names} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "21.3"},{"label": "1"},{"label": "Normalize function names to their canonical names, this was needed for projection query routing"}]}]}/>

将函数名称规范化为其标准名称。
## number_of_mutations_to_delay {#number_of_mutations_to_delay} 

如果被突变的表包含至少那么多未完成的突变，请人为减缓表的突变。0 - 禁用。
## number_of_mutations_to_throw {#number_of_mutations_to_throw} 

如果被突变的表包含至少那么多未完成的突变，则抛出 'Too many mutations ...' 异常。0 - 禁用。
## odbc_bridge_connection_pool_size {#odbc_bridge_connection_pool_size} 

ODBC 桥中每个连接设置字符串的连接池大小。
## odbc_bridge_use_connection_pooling {#odbc_bridge_use_connection_pooling} 

在 ODBC 桥中使用连接池。如果设置为 false，则每次都会创建新连接。
## offset {#offset} 

设置在开始返回查询结果之前要跳过的行数。它调整由 [OFFSET](/sql-reference/statements/select/offset) 子句设置的偏移量，使这两个值相加。

可能值：

- 0 — 不跳过任何行。
- 正整数。

**示例**

输入表：

```sql
CREATE TABLE test (i UInt64) ENGINE = MergeTree() ORDER BY i;
INSERT INTO test SELECT number FROM numbers(500);
```

查询：

```sql
SET limit = 5;
SET offset = 7;
SELECT * FROM test LIMIT 10 OFFSET 100;
```
结果：

```text
┌───i─┐
│ 107 │
│ 108 │
│ 109 │
└─────┘
```
## opentelemetry_start_trace_probability {#opentelemetry_start_trace_probability} 

<SettingsInfoBlock type="Float" default_value="0" />

设置 ClickHouse 在执行查询时可以启动跟踪的概率（前提是没有提供父级 [跟踪上下文](https://www.w3.org/TR/trace-context/)）。

可能值：

- 0 — 禁用所有执行查询的跟踪（如果没有提供父级跟踪上下文）。
- 在 [0..1] 范围内的正浮点数。例如，如果设置值为 `0.5`，则 ClickHouse 可以平均启动一半查询的跟踪。
- 1 — 启用所有执行查询的跟踪。
## opentelemetry_trace_cpu_scheduling {#opentelemetry_trace_cpu_scheduling} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "0"},{"label": "New setting to trace `cpu_slot_preemption` feature."}]}]}/>

为工作负载抢占式 CPU 调度收集 OpenTelemetry 跨度。
## opentelemetry_trace_processors {#opentelemetry_trace_processors} 

<SettingsInfoBlock type="Bool" default_value="0" />

收集 OpenTelemetry 跨度以获取处理器。
## optimize_aggregation_in_order {#optimize_aggregation_in_order} 

启用在 [SELECT](../../sql-reference/statements/select/index.md) 查询中的 [GROUP BY](/sql-reference/statements/select/group-by) 优化，以便在 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 表中按相应顺序聚合数据。

可能值：

- 0 — 禁用 `GROUP BY` 优化。
- 1 — 启用 `GROUP BY` 优化。

**另见**

- [GROUP BY 优化](/sql-reference/statements/select/group-by#group-by-optimization-depending-on-table-sorting-key)
## optimize_aggregators_of_group_by_keys {#optimize_aggregators_of_group_by_keys} 

消除在 SELECT 部分的 GROUP BY 键的 min/max/any/anyLast 聚合器。

## optimize_and_compare_chain {#optimize_and_compare_chain} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.2"},{"label": "1"},{"label": "A new setting"}]}]}/>

在 AND 链中填充常量比较以增强过滤能力。支持运算符 `<`, `<=`, `>`, `>=`, `=` 及其混合。例如，`(a < b) AND (b < c) AND (c < 5)` 将变为 `(a < b) AND (b < c) AND (c < 5) AND (b < 5) AND (a < 5)`。
## optimize_append_index {#optimize_append_index} 

<SettingsInfoBlock type="Bool" default_value="0" />

使用 [constraints](../../sql-reference/statements/create/table.md/#constraints) 来附加索引条件。默认值为 `false`。

可能的值：

- true, false
## optimize_arithmetic_operations_in_aggregate_functions {#optimize_arithmetic_operations_in_aggregate_functions} 

<SettingsInfoBlock type="Bool" default_value="1" />

将算术运算移出聚合函数。
## optimize_count_from_files {#optimize_count_from_files} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用或禁用从不同输入格式的文件中计算行数的优化。适用于表函数/引擎 `file`/`s3`/`url`/`hdfs`/`azureBlobStorage`。

可能的值：

- 0 — 优化禁用。
- 1 — 优化启用。
## optimize_distinct_in_order {#optimize_distinct_in_order} 

<SettingsInfoBlock type="Bool" default_value="1" />

如果 DISTINCT 中的一些列形成排序的前缀，则启用 DISTINCT 优化。例如，merge tree 中的排序键前缀或 ORDER BY 语句。
## optimize_distributed_group_by_sharding_key {#optimize_distributed_group_by_sharding_key} 

<SettingsInfoBlock type="Bool" default_value="1" />

通过避免在发起服务器上进行成本高昂的聚合，优化 `GROUP BY sharding_key` 查询（这将减少发起服务器上查询的内存使用）。

支持以下类型的查询（及其所有组合）：

- `SELECT DISTINCT [..., ]sharding_key[, ...] FROM dist`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...]`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] ORDER BY x`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] LIMIT 1`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] LIMIT 1 BY x`

不支持以下类型的查询（可能稍后会增加对某些查询的支持）：

- `SELECT ... GROUP BY sharding_key[, ...] WITH TOTALS`
- `SELECT ... GROUP BY sharding_key[, ...] WITH ROLLUP`
- `SELECT ... GROUP BY sharding_key[, ...] WITH CUBE`
- `SELECT ... GROUP BY sharding_key[, ...] SETTINGS extremes=1`

可能的值：

- 0 — 禁用。
- 1 — 启用。

另见：

- [distributed_group_by_no_merge](#distributed_group_by_no_merge)
- [distributed_push_down_limit](#distributed_push_down_limit)
- [optimize_skip_unused_shards](#optimize_skip_unused_shards)

:::note
现在需要 `optimize_skip_unused_shards`（原因是将来可能默认启用，如果数据是通过分布式表插入的，也就是说数据根据 sharding_key 分布，则将正常工作）。
:::
## optimize_empty_string_comparisons {#optimize_empty_string_comparisons} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.10"},{"label": "1"},{"label": "A new setting."}]}]}/>

将 col = '' 或 '' = col 的表达式转换为 empty(col)，并将 col != '' 或 '' != col 转换为 notEmpty(col)，仅当 col 的类型为 String 或 FixedString 时。
## optimize_extract_common_expressions {#optimize_extract_common_expressions} 

<SettingsInfoBlock type="Bool" default_value="1" />

允许从 WHERE、PREWHERE、ON、HAVING 和 QUALIFY 表达式中的析取中提取共同表达式。逻辑表达式如 `(A AND B) OR (A AND C)` 可以重写为 `A AND (B OR C)`，这可能有助于利用：
- 简单过滤表达式中的索引
- 交叉到内部连接的优化
## optimize_functions_to_subcolumns {#optimize_functions_to_subcolumns} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用或禁用通过将某些函数转换为读取子列的优化。这减少了读取的数据量。

以下函数可以被转换：

- [length](/sql-reference/functions/array-functions#length) 读取 [size0](../../sql-reference/data-types/array.md/#array-size) 子列。
- [empty](/sql-reference/functions/array-functions#empty) 读取 [size0](../../sql-reference/data-types/array.md/#array-size) 子列。
- [notEmpty](/sql-reference/functions/array-functions#notEmpty) 读取 [size0](../../sql-reference/data-types/array.md/#array-size) 子列。
- [isNull](/sql-reference/functions/functions-for-nulls#isNull) 读取 [null](../../sql-reference/data-types/nullable.md/#finding-null) 子列。
- [isNotNull](/sql-reference/functions/functions-for-nulls#isNotNull) 读取 [null](../../sql-reference/data-types/nullable.md/#finding-null) 子列。
- [count](/sql-reference/aggregate-functions/reference/count) 读取 [null](../../sql-reference/data-types/nullable.md/#finding-null) 子列。
- [mapKeys](/sql-reference/functions/tuple-map-functions#mapkeys) 读取 [keys](/sql-reference/data-types/map#reading-subcolumns-of-map) 子列。
- [mapValues](/sql-reference/functions/tuple-map-functions#mapvalues) 读取 [values](/sql-reference/data-types/map#reading-subcolumns-of-map) 子列。

可能的值：

- 0 — 优化禁用。
- 1 — 优化启用。
## optimize_group_by_constant_keys {#optimize_group_by_constant_keys} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.9"},{"label": "1"},{"label": "Optimize group by constant keys by default"}]}]}/>

当块中的所有键都是常量时优化 GROUP BY。
## optimize_group_by_function_keys {#optimize_group_by_function_keys} 

<SettingsInfoBlock type="Bool" default_value="1" />

在 GROUP BY 部分消除其他键的函数。
## optimize_if_chain_to_multiif {#optimize_if_chain_to_multiif} 

<SettingsInfoBlock type="Bool" default_value="0" />

将 `if(cond1, then1, if(cond2, ...))` 链替换为 multiIf。目前对于数值类型没有好处。
## optimize_if_transform_strings_to_enum {#optimize_if_transform_strings_to_enum} 

<SettingsInfoBlock type="Bool" default_value="0" />

在 If 和 Transform 中将字符串类型参数替换为枚举。默认禁用，因为这可能会导致分布式查询中的不一致变化，进而导致失败。
## optimize_injective_functions_in_group_by {#optimize_injective_functions_in_group_by} 

<SettingsInfoBlock type="Bool" default_value="1" />

在 GROUP BY 部分中用其参数替换注射函数。
## optimize_injective_functions_inside_uniq {#optimize_injective_functions_inside_uniq} 

删除 uniq*() 函数内部的单个参数注射函数。
## optimize_min_equality_disjunction_chain_length {#optimize_min_equality_disjunction_chain_length} 

表达式 `expr = x1 OR ... expr = xN` 的最小长度以进行优化。
## optimize_min_inequality_conjunction_chain_length {#optimize_min_inequality_conjunction_chain_length} 

表达式 `expr <> x1 AND ... expr <> xN` 的最小长度以进行优化。
## optimize_move_to_prewhere {#optimize_move_to_prewhere} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用或禁用在 [SELECT](../../sql-reference/statements/select/index.md) 查询中自动 [PREWHERE](../../sql-reference/statements/select/prewhere.md) 优化。

仅适用于 [*MergeTree](../../engines/table-engines/mergetree-family/index.md) 表。

可能的值：

- 0 — 自动 `PREWHERE` 优化禁用。
- 1 — 自动 `PREWHERE` 优化启用。
## optimize_move_to_prewhere_if_final {#optimize_move_to_prewhere_if_final} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在带有 [FINAL](/sql-reference/statements/select/from#final-modifier) 修饰符的 [SELECT](../../sql-reference/statements/select/index.md) 查询中自动 [PREWHERE](../../sql-reference/statements/select/prewhere.md) 优化。

仅适用于 [*MergeTree](../../engines/table-engines/mergetree-family/index.md) 表。

可能的值：

- 0 — 在带有 `FINAL` 修饰符的 `SELECT` 查询中自动 `PREWHERE` 优化禁用。
- 1 — 在带有 `FINAL` 修饰符的 `SELECT` 查询中自动 `PREWHERE` 优化启用。

**另见**

- [optimize_move_to_prewhere](#optimize_move_to_prewhere) 设置。
## optimize_multiif_to_if {#optimize_multiif_to_if} 

<SettingsInfoBlock type="Bool" default_value="1" />

将仅具有一个条件的 'multiIf' 替换为 'if'。
## optimize_normalize_count_variants {#optimize_normalize_count_variants} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "21.3"},{"label": "1"},{"label": "Rewrite aggregate functions that semantically equals to count() as count() by default"}]}]}/>

将语义上等同于 count() 的聚合函数重写为 count()。
## optimize_on_insert {#optimize_on_insert} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "21.1"},{"label": "1"},{"label": "Enable data optimization on INSERT by default for better user experience"}]}]}/>

启用或禁用在插入前对数据进行转换，仿佛在该块上进行了合并（根据表引擎）。

可能的值：

- 0 — 禁用。
- 1 — 启用。

**示例**

启用和禁用之间的区别：

查询：

```sql
SET optimize_on_insert = 1;

CREATE TABLE test1 (`FirstTable` UInt32) ENGINE = ReplacingMergeTree ORDER BY FirstTable;

INSERT INTO test1 SELECT number % 2 FROM numbers(5);

SELECT * FROM test1;

SET optimize_on_insert = 0;

CREATE TABLE test2 (`SecondTable` UInt32) ENGINE = ReplacingMergeTree ORDER BY SecondTable;

INSERT INTO test2 SELECT number % 2 FROM numbers(5);

SELECT * FROM test2;
```

结果：

```text
┌─FirstTable─┐
│          0 │
│          1 │
└────────────┘

┌─SecondTable─┐
│           0 │
│           0 │
│           0 │
│           1 │
│           1 │
└─────────────┘
```

注意，此设置会影响 [Materialized view](/sql-reference/statements/create/view#materialized-view) 的行为。
## optimize_or_like_chain {#optimize_or_like_chain} 

<SettingsInfoBlock type="Bool" default_value="0" />

将多个 OR LIKE 优化为 multiMatchAny。此优化不应默认启用，因为它在某些情况下会违反索引分析。
## optimize_qbit_distance_function_reads {#optimize_qbit_distance_function_reads} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.10"},{"label": "1"},{"label": "New setting"}]}]}/>

将 `QBit` 数据类型的距离函数替换为等效函数，仅从存储中读取计算所需的列。
## optimize_read_in_order {#optimize_read_in_order} 

<SettingsInfoBlock type="Bool" default_value="1" />

在 [SELECT](../../sql-reference/statements/select/index.md) 查询中启用 [ORDER BY](/sql-reference/statements/select/order-by#optimization-of-data-reading) 优化，以从 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 表中读取数据。

可能的值：

- 0 — `ORDER BY` 优化禁用。
- 1 — `ORDER BY` 优化启用。

**另见**

- [ORDER BY Clause](/sql-reference/statements/select/order-by#optimization-of-data-reading)
## optimize_read_in_window_order {#optimize_read_in_window_order} 

启用窗口子句中的 ORDER BY 优化，以在 MergeTree 表中以相应顺序读取数据。
## optimize_redundant_functions_in_order_by {#optimize_redundant_functions_in_order_by} 

从 ORDER BY 中移除函数，如果其参数也在 ORDER BY 中。
## optimize_respect_aliases {#optimize_respect_aliases} 

如果设置为 true，将尊重 WHERE/GROUP BY/ORDER BY 中的别名，以帮助进行分区修剪/二级索引/optimize_aggregation_in_order/optimize_read_in_order/optimize_trivial_count。
## optimize_rewrite_aggregate_function_with_if {#optimize_rewrite_aggregate_function_with_if} 

<SettingsInfoBlock type="Bool" default_value="1" />

当逻辑上等价时，将带有 if 表达式的聚合函数重写为参数。例如，`avg(if(cond, col, null))` 可以重写为 `avgOrNullIf(cond, col)`。这可能提高性能。

:::note
仅在启用分析器时支持（`enable_analyzer = 1`）。
:::
## optimize_rewrite_array_exists_to_has {#optimize_rewrite_array_exists_to_has} 

<SettingsInfoBlock type="Bool" default_value="0" />

将数组存在函数重写为 has()，当逻辑上等价时。例如，arrayExists(x -> x = 1, arr) 可以重写为 has(arr, 1)。
## optimize_rewrite_regexp_functions {#optimize_rewrite_regexp_functions} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "1"},{"label": "A new setting"}]}]}/>

将正则表达式相关函数重写为更简单和更高效的形式。
## optimize_rewrite_sum_if_to_count_if {#optimize_rewrite_sum_if_to_count_if} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.4"},{"label": "1"},{"label": "Only available for the analyzer, where it works correctly"}]}]}/>

当逻辑上等价时，将 sumIf() 和 sum(if()) 函数重写为 countIf() 函数。
## optimize_skip_merged_partitions {#optimize_skip_merged_partitions} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用针对 [OPTIMIZE TABLE ... FINAL](../../sql-reference/statements/optimize.md) 查询的优化，如果只有一个级别 > 0 的部分，并且没有过期的 TTL。

- `OPTIMIZE TABLE ... FINAL SETTINGS optimize_skip_merged_partitions=1`

默认情况下，即使只有一个部分，`OPTIMIZE TABLE ... FINAL` 查询也会重写这一部分。

可能的值：

- 1 - 启用优化。
- 0 - 禁用优化。
## optimize_skip_unused_shards {#optimize_skip_unused_shards} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在 `WHERE/PREWHERE` 中具有分片键条件的 [SELECT](../../sql-reference/statements/select/index.md) 查询中跳过未使用的分片（假设数据是根据分片键分布的，否则查询将产生不正确的结果）。

可能的值：

- 0 — 禁用。
- 1 — 启用。
## optimize_skip_unused_shards_limit {#optimize_skip_unused_shards_limit} 

达到限制时，分片键值的限制，关闭 `optimize_skip_unused_shards`。

过多的值可能需要大量处理，而收益值得怀疑，因为如果您在 `IN (...)` 中有大量值，那么查询很可能无论如何都会发送到所有的分片。
## optimize_skip_unused_shards_nesting {#optimize_skip_unused_shards_nesting} 

控制 [`optimize_skip_unused_shards`](#optimize_skip_unused_shards)（因此仍然需要 [`optimize_skip_unused_shards`](#optimize_skip_unused_shards)）取决于分布式查询的嵌套级别（假设您有 `Distributed` 表查找另一个 `Distributed` 表）。

可能的值：

- 0 — 禁用，`optimize_skip_unused_shards` 始终有效。
- 1 — 仅对第一层启用 `optimize_skip_unused_shards`。
- 2 — 允许 `optimize_skip_unused_shards` 直到第二层。
## optimize_skip_unused_shards_rewrite_in {#optimize_skip_unused_shards_rewrite_in} 

在查询中重写 IN 以排除不属于该分片的值（需要 optimize_skip_unused_shards）。

可能的值：

- 0 — 禁用。
- 1 — 启用。
## optimize_sorting_by_input_stream_properties {#optimize_sorting_by_input_stream_properties} 

根据输入流的排序属性优化排序。
## optimize_substitute_columns {#optimize_substitute_columns} 

使用 [constraints](../../sql-reference/statements/create/table.md/#constraints) 来替换列。默认值为 `false`。

可能的值：

- true, false
## optimize_syntax_fuse_functions {#optimize_syntax_fuse_functions} 

启用通过相同参数融合聚合函数。它重写查询中至少包含两个具有相同参数的聚合函数的查询，来自 [sum](/sql-reference/aggregate-functions/reference/sum), [count](/sql-reference/aggregate-functions/reference/count) 或 [avg](/sql-reference/aggregate-functions/reference/avg) 重写为 [sumCount](/sql-reference/aggregate-functions/reference/sumcount)。

可能的值：

- 0 — 不融合具有相同参数的函数。
- 1 — 融合具有相同参数的函数。

**示例**

查询：

```sql
CREATE TABLE fuse_tbl(a Int8, b Int8) Engine = Log;
SET optimize_syntax_fuse_functions = 1;
EXPLAIN SYNTAX SELECT sum(a), sum(b), count(b), avg(b) from fuse_tbl FORMAT TSV;
```

结果：

```text
SELECT
    sum(a),
    sumCount(b).1,
    sumCount(b).2,
    (sumCount(b).1) / (sumCount(b).2)
FROM fuse_tbl
```
## optimize_throw_if_noop {#optimize_throw_if_noop} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用如果 [OPTIMIZE](../../sql-reference/statements/optimize.md) 查询没有执行合并，则抛出异常。

默认情况下，`OPTIMIZE` 即使没有进行任何操作也会成功返回。此设置使您能够区分这些情况，并在异常消息中获取原因。

可能的值：

- 1 — 启用抛出异常。
- 0 — 禁用抛出异常。
## optimize_time_filter_with_preimage {#optimize_time_filter_with_preimage} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "1"},{"label": "通过将函数转换为等效比较来优化日期和日期时间谓词，而不进行转换（例如 `toYear(col) = 2023 -> col >= '2023-01-01' AND col <= '2023-12-31'`）"}]}]}/>

通过将函数转换为等效比较来优化日期和日期时间谓词，而不进行转换（例如 `toYear(col) = 2023 -> col >= '2023-01-01' AND col <= '2023-12-31'`）。
## optimize_trivial_approximate_count_query {#optimize_trivial_approximate_count_query} 

<SettingsInfoBlock type="Bool" default_value="0" />

对于支持这种估算的存储，使用近似值来优化简单计数，例如 EmbeddedRocksDB。

可能的值：

   - 0 — 优化禁用。
   - 1 — 优化启用。
## optimize_trivial_count_query {#optimize_trivial_count_query} 

启用或禁用使用来自 MergeTree 的元数据优化简单查询 `SELECT count() FROM table`。如果您需要使用行级安全性，请禁用此设置。

可能的值：

   - 0 — 优化禁用。
   - 1 — 优化启用。

另见：

- [optimize_functions_to_subcolumns](#optimize_functions_to_subcolumns)
## optimize_trivial_insert_select {#optimize_trivial_insert_select} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.7"},{"label": "0"},{"label": "The optimization does not make sense in many cases."}]}]}/>

优化简单的 'INSERT INTO table SELECT ... FROM TABLES' 查询。
## optimize_uniq_to_count {#optimize_uniq_to_count} 

将 uniq 及其变体（除 uniqUpTo 外）重写为 count，如果子查询有 distinct 或 group by 子句。
## optimize_use_implicit_projections {#optimize_use_implicit_projections} 

自动选择隐式投影来执行 SELECT 查询。
## optimize_use_projection_filtering {#optimize_use_projection_filtering} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.6"},{"label": "1"},{"label": "New setting"}]}]}/>

启用使用投影在执行 SELECT 查询时过滤部分范围，即使在选择查询时未选中投影。
## optimize_use_projections {#optimize_use_projections} 

启用或禁用在处理 `SELECT` 查询时的 [projection](../../engines/table-engines/mergetree-family/mergetree.md/#projections) 优化。

可能的值：

- 0 — 投影优化禁用。
- 1 — 投影优化启用。
## optimize_using_constraints {#optimize_using_constraints} 

使用 [constraints](../../sql-reference/statements/create/table.md/#constraints) 进行查询优化。默认值为 `false`。

可能的值：

- true, false
## os_threads_nice_value_materialized_view {#os_threads_nice_value_materialized_view} 

<SettingsInfoBlock type="Int32" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.9"},{"label": "0"},{"label": "New setting."}]}]}/>

Linux 物化视图线程的优先级值。较低的值意味着更高的 CPU 优先级。

需要 CAP_SYS_NICE 权限，否则不会生效。

可能的值： -20 到 19。
## os_threads_nice_value_query {#os_threads_nice_value_query} 

<SettingsInfoBlock type="Int32" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.9"},{"label": "0"},{"label": "New setting."}]}]}/>

Linux 查询处理线程的优先级值。较低的值意味着更高的 CPU 优先级。

需要 CAP_SYS_NICE 权限，否则不会生效。

可能的值： -20 到 19。
## output_format_compression_level {#output_format_compression_level} 

<SettingsInfoBlock type="UInt64" default_value="3" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.1"},{"label": "3"},{"label": "Allow to change compression level in the query output"}]}]}/>

如果查询输出被压缩，则默认压缩级别。当 `SELECT` 查询具有 `INTO OUTFILE` 时或写入表函数 `file`、`url`、`hdfs`、`s3` 或 `azureBlobStorage` 时应用此设置。

可能的值：从 `1` 到 `22`。
## output_format_compression_zstd_window_log {#output_format_compression_zstd_window_log} 

<SettingsInfoBlock type="UInt64" default_value="0" />

可以在输出压缩方法为 `zstd` 时使用。如果大于 `0`，此设置明确设置压缩窗口大小（2 的幂），并启用 zstd 压缩的长范围模式。这可以帮助实现更好的压缩率。

可能的值：非负数。请注意，如果值过小或过大，`zstdlib` 将抛出异常。典型值为从 `20`（窗口大小 = `1MB`）到 `30`（窗口大小 = `1GB`）。
## output_format_parallel_formatting {#output_format_parallel_formatting} 

启用或禁用数据格式的并行格式化。仅支持 [TSV](../../interfaces/formats.md/#tabseparated)、[TSKV](../../interfaces/formats.md/#tskv)、[CSV](../../interfaces/formats.md/#csv) 和 [JSONEachRow](../../interfaces/formats.md/#jsoneachrow) 格式。

可能的值：

- 1 — 启用。
- 0 — 禁用。
## page_cache_block_size {#page_cache_block_size} 

<SettingsInfoBlock type="UInt64" default_value="1048576" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "1048576"},{"label": "Made this setting adjustable on a per-query level."}]}]}/>

存储在用户空间页缓存中的文件块大小，以字节为单位。所有通过缓存的读取将被舍入为该大小的倍数。

此设置可以在每个查询级别进行调整，但具有不同块大小的缓存条目不能重用。改变此设置实际上会使缓存中现有条目失效。

较高的值，如 1 MiB 适用于高吞吐量查询，而较低的值，如 64 KiB 适用于低延迟的点查询。
## page_cache_inject_eviction {#page_cache_inject_eviction} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "0"},{"label": "Added userspace page cache"}]}]}/>

用户空间页缓存有时会随机失效某些页面。用于测试。
## page_cache_lookahead_blocks {#page_cache_lookahead_blocks} 

<SettingsInfoBlock type="UInt64" default_value="16" />

在用户空间页缓存缺失时，从底层存储一次读取最多此数量的连续块，如果它们也不在缓存中。每个块是 page_cache_block_size 字节。

较高的值适用于高吞吐量查询，而低延迟的点查询在没有预读取的情况下会更好。
## parallel_distributed_insert_select {#parallel_distributed_insert_select} 

<SettingsInfoBlock type="UInt64" default_value="2" />

启用并行分布式 `INSERT ... SELECT` 查询。

如果我们执行 `INSERT INTO distributed_table_a SELECT ... FROM distributed_table_b` 查询并且两个表使用相同的集群，并且两个表都是 [replicated](../../engines/table-engines/mergetree-family/replication.md) 或非复制的，那么此查询将在每个分片上本地处理。

可能的值：

- `0` — 禁用。
- `1` — `SELECT` 将在分布式引擎的基础表的每个分片上执行。
- `2` — `SELECT` 和 `INSERT` 将在分布式引擎的基础表的每个分片上执行。

使用此设置需要将 `enable_parallel_replicas` 设置为 `1`。
## parallel_hash_join_threshold {#parallel_hash_join_threshold} 

<SettingsInfoBlock type="UInt64" default_value="100000" />

当应用基于哈希的连接算法时，此阈值帮助决定使用 `hash` 还是 `parallel_hash`（仅在可用右表大小估计的情况下）。

前者在我们知道右表大小低于阈值时使用。
## parallel_replica_offset {#parallel_replica_offset} 

<BetaBadge/>

<SettingsInfoBlock type="UInt64" default_value="0" />

这是一个内部设置，不应直接使用，表示“并行副本”模式的实现细节。此设置将由发起服务器自动为分布式查询设置为参与查询处理的副本索引。
## parallel_replicas_allow_in_with_subquery {#parallel_replicas_allow_in_with_subquery} 

<BetaBadge/>

如果为真，IN 的子查询将在每个跟随副本上执行。
## parallel_replicas_connect_timeout_ms {#parallel_replicas_connect_timeout_ms} 

<BetaBadge/>

<SettingsInfoBlock type="Milliseconds" default_value="300" />

查询执行时连接到远程副本的超时时间（以毫秒为单位）。如果超时，则不使用相应副本进行查询执行。
## parallel_replicas_count {#parallel_replicas_count} 

<BetaBadge/>

这是一个内部设置，不应直接使用，表示“并行副本”模式的实现细节。此设置将由发起服务器自动为分布式查询设置为参与查询处理的并行副本数量。
## parallel_replicas_custom_key {#parallel_replicas_custom_key} 

<BetaBadge/>

可以用于在特定表之间划分副本工作的任意整数表达式。
值可以是任何整数表达式。

简单的使用主键的表达式更受欢迎。

如果在由单个分片和多个副本组成的集群上使用此设置，则这些副本将转换为虚拟分片。
否则，它将与 `SAMPLE` 键的行为相同，它将使用每个分片的多个副本。
## parallel_replicas_custom_key_range_lower {#parallel_replicas_custom_key_range_lower} 

<BetaBadge/>

<SettingsInfoBlock type="UInt64" default_value="0" />

允许过滤器类型 `range` 基于自定义范围 `[parallel_replicas_custom_key_range_lower, INT_MAX]` 在副本之间均匀划分工作。

与 [parallel_replicas_custom_key_range_upper](#parallel_replicas_custom_key_range_upper) 一起使用时，可以使过滤器对范围 `[parallel_replicas_custom_key_range_lower, parallel_replicas_custom_key_range_upper]` 平均分配工作。

注意：此设置不会在查询处理期间造成额外的数据过滤，而是更改了范围过滤器在进行并行处理时将范围 `[0, INT_MAX]` 划分的点。
## parallel_replicas_custom_key_range_upper {#parallel_replicas_custom_key_range_upper} 

<BetaBadge/>

<SettingsInfoBlock type="UInt64" default_value="0" />

允许过滤器类型 `range` 基于自定义范围 `[0, parallel_replicas_custom_key_range_upper]` 在副本之间均匀划分工作。值为 0 禁用上限，将其设置为自定义键表达式的最大值。

与 [parallel_replicas_custom_key_range_lower](#parallel_replicas_custom_key_range_lower) 一起使用时，可以使过滤器对范围 `[parallel_replicas_custom_key_range_lower, parallel_replicas_custom_key_range_upper]` 平均分配工作。

注意：此设置不会在查询处理期间造成额外的数据过滤，而是更改了范围过滤器在进行并行处理时将范围 `[0, INT_MAX]` 划分的点。
## parallel_replicas_for_cluster_engines {#parallel_replicas_for_cluster_engines} 

<SettingsInfoBlock type="Bool" default_value="1" />

将表函数引擎替换为其 -Cluster 替代品。
## parallel_replicas_for_non_replicated_merge_tree {#parallel_replicas_for_non_replicated_merge_tree} 

<BetaBadge/>

如果为真，ClickHouse 将对非复制的 MergeTree 表使用并行副本算法。
## parallel_replicas_index_analysis_only_on_coordinator {#parallel_replicas_index_analysis_only_on_coordinator} 

<BetaBadge/>

仅在副本协调器上执行索引分析，其他副本则省略。仅在启用 parallel_replicas_local_plan 时有效。
## parallel_replicas_insert_select_local_pipeline {#parallel_replicas_insert_select_local_pipeline} 

<BetaBadge/>

在分布式 INSERT SELECT 中的并行副本中使用本地管道。
## parallel_replicas_local_plan {#parallel_replicas_local_plan} 

<BetaBadge/>

为本地副本构建本地计划。
## parallel_replicas_mark_segment_size {#parallel_replicas_mark_segment_size} 

<BetaBadge/>

部分虚拟地划分为要在副本之间分发的段。此设置控制这些段的大小。在完全确定您在做什么之前，不建议更改。值应在 [128; 16384] 范围内。
## parallel_replicas_min_number_of_rows_per_replica {#parallel_replicas_min_number_of_rows_per_replica} 

<BetaBadge/>

限制查询中使用的副本数量为（估计读取行数 / min_number_of_rows_per_replica）。最大值仍然受 'max_parallel_replicas' 的限制。
## parallel_replicas_mode {#parallel_replicas_mode} 

<BetaBadge/>

用于具有自定义键的并行副本的过滤器类型。默认值 - 使用自定义键的模运算，范围 - 对自定义键使用范围过滤器，使用自定义键值类型的所有可能值。
## parallel_replicas_only_with_analyzer {#parallel_replicas_only_with_analyzer} 

<BetaBadge/>

必须启用分析器才能使用并行副本。在禁用分析器时，查询执行回落到本地执行，即使启用了从副本并行读取。禁用分析器不支持使用并行副本。
## parallel_replicas_prefer_local_join {#parallel_replicas_prefer_local_join} 

<BetaBadge/>

如果为真，并且 JOIN 可以使用并行副本算法执行，并且右 JOIN 部分的所有存储都是 *MergeTree，则将使用本地 JOIN，而不是 GLOBAL JOIN。
## parallel_replicas_support_projection {#parallel_replicas_support_projection} 

<BetaBadge/>

可以在并行副本中应用投影的优化。仅在启用 parallel_replicas_local_plan 且 aggregation_in_order 不激活时有效。
## parallel_view_processing {#parallel_view_processing} 

启用同步推送到附加视图，而不是顺序推送。
## parallelize_output_from_storages {#parallelize_output_from_storages} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.5"},{"label": "1"},{"label": "Allow parallelism when executing queries that read from file/url/s3/etc. This may reorder rows."}]}]}/>

并行化从存储的读取步骤的输出。如果可能，它允许在从存储读取后立即并行化查询处理。
## parsedatetime_e_requires_space_padding {#parsedatetime_e_requires_space_padding} 

<SettingsInfoBlock type="Bool" default_value="0" />

格式化器 `%e` 在函数 'parseDateTime' 中期望单数字的日期为填充空格，例如，接受 ' 2'，但 '2' 会引发错误。
## parsedatetime_parse_without_leading_zeros {#parsedatetime_parse_without_leading_zeros} 

<SettingsInfoBlock type="Bool" default_value="1" />

格式化器 `%c`、`%l` 和 `%k` 在函数 'parseDateTime' 中解析月份和小时时不使用前导零。
## partial_merge_join_left_table_buffer_bytes {#partial_merge_join_left_table_buffer_bytes} 

<SettingsInfoBlock type="UInt64" default_value="0" />

如果不为 0，在部分合并连接中将左表块组合为较大的块。它每个连接线程最多使用指定内存的 2 倍。
## partial_merge_join_rows_in_right_blocks {#partial_merge_join_rows_in_right_blocks} 

<SettingsInfoBlock type="UInt64" default_value="65536" />

限制在部分合并连接算法中用于 [JOIN](../../sql-reference/statements/select/join.md) 查询的右侧连接数据块的大小。

ClickHouse 服务器:

1. 将右侧连接数据分成最多指定行数的块。
2. 用每个块的最小和最大值为其建立索引。
3. 如果可能，将准备好的块卸载到磁盘。

可能的值：

- 任何正整数。推荐的值范围: \[1000, 100000\]。

## partial_result_on_first_cancel {#partial_result_on_first_cancel} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许在取消后返回部分结果。

## parts_to_delay_insert {#parts_to_delay_insert} 

<SettingsInfoBlock type="UInt64" default_value="0" />

如果目标表在单个分区中至少包含如此多的活动部分，则人为地减慢插入到表的速度。

## parts_to_throw_insert {#parts_to_throw_insert} 

<SettingsInfoBlock type="UInt64" default_value="0" />

如果目标表的单个分区中活动部分超过此数量，则抛出 'Too many parts ...' 异常。

## per_part_index_stats {#per_part_index_stats} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "0"},{"label": "New setting."}]}]}/>

        记录每部分的索引统计信息

## periodic_live_view_refresh {#periodic_live_view_refresh} 

<SettingsInfoBlock type="Seconds" default_value="60" />

在该间隔后强制刷新定期更新的物化视图。

## poll_interval {#poll_interval} 

<SettingsInfoBlock type="UInt64" default_value="10" />

在服务器上阻塞查询等待循环，持续指定的秒数。

## postgresql_connection_attempt_timeout {#postgresql_connection_attempt_timeout} 

<SettingsInfoBlock type="UInt64" default_value="2" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.7"},{"label": "2"},{"label": "Allow to control 'connect_timeout' parameter of PostgreSQL connection."}]}]}/>

单次尝试连接 PostgreSQL 端点的超时时间（秒）。
该值作为连接 URL 的 `connect_timeout` 参数传递。

## postgresql_connection_pool_auto_close_connection {#postgresql_connection_pool_auto_close_connection} 

<SettingsInfoBlock type="Bool" default_value="0" />

在将连接返回到连接池之前关闭连接。

## postgresql_connection_pool_retries {#postgresql_connection_pool_retries} 

<SettingsInfoBlock type="UInt64" default_value="2" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.7"},{"label": "2"},{"label": "Allow to control the number of retries in PostgreSQL connection pool."}]}]}/>

PostgreSQL 表引擎和数据库引擎的连接池推送/获取重试次数。

## postgresql_connection_pool_size {#postgresql_connection_pool_size} 

<SettingsInfoBlock type="UInt64" default_value="16" />

PostgreSQL 表引擎和数据库引擎的连接池大小。

## postgresql_connection_pool_wait_timeout {#postgresql_connection_pool_wait_timeout} 

<SettingsInfoBlock type="UInt64" default_value="5000" />

PostgreSQL 表引擎和数据库引擎的空连接池推送/获取超时。默认情况下，在空池上将阻塞。

## postgresql_fault_injection_probability {#postgresql_fault_injection_probability} 

<SettingsInfoBlock type="Float" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.2"},{"label": "0"},{"label": "New setting"}]}]}/>

内部（用于复制）PostgreSQL 查询失败的近似概率。有效值在区间 [0.0f, 1.0f] 之间。

## prefer_column_name_to_alias {#prefer_column_name_to_alias} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在查询表达式和子句中使用原始列名而不是别名。尤其是在别名与列名相同的情况下，这一设置尤其重要，见 [表达式别名](/sql-reference/syntax#notes-on-usage)。启用该设置可使 ClickHouse 中的别名语法规则与大多数其他数据库引擎更加兼容。

可能的值：

- 0 — 列名将被别名替换。
- 1 — 列名不被别名替换。

**示例**

已启用和已禁用之间的差异：

查询：

```sql
SET prefer_column_name_to_alias = 0;
SELECT avg(number) AS number, max(number) FROM numbers(10);
```

结果：

```text
Received exception from server (version 21.5.1):
Code: 184. DB::Exception: Received from localhost:9000. DB::Exception: Aggregate function avg(number) is found inside another aggregate function in query: While processing avg(number) AS number.
```

查询：

```sql
SET prefer_column_name_to_alias = 1;
SELECT avg(number) AS number, max(number) FROM numbers(10);
```

结果：

```text
┌─number─┬─max(number)─┐
│    4.5 │           9 │
└────────┴─────────────┘
```

## prefer_external_sort_block_bytes {#prefer_external_sort_block_bytes} 

<SettingsInfoBlock type="UInt64" default_value="16744704" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.5"},{"label": "16744704"},{"label": "Prefer maximum block bytes for external sort, reduce the memory usage during merging."}]}]}/>

偏好用于外部排序的最大块字节，减少合并过程中的内存使用。

## prefer_global_in_and_join {#prefer_global_in_and_join} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用将 `IN`/`JOIN` 操作符替换为 `GLOBAL IN`/`GLOBAL JOIN`。

可能的值：

- 0 — 禁用。`IN`/`JOIN` 操作符不被替换为 `GLOBAL IN`/`GLOBAL JOIN`。
- 1 — 启用。`IN`/`JOIN` 操作符被替换为 `GLOBAL IN`/`GLOBAL JOIN`。

**用法**

尽管 `SET distributed_product_mode=global` 可以改变分布式表的查询行为，但它不适用于本地表或外部资源中的表。这就是 `prefer_global_in_and_join` 设置发挥作用的地方。

例如，我们有服务节点查询本地表，这些表并不适合分发。在分布式处理时，我们需要动态分散这些数据，使用 `GLOBAL` 关键词 — `GLOBAL IN`/`GLOBAL JOIN`。

`prefer_global_in_and_join` 的另一个用例是访问由外部引擎创建的表。该设置有助于在连接此类表时减少对外部资源的调用次数：每个查询仅进行一次调用。

**另外请参见:**

- [分布式子查询](/sql-reference/operators/in#distributed-subqueries)，了解如何使用 `GLOBAL IN`/`GLOBAL JOIN`

## prefer_localhost_replica {#prefer_localhost_replica} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用/禁用在处理分布式查询时优先使用本地地址副本。

可能的值：

- 1 — 如果存在，ClickHouse 将始终将查询发送到本地地址副本。
- 0 — ClickHouse 使用 [load_balancing](#load_balancing) 设置指定的负载平衡策略。

:::note
如果您使用 [max_parallel_replicas](#max_parallel_replicas) 而不使用 [parallel_replicas_custom_key](#parallel_replicas_custom_key)，则禁用此设置。
如果设置了 [parallel_replicas_custom_key](#parallel_replicas_custom_key)，仅在多个副本的多个分片包含的集群上禁用此设置。
在包含单个分片和多个副本的集群上使用此设置将产生负面影响。
:::

## prefer_warmed_unmerged_parts_seconds {#prefer_warmed_unmerged_parts_seconds} 

<CloudOnlyBadge/>

<SettingsInfoBlock type="Int64" default_value="0" />

仅对 ClickHouse Cloud 有效。 如果合并的部分小于这一秒数且未预热（参见 [cache_populated_by_fetch](merge-tree-settings.md/#cache_populated_by_fetch)），但其所有源部分可用且已预热，则 SELECT 查询将从这些部分读取。 仅适用于 Replicated-/SharedMergeTree。 请注意，这仅检查 CacheWarmer 是否处理了该部分；如果该部分已由其他内容加载到缓存中，则在 CacheWarmer 处理之前，它仍将被视为冷部分；如果它已被预热，然后从缓存中驱逐，将仍被视为热部分。

## preferred_block_size_bytes {#preferred_block_size_bytes} 

<SettingsInfoBlock type="UInt64" default_value="1000000" />

该设置调整查询处理的数据块大小，并对粗略的 `max_block_size` 设置进行额外微调。如果列很大且具有 `max_block_size` 行，则块大小可能大于指定的字节数，其大小将被降低以提高 CPU 缓存局部性。

## preferred_max_column_in_block_size_bytes {#preferred_max_column_in_block_size_bytes} 

<SettingsInfoBlock type="UInt64" default_value="0" />

读取时块内最大列大小的限制。 有助于减少缓存未命中的次数。 应该接近 L2 缓存大小。

## preferred_optimize_projection_name {#preferred_optimize_projection_name} 

如果设置为非空字符串，ClickHouse 将尝试在查询中应用指定的投影。

可能的值：

- 字符串：首选投影的名称

## prefetch_buffer_size {#prefetch_buffer_size} 

<SettingsInfoBlock type="UInt64" default_value="1048576" />

从文件系统读取的预取缓冲区的最大大小。

## print_pretty_type_names {#print_pretty_type_names} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.1"},{"label": "1"},{"label": "Better user experience."}]}]}/>

允许在 `DESCRIBE` 查询和 `toTypeName()` 函数中以漂亮的方式打印深层嵌套的类型名称并进行缩进。

示例：

```sql
CREATE TABLE test (a Tuple(b String, c Tuple(d Nullable(UInt64), e Array(UInt32), f Array(Tuple(g String, h Map(String, Array(Tuple(i String, j UInt64))))), k Date), l Nullable(String))) ENGINE=Memory;
DESCRIBE TABLE test FORMAT TSVRaw SETTINGS print_pretty_type_names=1;
```

```
a   Tuple(
    b String,
    c Tuple(
        d Nullable(UInt64),
        e Array(UInt32),
        f Array(Tuple(
            g String,
            h Map(
                String,
                Array(Tuple(
                    i String,
                    j UInt64
                ))
            )
        )),
        k Date
    ),
    l Nullable(String)
)
```

## priority {#priority} 

<SettingsInfoBlock type="UInt64" default_value="0" />

查询的优先级。1 - 最高，较高的数值 - 较低的优先级；0 - 不使用优先级。

## promql_database {#promql_database} 

<ExperimentalBadge/>

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": ""},{"label": "New experimental setting"}]}]}/>

指定 'promql' 方言使用的数据库名称。空字符串表示当前数据库。

## promql_evaluation_time {#promql_evaluation_time} 

<ExperimentalBadge/>

<SettingsInfoBlock type="FloatAuto" default_value="auto" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.9"},{"label": "auto"},{"label": "The setting was renamed. The previous name is `evaluation_time`."}]}]}/>

设置将与 promql 方言一起使用的评估时间。 'auto' 表示当前时间。

## promql_table {#promql_table} 

<ExperimentalBadge/>

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": ""},{"label": "New experimental setting"}]}]}/>

指定 'promql' 方言使用的时间序列表的名称。

## push_external_roles_in_interserver_queries {#push_external_roles_in_interserver_queries} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "1"},{"label": "New setting."}]}]}/>

在执行查询时启用将用户角色从发起方推送到其他节点。

## query_cache_compress_entries {#query_cache_compress_entries} 

<SettingsInfoBlock type="Bool" default_value="1" />

压缩 [查询缓存](../query-cache.md) 中的条目。 以减小查询缓存的内存消耗，但代价是插入/读取速度变慢。

可能的值：

- 0 - 禁用
- 1 - 启用

## query_cache_max_entries {#query_cache_max_entries} 

<SettingsInfoBlock type="UInt64" default_value="0" />

当前用户可以在 [查询缓存](../query-cache.md) 中存储的查询结果的最大数量。0 表示无限制。

可能的值：

- 正整数 >= 0。

## query_cache_max_size_in_bytes {#query_cache_max_size_in_bytes} 

<SettingsInfoBlock type="UInt64" default_value="0" />

当前用户可以在 [查询缓存](../query-cache.md) 中分配的最大内存（以字节为单位）。0 表示无限制。

可能的值：

- 正整数 >= 0。

## query_cache_min_query_duration {#query_cache_min_query_duration} 

<SettingsInfoBlock type="Milliseconds" default_value="0" />

查询需要运行的最小持续时间（毫秒），才能将其结果存储在 [查询缓存](../query-cache.md) 中。

可能的值：

- 正整数 >= 0。

## query_cache_min_query_runs {#query_cache_min_query_runs} 

<SettingsInfoBlock type="UInt64" default_value="0" />

必须运行的 `SELECT` 查询的最小次数，才能将其结果存储在 [查询缓存](../query-cache.md) 中。

可能的值：

- 正整数 >= 0。

## query_cache_nondeterministic_function_handling {#query_cache_nondeterministic_function_handling} 

<SettingsInfoBlock type="QueryResultCacheNondeterministicFunctionHandling" default_value="throw" />

控制 [查询缓存](../query-cache.md) 如何处理带有非确定性函数的 `SELECT` 查询，如 `rand()` 或 `now()`。

可能的值：

- `'throw'` - 抛出异常并不缓存查询结果。
- `'save'` - 缓存查询结果。
- `'ignore'` - 不缓存查询结果也不抛出异常。

## query_cache_share_between_users {#query_cache_share_between_users} 

<SettingsInfoBlock type="Bool" default_value="0" />

如果启用，缓存的 `SELECT` 查询结果可以被其他用户读取。
出于安全原因，不建议启用此设置。

可能的值：

- 0 - 禁用
- 1 - 启用

## query_cache_squash_partial_results {#query_cache_squash_partial_results} 

<SettingsInfoBlock type="Bool" default_value="1" />

将部分结果块压缩为 [max_block_size](#max_block_size) 的大小。 减少插入到 [查询缓存](../query-cache.md) 的性能，但提高了缓存条目的可压缩性（见 [query_cache_compress-entries](#query_cache_compress_entries)）。

可能的值：

- 0 - 禁用
- 1 - 启用

## query_cache_system_table_handling {#query_cache_system_table_handling} 

<SettingsInfoBlock type="QueryResultCacheSystemTableHandling" default_value="throw" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.4"},{"label": "throw"},{"label": "The query cache no longer caches results of queries against system tables"}]}]}/>

控制 [查询缓存](../query-cache.md)如何处理针对系统表的 `SELECT` 查询，即数据库 `system.*` 和 `information_schema.*` 中的表。

可能的值：

- `'throw'` - 抛出异常并不缓存查询结果。
- `'save'` - 缓存查询结果。
- `'ignore'` - 不缓存查询结果也不抛出异常。

## query_cache_tag {#query_cache_tag} 

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.8"},{"label": ""},{"label": "New setting for labeling query cache settings."}]}]}/>

作为 [查询缓存](../query-cache.md) 条目的标签的字符串。 具有不同标签的相同查询被查询缓存视为不同查询。

可能的值：

- 任何字符串

## query_cache_ttl {#query_cache_ttl} 

<SettingsInfoBlock type="Seconds" default_value="60" />

在此时间（秒）后，[查询缓存](../query-cache.md) 中的条目变为过时。

可能的值：

- 正整数 >= 0。

## query_condition_cache_store_conditions_as_plaintext {#query_condition_cache_store_conditions_as_plaintext} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "0"},{"label": "New setting"}]}]}/>

以纯文本格式存储 [查询条件缓存](/operations/query-condition-cache) 的过滤条件。
如果启用，system.query_condition_cache 显示逐字过滤条件，这使得调试缓存问题更加容易。
默认情况下禁用，因为纯文本过滤条件可能会暴露敏感信息。

可能的值：

- 0 - 禁用
- 1 - 启用

## query_metric_log_interval {#query_metric_log_interval} 

<SettingsInfoBlock type="Int64" default_value="-1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "-1"},{"label": "New setting."}]}]}/>

收集每个查询的 [query_metric_log](../../operations/system-tables/query_metric_log.md) 的间隔（毫秒）。

如果设置为任何负值，将使用 [query_metric_log 设置](/operations/server-configuration-parameters/settings#query_metric_log) 中的 `collect_interval_milliseconds` 取值，如果没有，则默认为1000。

要禁用单个查询的收集，将 `query_metric_log_interval` 设置为 0。

默认值: -1

## query_plan_aggregation_in_order {#query_plan_aggregation_in_order} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "22.12"},{"label": "1"},{"label": "Enable some refactoring around query plan"}]}]}/>

切换按顺序聚合的查询计划级优化。
仅在设置 [`query_plan_enable_optimizations`](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置在未来可能会以不向后兼容的方式发生更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_convert_any_join_to_semi_or_anti_join {#query_plan_convert_any_join_to_semi_or_anti_join} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.9"},{"label": "1"},{"label": "New setting."}]}]}/>

允许将 ANY JOIN 转换为 SEMI 或 ANTI JOIN，如果 JOIN 后的过滤器始终对未匹配或匹配的行评估为 false。

## query_plan_convert_join_to_in {#query_plan_convert_join_to_in} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "0"},{"label": "New setting"}]}]}/>

允许将 `JOIN` 转换为带 `IN` 的子查询，如果输出列仅与左表相关。 可能导致非 ANY JOIN 的错误结果（例如，ALL JOIN，后者是默认情况）。

## query_plan_convert_outer_join_to_inner_join {#query_plan_convert_outer_join_to_inner_join} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.4"},{"label": "1"},{"label": "Allow to convert OUTER JOIN to INNER JOIN if filter after JOIN always filters default values"}]}]}/>

如果 JOIN 后的过滤器始终过滤默认值，则允许将 `OUTER JOIN` 转换为 `INNER JOIN`。

## query_plan_direct_read_from_text_index {#query_plan_direct_read_from_text_index} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.9"},{"label": "1"},{"label": "New setting."}]}]}/>

允许在查询计划中仅使用倒排索引执行全文搜索过滤。

## query_plan_display_internal_aliases {#query_plan_display_internal_aliases} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.9"},{"label": "0"},{"label": "New setting"}]}]}/>

在 EXPLAIN PLAN 中显示内部别名（如 __table1），而不是原始查询中指定的那些。

## query_plan_enable_multithreading_after_window_functions {#query_plan_enable_multithreading_after_window_functions} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用在评估窗口函数后进行多线程处理以允许并行流处理。

## query_plan_enable_optimizations {#query_plan_enable_optimizations} 

<SettingsInfoBlock type="Bool" default_value="1" />

切换查询计划级别的查询优化。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置在未来可能会以不向后兼容的方式发生更改或被移除。
:::

可能的值：

- 0 - 禁用查询计划级别的所有优化
- 1 - 启用查询计划级别的优化（但个别优化仍可以通过其个别设置禁用）。

## query_plan_execute_functions_after_sorting {#query_plan_execute_functions_after_sorting} 

<SettingsInfoBlock type="Bool" default_value="1" />

切换查询计划级别的优化，将表达式移动到排序步骤后面。
仅在设置 [`query_plan_enable_optimizations`](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置在未来可能会以不向后兼容的方式发生更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_filter_push_down {#query_plan_filter_push_down} 

<SettingsInfoBlock type="Bool" default_value="1" />

切换查询计划级别的优化，该优化将过滤器向下移动到执行计划中。
仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置在未来可能会以不向后兼容的方式发生更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_join_shard_by_pk_ranges {#query_plan_join_shard_by_pk_ranges} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "0"},{"label": "New setting"}]}]}/>

如果连接键包含两个表的主键前缀，则在连接时应用分片。 支持散列、平行散列和全排序合并算法。 通常不会加速查询，但可能会降低内存消耗。

## query_plan_join_swap_table {#query_plan_join_swap_table} 

<SettingsInfoBlock type="BoolAuto" default_value="auto" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.12"},{"label": "auto"},{"label": "New setting. Right table was always chosen before."}]}]}/>

确定连接的一侧应成为构建表（也称为内侧，在哈希连接中将其插入哈希表中的表）在查询计划中。 此设置仅支持使用 `JOIN ON` 子句的 `ALL` 连接严格性。可能的值：

- 'auto': 让规划器决定使用哪个表作为构建表。
- 'false': 绝不交换表（右表是构建表）。
- 'true': 始终交换表（左表是构建表）。

## query_plan_lift_up_array_join {#query_plan_lift_up_array_join} 

<SettingsInfoBlock type="Bool" default_value="1" />

切换查询计划级别的优化，将 ARRAY JOIN 提升到执行计划中。
仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置在未来可能会以不向后兼容的方式发生更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_lift_up_union {#query_plan_lift_up_union} 

<SettingsInfoBlock type="Bool" default_value="1" />

切换查询计划级别的优化，将更大的子树移动到联合中，以启用进一步的优化。
仅在设置 [`query_plan_enable_optimizations`](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置在未来可能会以不向后兼容的方式发生更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_max_limit_for_lazy_materialization {#query_plan_max_limit_for_lazy_materialization} 

<SettingsInfoBlock type="UInt64" default_value="10" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "10"},{"label": "Added new setting to control maximum limit value that allows to use query plan for lazy materialization optimisation. If zero, there is no limit"}]}]}/>

控制允许使用查询计划进行懒惰物化优化的最大限制值。如果为零，则没有限制。

## query_plan_max_optimizations_to_apply {#query_plan_max_optimizations_to_apply} 

<SettingsInfoBlock type="UInt64" default_value="10000" />

限制应用于查询计划的优化总数，详细信息见设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations)。
有助于避免复杂查询的长期优化时间。
在 EXPLAIN PLAN 查询中，达到此限制后停止应用优化，并按原样返回计划。
对于常规查询执行，如果实际应用的优化数量超过此设置，将抛出异常。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置在未来可能会以不向后兼容的方式发生更改或被移除。
:::

## query_plan_max_step_description_length {#query_plan_max_step_description_length} 

<SettingsInfoBlock type="UInt64" default_value="500" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.9"},{"label": "500"},{"label": "New setting"}]}]}/>

EXPLAIN PLAN 中步骤描述的最大长度。

## query_plan_merge_expressions {#query_plan_merge_expressions} 

<SettingsInfoBlock type="Bool" default_value="1" />

切换查询计划级别的优化，合并连续的过滤器。
仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置在未来可能会以不向后兼容的方式发生更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_merge_filter_into_join_condition {#query_plan_merge_filter_into_join_condition} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "1"},{"label": "Added new setting to merge filter into join condition"}]}]}/>

允许将过滤器合并到 `JOIN` 条件中，并将 `CROSS JOIN` 转换为 `INNER`。

## query_plan_merge_filters {#query_plan_merge_filters} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.7"},{"label": "0"},{"label": "Allow to merge filters in the query plan"}]}, {"id": "row-2","items": [{"label": "24.11"},{"label": "1"},{"label": "Allow to merge filters in the query plan. This is required to properly support filter-push-down with a new analyzer."}]}]}/>

允许在查询计划中合并过滤器。

## query_plan_optimize_join_order_limit {#query_plan_optimize_join_order_limit} 

<SettingsInfoBlock type="UInt64" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.9"},{"label": "0"},{"label": "New setting"}]}]}/>

在相同子查询内优化连接顺序。 当前仅支持非常有限的情况。
值是要优化的最大表数。

## query_plan_optimize_lazy_materialization {#query_plan_optimize_lazy_materialization} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "1"},{"label": "Added new setting to use query plan for lazy materialization optimisation"}]}]}/>

使用查询计划进行懒惰物化优化。

## query_plan_optimize_prewhere {#query_plan_optimize_prewhere} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "1"},{"label": "Allow to push down filter to PREWHERE expression for supported storages"}]}]}/>

允许将过滤器向下推送到支持的存储的 PREWHERE 表达式。

## query_plan_push_down_limit {#query_plan_push_down_limit} 

<SettingsInfoBlock type="Bool" default_value="1" />

切换查询计划级别的优化，将 LIMIT 向下移动到执行计划中。
仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置在未来可能会以不向后兼容的方式发生更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_read_in_order {#query_plan_read_in_order} 

<SettingsInfoBlock type="Bool" default_value="1" />

切换按顺序读取优化的查询计划级别的优化。
仅在设置 [`query_plan_enable_optimizations`](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置在未来可能会以不向后兼容的方式发生更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_remove_redundant_distinct {#query_plan_remove_redundant_distinct} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.2"},{"label": "1"},{"label": "Remove redundant Distinct step in query plan"}]}]}/>

切换查询计划级别的优化，删除冗余的 DISTINCT 步骤。
仅在设置 [`query_plan_enable_optimizations`](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置在未来可能会以不向后兼容的方式发生更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_remove_redundant_sorting {#query_plan_remove_redundant_sorting} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.1"},{"label": "1"},{"label": "Remove redundant sorting in query plan. For example, sorting steps related to ORDER BY clauses in subqueries"}]}]}/>

切换查询计划级别的优化，删除冗余的排序步骤，例如在子查询中。
仅在设置 [`query_plan_enable_optimizations`](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置在未来可能会以不向后兼容的方式发生更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_reuse_storage_ordering_for_window_functions {#query_plan_reuse_storage_ordering_for_window_functions} 

<SettingsInfoBlock type="Bool" default_value="1" />

切换查询计划级别的优化，在排序窗口函数时使用存储排序。
仅在设置 [`query_plan_enable_optimizations`](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置在未来可能会以不向后兼容的方式发生更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_split_filter {#query_plan_split_filter} 

<SettingsInfoBlock type="Bool" default_value="1" />

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置在未来可能会以不向后兼容的方式发生更改或被移除。
:::

切换查询计划级别的优化，将过滤器拆分为表达式。
仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_try_use_vector_search {#query_plan_try_use_vector_search} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "1"},{"label": "New setting."}]}]}/>

切换查询计划级别的优化，尝试使用向量相似度索引。
仅在设置 [`query_plan_enable_optimizations`](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置在未来可能会以不向后兼容的方式发生更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_use_new_logical_join_step {#query_plan_use_new_logical_join_step} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.2"},{"label": "1"},{"label": "Enable new step"}]}, {"id": "row-2","items": [{"label": "25.1"},{"label": "0"},{"label": "New join step, internal change"}]}]}/>

在查询计划中使用逻辑连接步骤。
注意: 设置 `query_plan_use_new_logical_join_step` 已被弃用，请使用 `query_plan_use_logical_join_step` 替代。

## query_profiler_cpu_time_period_ns {#query_profiler_cpu_time_period_ns} 

<SettingsInfoBlock type="UInt64" default_value="1000000000" />

设置 [查询分析器](../../operations/optimizing-performance/sampling-query-profiler.md) 的 CPU 时钟计时器周期。该计时器仅计算 CPU 时间。

可能的值：

- 正整数（以纳秒为单位）。

    推荐值：

            - 10000000（每秒 100 次）纳秒及以上用于单个查询。
            - 1000000000（每秒一次）用于集群范围的分析。

- 0 用于关闭计时器。

**在 ClickHouse Cloud 中暂时禁用。**

另请参见：

- 系统表 [trace_log](/operations/system-tables/trace_log)

## query_profiler_real_time_period_ns {#query_profiler_real_time_period_ns} 

<SettingsInfoBlock type="UInt64" default_value="1000000000" />

设置 [查询分析器](../../operations/optimizing-performance/sampling-query-profiler.md) 的真实时钟计时器周期。真实时钟计时器计算实际时间。

可能的值：

- 正整数（以纳秒为单位）。

    推荐值：

            - 10000000（每秒 100 次）纳秒及以下用于单个查询。
            - 1000000000（每秒一次）用于集群范围的分析。

- 0 用于关闭计时器。

**在 ClickHouse Cloud 中暂时禁用。**

另请参见：

- 系统表 [trace_log](/operations/system-tables/trace_log)

## queue_max_wait_ms {#queue_max_wait_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="0" />

请求队列的等待时间，如果并发请求的数量超过最大值。

## rabbitmq_max_wait_ms {#rabbitmq_max_wait_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="5000" />

从 RabbitMQ 读取的等待时间，直到重试。

## read_backoff_max_throughput {#read_backoff_max_throughput} 

<SettingsInfoBlock type="UInt64" default_value="1048576" />

在读取缓慢时减少线程数量的设置。 计数事件，当读取带宽低于该数量字节/秒时。

## read_backoff_min_concurrency {#read_backoff_min_concurrency} 

<SettingsInfoBlock type="UInt64" default_value="1" />

在读取缓慢时尝试保持最小线程数量的设置。

## read_backoff_min_events {#read_backoff_min_events} 

<SettingsInfoBlock type="UInt64" default_value="2" />

在读取缓慢时减少线程数量的设置。 触发减少线程数量的事件数。

## read_backoff_min_interval_between_events_ms {#read_backoff_min_interval_between_events_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="1000" />

在读取缓慢时减少线程数量的设置。 如果前一个事件经过的时间少于一定时间，则不关注该事件。

## read_backoff_min_latency_ms {#read_backoff_min_latency_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="1000" />

在读取缓慢时减少线程数量的设置。 仅关注至少花费如此多时间的读取。

## read_from_filesystem_cache_if_exists_otherwise_bypass_cache {#read_from_filesystem_cache_if_exists_otherwise_bypass_cache} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许在被动模式下使用文件系统缓存 - 从现有缓存条目中受益，但不再将新的条目放入缓存中。 如果您为重型临时查询设置此设置，并为短实时查询保持禁用，这将避免过重查询导致的缓存颠簸，并提高整体系统效率。

## read_from_page_cache_if_exists_otherwise_bypass_cache {#read_from_page_cache_if_exists_otherwise_bypass_cache} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "0"},{"label": "Added userspace page cache"}]}]}/>

以被动模式使用用户空间页面缓存，类似于 read_from_filesystem_cache_if_exists_otherwise_bypass_cache。

## read_in_order_two_level_merge_threshold {#read_in_order_two_level_merge_threshold} 

<SettingsInfoBlock type="UInt64" default_value="100" />

在按主键顺序读取时，运行初步合并步骤所需读取的最小部分数量。

## read_in_order_use_buffering {#read_in_order_use_buffering} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.7"},{"label": "1"},{"label": "Use buffering before merging while reading in order of primary key"}]}]}/>

在按主键顺序读取时，在合并之前使用缓存。它增加了查询执行的并行性。

## read_in_order_use_virtual_row {#read_in_order_use_virtual_row} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "0"},{"label": "Use virtual row while reading in order of primary key or its monotonic function fashion. It is useful when searching over multiple parts as only relevant ones are touched."}]}]}/>

在按主键或其单调函数顺序读取时使用虚拟行。 当遍历多个部分时，这很有用，因为只会触及相关部分。

## read_overflow_mode {#read_overflow_mode} 

<SettingsInfoBlock type="OverflowMode" default_value="throw" />

超出限制时该如何处理。

## read_overflow_mode_leaf {#read_overflow_mode_leaf} 

<SettingsInfoBlock type="OverflowMode" default_value="throw" />

设置当读取的数据量超过其中一个叶子限制时会发生什么。

可能的选项：
- `throw`: 抛出异常（默认）。
- `break`: 停止执行查询并返回部分结果。
## read_priority {#read_priority} 

<SettingsInfoBlock type="Int64" default_value="0" />

从本地文件系统或远程文件系统读取数据的优先级。仅支持用于本地文件系统的 'pread_threadpool' 方法和用于远程文件系统的 `threadpool` 方法。

## read_through_distributed_cache {#read_through_distributed_cache} 

<CloudOnlyBadge/>

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "0"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。允许从分布式缓存中读取。

## readonly {#readonly} 

<SettingsInfoBlock type="UInt64" default_value="0" />

0 - 没有只读限制。 1 - 仅允许读取请求，以及改变显式允许的设置。 2 - 仅允许读取请求，以及改变设置，但不包括 'readonly' 设置。

## receive_data_timeout_ms {#receive_data_timeout_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="2000" />

接收来自副本的第一段数据或带有正向进程的数据包的连接超时。

## receive_timeout {#receive_timeout} 

<SettingsInfoBlock type="Seconds" default_value="300" />

从网络接收数据的超时，单位为秒。如果在此时间段内未接收到字节，则会抛出异常。如果在客户端设置了该设置，相应的服务器连接端口的套接字'send_timeout'也将被设置。

## regexp_max_matches_per_row {#regexp_max_matches_per_row} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

设置每行单个正则表达式的最大匹配数。可用于在使用贪婪正则表达式的 [extractAllGroupsHorizontal](/sql-reference/functions/string-search-functions#extractallgroupshorizontal) 函数中防止内存过载。

可能的值：

- 正整数。

## reject_expensive_hyperscan_regexps {#reject_expensive_hyperscan_regexps} 

<SettingsInfoBlock type="Bool" default_value="1" />

拒绝在使用 hyperscan 时评估可能会很昂贵的模式（由于 NFA 状态膨胀）。

## remerge_sort_lowered_memory_bytes_ratio {#remerge_sort_lowered_memory_bytes_ratio} 

<SettingsInfoBlock type="Float" default_value="2" />

如果在重新合并后内存使用量未降低到该比率，则将禁用重新合并。

## remote_filesystem_read_method {#remote_filesystem_read_method} 

<SettingsInfoBlock type="String" default_value="threadpool" />

从远程文件系统读取数据的方法之一：read、threadpool。

## remote_filesystem_read_prefetch {#remote_filesystem_read_prefetch} 

<SettingsInfoBlock type="Bool" default_value="1" />

在从远程文件系统读取数据时应使用预取。

## remote_fs_read_backoff_max_tries {#remote_fs_read_backoff_max_tries} 

<SettingsInfoBlock type="UInt64" default_value="5" />

最大读取尝试次数（带有回退）。

## remote_fs_read_max_backoff_ms {#remote_fs_read_max_backoff_ms} 

<SettingsInfoBlock type="UInt64" default_value="10000" />

尝试从远程磁盘读取数据时的最大等待时间。

## remote_read_min_bytes_for_seek {#remote_read_min_bytes_for_seek} 

<SettingsInfoBlock type="UInt64" default_value="4194304" />

进行寻址所需的远程读取 (url, s3) 的最少字节，代替使用 ignore 读取。

## rename_files_after_processing {#rename_files_after_processing} 

- **类型：** 字符串

- **默认值：** 空字符串

此设置允许指定由 `file` 表函数处理的文件的重命名模式。当选项被设置时，所有通过 `file` 表函数读取的文件将在处理成功时根据指定的模式和占位符重命名。

### 占位符

- `%a` — 完整的原始文件名（例如，“sample.csv”）。
- `%f` — 不带扩展名的原始文件名（例如，“sample”）。
- `%e` — 带点的原始文件扩展名（例如，“.csv”）。
- `%t` — 时间戳（以微秒为单位）。
- `%%` — 百分号 ("%")。

### 示例
- 选项： `--rename_files_after_processing="processed_%f_%t%e"`

- 查询： `SELECT * FROM file('sample.csv')`

如果读取 `sample.csv` 成功，文件将被重命名为 `processed_sample_1683473210851438.csv`。

## replace_running_query {#replace_running_query} 

<SettingsInfoBlock type="Bool" default_value="0" />

在使用 HTTP 接口时，可以传递 'query_id' 参数。这个参数是任何作为查询标识符的字符串。如果同一用户的同一 'query_id' 的查询已在运行，行为将取决于 'replace_running_query' 参数。

`0`（默认） – 抛出异常（如果同一 'query_id' 的查询已在运行，则不允许查询运行）。

`1` – 取消旧查询并开始运行新查询。

将此参数设置为 1，以实现对分段条件的建议。在输入下一个字符后，如果旧查询尚未完成，应该取消它。

## replace_running_query_max_wait_ms {#replace_running_query_max_wait_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="5000" />

当 [replace_running_query](#replace_running_query) 设置处于活动状态时，等待相同 `query_id` 的查询完成的等待时间。

可能的值：

- 正整数。
- 0 — 在服务器中执行相同 `query_id` 的新查询时抛出异常。

## replication_wait_for_inactive_replica_timeout {#replication_wait_for_inactive_replica_timeout} 

<SettingsInfoBlock type="Int64" default_value="120" />

指定等待非活动副本执行 [`ALTER`](../../sql-reference/statements/alter/index.md)、[`OPTIMIZE`](../../sql-reference/statements/optimize.md) 或 [`TRUNCATE`](../../sql-reference/statements/truncate.md) 查询的时间（以秒为单位）。

可能的值：

- `0` — 不等待。
- 负整数 — 等待无限时间。
- 正整数 — 等待的秒数。

## restore_replace_external_dictionary_source_to_null {#restore_replace_external_dictionary_source_to_null} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "0"},{"label": "New setting."}]}]}/>

在恢复时将外部字典源替换为 Null。对测试有用。

## restore_replace_external_engines_to_null {#restore_replace_external_engines_to_null} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.8"},{"label": "0"},{"label": "New setting."}]}]}/>

用于测试目的。将所有外部引擎替换为 Null，以避免启动外部连接。

## restore_replace_external_table_functions_to_null {#restore_replace_external_table_functions_to_null} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.8"},{"label": "0"},{"label": "New setting."}]}]}/>

用于测试目的。将所有外部表函数替换为 Null，以避免启动外部连接。

## restore_replicated_merge_tree_to_shared_merge_tree {#restore_replicated_merge_tree_to_shared_merge_tree} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.2"},{"label": "0"},{"label": "New setting."}]}]}/>

在 RESTORE 期间将表引擎从 Replicated*MergeTree 替换为 Shared*MergeTree。

## result_overflow_mode {#result_overflow_mode} 

<SettingsInfoBlock type="OverflowMode" default_value="throw" />

Cloud 默认值： `throw`

设置结果量超过其中一个限制时的处理方式。

可能的值：
- `throw`：抛出异常（默认）。
- `break`：停止执行查询并返回部分结果，仿佛源数据已经耗尽。

使用 'break' 类似于使用 LIMIT。`Break` 仅在块级别中中断执行。这意味着返回的行数大于
[`max_result_rows`](/operations/settings/settings#max_result_rows)、[`max_block_size`](/operations/settings/settings#max_block_size) 的倍数，并取决于 [`max_threads`](/operations/settings/settings#max_threads)。

**示例**

```sql title="Query"
SET max_threads = 3, max_block_size = 3333;
SET max_result_rows = 3334, result_overflow_mode = 'break';

SELECT *
FROM numbers_mt(100000)
FORMAT Null;
```

```text title="Result"
6666 rows in set. ...
```

## rewrite_count_distinct_if_with_count_distinct_implementation {#rewrite_count_distinct_if_with_count_distinct_implementation} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.8"},{"label": "1"},{"label": "Rewrite countDistinctIf with count_distinct_implementation configuration"}]}]}/>

允许您将 `countDistcintIf` 重写为 [count_distinct_implementation](#count_distinct_implementation) 设置。

可能的值：

- true — 允许。
- false — 不允许。

## rewrite_in_to_join {#rewrite_in_to_join} 

<ExperimentalBadge/>

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.10"},{"label": "0"},{"label": "New experimental setting"}]}]}/>

重写表达式，例如 'x IN subquery' 为 JOIN。这可能对于优化整个查询的连接重排序有用。

## s3_allow_multipart_copy {#s3_allow_multipart_copy} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.2"},{"label": "1"},{"label": "New setting."}]}]}/>

允许在 S3 中进行多部分复制。

## s3_allow_parallel_part_upload {#s3_allow_parallel_part_upload} 

<SettingsInfoBlock type="Bool" default_value="1" />

为 S3 多部分上传使用多个线程。可能会导致稍微更高的内存使用。

## s3_check_objects_after_upload {#s3_check_objects_after_upload} 

<SettingsInfoBlock type="Bool" default_value="0" />

使用头请求检查每个已上传对象以确保上传成功。

## s3_connect_timeout_ms {#s3_connect_timeout_ms} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "1000"},{"label": "Introduce new dedicated setting for s3 connection timeout"}]}]}/>

来自 s3 磁盘的主机的连接超时。

## s3_create_new_file_on_insert {#s3_create_new_file_on_insert} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在每次插入到 s3 引擎表时创建一个新文件。如果启用，则每次插入都会创建一个新的 S3 对象，其键类似于以下模式：

初始: `data.Parquet.gz` -> `data.1.Parquet.gz` -> `data.2.Parquet.gz` 等等。

可能的值：
- 0 — `INSERT` 查询创建一个新文件，或者在文件已存在且未设置 s3_truncate_on_insert 时失败。
- 1 — `INSERT` 查询在每次插入时创建一个新文件，使用后缀（从第二个开始），如果 s3_truncate_on_insert 未设置。

更多详细信息 [在这里](/integrations/s3#inserting-data)。

## s3_disable_checksum {#s3_disable_checksum} 

<SettingsInfoBlock type="Bool" default_value="0" />

向 S3 发送文件时不计算校验和。这通过避免文件上的过度处理传递来加速写入。这大体上是安全的，因为 MergeTree 表的数据是由 ClickHouse 检查的，并且当以 HTTPS 访问 S3 时，TLS 层在通过网络传输时已经提供了完整性。虽然在 S3 上进行额外的校验和提供了深层防御。

## s3_ignore_file_doesnt_exist {#s3_ignore_file_doesnt_exist} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "0"},{"label": "Allow to return 0 rows when the requested files don't exist instead of throwing an exception in S3 table engine"}]}]}/>

如果在读取某些键时文件不存在，则忽略文件缺失。

可能的值：
- 1 — `SELECT` 返回空结果。
- 0 — `SELECT` 抛出异常。

## s3_list_object_keys_size {#s3_list_object_keys_size} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

ListObject 请求中批量返回的最大文件数。

## s3_max_connections {#s3_max_connections} 

<SettingsInfoBlock type="UInt64" default_value="1024" />

每个服务器的最大连接数。

## s3_max_get_burst {#s3_max_get_burst} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在达到每秒请求限制之前可以同时发出的最大请求数。默认值为 (0) 等于 `s3_max_get_rps`。

## s3_max_get_rps {#s3_max_get_rps} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在达到限制之前的 S3 GET 请求每秒速率限制。零表示无限制。

## s3_max_inflight_parts_for_one_file {#s3_max_inflight_parts_for_one_file} 

<SettingsInfoBlock type="UInt64" default_value="20" />

在多部分上传请求中同时加载的部分的最大数量。0表示无限制。

## s3_max_part_number {#s3_max_part_number} 

<SettingsInfoBlock type="UInt64" default_value="10000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "10000"},{"label": "Maximum part number number for s3 upload part"}]}]}/>

S3 上传部分的最大部分编号。

## s3_max_put_burst {#s3_max_put_burst} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在达到每秒请求限制之前可以同时发出的最大请求数。默认值为 (0) 等于 `s3_max_put_rps`。

## s3_max_put_rps {#s3_max_put_rps} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在达到限制之前的 S3 PUT 请求每秒速率限制。零表示无限制。

## s3_max_single_operation_copy_size {#s3_max_single_operation_copy_size} 

<SettingsInfoBlock type="UInt64" default_value="33554432" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "33554432"},{"label": "Maximum size for a single copy operation in s3"}]}]}/>

S3 中单次操作复制的最大大小。仅在 s3_allow_multipart_copy 为 true 的情况下使用此设置。

## s3_max_single_part_upload_size {#s3_max_single_part_upload_size} 

<SettingsInfoBlock type="UInt64" default_value="33554432" />

使用单个部分上传到 S3 的对象的最大大小。

## s3_max_single_read_retries {#s3_max_single_read_retries} 

<SettingsInfoBlock type="UInt64" default_value="4" />

单次 S3 读取期间的最大重试次数。

## s3_max_unexpected_write_error_retries {#s3_max_unexpected_write_error_retries} 

<SettingsInfoBlock type="UInt64" default_value="4" />

在 S3 写入期间发生意外错误时的最大重试次数。

## s3_max_upload_part_size {#s3_max_upload_part_size} 

<SettingsInfoBlock type="UInt64" default_value="5368709120" />

多部分上传至 S3 的最大部分大小。

## s3_min_upload_part_size {#s3_min_upload_part_size} 

<SettingsInfoBlock type="UInt64" default_value="16777216" />

多部分上传至 S3 的最小部分大小。

## s3_request_timeout_ms {#s3_request_timeout_ms} 

<SettingsInfoBlock type="UInt64" default_value="30000" />

向 S3 发送和接收数据的空闲超时。如果单次 TCP 读取或写入调用阻塞时间超过此时长，则失败。

## s3_skip_empty_files {#s3_skip_empty_files} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "1"},{"label": "We hope it will provide better UX"}]}]}/>

启用或禁用在 [S3](../../engines/table-engines/integrations/s3.md) 引擎表中跳过空文件。

可能的值：
- 0 — 如果空文件与请求的格式不兼容，`SELECT` 将抛出异常。
- 1 — 对于空文件，`SELECT` 返回空结果。

## s3_slow_all_threads_after_network_error {#s3_slow_all_threads_after_network_error} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "1"},{"label": "New setting"}]}]}/>

设置为 `true` 时，任何单个 S3 请求遇到可重试网络错误（例如套接字超时）后，所有执行 S3 请求的线程都会变慢。设置为 `false` 时，每个线程独立处理 S3 请求的回退。

## s3_slow_all_threads_after_retryable_error {#s3_slow_all_threads_after_retryable_error} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.9"},{"label": "0"},{"label": "Added an alias for setting `backup_slow_all_threads_after_retryable_s3_error`"}]}, {"id": "row-2","items": [{"label": "25.8"},{"label": "0"},{"label": "Added an alias for setting `backup_slow_all_threads_after_retryable_s3_error`"}]}, {"id": "row-3","items": [{"label": "25.6"},{"label": "0"},{"label": "Added an alias for setting `backup_slow_all_threads_after_retryable_s3_error`"}]}, {"id": "row-4","items": [{"label": "25.10"},{"label": "0"},{"label": "Disable the setting by default"}]}]}/>

设置为 `true` 时，任何单个 S3 请求遇到可重试 S3 错误（例如 'Slow Down'）后，所有执行 S3 请求的线程都会变慢。设置为 `false` 时，每个线程独立处理 S3 请求的回退。

## s3_strict_upload_part_size {#s3_strict_upload_part_size} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在对 S3 进行多部分上传时，上传部分的确切大小（某些实现不支持可变大小部分）。

## s3_throw_on_zero_files_match {#s3_throw_on_zero_files_match} 

<SettingsInfoBlock type="Bool" default_value="0" />

当 ListObjects 请求无法匹配到文件时抛出错误。

## s3_truncate_on_insert {#s3_truncate_on_insert} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在 S3 引擎表中进行插入前的截断。如果禁用，在插入尝试时，如果 S3 对象已存在，将抛出异常。

可能的值：
- 0 — `INSERT` 查询创建一个新文件，或者在文件已存在且未设置 s3_create_new_file_on_insert 时失败。
- 1 — `INSERT` 查询用新数据替换文件的现有内容。

更多详细信息 [在这里](/integrations/s3#inserting-data)。

## s3_upload_part_size_multiply_factor {#s3_upload_part_size_multiply_factor} 

<SettingsInfoBlock type="UInt64" default_value="2" />

在从单个写入 S3 上传了 s3_multiply_parts_count_threshold 个部分时，每次将 s3_min_upload_part_size 乘以该因子。

## s3_upload_part_size_multiply_parts_count_threshold {#s3_upload_part_size_multiply_parts_count_threshold} 

<SettingsInfoBlock type="UInt64" default_value="500" />

每次上传到 S3 的部分数达到该数字时，s3_min_upload_part_size 乘以 s3_upload_part_size_multiply_factor。

## s3_use_adaptive_timeouts {#s3_use_adaptive_timeouts} 

<SettingsInfoBlock type="Bool" default_value="1" />

设置为 `true` 时，所有 S3 请求的前两次尝试都在低发送和接收超时中进行。设置为 `false` 时，所有尝试都在相同的超时时间内进行。

## s3_validate_request_settings {#s3_validate_request_settings} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "1"},{"label": "Allow to disable S3 request settings validation"}]}]}/>

启用 S3 请求设置验证。
可能的值：
- 1 — 验证设置。
- 0 — 不验证设置。

## s3queue_default_zookeeper_path {#s3queue_default_zookeeper_path} 

<SettingsInfoBlock type="String" default_value="/clickhouse/s3queue/" />

S3Queue 引擎的默认 zookeeper 路径前缀。

## s3queue_enable_logging_to_s3queue_log {#s3queue_enable_logging_to_s3queue_log} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用写入 system.s3queue_log。可以通过表设置覆盖该值。

## s3queue_migrate_old_metadata_to_buckets {#s3queue_migrate_old_metadata_to_buckets} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0"},{"label": "New setting."}]}]}/>

将 S3Queue 表的旧元数据结构迁移到新结构。

## schema_inference_cache_require_modification_time_for_url {#schema_inference_cache_require_modification_time_for_url} 

<SettingsInfoBlock type="Bool" default_value="1" />

在 URL 的最后修改时间验证中使用缓存中的模式（对于带有 Last-Modified 头的 URL）。

## schema_inference_use_cache_for_azure {#schema_inference_use_cache_for_azure} 

<SettingsInfoBlock type="Bool" default_value="1" />

在使用 Azure 表函数时，使用模式推断中的缓存。

## schema_inference_use_cache_for_file {#schema_inference_use_cache_for_file} 

<SettingsInfoBlock type="Bool" default_value="1" />

在使用文件表函数时，使用模式推断中的缓存。

## schema_inference_use_cache_for_hdfs {#schema_inference_use_cache_for_hdfs} 

<SettingsInfoBlock type="Bool" default_value="1" />

在使用 HDFS 表函数时，使用模式推断中的缓存。

## schema_inference_use_cache_for_s3 {#schema_inference_use_cache_for_s3} 

<SettingsInfoBlock type="Bool" default_value="1" />

在使用 S3 表函数时，使用模式推断中的缓存。

## schema_inference_use_cache_for_url {#schema_inference_use_cache_for_url} 

<SettingsInfoBlock type="Bool" default_value="1" />

在使用 URL 表函数时，使用模式推断中的缓存。

## secondary_indices_enable_bulk_filtering {#secondary_indices_enable_bulk_filtering} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "1"},{"label": "A new algorithm for filtering by data skipping indices"}]}]}/>

启用索引的批量过滤算法。预计总是更好，但我们保留此设置以便于兼容性和控制。

## select_sequential_consistency {#select_sequential_consistency} 

:::note
此设置在 SharedMergeTree 和 ReplicatedMergeTree 之间的行为不同，详情请参阅 [SharedMergeTree 一致性](/cloud/reference/shared-merge-tree#consistency)。
:::

启用或禁用 `SELECT` 查询的顺序一致性。要求禁用 `insert_quorum_parallel`（默认启用）。

可能的值：

- 0 — 禁用。
- 1 — 启用。

用法

当启用顺序一致性时，ClickHouse 仅允许客户端对来自所有先前使用 `insert_quorum` 执行的 `INSERT` 查询的副本执行 `SELECT` 查询。如果客户端引用了部分副本，ClickHouse 将生成异常。SELECT 查询将不包括尚未写入副本仲裁的副本中的数据。

当启用 `insert_quorum_parallel`（默认值）时，`select_sequential_consistency` 不起作用。这是因为并行的 `INSERT` 查询可以写入不同集合的法定副本，因此无法保证单个副本会接收所有写入。

另请参见：

- [insert_quorum](#insert_quorum)
- [insert_quorum_timeout](#insert_quorum_timeout)
- [insert_quorum_parallel](#insert_quorum_parallel)

## send_logs_level {#send_logs_level} 

发送带有指定最低级别的服务器文本日志到客户端。有效值：'trace'，'debug'，'information'，'warning'，'error'，'fatal'，'none'。

## send_logs_source_regexp {#send_logs_source_regexp} 

发送带有指定正则表达式以匹配日志源名称的服务器文本日志。空表示所有来源。

## send_progress_in_http_headers {#send_progress_in_http_headers} 

启用或禁用 `clickhouse-server` 响应中的 `X-ClickHouse-Progress` HTTP 响应头。

有关更多信息，请参阅 [HTTP 接口描述](../../interfaces/http.md)。

可能的值：

- 0 — 禁用。
- 1 — 启用。

## send_timeout {#send_timeout} 

在网络中发送数据的超时，单位为秒。如果客户端需要发送一些数据但在此时间段内无法发送任何字节，则会抛出异常。如果在客户端设置了此设置，则套接字的'receive_timeout'也将设置在服务器上相应的连接端。

## serialize_query_plan {#serialize_query_plan} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "0"},{"label": "NewSetting"}]}]}/>

序列化查询计划以便于分布式处理。

## session_timezone {#session_timezone} 

<BetaBadge/>

设置当前会话或查询的隐式时区。
隐式时区应用于没有显式指定时区的 DateTime/DateTime64 类型的值。
该设置优先于全局配置的（服务器级）隐式时区。
空字符串值表示当前会话或查询的隐式时区等于 [服务器时区](../server-configuration-parameters/settings.md/#timezone)。

您可以使用函数 `timeZone()` 和 `serverTimeZone()` 获取会话时区和服务器时区。

可能的值：

- 任何 `system.time_zones` 中的时区名称，例如 `Europe/Berlin`、`UTC` 或 `Zulu`。

示例：

```sql
SELECT timeZone(), serverTimeZone() FORMAT CSV

"Europe/Berlin","Europe/Berlin"
```

```sql
SELECT timeZone(), serverTimeZone() SETTINGS session_timezone = 'Asia/Novosibirsk' FORMAT CSV

"Asia/Novosibirsk","Europe/Berlin"
```

将会话时区 'America/Denver' 分配给内含的没有显式指定时区的 DateTime：

```sql
SELECT toDateTime64(toDateTime64('1999-12-12 23:23:23.123', 3), 3, 'Europe/Zurich') SETTINGS session_timezone = 'America/Denver' FORMAT TSV

1999-12-13 07:23:23.123
```

:::warning
并非所有解析 DateTime/DateTime64 的函数都遵循 `session_timezone`。这可能会导致微妙的错误。
请参见以下示例和说明。
:::

```sql
CREATE TABLE test_tz (`d` DateTime('UTC')) ENGINE = Memory AS SELECT toDateTime('2000-01-01 00:00:00', 'UTC');

SELECT *, timeZone() FROM test_tz WHERE d = toDateTime('2000-01-01 00:00:00') SETTINGS session_timezone = 'Asia/Novosibirsk'
0 rows in set.

SELECT *, timeZone() FROM test_tz WHERE d = '2000-01-01 00:00:00' SETTINGS session_timezone = 'Asia/Novosibirsk'
┌───────────────────d─┬─timeZone()───────┐
│ 2000-01-01 00:00:00 │ Asia/Novosibirsk │
└─────────────────────┴──────────────────┘
```

这是由于不同的解析管道造成的：

- 第一个 `SELECT` 查询中不带显式给出的时区的 `toDateTime()` 尊重 `session_timezone` 和全局时区。
- 在第二个查询中，从字符串解析的 DateTime 继承现有列 `d` 的类型和时区。因此，设置 `session_timezone` 和全局时区将不被遵循。

**另请参见**

- [timezone](../server-configuration-parameters/settings.md/#timezone)

## set_overflow_mode {#set_overflow_mode} 

设置当数据量超过某个限制时的处理方式。

可能的值：
- `throw`：抛出异常（默认）。
- `break`：停止执行查询并返回部分结果，就好像源数据已经耗尽。

## shared_merge_tree_sync_parts_on_partition_operations {#shared_merge_tree_sync_parts_on_partition_operations} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.12"},{"label": "1"},{"label": "New setting. By default parts are always synchronized"}]}]}/>

在 SMT 表中 MOVE|REPLACE|ATTACH 分区操作后自动同步数据部分的集合。仅限 Cloud。

## short_circuit_function_evaluation {#short_circuit_function_evaluation} 

允许根据短路方案优化执行这些函数的复杂表达式，并防止可能的异常（例如在意外情况下的除零）。

可能的值：

- `enable` — 为适合的函数启用短路函数评估（可能抛出异常或计算负担重）。
- `force_enable` — 对所有函数启用短路函数评估。
- `disable` — 禁用短路函数评估。

## short_circuit_function_evaluation_for_nulls {#short_circuit_function_evaluation_for_nulls} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "1"},{"label": "Allow to execute functions with Nullable arguments only on rows with non-NULL values in all arguments"}]}]}/>

优化在任何参数为 NULL 时返回 NULL 的函数的评估。当函数参数中 NULL 值的百分比超过 short_circuit_function_evaluation_for_nulls_threshold 时，系统将跳过逐行评估该函数。相反，该行将立即返回 NULL，避免不必要的计算。

## short_circuit_function_evaluation_for_nulls_threshold {#short_circuit_function_evaluation_for_nulls_threshold} 

<SettingsInfoBlock type="Double" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "1"},{"label": "Ratio threshold of NULL values to execute functions with Nullable arguments only on rows with non-NULL values in all arguments. Applies when setting short_circuit_function_evaluation_for_nulls is enabled."}]}]}/>

仅在所有参数中都包含非 NULL 值的行上执行带有 Nullable 参数的函数的 NULL 值比率阈值。当包含 NULL 值的行数占总行数的比例超过此阈值时，将不评估这些包含 NULL 值的行。

## show_data_lake_catalogs_in_system_tables {#show_data_lake_catalogs_in_system_tables} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "1"},{"label": "New setting"}]}]}/>

启用在系统表中显示数据湖目录。

## show_table_uuid_in_table_create_query_if_not_nil {#show_table_uuid_in_table_create_query_if_not_nil} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "20.7"},{"label": "0"},{"label": "Stop showing  UID of the table in its CREATE query for Engine=Atomic"}]}]}/>

设置 `SHOW TABLE` 查询的显示。

可能的值：

- 0 — 查询将不显示表 UUID。
- 1 — 查询将显示表 UUID。

## single_join_prefer_left_table {#single_join_prefer_left_table} 

在单个 JOIN 情况下，如遇标识符歧义时，优先选择左侧表。

## skip_redundant_aliases_in_udf {#skip_redundant_aliases_in_udf} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.12"},{"label": "0"},{"label": "When enabled, this allows you to use the same user defined function several times for several materialized columns in the same table."}]}]}/>

在用户定义的函数中不使用（替代）多余的别名，以简化其使用。

可能的值：

- 1 — 在 UDF 中跳过（替代）别名。
- 0 — 不在 UDF 中跳过（替代）别名。

**示例**

启用与禁用之间的差异：

查询：

```sql
SET skip_redundant_aliases_in_udf = 0;
CREATE FUNCTION IF NOT EXISTS test_03274 AS ( x ) -> ((x + 1 as y, y + 2));

EXPLAIN SYNTAX SELECT test_03274(4 + 2);
```

结果：

```text
SELECT ((4 + 2) + 1 AS y, y + 2)
```

查询：

```sql
SET skip_redundant_aliases_in_udf = 1;
CREATE FUNCTION IF NOT EXISTS test_03274 AS ( x ) -> ((x + 1 as y, y + 2));

EXPLAIN SYNTAX SELECT test_03274(4 + 2);
```

结果：

```text
SELECT ((4 + 2) + 1, ((4 + 2) + 1) + 2)
```

## skip_unavailable_shards {#skip_unavailable_shards} 

启用或禁用静默跳过不可用的分片。

如果所有副本均不可用，则分片被视为不可用。副本不可用的情况包括：

- ClickHouse 无法因任何原因连接到副本。

    在连接到副本时，ClickHouse 会进行多次尝试。如果所有尝试均失败，则该副本被视为不可用。

- 无法通过 DNS 解析副本。

    如果无法通过 DNS 解析副本的主机名，可能表示以下几种情况：

    - 副本主机没有 DNS 记录。这可能发生在动态 DNS 系统中，例如 [Kubernetes](https://kubernetes.io)，在此期间节点可能无法解析，并且这不是错误。

    - 配置错误。ClickHouse 配置文件包含错误的主机名。

可能的值：

- 1 — 启用跳过。

    如果一个分片不可用，ClickHouse 返回基于部分数据的结果，不报告节点可用性问题。

- 0 — 禁用跳过。

    如果一个分片不可用，ClickHouse 抛出异常。

## sleep_after_receiving_query_ms {#sleep_after_receiving_query_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="0" />

在 TCPHandler 中接收查询后睡眠的时间。

## sleep_in_send_data_ms {#sleep_in_send_data_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="0" />

在 TCPHandler 中发送数据时的睡眠时间。

## sleep_in_send_tables_status_ms {#sleep_in_send_tables_status_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="0" />

在 TCPHandler 中发送表状态响应时的睡眠时间。

## sort_overflow_mode {#sort_overflow_mode} 

<SettingsInfoBlock type="OverflowMode" default_value="throw" />

设置在排序前接收到的行数超过某个限制时发生的情况。

可能的值：
- `throw`：抛出异常。
- `break`：停止执行查询并返回部分结果。

## split_intersecting_parts_ranges_into_layers_final {#split_intersecting_parts_ranges_into_layers_final} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "1"},{"label": "Allow to split intersecting parts ranges into layers during FINAL optimization"}]}, {"id": "row-2","items": [{"label": "24.1"},{"label": "1"},{"label": "Allow to split intersecting parts ranges into layers during FINAL optimization"}]}]}/>

在 FINAL 优化期间将相交的部分范围拆分成层。

## split_parts_ranges_into_intersecting_and_non_intersecting_final {#split_parts_ranges_into_intersecting_and_non_intersecting_final} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "1"},{"label": "Allow to split parts ranges into intersecting and non intersecting during FINAL optimization"}]}, {"id": "row-2","items": [{"label": "24.1"},{"label": "1"},{"label": "Allow to split parts ranges into intersecting and non intersecting during FINAL optimization"}]}]}/>

在 FINAL 优化期间将部分范围拆分为相交和不相交部分。

## splitby_max_substrings_includes_remaining_string {#splitby_max_substrings_includes_remaining_string} 

控制带有参数 `max_substrings` > 0 的函数 [splitBy*()](../../sql-reference/functions/splitting-merging-functions.md) 是否将在结果数组的最后一个元素中包含剩余字符串。

可能的值：

- `0` - 剩余字符串不会包含在结果数组的最后一个元素中。
- `1` - 剩余字符串将包括在结果数组的最后一个元素中。这是 Spark 的 [`split()`](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.split.html) 函数和 Python 的 ['string.split()'](https://docs.python.org/3/library/stdtypes.html#str.split) 方法的行为。

## stop_refreshable_materialized_views_on_startup {#stop_refreshable_materialized_views_on_startup} 

<ExperimentalBadge/>

<SettingsInfoBlock type="Bool" default_value="0" />

在服务器启动时，防止安排可刷新的物化视图，仿佛使用 SYSTEM STOP VIEWS。您可以在此后手动启动它们，使用 `SYSTEM START VIEWS` 或 `SYSTEM START VIEW <name>`。这也适用于新创建的视图。对不可刷新的物化视图没有影响。

## storage_file_read_method {#storage_file_read_method} 

<SettingsInfoBlock type="LocalFSReadMethod" default_value="pread" />

从存储文件读取数据的方法之一：`read`、`pread`、`mmap`。mmap 方法不适用于 clickhouse-server（它用于 clickhouse-local）。

## storage_system_stack_trace_pipe_read_timeout_ms {#storage_system_stack_trace_pipe_read_timeout_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="100" />

从管道读取信息以接收线程信息的最大时间，在查询 `system.stack_trace` 表时。此设置用于测试目的，不应由用户更改。

## stream_flush_interval_ms {#stream_flush_interval_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="7500" />

对具有流动性的表，在超时情况下，或者当线程生成 [max_insert_block_size](#max_insert_block_size) 行时起作用。

默认值为 7500。

值越小，数据越频繁地刷新到表中。将该值设置得过低会导致性能下降。

## stream_like_engine_allow_direct_select {#stream_like_engine_allow_direct_select} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "21.12"},{"label": "0"},{"label": "Do not allow direct select for Kafka/RabbitMQ/FileLog by default"}]}]}/>

允许对 Kafka、RabbitMQ、FileLog、Redis Streams 和 NATS 引擎进行直接 SELECT 查询。如果有附加的物化视图，即使启用了此设置也不允许进行 SELECT 查询。

## stream_like_engine_insert_queue {#stream_like_engine_insert_queue} 

当流状引擎从多个队列中读取时，用户需要在写入时选择一个队列进行插入。由 Redis Streams 和 NATS 使用。

## stream_poll_timeout_ms {#stream_poll_timeout_ms} 

从/到流存储轮询数据的超时。
## system_events_show_zero_values {#system_events_show_zero_values} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许从 [`system.events`](../../operations/system-tables/events.md) 中选择零值事件。

一些监控系统要求在每个检查点将所有指标值传递给它们，即使指标值为零。

可能的值：

- 0 — 禁用。
- 1 — 启用。

**示例**

查询

```sql
SELECT * FROM system.events WHERE event='QueryMemoryLimitExceeded';
```

结果

```text
Ok.
```

查询
```sql
SET system_events_show_zero_values = 1;
SELECT * FROM system.events WHERE event='QueryMemoryLimitExceeded';
```

结果

```text
┌─event────────────────────┬─value─┬─description───────────────────────────────────────────┐
│ QueryMemoryLimitExceeded │     0 │ Number of times when memory limit exceeded for query. │
└──────────────────────────┴───────┴───────────────────────────────────────────────────────┘
```

## table_engine_read_through_distributed_cache {#table_engine_read_through_distributed_cache} 

<CloudOnlyBadge/>

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.7"},{"label": "0"},{"label": "New setting"}]}]}/>

仅在 ClickHouse Cloud 中有效。允许通过表引擎 / 表函数 (s3, azure 等) 从分布式缓存读取。

## table_function_remote_max_addresses {#table_function_remote_max_addresses} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

设置通过 [remote](../../sql-reference/table-functions/remote.md) 函数从模式生成的最大地址数量。

可能的值：

- 正整数。

## tcp_keep_alive_timeout {#tcp_keep_alive_timeout} 

<SettingsInfoBlock type="Seconds" default_value="290" />

连接需要保持空闲的秒数，才会开始发送 TCP 保活探测。

## temporary_data_in_cache_reserve_space_wait_lock_timeout_milliseconds {#temporary_data_in_cache_reserve_space_wait_lock_timeout_milliseconds} 

<SettingsInfoBlock type="UInt64" default_value="600000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.4"},{"label": "600000"},{"label": "Wait time to lock cache for space reservation in temporary data in filesystem cache"}]}]}/>

为临时数据在文件系统缓存中锁定空间预留的等待时间。

## temporary_files_codec {#temporary_files_codec} 

<SettingsInfoBlock type="String" default_value="LZ4" />

设置在磁盘上的排序和连接操作中使用的临时文件的压缩编解码器。

可能的值：

- LZ4 — 应用 [LZ4](https://en.wikipedia.org/wiki/LZ4_(compression_algorithm)) 压缩。
- NONE — 不应用压缩。

## text_index_use_bloom_filter {#text_index_use_bloom_filter} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.9"},{"label": "1"},{"label": "New setting."}]}]}/>

用于测试目的，启用或禁用在文本索引中使用布隆过滤器。

## throw_if_deduplication_in_dependent_materialized_views_enabled_with_async_insert {#throw_if_deduplication_in_dependent_materialized_views_enabled_with_async_insert} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "1"},{"label": "Deduplication in dependent materialized view cannot work together with async inserts."}]}]}/>

当设置 `deduplicate_blocks_in_dependent_materialized_views` 与 `async_insert` 一起启用时，在 INSERT 查询时抛出异常。它保证了正确性，因为这两个功能不能一起工作。

## throw_if_no_data_to_insert {#throw_if_no_data_to_insert} 

<SettingsInfoBlock type="Bool" default_value="1" />

允许或禁止空 INSERT，默认启用（在空插入时抛出错误）。仅适用于使用 [`clickhouse-client`](/interfaces/cli) 或使用 [gRPC 接口](/interfaces/grpc) 的 INSERT。

## throw_on_error_from_cache_on_write_operations {#throw_on_error_from_cache_on_write_operations} 

<SettingsInfoBlock type="Bool" default_value="0" />

在写操作（INSERT, 合并）时忽略缓存中的错误。

## throw_on_max_partitions_per_insert_block {#throw_on_max_partitions_per_insert_block} 

<SettingsInfoBlock type="Bool" default_value="1" />

允许控制达到 `max_partitions_per_insert_block` 时的行为。

可能的值：
- `true`  - 当插入块达到 `max_partitions_per_insert_block` 时，抛出异常。
- `false` - 当达到 `max_partitions_per_insert_block` 时记录警告。

:::tip
如果您试图理解更改 [`max_partitions_per_insert_block`](/operations/settings/settings#max_partitions_per_insert_block) 对用户的影响，这可能会很有用。
:::

## throw_on_unsupported_query_inside_transaction {#throw_on_unsupported_query_inside_transaction} 

<ExperimentalBadge/>

<SettingsInfoBlock type="Bool" default_value="1" />

如果在事务内部使用不支持的查询则抛出异常。

## timeout_before_checking_execution_speed {#timeout_before_checking_execution_speed} 

<SettingsInfoBlock type="Seconds" default_value="10" />

在指定的秒数到期后检查执行速度是否太慢（不低于 `min_execution_speed`）。

## timeout_overflow_mode {#timeout_overflow_mode} 

<SettingsInfoBlock type="OverflowMode" default_value="throw" />

设置查询运行时间超过 `max_execution_time` 或预估运行时间超过 `max_estimated_execution_time` 时该如何处理。

可能的值：
- `throw`: 抛出异常（默认）。
- `break`: 停止执行查询并返回部分结果，就像源数据用尽一样。

## timeout_overflow_mode_leaf {#timeout_overflow_mode_leaf} 

<SettingsInfoBlock type="OverflowMode" default_value="throw" />

设置当叶子节点中的查询运行时间超过 `max_execution_time_leaf` 时发生什么。

可能的值：
- `throw`: 抛出异常（默认）。
- `break`: 停止执行查询并返回部分结果，就像源数据用尽一样。

## totals_auto_threshold {#totals_auto_threshold} 

<SettingsInfoBlock type="Float" default_value="0.5" />

`totals_mode = 'auto'` 的阈值。请参见 "WITH TOTALS 修饰符" 一节。

## totals_mode {#totals_mode} 

在存在 HAVING 的情况下以及当 `max_rows_to_group_by` 和 `group_by_overflow_mode = 'any'` 存在时，如何计算 TOTALS。请参见 "WITH TOTALS 修饰符" 一节。

## trace_profile_events {#trace_profile_events} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在每次更新配置文件事件时收集堆栈跟踪及配置文件事件的名称和增量值，并将它们发送到 [trace_log](/operations/system-tables/trace_log)。

可能的值：

- 1 — 启用配置文件事件跟踪。
- 0 — 禁用配置文件事件跟踪。

## transfer_overflow_mode {#transfer_overflow_mode} 

<SettingsInfoBlock type="OverflowMode" default_value="throw" />

设置当数据量超过某个限制时会发生什么。

可能的值：
- `throw`: 抛出异常（默认）。
- `break`: 停止执行查询并返回部分结果，就像源数据用尽一样。

## transform_null_in {#transform_null_in} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用 [NULL](/sql-reference/syntax#null) 值在 [IN](../../sql-reference/operators/in.md) 操作符中的相等性比较。

默认情况下，`NULL` 值无法进行比较，因为 `NULL` 代表未定义的值。因此，比较 `expr = NULL` 必须始终返回 `false`。使用此设置时，`NULL = NULL` 在 `IN` 操作符中返回 `true`。

可能的值：

- 0 — 在 `IN` 操作符中比较 `NULL` 值返回 `false`。
- 1 — 在 `IN` 操作符中比较 `NULL` 值返回 `true`。

**示例**

考虑 `null_in` 表：

```text
┌──idx─┬─────i─┐
│    1 │     1 │
│    2 │  NULL │
│    3 │     3 │
└──────┴───────┘
```

查询：

```sql
SELECT idx, i FROM null_in WHERE i IN (1, NULL) SETTINGS transform_null_in = 0;
```

结果：

```text
┌──idx─┬────i─┐
│    1 │    1 │
└──────┴──────┘
```

查询：

```sql
SELECT idx, i FROM null_in WHERE i IN (1, NULL) SETTINGS transform_null_in = 1;
```

结果：

```text
┌──idx─┬─────i─┐
│    1 │     1 │
│    2 │  NULL │
└──────┴───────┘
```

**另见**

- [IN 操作符中的 NULL 处理](/sql-reference/operators/in#null-processing)

## traverse_shadow_remote_data_paths {#traverse_shadow_remote_data_paths} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "0"},{"label": "Traverse shadow directory when query system.remote_data_paths."}]}]}/>

遍历冻结数据（影子目录）以及实际表数据时的查询 `system.remote_data_paths`。

## union_default_mode {#union_default_mode} 

设置 `SELECT` 查询结果组合的模式。仅在与 [UNION](../../sql-reference/statements/select/union.md) 共享时使用，且未显式指定 `UNION ALL` 或 `UNION DISTINCT`。

可能的值：

- `'DISTINCT'` — ClickHouse 在合并查询时输出去重后的行。
- `'ALL'` — ClickHouse 输出合并查询时的所有行，包括重复行。
- `''` — 如果与 `UNION` 一起使用，将生成异常。

请在 [UNION](../../sql-reference/statements/select/union.md) 中查看示例。

## unknown_packet_in_send_data {#unknown_packet_in_send_data} 

<SettingsInfoBlock type="UInt64" default_value="0" />

发送未知数据包代替第 N 个数据包。

## update_parallel_mode {#update_parallel_mode} 

<SettingsInfoBlock type="UpdateParallelMode" default_value="auto" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "auto"},{"label": "A new setting"}]}]}/>

确定并发更新查询的行为。

可能的值：
- `sync` - 顺序运行所有 `UPDATE` 查询。
- `auto` - 仅顺序运行在一个查询中更新的列与另一个查询中的表达式使用的列之间存在依赖关系的 `UPDATE` 查询。
- `async` - 不同步更新查询。

## update_sequential_consistency {#update_sequential_consistency} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "1"},{"label": "A new setting"}]}]}/>

如果为真，则在执行更新前将部分集更新至最新版本。

## use_async_executor_for_materialized_views {#use_async_executor_for_materialized_views} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.12"},{"label": "0"},{"label": "New setting."}]}]}/>

使用异步和可能的多线程执行物化视图查询，可以在插入期间加速视图处理，但也会消耗更多内存。

## use_cache_for_count_from_files {#use_cache_for_count_from_files} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用在表函数 `file`/`s3`/`url`/`hdfs`/`azureBlobStorage` 中从文件计数时的行数缓存。

默认启用。

## use_client_time_zone {#use_client_time_zone} 

使用客户端时区解释 DateTime 字符串值，而不是采用服务器时区。

## use_compact_format_in_distributed_parts_names {#use_compact_format_in_distributed_parts_names} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "21.1"},{"label": "1"},{"label": "Use compact format for async INSERT into Distributed tables by default"}]}]}/>

在 `Distributed` 引擎的表中，后台 (`distributed_foreground_insert`) INSERT 存储块时使用紧凑格式。

可能的值：

- 0 — 使用 `user[:password]@host:port#default_database` 目录格式。
- 1 — 使用 `[shard{shard_index}[_replica{replica_index}]]` 目录格式。

:::note
- 当 `use_compact_format_in_distributed_parts_names=0` 时，集群定义中的更改将不应用于后台 INSERT。
- 当 `use_compact_format_in_distributed_parts_names=1` 时，更改集群定义中的节点顺序将更改 `shard_index`/`replica_index`，请注意。

:::

## use_concurrency_control {#use_concurrency_control} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.12"},{"label": "1"},{"label": "Enable concurrency control by default"}]}]}/>

遵守服务器的并发控制（见 `concurrent_threads_soft_limit_num` 和 `concurrent_threads_soft_limit_ratio_to_cores` 全局服务器设置）。如果禁用，则允许使用更多线程，即使服务器过载（不建议在正常使用中，主要用于测试）。

## use_hedged_requests {#use_hedged_requests} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "21.9"},{"label": "1"},{"label": "Enable Hedged Requests feature by default"}]}]}/>

为远程查询启用 hedged 请求逻辑。它允许为查询建立与不同副本的多个连接。
如果在 `hedged_connection_timeout` 内未建立与副本的现有连接，或者在 `receive_data_timeout` 内未收到数据，则启用新连接。查询使用发送非空进度数据包（或数据包，如果 `allow_changing_replica_until_first_data_packet`）的第一个连接；其他连接将被取消。支持 `max_parallel_replicas > 1` 的查询。

默认启用。

云中的默认值： `1`

## use_hive_partitioning {#use_hive_partitioning} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "1"},{"label": "Enabled the setting by default."}]}, {"id": "row-2","items": [{"label": "24.8"},{"label": "0"},{"label": "Allows to use hive partitioning for File, URL, S3, AzureBlobStorage and HDFS engines."}]}]}/>

当启用时，ClickHouse 将在类似文件的表引擎 [File](/sql-reference/table-functions/file#hive-style-partitioning)/[S3](/sql-reference/table-functions/s3#hive-style-partitioning)/[URL](/sql-reference/table-functions/url#hive-style-partitioning)/[HDFS](/sql-reference/table-functions/hdfs#hive-style-partitioning)/[AzureBlobStorage](/sql-reference/table-functions/azureBlobStorage#hive-style-partitioning) 的路径中检测类似 Hive 的分区 (`/name=value/`)，并允许在查询中使用分区列作为虚拟列。这些虚拟列将具有与分区路径中相同的名称，但以 `_` 开头。

## use_iceberg_metadata_files_cache {#use_iceberg_metadata_files_cache} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "1"},{"label": "New setting"}]}]}/>

如果开启，冰山表函数和冰山存储可能利用冰山元数据文件缓存。

可能的值：

- 0 - 禁用
- 1 - 启用

## use_iceberg_partition_pruning {#use_iceberg_partition_pruning} 

<SettingsInfoBlock type="Bool" default_value="1" />

使用冰山表的分区修剪。

## use_index_for_in_with_subqueries {#use_index_for_in_with_subqueries} 

如果 IN 操作符的右侧有子查询或表表达式，则尝试使用索引。

## use_index_for_in_with_subqueries_max_values {#use_index_for_in_with_subqueries_max_values} 

<SettingsInfoBlock type="UInt64" default_value="0" />

IN 操作符右侧集合的最大大小，以使用表索引进行过滤。它可以避免由于准备大型查询所需的额外数据结构而导致的性能下降和更高内存使用。零意味着没有限制。

## use_join_disjunctions_push_down {#use_join_disjunctions_push_down} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用将 JOIN 条件中与 OR 连接的部分下推到相应输入端（“部分下推”）。
这允许存储引擎更早过滤，从而可以减少读取的数据量。
该优化保持语义的，但仅在每个顶层 OR 分支至少对目标侧贡献一个确定性谓词时应用。

## use_json_alias_for_old_object_type {#use_json_alias_for_old_object_type} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.8"},{"label": "0"},{"label": "Use JSON type alias to create new JSON type"}]}]}/>

启用时，将使用 `JSON` 数据类型别名创建旧 [Object('json')](../../sql-reference/data-types/json.md) 类型，而不是新的 [JSON](../../sql-reference/data-types/newjson.md) 类型。

## use_legacy_to_time {#use_legacy_to_time} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.6"},{"label": "1"},{"label": "New setting. Allows for user to use the old function logic for toTime, which works as toTimeWithFixedDate."}]}]}/>

当启用时，允许使用遗留的 toTime 函数，该函数将带时间的日期转换为某个固定日期，同时保留时间。
否则，使用新的 toTime 函数，它将不同类型的数据转换为时间类型。
旧的遗留函数在任何情况下也可以无条件访问，作为 toTimeWithFixedDate。

## use_page_cache_for_disks_without_file_cache {#use_page_cache_for_disks_without_file_cache} 

<SettingsInfoBlock type="Bool" default_value="0" />

对没有启用文件系统缓存的远程磁盘使用用户空间页缓存。

## use_page_cache_with_distributed_cache {#use_page_cache_with_distributed_cache} 

<SettingsInfoBlock type="Bool" default_value="0" />

在使用分布式缓存时使用用户空间页缓存。

## use_query_cache {#use_query_cache} 

如果开启，`SELECT` 查询可能利用 [查询缓存](../query-cache.md)。参数 [enable_reads_from_query_cache](#enable_reads_from_query_cache) 和 [enable_writes_to_query_cache](#enable_writes_to_query_cache) 更详细地控制缓存的使用。

可能的值：

- 0 - 禁用
- 1 - 启用

## use_query_condition_cache {#use_query_condition_cache} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用 [查询条件缓存](/operations/query-condition-cache)。缓存存储不满足 `WHERE` 子句条件的数据部分中的聚合范围，并在后续查询中重用这些信息作为临时索引。

可能的值：

- 0 - 禁用
- 1 - 启用

## use_roaring_bitmap_iceberg_positional_deletes {#use_roaring_bitmap_iceberg_positional_deletes} 

<SettingsInfoBlock type="Bool" default_value="0" />

使用 roaring 位图进行冰山的定位删除。

## use_skip_indexes {#use_skip_indexes} 

在查询执行期间使用数据跳过索引。

可能的值：

- 0 — 禁用。
- 1 — 启用。

## use_skip_indexes_if_final {#use_skip_indexes_if_final} 

<SettingsInfoBlock type="Bool" default_value="1" />

控制在执行带有 FINAL 修饰符的查询时是否使用跳过索引。

跳过索引可能排除包含最新数据的行（颗粒），这可能导致带有 FINAL 修饰符的查询结果不正确。当此设置启用时，即使在使用 FINAL 修饰符的情况下，也会应用跳过索引，从而可能提高性能，但存在遗漏最近更新的风险。此设置应与设置 use_skip_indexes_if_final_exact_mode 同步启用（默认启用）。

可能的值：

- 0 — 禁用。
- 1 — 启用。

## use_skip_indexes_if_final_exact_mode {#use_skip_indexes_if_final_exact_mode} 

<SettingsInfoBlock type="Bool" default_value="1" />

控制当跳过索引返回的颗粒在较新部分中扩展时，在执行带有 FINAL 修饰符的查询时返回正确的结果。

使用跳过索引可能排除包含最新数据的行（颗粒），这可能导致结果不正确。此设置可以确保通过扫描与跳过索引返回的范围重叠的较新部分返回正确的结果。仅在应用程序对基于跳过索引查找的近似结果无所谓的情况下，才应禁用此设置。

可能的值：

- 0 — 禁用。
- 1 — 启用。

## use_skip_indexes_on_data_read {#use_skip_indexes_on_data_read} 

<SettingsInfoBlock type="Bool" default_value="1" />

在数据读取过程中启用使用数据跳过索引。

启用时，跳过索引会在读取每个数据颗粒时动态评估，而不是在查询执行开始前提前分析。这可以减少查询启动延迟。

可能的值：

- 0 — 禁用。
- 1 — 启用。

## use_structure_from_insertion_table_in_table_functions {#use_structure_from_insertion_table_in_table_functions} 

<SettingsInfoBlock type="UInt64" default_value="2" />

使用插入表的结构，而不是从数据推断的模式。可能的值： 0 - 禁用，1 - 启用，2 - 自动。

## use_uncompressed_cache {#use_uncompressed_cache} 

<SettingsInfoBlock type="Bool" default_value="0" />

是否使用未压缩块的缓存。接受 0 或 1。默认情况下，0（禁用）。
使用未压缩缓存（仅适用于 MergeTree 家族中的表）可以显著降低延迟并提高处理大量短查询时的吞吐量。对频繁发送短请求的用户启用此设置。还请注意配置参数 [uncompressed_cache_size](/operations/server-configuration-parameters/settings#uncompressed_cache_size)（仅在配置文件中设置）– 未压缩缓存块的大小。默认情况下为 8 GiB。未压缩缓存根据需要填充，并自动删除使用最少的数据。

对于至少读取一定量数据（百万行或更多）的查询，未压缩缓存会自动禁用，以节省真正小查询所需的空间。这意味着您可以将 'use_uncompressed_cache' 设置始终设置为 1。

## use_variant_as_common_type {#use_variant_as_common_type} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许在没有公共类型的情况下使用 `Variant` 类型作为 [if](../../sql-reference/functions/conditional-functions.md/#if)/[multiIf](../../sql-reference/functions/conditional-functions.md/#multiIf)/[array](../../sql-reference/functions/array-functions.md)/[map](../../sql-reference/functions/tuple-map-functions.md) 函数的结果类型。

示例：

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(if(number % 2, number, range(number))) as variant_type FROM numbers(1);
SELECT if(number % 2, number, range(number)) as variant FROM numbers(5);
```

```text
┌─variant_type───────────────────┐
│ Variant(Array(UInt64), UInt64) │
└────────────────────────────────┘
┌─variant───┐
│ []        │
│ 1         │
│ [0,1]     │
│ 3         │
│ [0,1,2,3] │
└───────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(multiIf((number % 4) = 0, 42, (number % 4) = 1, [1, 2, 3], (number % 4) = 2, 'Hello, World!', NULL)) AS variant_type FROM numbers(1);
SELECT multiIf((number % 4) = 0, 42, (number % 4) = 1, [1, 2, 3], (number % 4) = 2, 'Hello, World!', NULL) AS variant FROM numbers(4);
```

```text
─variant_type─────────────────────────┐
│ Variant(Array(UInt8), String, UInt8) │
└──────────────────────────────────────┘

┌─variant───────┐
│ 42            │
│ [1,2,3]       │
│ Hello, World! │
│ ᴺᵁᴸᴸ          │
└───────────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(array(range(number), number, 'str_' || toString(number))) as array_of_variants_type from numbers(1);
SELECT array(range(number), number, 'str_' || toString(number)) as array_of_variants FROM numbers(3);
```

```text
┌─array_of_variants_type────────────────────────┐
│ Array(Variant(Array(UInt64), String, UInt64)) │
└───────────────────────────────────────────────┘

┌─array_of_variants─┐
│ [[],0,'str_0']    │
│ [[0],1,'str_1']   │
│ [[0,1],2,'str_2'] │
└───────────────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(map('a', range(number), 'b', number, 'c', 'str_' || toString(number))) as map_of_variants_type from numbers(1);
SELECT map('a', range(number), 'b', number, 'c', 'str_' || toString(number)) as map_of_variants FROM numbers(3);
```

```text
┌─map_of_variants_type────────────────────────────────┐
│ Map(String, Variant(Array(UInt64), String, UInt64)) │
└─────────────────────────────────────────────────────┘

┌─map_of_variants───────────────┐
│ {'a':[],'b':0,'c':'str_0'}    │
│ {'a':[0],'b':1,'c':'str_1'}   │
│ {'a':[0,1],'b':2,'c':'str_2'} │
└───────────────────────────────┘
```

## use_with_fill_by_sorting_prefix {#use_with_fill_by_sorting_prefix} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.5"},{"label": "1"},{"label": "Columns preceding WITH FILL columns in ORDER BY clause form sorting prefix. Rows with different values in sorting prefix are filled independently"}]}]}/>

在 ORDER BY 子句中排在 WITH FILL 列之前的列形成排序前缀。排序前缀中具有不同值的行会独立填充。

## validate_enum_literals_in_operators {#validate_enum_literals_in_operators} 

<SettingsInfoBlock type="Bool" default_value="0" />

如果启用，验证操作符中如 `IN`、`NOT IN`、`==`、`!=` 的枚举字面量与枚举类型的一致性，如果字面量不是有效的枚举值，则抛出异常。

## validate_mutation_query {#validate_mutation_query} 

<SettingsInfoBlock type="Bool" default_value="1" />

在接受变更查询之前进行验证。变更在后台执行，运行无效查询将导致变更被卡住，需要手动干预。

只有在遇到不向后兼容的 bug 时才更改此设置。

## validate_polygons {#validate_polygons} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用或禁用在 [pointInPolygon](/sql-reference/functions/geo/coordinates#pointinpolygon) 函数中抛出异常，如果多边形自相交或自切。

可能的值：

- 0 — 禁用抛出异常。`pointInPolygon` 接受无效的多边形，并返回可能不正确的结果。
- 1 — 启用抛出异常。

## vector_search_filter_strategy {#vector_search_filter_strategy} 

<SettingsInfoBlock type="VectorSearchFilterStrategy" default_value="auto" />

如果向量搜索查询具有 WHERE 子句，此设置决定是先评估该子句（预过滤）还是首先检查向量相似度索引（后过滤）。可能的值：
- 'auto' - 后过滤（确切语义可能在未来改变）。
- 'postfilter' - 使用向量相似度索引确定最近邻，然后应用其他过滤条件。
- 'prefilter' - 先评估其他过滤条件，然后执行暴力搜索以识别邻近点。

## vector_search_index_fetch_multiplier {#vector_search_index_fetch_multiplier} 

<SettingsInfoBlock type="Float" default_value="1" />

将从向量相似度索引中获取的最近邻数量乘以该数字。仅在后过滤与其他谓词组合或设置 'vector_search_with_rescoring = 1' 时适用。

## vector_search_with_rescoring {#vector_search_with_rescoring} 

<SettingsInfoBlock type="Bool" default_value="0" />

如果 ClickHouse 对使用向量相似度索引的查询执行重新评分。
不进行重新评分时，向量相似度索引直接返回包含最佳匹配的行。
进行重新评分时，行被推断到颗粒级，颗粒中的所有行将再次被检查。
在大多数情况下，重新评分仅对准确性有微小帮助，但会显著降低向量搜索查询的性能。
注意：没有重新评分且启用并行副本的查询可能会回退至重新评分。

## wait_changes_become_visible_after_commit_mode {#wait_changes_become_visible_after_commit_mode} 

<ExperimentalBadge/>

<SettingsInfoBlock type="TransactionsWaitCSNMode" default_value="wait_unknown" />

等待已提交的更改在最新快照中实际可见。

## wait_for_async_insert {#wait_for_async_insert} 

如果为真，则等待异步插入处理完成。

## wait_for_async_insert_timeout {#wait_for_async_insert_timeout} 

等待异步插入处理的超时。

## wait_for_window_view_fire_signal_timeout {#wait_for_window_view_fire_signal_timeout} 

<ExperimentalBadge/>

<SettingsInfoBlock type="Seconds" default_value="10" />

等待事件时间处理中窗口视图触发信号的超时。

## window_view_clean_interval {#window_view_clean_interval} 

<ExperimentalBadge/>

<SettingsInfoBlock type="Seconds" default_value="60" />

窗口视图的清理间隔，单位为秒，以释放过时的数据。

## window_view_heartbeat_interval {#window_view_heartbeat_interval} 

<ExperimentalBadge/>

<SettingsInfoBlock type="Seconds" default_value="15" />

心跳间隔，单位为秒，以指示监视查询仍处于活动状态。

## workload {#workload} 

资源访问所使用的工作负载名称。

## write_full_path_in_iceberg_metadata {#write_full_path_in_iceberg_metadata} 

<ExperimentalBadge/>

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.8"},{"label": "0"},{"label": "New setting."}]}]}/>

将完整路径（包括 s3://）写入冰山元数据文件。

## write_through_distributed_cache {#write_through_distributed_cache} 

<CloudOnlyBadge/>

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "0"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。允许写入分布式缓存（写入 s3 也将通过分布式缓存完成）。

## write_through_distributed_cache_buffer_size {#write_through_distributed_cache_buffer_size} 

<CloudOnlyBadge/>

<SettingsInfoBlock type="UInt64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.7"},{"label": "0"},{"label": "New cloud setting"}]}]}/>

仅在 ClickHouse Cloud 中有效。设置写透分布式缓存的缓冲区大小。如果为 0，将使用如果没有分布式缓存将使用的缓冲区大小。

## zstd_window_log_max {#zstd_window_log_max} 

<SettingsInfoBlock type="Int64" default_value="0" />

允许选择 ZSTD 的最大窗口日志（不适用于 MergeTree 家族）。
