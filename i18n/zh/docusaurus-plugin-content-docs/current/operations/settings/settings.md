---
title: 会话设置
sidebar_label: 会话设置
slug: /operations/settings/settings
toc_max_heading_level: 2
description: 在 system.settings 表中找到的设置。
---

import ExperimentalBadge from '@theme/badges/ExperimentalBadge';
import BetaBadge from '@theme/badges/BetaBadge';
import CloudAvailableBadge from '@theme/badges/CloudAvailableBadge';

<!-- Autogenerated -->
以下所有设置也可以在表 [system.settings](/docs/operations/system-tables/settings) 中找到。这些设置是从 [source](https://github.com/ClickHouse/ClickHouse/blob/master/src/Core/Settings.cpp) 自动生成的。
## add_http_cors_header {#add_http_cors_header}



类型: Bool

默认值: 0

写入添加 HTTP CORS 头。
## additional_result_filter {#additional_result_filter}



类型: String

默认值: 

要应用于 `SELECT` 查询结果的额外过滤表达式。
此设置不适用于任何子查询。

**示例**

``` sql
INSERT INTO table_1 VALUES (1, 'a'), (2, 'bb'), (3, 'ccc'), (4, 'dddd');
SELECT * FROM table_1;
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 2 │ bb   │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
```sql
SELECT *
FROM table_1
SETTINGS additional_result_filter = 'x != 2'
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
## additional_table_filters {#additional_table_filters}



类型: Map

默认值: {}

从指定表读取后应用的额外过滤表达式。

**示例**

``` sql
INSERT INTO table_1 VALUES (1, 'a'), (2, 'bb'), (3, 'ccc'), (4, 'dddd');
SELECT * FROM table_1;
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 2 │ bb   │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
```sql
SELECT *
FROM table_1
SETTINGS additional_table_filters = {'table_1': 'x != 2'}
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
## aggregate_functions_null_for_empty {#aggregate_functions_null_for_empty}



类型: Bool

默认值: 0

启用或禁用在查询中重写所有聚合函数，向它们添加 [-OrNull](/sql-reference/aggregate-functions/combinators#-ornull) 后缀。为 SQL 标准兼容性启用它。
通过查询重写来实现（类似于 [count_distinct_implementation](#count_distinct_implementation) 设置），以获得分布式查询的一致结果。

可能值：

- 0 — 禁用。
- 1 — 启用。

**示例**

考虑以下带有聚合函数的查询：
```sql
SELECT SUM(-1), MAX(0) FROM system.one WHERE 0;
```

在 `aggregate_functions_null_for_empty = 0` 时，它将生成：
```text
┌─SUM(-1)─┬─MAX(0)─┐
│       0 │      0 │
└─────────┴────────┘
```

在 `aggregate_functions_null_for_empty = 1` 时，结果将是：
```text
┌─SUMOrNull(-1)─┬─MAXOrNull(0)─┐
│          NULL │         NULL │
└───────────────┴──────────────┘
```
## aggregation_in_order_max_block_bytes {#aggregation_in_order_max_block_bytes}



类型: UInt64

默认值: 50000000

在按主键的顺序聚合期间累积的块的最大大小（以字节为单位）。较小的块大小允许更多的最终合并阶段并行化聚合。
## aggregation_memory_efficient_merge_threads {#aggregation_memory_efficient_merge_threads}



类型: UInt64

默认值: 0

用于在内存高效模式下合并中间聚合结果的线程数。增加时，消耗更多内存。0 意味着——与 'max_threads' 相同。
## allow_aggregate_partitions_independently {#allow_aggregate_partitions_independently}



类型: Bool

默认值: 0

当分区键适合分组键时，启用在单独线程上独立聚合分区。当分区数量接近核心数量并且分区大小大致相同时，有利于此设置。
## allow_archive_path_syntax {#allow_archive_path_syntax}
<ExperimentalBadge/>


类型: Bool

默认值: 1

文件/S3 引擎/表函数将解析带有 '::' 的路径为 `<archive> :: <file>\`，如果归档具有正确的扩展名。
## allow_asynchronous_read_from_io_pool_for_merge_tree {#allow_asynchronous_read_from_io_pool_for_merge_tree}



类型: Bool

默认值: 0

使用后台 I/O 池从 MergeTree 表读取。此设置可能会提高 I/O 密集型查询的性能。
## allow_changing_replica_until_first_data_packet {#allow_changing_replica_until_first_data_packet}



类型: Bool

默认值: 0

如果启用，在冗余请求中，我们可以在接收第一个数据包之前开始新连接，即使我们已经取得了一些进展（但进展在 `receive_data_timeout` 超时内未更新），否则在第一次取得进展后，我们禁用更改副本。
## allow_create_index_without_type {#allow_create_index_without_type}



类型: Bool

默认值: 0

允许 CREATE INDEX 查询不带 TYPE。该查询将被忽略。为 SQL 兼容性测试而设置。
## allow_custom_error_code_in_throwif {#allow_custom_error_code_in_throwif}



类型: Bool

默认值: 0

启用 throwIf() 函数中的自定义错误代码。如果为 true，抛出的异常可能具有意外的错误代码。
## allow_ddl {#allow_ddl}



类型: Bool

默认值: 1

如果设置为 true，则用户可以执行 DDL 查询。
## allow_deprecated_database_ordinary {#allow_deprecated_database_ordinary}



类型: Bool

默认值: 0

允许使用过时的普通引擎创建数据库。
## allow_deprecated_error_prone_window_functions {#allow_deprecated_error_prone_window_functions}



类型: Bool

默认值: 0

允许使用过时的易出错窗口函数（neighbor, runningAccumulate, runningDifferenceStartingWithFirstValue, runningDifference）。
## allow_deprecated_snowflake_conversion_functions {#allow_deprecated_snowflake_conversion_functions}



类型: Bool

默认值: 0

函数 `snowflakeToDateTime`、`snowflakeToDateTime64`、`dateTimeToSnowflake` 和 `dateTime64ToSnowflake` 被弃用并默认禁用。
请使用函数 `snowflakeIDToDateTime`、`snowflakeIDToDateTime64`、`dateTimeToSnowflakeID` 和 `dateTime64ToSnowflakeID` 代替。

要重新启用被弃用的函数（例如，在过渡期间），请将此设置设为 `true`。
## allow_deprecated_syntax_for_merge_tree {#allow_deprecated_syntax_for_merge_tree}



类型: Bool

默认值: 0

允许使用已弃用的引擎定义语法创建 *MergeTree 表。
## allow_distributed_ddl {#allow_distributed_ddl}



类型: Bool

默认值: 1

如果设置为 true，则用户可以执行分布式 DDL 查询。
## allow_drop_detached {#allow_drop_detached}



类型: Bool

默认值: 0

允许 ALTER TABLE ... DROP DETACHED PART[ITION] ... 查询。
## allow_execute_multiif_columnar {#allow_execute_multiif_columnar}



类型: Bool

默认值: 1

允许以列式执行 multiIf 函数。
## allow_experimental_analyzer {#allow_experimental_analyzer}



类型: Bool

默认值: 1

允许新的查询分析器。
## allow_experimental_codecs {#allow_experimental_codecs}
<ExperimentalBadge/>


类型: Bool

默认值: 0

如果设置为 true，允许指定实验性压缩编解码器（但我们还没有这些功能，此选项不执行任何操作）。
## allow_experimental_database_iceberg {#allow_experimental_database_iceberg}
<ExperimentalBadge/>


类型: Bool

默认值: 0

允许实验性数据库引擎 Iceberg。
## allow_experimental_database_materialized_postgresql {#allow_experimental_database_materialized_postgresql}
<ExperimentalBadge/>


类型: Bool

默认值: 0

允许创建 Engine=MaterializedPostgreSQL(...) 的数据库。
## allow_experimental_dynamic_type {#allow_experimental_dynamic_type}
<BetaBadge/>


类型: Bool

默认值: 0

允许创建 [Dynamic](../../sql-reference/data-types/dynamic.md) 数据类型。
## allow_experimental_full_text_index {#allow_experimental_full_text_index}
<ExperimentalBadge/>


类型: Bool

默认值: 0

如果设置为 true，允许使用实验性全文索引。
## allow_experimental_funnel_functions {#allow_experimental_funnel_functions}
<ExperimentalBadge/>


类型: Bool

默认值: 0

启用实验性漏斗分析功能。
## allow_experimental_hash_functions {#allow_experimental_hash_functions}
<ExperimentalBadge/>


类型: Bool

默认值: 0

启用实验性哈希函数。
## allow_experimental_inverted_index {#allow_experimental_inverted_index}
<ExperimentalBadge/>


类型: Bool

默认值: 0

如果设置为 true，允许使用实验性倒排索引。
## allow_experimental_join_condition {#allow_experimental_join_condition}
<ExperimentalBadge/>


类型: Bool

默认值: 0

支持涉及左右两表列的非等条件连接。例如 `t1.y < t2.y`。
## allow_experimental_join_right_table_sorting {#allow_experimental_join_right_table_sorting}
<ExperimentalBadge/>


类型: Bool

默认值: 0

如果设置为 true，并且满足 `join_to_sort_minimum_perkey_rows` 和 `join_to_sort_maximum_table_rows` 的条件，则通过键对右表进行重新排序，以提高左连接或内部哈希连接的性能。
## allow_experimental_json_type {#allow_experimental_json_type}
<BetaBadge/>


类型: Bool

默认值: 0

允许创建 [JSON](../../sql-reference/data-types/newjson.md) 数据类型。
## allow_experimental_kafka_offsets_storage_in_keeper {#allow_experimental_kafka_offsets_storage_in_keeper}
<ExperimentalBadge/>


类型: Bool

默认值: 0

允许实验性功能将 Kafka 相关偏移存储在 ClickHouse Keeper 中。启用后，可以将 ClickHouse Keeper 路径和副本名称指定给 Kafka 表引擎。因此，与常规 Kafka 引擎不同，将使用一种新类型的存储引擎，主要将已提交的偏移存储在 ClickHouse Keeper 中。
## allow_experimental_kusto_dialect {#allow_experimental_kusto_dialect}
<ExperimentalBadge/>


类型: Bool

默认值: 0

启用 Kusto 查询语言 (KQL) - SQL 的替代方案。
## allow_experimental_live_view {#allow_experimental_live_view}
<ExperimentalBadge/>


类型: Bool

默认值: 0

允许创建弃用的实时视图。

可能值：

- 0 — 禁用实时视图的工作。
- 1 — 启用实时视图的工作。
## allow_experimental_materialized_postgresql_table {#allow_experimental_materialized_postgresql_table}
<ExperimentalBadge/>


类型: Bool

默认值: 0

允许使用 MaterializedPostgreSQL 表引擎。默认禁用，因为此功能是实验性的。
## allow_experimental_nlp_functions {#allow_experimental_nlp_functions}
<ExperimentalBadge/>


类型: Bool

默认值: 0

启用实验性自然语言处理功能。
## allow_experimental_object_type {#allow_experimental_object_type}
<ExperimentalBadge/>


类型: Bool

默认值: 0

允许过时的对象数据类型。
## allow_experimental_parallel_reading_from_replicas {#allow_experimental_parallel_reading_from_replicas}
<BetaBadge/>


类型: UInt64

默认值: 0

在执行 SELECT 查询时，从每个分片使用多达 `max_parallel_replicas` 的副本。读取通过协调动态并行化。0 - 禁用，1 - 启用，失败时默默禁用，2 - 启用，失败时抛出异常。
## allow_experimental_prql_dialect {#allow_experimental_prql_dialect}
<ExperimentalBadge/>


类型: Bool

默认值: 0

启用 PRQL - SQL 的替代方案。
## allow_experimental_query_deduplication {#allow_experimental_query_deduplication}
<ExperimentalBadge/>


类型: Bool

默认值: 0

基于部分 UUID 的 SELECT 查询实验性去重。
## allow_experimental_shared_set_join {#allow_experimental_shared_set_join}
<ExperimentalBadge/>

<CloudAvailableBadge/>

类型: Bool

默认值: 0

仅在 ClickHouse Cloud 中有效。允许创建 ShareSet 和 SharedJoin。
## allow_experimental_statistics {#allow_experimental_statistics}
<ExperimentalBadge/>


类型: Bool

默认值: 0

允许定义带有 [统计信息](../../engines/table-engines/mergetree-family/mergetree.md/#table_engine-mergetree-creating-a-table) 的列，并且 [操作统计信息](../../engines/table-engines/mergetree-family/mergetree.md/#column-statistics)。
## allow_experimental_time_series_table {#allow_experimental_time_series_table}
<ExperimentalBadge/>


类型: Bool

默认值: 0

允许创建带有 [TimeSeries](../../engines/table-engines/integrations/time-series.md) 表引擎的表。

可能值：

- 0 — 禁用 [TimeSeries](../../engines/table-engines/integrations/time-series.md) 表引擎。
- 1 — 启用 [TimeSeries](../../engines/table-engines/integrations/time-series.md) 表引擎。
## allow_experimental_ts_to_grid_aggregate_function {#allow_experimental_ts_to_grid_aggregate_function}
<ExperimentalBadge/>


类型: Bool

默认值: 0

实验性 tsToGrid 聚合函数，用于类似于 Prometheus 的时间序列重采样。仅在云中可用。
## allow_experimental_variant_type {#allow_experimental_variant_type}
<BetaBadge/>


类型: Bool

默认值: 0

允许创建 [Variant](../../sql-reference/data-types/variant.md) 数据类型。
## allow_experimental_vector_similarity_index {#allow_experimental_vector_similarity_index}
<ExperimentalBadge/>


类型: Bool

默认值: 0

允许实验性向量相似性索引。
## allow_experimental_window_view {#allow_experimental_window_view}
<ExperimentalBadge/>


类型: Bool

默认值: 0

启用窗口视图。还不够成熟。
## allow_general_join_planning {#allow_general_join_planning}



类型: Bool

默认值: 1

允许更通用的连接规划算法，可以处理更复杂的条件，但仅适用于哈希连接。如果哈希连接未启用，则使用常规连接规划算法，而不管此设置的值如何。
## allow_get_client_http_header {#allow_get_client_http_header}



类型: Bool

默认值: 0

允许使用函数 `getClientHTTPHeader`，该函数允许获取当前 HTTP 请求头的值。出于安全原因，默认情况下未启用此功能，因为某些标头（例如 `Cookie`）可能包含敏感信息。注意，`X-ClickHouse-*` 和 `Authentication` 标头始终受到限制，无法通过此函数获取。
## allow_hyperscan {#allow_hyperscan}



类型: Bool

默认值: 1

允许使用 Hyperscan 库的函数。禁用以避免潜在的长时间编译和过度资源使用。
## allow_introspection_functions {#allow_introspection_functions}



类型: Bool

默认值: 0

启用或禁用用于查询分析的 [自我检查函数](../../sql-reference/functions/introspection.md)。

可能值：

- 1 — 启用自我检查函数。
- 0 — 禁用自我检查函数。

**另请参阅**

- [采样查询分析器](../../operations/optimizing-performance/sampling-query-profiler.md)
- 系统表 [trace_log](/operations/system-tables/trace_log)
## allow_materialized_view_with_bad_select {#allow_materialized_view_with_bad_select}



类型: Bool

默认值: 1

允许使用引用不存在的表或列的 SELECT 查询创建物化视图。它仍然必须在语法上有效。不适用于可刷新的物化视图。如果 MV 模式必须从 SELECT 查询推断（即，如果 CREATE 没有列列表且没有 TO 表）。可用于在其源表之前创建 MV。
## allow_named_collection_override_by_default {#allow_named_collection_override_by_default}



类型: Bool

默认值: 1

默认允许覆盖命名集合的字段。
## allow_non_metadata_alters {#allow_non_metadata_alters}



类型: Bool

默认值: 1

允许执行不仅影响表元数据，还影响磁盘上数据的修改。
## allow_nonconst_timezone_arguments {#allow_nonconst_timezone_arguments}



类型: Bool

默认值: 0

在某些时间相关函数中允许非常量时区参数，例如 toTimeZone()、fromUnixTimestamp*()、snowflakeToDateTime*()。
## allow_nondeterministic_mutations {#allow_nondeterministic_mutations}



类型: Bool

默认值: 0

用户级设置，允许在复制表上使用非确定性函数进行变更，例如 `dictGet`。

例如，字典可能在节点之间不同步，因此默认情况下不允许通过它们拉取值的变更在复制表上进行。启用此设置允许这种行为，使用户有责任确保所使用的数据在所有节点之间保持同步。

**示例**

``` xml
<profiles>
    <default>
        <allow_nondeterministic_mutations>1</allow_nondeterministic_mutations>

        <!-- ... -->
    </default>

    <!-- ... -->

</profiles>
```
## allow_nondeterministic_optimize_skip_unused_shards {#allow_nondeterministic_optimize_skip_unused_shards}



类型: Bool

默认值: 0

允许在分片键中使用非确定性功能（如 `rand` 或 `dictGet`，因为后者在更新时存在一些注意事项）。

可能值：

- 0 — 不允许。
- 1 — 允许。
## allow_not_comparable_types_in_comparison_functions {#allow_not_comparable_types_in_comparison_functions}



类型: Bool

默认值: 0

允许或限制在比较函数 `equal/less/greater/etc` 中使用不可比较的类型（如 JSON/Object/AggregateFunction）。
## allow_not_comparable_types_in_order_by {#allow_not_comparable_types_in_order_by}



类型: Bool

默认值: 0

允许或限制在 ORDER BY 键中使用不可比较的类型（如 JSON/Object/AggregateFunction）。
## allow_prefetched_read_pool_for_local_filesystem {#allow_prefetched_read_pool_for_local_filesystem}



类型: Bool

默认值: 0

如果所有部分都在本地文件系统中，则优先使用预取的线程池。
## allow_prefetched_read_pool_for_remote_filesystem {#allow_prefetched_read_pool_for_remote_filesystem}



类型: Bool

默认值: 1

如果所有部分都在远程文件系统中，则优先使用预取的线程池。
## allow_push_predicate_ast_for_distributed_subqueries {#allow_push_predicate_ast_for_distributed_subqueries}



类型: Bool

默认值: 1

允许在启用分析器的分布式子查询的 AST 级别推动谓词。
## allow_push_predicate_when_subquery_contains_with {#allow_push_predicate_when_subquery_contains_with}



类型: Bool

默认值: 1

允许在子查询包含 WITH 子句时推动谓词。
## allow_reorder_prewhere_conditions {#allow_reorder_prewhere_conditions}



类型: Bool

默认值: 1

在将条件从 WHERE 移动到 PREWHERE 时，允许重新排序以优化过滤。
## allow_settings_after_format_in_insert {#allow_settings_after_format_in_insert}



类型: Bool

默认值: 0

控制是否允许在 `INSERT` 查询中的 `FORMAT` 后使用 `SETTINGS`。不建议使用此功能，因为这可能将部分 `SETTINGS` 解释为值。

示例：

```sql
INSERT INTO FUNCTION null('foo String') SETTINGS max_threads=1 VALUES ('bar');
```

但是，以下查询仅在 `allow_settings_after_format_in_insert` 为 true 时会生效：

```sql
SET allow_settings_after_format_in_insert=1;
INSERT INTO FUNCTION null('foo String') VALUES ('bar') SETTINGS max_threads=1;
```

可能值：

- 0 — 禁止。
- 1 — 允许。

:::note
仅在后向兼容性情况下使用此设置，如果您的用例依赖于旧语法。
:::
## allow_simdjson {#allow_simdjson}



类型: Bool

默认值: 1

允许在 'JSON*' 函数中使用 simdjson 库（如果 AVX2 指令可用）。如果禁用，将使用 rapidjson。
## allow_statistics_optimize {#allow_statistics_optimize}
<ExperimentalBadge/>


类型: Bool

默认值: 0

允许使用统计信息来优化查询。
## allow_suspicious_codecs {#allow_suspicious_codecs}



类型: Bool

默认值: 0

如果设置为 true，允许指定无意义的压缩编解码器。
## allow_suspicious_fixed_string_types {#allow_suspicious_fixed_string_types}



类型: Bool

默认值: 0

在 CREATE TABLE 语句中，允许创建类型为 FixedString(n) 且 n > 256 的列。长度 >= 256 的 FixedString 可能会引起怀疑，并且很可能表示误用。
## allow_suspicious_indices {#allow_suspicious_indices}



类型: Bool

默认值: 0

拒绝具有相同表达式的主/次索引和排序键。
## allow_suspicious_low_cardinality_types {#allow_suspicious_low_cardinality_types}



类型: Bool

默认值: 0

允许或限制在固定大小为 8 字节或更小的数据类型中使用 [LowCardinality](../../sql-reference/data-types/lowcardinality.md)：数值数据类型和 `FixedString(8_bytes_or_less)`。

对于使用小的固定值，使用 `LowCardinality` 通常效率不高，因为 ClickHouse 为每一行存储一个数值索引。结果：

- 磁盘空间使用量可能增加。
- 内存消耗可能更高，具体取决于字典大小。
- 由于额外的编码/解码操作，某些函数可能运行更慢。

在 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 引擎表中的合并时间可能因上述所有原因而增加。

可能值：

- 1 — 不限制使用 `LowCardinality`。
- 0 — 限制使用 `LowCardinality`。
## allow_suspicious_primary_key {#allow_suspicious_primary_key}



类型: Bool

默认值: 0

允许可疑的 `PRIMARY KEY`/`ORDER BY` 用于 MergeTree（即 SimpleAggregateFunction）。
## allow_suspicious_ttl_expressions {#allow_suspicious_ttl_expressions}



类型: Bool

默认值: 0

拒绝不依赖于任何表列的 TTL 表达式。这通常表示用户错误。
## allow_suspicious_types_in_group_by {#allow_suspicious_types_in_group_by}



类型: Bool

默认值: 0

允许或限制在 GROUP BY 键中使用 [Variant](../../sql-reference/data-types/variant.md) 和 [Dynamic](../../sql-reference/data-types/dynamic.md) 类型。
## allow_suspicious_types_in_order_by {#allow_suspicious_types_in_order_by}



类型: Bool

默认值: 0

允许或限制在 ORDER BY 键中使用 [Variant](../../sql-reference/data-types/variant.md) 和 [Dynamic](../../sql-reference/data-types/dynamic.md) 类型。
## allow_suspicious_variant_types {#allow_suspicious_variant_types}



类型: Bool

默认值: 0

在 CREATE TABLE 语句中，允许指定具有相似变体类型的 Variant 类型（例如，具有不同的数值或日期类型）。启用此设置可能在处理具有相似类型的值时引入一些歧义。
## allow_unrestricted_reads_from_keeper {#allow_unrestricted_reads_from_keeper}



类型: Bool

默认值: 0

允许从 system.zookeeper 表中进行不受限制（没有条件路径）的读取，可能很方便，但对 zookeeper 而言不安全。
## alter_move_to_space_execute_async {#alter_move_to_space_execute_async}



类型: Bool

默认值: 0

异步执行 ALTER TABLE MOVE ... TO [DISK|VOLUME]。
## alter_partition_verbose_result {#alter_partition_verbose_result}



类型: Bool

默认值: 0

启用或禁用显示对分区和部分成功应用操作的信息。
适用于 [ATTACH PARTITION|PART](/sql-reference/statements/alter/partition#attach-partitionpart) 和 [FREEZE PARTITION](/sql-reference/statements/alter/partition#freeze-partition)。

可能值：

- 0 — 禁用详细信息。
- 1 — 启用详细信息。

**示例**

```sql
CREATE TABLE test(a Int64, d Date, s String) ENGINE = MergeTree PARTITION BY toYYYYMDECLARE(d) ORDER BY a;
INSERT INTO test VALUES(1, '2021-01-01', '');
INSERT INTO test VALUES(1, '2021-01-01', '');
ALTER TABLE test DETACH PARTITION ID '202101';

ALTER TABLE test ATTACH PARTITION ID '202101' SETTINGS alter_partition_verbose_result = 1;

┌─command_type─────┬─partition_id─┬─part_name────┬─old_part_name─┐
│ ATTACH PARTITION │ 202101       │ 202101_7_7_0 │ 202101_5_5_0  │
│ ATTACH PARTITION │ 202101       │ 202101_8_8_0 │ 202101_6_6_0  │
└──────────────────┴──────────────┴──────────────┴───────────────┘

ALTER TABLE test FREEZE SETTINGS alter_partition_verbose_result = 1;

┌─command_type─┬─partition_id─┬─part_name────┬─backup_name─┬─backup_path───────────────────┬─part_backup_path────────────────────────────────────────────┐
│ FREEZE ALL   │ 202101       │ 202101_7_7_0 │ 8           │ /var/lib/clickhouse/shadow/8/ │ /var/lib/clickhouse/shadow/8/data/default/test/202101_7_7_0 │
│ FREEZE ALL   │ 202101       │ 202101_8_8_0 │ 8           │ /var/lib/clickhouse/shadow/8/ │ /var/lib/clickhouse/shadow/8/data/default/test/202101_8_8_0 │
└──────────────┴──────────────┴──────────────┴─────────────┴───────────────────────────────┴─────────────────────────────────────────────────────────────┘
```
## alter_sync {#alter_sync}



类型: UInt64

默认值: 1

允许通过 [ALTER](../../sql-reference/statements/alter/index.md)、[OPTIMIZE](../../sql-reference/statements/optimize.md) 或 [TRUNCATE](../../sql-reference/statements/truncate.md) 查询设置等待在副本上执行操作。

可能值：

- 0 — 不等待。
- 1 — 等待自身执行。
- 2 — 等待所有人。

云默认值: `0`。

:::note
`alter_sync` 仅适用于 `Replicated` 表，对非 `Replicated` 表的更改无效。
:::
## analyze_index_with_space_filling_curves {#analyze_index_with_space_filling_curves}



类型: Bool

默认值: 1

如果表的索引中具有空间填充曲线，例如 `ORDER BY mortonEncode(x, y)` 或 `ORDER BY hilbertEncode(x, y)`，并且查询在其参数上有条件，例如 `x >= 10 AND x <= 20 AND y >= 20 AND y <= 30`，则使用空间填充曲线进行索引分析。
## analyzer_compatibility_join_using_top_level_identifier {#analyzer_compatibility_join_using_top_level_identifier}



类型: Bool

默认值: 0

强制在 JOIN USING 中从投影解决标识符（例如，在 `SELECT a + 1 AS b FROM t1 JOIN t2 USING (b)` 中，连接将通过 `t1.a + 1 = t2.b` 执行，而不是 `t1.b = t2.b`）。
## any_join_distinct_right_table_keys {#any_join_distinct_right_table_keys}



类型: Bool

默认值: 0

在 `ANY INNER|LEFT JOIN` 操作中启用传统的 ClickHouse 服务器行为。

:::note
仅在您的用例依赖于传统 `JOIN` 行为时使用此设置。
:::

启用传统行为时：

- `t1 ANY LEFT JOIN t2` 和 `t2 ANY RIGHT JOIN t1` 操作的结果不相等，因为 ClickHouse 使用的是从左到右的表键映射逻辑。
- `ANY INNER JOIN` 操作的结果包含来自左表的所有行，类似于 `SEMI LEFT JOIN` 操作。

禁用传统行为时：

- `t1 ANY LEFT JOIN t2` 和 `t2 ANY RIGHT JOIN t1` 操作的结果相等，因为 ClickHouse 使用的是提供一次对多键映射的逻辑。
- `ANY INNER JOIN` 操作的结果包含来自左表和右表的每个键的一行。

可能值：

- 0 — 传统行为禁用。
- 1 — 传统行为启用。

另请参阅：

- [JOIN 严格性](/sql-reference/statements/select/join#settings)
## apply_deleted_mask {#apply_deleted_mask}



类型: Bool

默认值: 1

启用通过轻量级删除删除的行过滤。如果禁用，则查询将能够读取这些行。这对于调试和 "恢复删除" 场景很有用。
## apply_mutations_on_fly {#apply_mutations_on_fly}



类型: Bool

默认值: 0

如果为 true，未在数据部分中物化的变更（UPDATE 和 DELETE）将在 SELECT 时应用。
## apply_settings_from_server {#apply_settings_from_server}



类型: Bool

默认值: 1

客户端是否应接受来自服务器的设置。

这仅影响在客户端执行的操作，特别是在解析 INSERT 输入数据和格式化查询结果时。大多数查询执行发生在服务器上，并不受此设置的影响。

通常，此设置应在用户配置文件中设置（users.xml 或 `ALTER USER` 等查询），而不是通过客户端（客户端命令行参数、`SET` 查询或 `SELECT` 查询的 `SETTINGS` 部分）。通过客户端可以将其更改为 false，但不能更改为 true（因为如果用户配置文件具有 `apply_settings_from_server = false`，则服务器不会发送设置）。

请注意，最初（24.12）存在一个服务器设置（`send_settings_to_client`），但随后它被替换为此客户端设置，以提高可用性。
## asterisk_include_alias_columns {#asterisk_include_alias_columns}



类型: Bool

默认值: 0

为通配符查询 (`SELECT *`) 包含 [ALIAS](../../sql-reference/statements/create/table.md/#alias) 列。

可能值：

- 0 - 禁用
- 1 - 启用
## asterisk_include_materialized_columns {#asterisk_include_materialized_columns}



类型: Bool

默认值: 0

为通配符查询 (`SELECT *`) 包含 [MATERIALIZED](/sql-reference/statements/create/view#materialized-view) 列。

可能值：

- 0 - 禁用
- 1 - 启用
## async_insert {#async_insert}



类型: Bool

默认值: 0

如果为 true，来自 INSERT 查询的数据将存储在队列中，稍后在后台刷新到表中。如果 wait_for_async_insert 为 false，INSERT 查询几乎立即处理，而客户端将等待数据刷新到表中。
## async_insert_busy_timeout_decrease_rate {#async_insert_busy_timeout_decrease_rate}



类型: Double

默认值: 0.2

自适应异步插入超时降低的指数增长率。
## async_insert_busy_timeout_increase_rate {#async_insert_busy_timeout_increase_rate



类型: Double

默认值: 0.2

自适应异步插入超时增加的指数增长率。

## async_insert_busy_timeout_max_ms {#async_insert_busy_timeout_max_ms}

类型：毫秒

默认值：200

在第一次数据出现后，每个查询收集的数据转储之前的最大等待时间。

## async_insert_busy_timeout_min_ms {#async_insert_busy_timeout_min_ms}

类型：毫秒

默认值：50

如果通过 async_insert_use_adaptive_busy_timeout 启用了自动调整，则在第一次数据出现后，每个查询收集的数据转储之前的最小等待时间。它还作为自适应算法的初始值。

## async_insert_deduplicate {#async_insert_deduplicate}

类型：布尔值

默认值：0

对于复制表中的异步 INSERT 查询，指定应执行插入块的去重。

## async_insert_max_data_size {#async_insert_max_data_size}

类型：UInt64

默认值：10485760

在插入之前，每个查询收集的未解析数据的最大字节数。

## async_insert_max_query_number {#async_insert_max_query_number}

类型：UInt64

默认值：450

在插入之前的最大插入查询数量。

## async_insert_poll_timeout_ms {#async_insert_poll_timeout_ms}

类型：毫秒

默认值：10

从异步插入队列轮询数据的超时时间。

## async_insert_use_adaptive_busy_timeout {#async_insert_use_adaptive_busy_timeout}

类型：布尔值

默认值：1

如果设置为 true，则使用自适应忙碌超时进行异步插入。

## async_query_sending_for_remote {#async_query_sending_for_remote}

类型：布尔值

默认值：1

在执行远程查询时启用异步连接创建和查询发送。

默认情况下启用。

## async_socket_for_remote {#async_socket_for_remote}

类型：布尔值

默认值：1

在执行远程查询时启用从套接字异步读取。

默认情况下启用。

## azure_allow_parallel_part_upload {#azure_allow_parallel_part_upload}

类型：布尔值

默认值：1

对 Azure 多部分上传使用多个线程。

## azure_check_objects_after_upload {#azure_check_objects_after_upload}

类型：布尔值

默认值：0

检查在 Azure Blob 存储中每个上传对象，以确保上传成功。

## azure_create_new_file_on_insert {#azure_create_new_file_on_insert}

类型：布尔值

默认值：0

启用或禁用在 Azure 引擎表中每次插入时创建新文件。

## azure_ignore_file_doesnt_exist {#azure_ignore_file_doesnt_exist}

类型：布尔值

默认值：0

当读取某些键时，如果文件不存在，则忽略文件的缺失。

可能的值：
- 1 — `SELECT` 返回空结果。
- 0 — `SELECT` 抛出异常。

## azure_list_object_keys_size {#azure_list_object_keys_size}

类型：UInt64

默认值：1000

ListObject 请求中可批量返回的最大文件数量。

## azure_max_blocks_in_multipart_upload {#azure_max_blocks_in_multipart_upload}

类型：UInt64

默认值：50000

Azure 多部分上传中的最大块数。

## azure_max_inflight_parts_for_one_file {#azure_max_inflight_parts_for_one_file}

类型：UInt64

默认值：20

多部分上传请求中并行加载的最大块数量。0 表示无限制。

## azure_max_single_part_copy_size {#azure_max_single_part_copy_size}

类型：UInt64

默认值：268435456

使用单个部分复制到 Azure Blob 存储的对象的最大大小。

## azure_max_single_part_upload_size {#azure_max_single_part_upload_size}

类型：UInt64

默认值：104857600

使用单个部分上传到 Azure Blob 存储的对象的最大大小。

## azure_max_single_read_retries {#azure_max_single_read_retries}

类型：UInt64

默认值：4

在单次 Azure Blob 存储读取期间的最大重试次数。

## azure_max_unexpected_write_error_retries {#azure_max_unexpected_write_error_retries}

类型：UInt64

默认值：4

在 Azure Blob 存储写入期间出现意外错误时的最大重试次数。

## azure_max_upload_part_size {#azure_max_upload_part_size}

类型：UInt64

默认值：5368709120

在 Azure Blob 存储中进行多部分上传期间要上传的部分的最大大小。

## azure_min_upload_part_size {#azure_min_upload_part_size}

类型：UInt64

默认值：16777216

在 Azure Blob 存储中进行多部分上传期间要上传的部分的最小大小。

## azure_sdk_max_retries {#azure_sdk_max_retries}

类型：UInt64

默认值：10

在 Azure SDK 中的最大重试次数。

## azure_sdk_retry_initial_backoff_ms {#azure_sdk_retry_initial_backoff_ms}

类型：UInt64

默认值：10

在 Azure SDK 中重试之间的最小回退时间。

## azure_sdk_retry_max_backoff_ms {#azure_sdk_retry_max_backoff_ms}

类型：UInt64

默认值：1000

在 Azure SDK 中重试之间的最大回退时间。

## azure_skip_empty_files {#azure_skip_empty_files}

类型：布尔值

默认值：0

启用或禁用在 S3 引擎中跳过空文件。

可能的值：
- 0 — 如果空文件与请求格式不兼容，`SELECT` 将抛出异常。
- 1 — 空文件返回空结果。

## azure_strict_upload_part_size {#azure_strict_upload_part_size}

类型：UInt64

默认值：0

在 Azure Blob 存储中进行多部分上传期间要上传的部分的确切大小。

## azure_throw_on_zero_files_match {#azure_throw_on_zero_files_match}

类型：布尔值

默认值：0

如果根据通配符扩展规则匹配到零个文件，则抛出错误。

可能的值：
- 1 — `SELECT` 抛出异常。
- 0 — `SELECT` 返回空结果。

## azure_truncate_on_insert {#azure_truncate_on_insert}

类型：布尔值

默认值：0

启用或禁用在 Azure 引擎表中的插入之前截断。

## azure_upload_part_size_multiply_factor {#azure_upload_part_size_multiply_factor}

类型：UInt64

默认值：2

在从 Azure Blob 存储的单次写入中上传azure_multiply_parts_count_threshold的部分数量时，每次将 azure_min_upload_part_size 乘以此因子。

## azure_upload_part_size_multiply_parts_count_threshold {#azure_upload_part_size_multiply_parts_count_threshold}

类型：UInt64

默认值：500

每当此数量的部分被上传到 Azure Blob 存储时，azure_min_upload_part_size 乘以 azure_upload_part_size_multiply_factor。

## backup_restore_batch_size_for_keeper_multi {#backup_restore_batch_size_for_keeper_multi}

类型：UInt64

默认值：1000

备份或恢复期间对 [Zoo]Keeper 的多请求的最大批大小。

## backup_restore_batch_size_for_keeper_multiread {#backup_restore_batch_size_for_keeper_multiread}

类型：UInt64

默认值：10000

备份或恢复期间对 [Zoo]Keeper 的多读取请求的最大批大小。

## backup_restore_failure_after_host_disconnected_for_seconds {#backup_restore_failure_after_host_disconnected_for_seconds}

类型：UInt64

默认值：3600

如果在进行 BACKUP ON CLUSTER 或 RESTORE ON CLUSTER 操作期间，主机在此时间量内没有在 ZooKeeper 中重新创建其临时的“存活”节点，则整个备份或恢复被视为失败。
此值应大于主机在失败后重新连接到 ZooKeeper 的合理时间。
零表示无限制。

## backup_restore_finish_timeout_after_error_sec {#backup_restore_finish_timeout_after_error_sec}

类型：UInt64

默认值：180

发起者应等待多长时间，以便其他主机对“错误”节点做出反应并停止当前的 BACKUP ON CLUSTER 或 RESTORE ON CLUSTER 操作的工作。

## backup_restore_keeper_fault_injection_probability {#backup_restore_keeper_fault_injection_probability}

类型：浮点数

默认值：0

备份或恢复期间 keeper 请求失败的近似概率。有效值在区间 [0.0f, 1.0f] 内。

## backup_restore_keeper_fault_injection_seed {#backup_restore_keeper_fault_injection_seed}

类型：UInt64

默认值：0

0 - 随机种子，其他值为设置值。

## backup_restore_keeper_max_retries {#backup_restore_keeper_max_retries}

类型：UInt64

默认值：1000

在 BACKUP 或 RESTORE 操作中进行的 [Zoo]Keeper 操作的最高重试次数。
应足够大，以便整个操作在临时的 [Zoo]Keeper 故障期间不会失败。

## backup_restore_keeper_max_retries_while_handling_error {#backup_restore_keeper_max_retries_while_handling_error}

类型：UInt64

默认值：20

在处理 BACKUP ON CLUSTER 或 RESTORE ON CLUSTER 操作的错误期间，进行的 [Zoo]Keeper 操作的最大重试次数。

## backup_restore_keeper_max_retries_while_initializing {#backup_restore_keeper_max_retries_while_initializing}

类型：UInt64

默认值：20

在初始化 BACKUP ON CLUSTER 或 RESTORE ON CLUSTER 操作期间，进行的 [Zoo]Keeper 操作的最大重试次数。

## backup_restore_keeper_retry_initial_backoff_ms {#backup_restore_keeper_retry_initial_backoff_ms}

类型：UInt64

默认值：100

在备份或恢复期间进行的 [Zoo]Keeper 操作的初始回退超时时间。

## backup_restore_keeper_retry_max_backoff_ms {#backup_restore_keeper_retry_max_backoff_ms}

类型：UInt64

默认值：5000

在备份或恢复期间进行的 [Zoo]Keeper 操作的最大回退超时时间。

## backup_restore_keeper_value_max_size {#backup_restore_keeper_value_max_size}

类型：UInt64

默认值：1048576

备份期间 [Zoo]Keeper 节点的数据的最大大小。

## backup_restore_s3_retry_attempts {#backup_restore_s3_retry_attempts}

类型：UInt64

默认值：1000

针对 Aws::Client::RetryStrategy 的设置，Aws::Client 本身会重试，0 表示没有重试。仅适用于备份/恢复。

## cache_warmer_threads {#cache_warmer_threads}

<CloudAvailableBadge/>

类型：UInt64

默认值：4

仅在 ClickHouse Cloud 中生效。用于在启用 [cache_populated_by_fetch](merge-tree-settings.md/#cache_populated_by_fetch) 时，后台线程的数量，用于推测性地下载新数据部分到文件缓存中。设置为零以禁用。

## calculate_text_stack_trace {#calculate_text_stack_trace}

类型：布尔值

默认值：1

在查询执行期间发生异常时计算文本堆栈跟踪。这是默认值。它需要符号查找，当执行大量错误查询时，可能会减慢模糊测试。在正常情况下，不应禁用此选项。

## cancel_http_readonly_queries_on_client_close {#cancel_http_readonly_queries_on_client_close}

类型：布尔值

默认值：0

当客户端在不等待响应的情况下关闭连接时，取消 HTTP 只读查询（例如 SELECT）。

云的默认值为：`1`。

## cast_ipv4_ipv6_default_on_conversion_error {#cast_ipv4_ipv6_default_on_conversion_error}

类型：布尔值

默认值：0

在转换错误时，CAST 运算符转换为 IPv4、转换为 IPV6 类型，toIPv4、toIPv6 函数将返回默认值，而不是抛出异常。

## cast_keep_nullable {#cast_keep_nullable}

类型：布尔值

默认值：0

在 [CAST](/sql-reference/functions/type-conversion-functions#cast) 操作中启用或禁用保留 `Nullable` 数据类型。

当设置为启用时，如果 `CAST` 函数的参数为 `Nullable`，结果也将转换为 `Nullable` 类型。当设置为禁用时，结果始终严格具有目标类型。

可能的值：

- 0 — `CAST` 结果具有指定的确切目标类型。
- 1 — 如果参数类型为 `Nullable`，则 `CAST` 结果转换为 `Nullable(DestinationDataType)`。

**示例**

以下查询的结果为目标数据类型：

```sql
SET cast_keep_nullable = 0;
SELECT CAST(toNullable(toInt32(0)) AS Int32) as x, toTypeName(x);
```

结果：

```text
┌─x─┬─toTypeName(CAST(toNullable(toInt32(0)), 'Int32'))─┐
│ 0 │ Int32                                             │
└───┴───────────────────────────────────────────────────┘
```

以下查询的结果为目标数据类型的 `Nullable` 修改：

```sql
SET cast_keep_nullable = 1;
SELECT CAST(toNullable(toInt32(0)) AS Int32) as x, toTypeName(x);
```

结果：

```text
┌─x─┬─toTypeName(CAST(toNullable(toInt32(0)), 'Int32'))─┐
│ 0 │ Nullable(Int32)                                   │
└───┴───────────────────────────────────────────────────┘
```

**另请参见**

- [CAST](/sql-reference/functions/type-conversion-functions#cast) 函数

## cast_string_to_dynamic_use_inference {#cast_string_to_dynamic_use_inference}

类型：布尔值

默认值：0

在字符串到动态转换期间使用类型推断。

## check_query_single_value_result {#check_query_single_value_result}

类型：布尔值

默认值：1

定义 MergeTree 家族引擎的 [CHECK TABLE](/sql-reference/statements/check-table) 查询结果的详细级别。

可能的值：

- 0 — 查询显示表中每个单独数据部分的检查状态。
- 1 — 查询显示一般的表检查状态。

## check_referential_table_dependencies {#check_referential_table_dependencies}

类型：布尔值

默认值：0

检查DDL查询（例如 DROP TABLE 或 RENAME）不会破坏引用依赖关系。

## check_table_dependencies {#check_table_dependencies}

类型：布尔值

默认值：1

检查DDL查询（例如 DROP TABLE 或 RENAME）不会破坏依赖关系。

## checksum_on_read {#checksum_on_read}

类型：布尔值

默认值：1

读取时验证校验和。默认情况下启用，并且应始终在生产环境中启用。请不要期望禁用此设置会带来任何好处。它只能用于实验和基准测试。该设置仅适用于 MergeTree 家族的表。对于其他表引擎以及通过网络接收的数据，总是会验证校验和。

## cloud_mode {#cloud_mode}

类型：布尔值

默认值：0

云模式。

## cloud_mode_database_engine {#cloud_mode_database_engine}

类型：UInt64

默认值：1

云中允许的数据库引擎。1 - 重写 DDL 使用 Replicated 数据库，2 - 重写 DDL 使用 Shared 数据库。

## cloud_mode_engine {#cloud_mode_engine}

类型：UInt64

默认值：1

云中允许的引擎系列。

- 0 - 允许所有。
- 1 - 重写 DDL 使用 *ReplicatedMergeTree。
- 2 - 重写 DDL 使用 SharedMergeTree。
- 3 - 重写 DDL 使用 SharedMergeTree，除非显式指定远程磁盘。

UInt64 最小化公共部分。

## cluster_for_parallel_replicas {#cluster_for_parallel_replicas}
<BetaBadge/>

类型：字符串

默认值：

当前服务器所在分片的集群。

## collect_hash_table_stats_during_aggregation {#collect_hash_table_stats_during_aggregation}

类型：布尔值

默认值：1

启用收集哈希表统计信息以优化内存分配。

## collect_hash_table_stats_during_joins {#collect_hash_table_stats_during_joins}

类型：布尔值

默认值：1

启用收集哈希表统计信息以优化内存分配。

## compatibility {#compatibility}

类型：字符串

默认值：

`compatibility` 设置使 ClickHouse 使用之前版本的 ClickHouse 的默认设置，之前版本作为设置提供。

如果设置为非默认值，则会遵循那些设置（只有未修改的设置会受到 `compatibility` 设置的影响）。

此设置需要一个 ClickHouse 版本号作为字符串，例如 `22.3`、`22.8`。空值意味着该设置被禁用。

默认情况下禁用。

:::note
在 ClickHouse Cloud 中，兼容性设置必须由 ClickHouse Cloud 支持设置。请 [打开一个案例](https://clickhouse.cloud/support) 以设置它。
:::

## compatibility_ignore_auto_increment_in_create_table {#compatibility_ignore_auto_increment_in_create_table}

类型：布尔值

默认值：0

如果为 true，则在列声明中忽略 AUTO_INCREMENT 关键字，否则返回错误。这简化了从 MySQL 的迁移。

## compatibility_ignore_collation_in_create_table {#compatibility_ignore_collation_in_create_table}

类型：布尔值

默认值：1

兼容性忽略创建表中的排序规则。

## compile_aggregate_expressions {#compile_aggregate_expressions}

类型：布尔值

默认值：1

启用或禁用聚合函数的 JIT 编译为本机代码。启用此设置可以提高性能。

可能的值：

- 0 — 无 JIT 编译进行聚合。
- 1 — 使用 JIT 编译进行聚合。

**另请参见**

- [min_count_to_compile_aggregate_expression](#min_count_to_compile_aggregate_expression)

## compile_expressions {#compile_expressions}

类型：布尔值

默认值：0

将某些标量函数和运算符编译为本机代码。由于 LLVM 编译器基础设施中的错误，在 AArch64 机器上已知会导致 nullptr 解引用，从而导致服务器崩溃。请不要启用此设置。

## compile_sort_description {#compile_sort_description}

类型：布尔值

默认值：1

将排序描述编译为本机代码。

## connect_timeout {#connect_timeout}

类型：秒

默认值：10

如果没有副本，则为连接超时时间。

## connect_timeout_with_failover_ms {#connect_timeout_with_failover_ms}

类型：毫秒

默认值：1000

对于分布式表引擎，在集群定义中使用“shard”和“replica”部分时，连接到远程服务器的超时时间（以毫秒为单位）。如果不成功，将尝试多次与不同副本建立连接。

## connect_timeout_with_failover_secure_ms {#connect_timeout_with_failover_secure_ms}

类型：毫秒

默认值：1000

选择第一个健康副本的超时时间（适用于安全连接）。

## connection_pool_max_wait_ms {#connection_pool_max_wait_ms}

类型：毫秒

默认值：0

连接池满时连接的等待时间（以毫秒为单位）。

可能的值：

- 正整数。
- 0 — 无限超时。

## connections_with_failover_max_tries {#connections_with_failover_max_tries}

类型：UInt64

默认值：3

对于分布式表引擎，每个副本的最大连接尝试次数。

## convert_query_to_cnf {#convert_query_to_cnf}

类型：布尔值

默认值：0

设置为 `true` 时，`SELECT` 查询将转换为合取范式（CNF）。在某些情况下，使用 CNF 重写查询可能会更快执行（查看此 [Github 问题](https://github.com/ClickHouse/ClickHouse/issues/11749) 进行解释）。

例如，注意以下 `SELECT` 查询未被修改（默认行为）：

```sql
EXPLAIN SYNTAX
SELECT *
FROM
(
    SELECT number AS x
    FROM numbers(20)
) AS a
WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))
SETTINGS convert_query_to_cnf = false;
```

结果是：

```response
┌─explain────────────────────────────────────────────────────────┐
│ SELECT x                                                       │
│ FROM                                                           │
│ (                                                              │
│     SELECT number AS x                                         │
│     FROM numbers(20)                                           │
│     WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15)) │
│ ) AS a                                                         │
│ WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))     │
│ SETTINGS convert_query_to_cnf = 0                              │
└────────────────────────────────────────────────────────────────┘
```

让我们将 `convert_query_to_cnf` 设置为 `true`，看看会发生什么变化：

```sql
EXPLAIN SYNTAX
SELECT *
FROM
(
    SELECT number AS x
    FROM numbers(20)
) AS a
WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))
SETTINGS convert_query_to_cnf = true;
```

注意，`WHERE` 子句被重写为 CNF，但结果集是相同的 - 布尔逻辑不变：

```response
┌─explain───────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ SELECT x                                                                                                              │
│ FROM                                                                                                                  │
│ (                                                                                                                     │
│     SELECT number AS x                                                                                                │
│     FROM numbers(20)                                                                                                  │
│     WHERE ((x <= 15) OR (x <= 5)) AND ((x <= 15) OR (x >= 1)) AND ((x >= 10) OR (x <= 5)) AND ((x >= 10) OR (x >= 1)) │
│ ) AS a                                                                                                                │
│ WHERE ((x >= 10) OR (x >= 1)) AND ((x >= 10) OR (x <= 5)) AND ((x <= 15) OR (x >= 1)) AND ((x <= 15) OR (x <= 5))     │
│ SETTINGS convert_query_to_cnf = 1                                                                                     │
└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

可能的值：true, false

## count_distinct_implementation {#count_distinct_implementation}

类型：字符串

默认值：uniqExact

指定应使用哪个 `uniq*` 函数来执行 [COUNT(DISTINCT ...)](/sql-reference/aggregate-functions/reference/count) 结构。

可能的值：

- [uniq](/sql-reference/aggregate-functions/reference/uniq)
- [uniqCombined](/sql-reference/aggregate-functions/reference/uniqcombined)
- [uniqCombined64](/sql-reference/aggregate-functions/reference/uniqcombined64)
- [uniqHLL12](/sql-reference/aggregate-functions/reference/uniqhll12)
- [uniqExact](/sql-reference/aggregate-functions/reference/uniqexact)

## count_distinct_optimization {#count_distinct_optimization}

类型：布尔值

默认值：0

重写计数去重为分组子查询。

## create_if_not_exists {#create_if_not_exists}

类型：布尔值

默认值：0

 默认为 `CREATE` 语句启用 `IF NOT EXISTS`。如果指定了此设置或 `IF NOT EXISTS` 并且已存在提供的名称的表，则不会抛出异常。

## create_index_ignore_unique {#create_index_ignore_unique}

类型：布尔值

默认值：0

在创建唯一索引时忽略 UNIQUE 关键字。用于 SQL 兼容性测试。

## create_replicated_merge_tree_fault_injection_probability {#create_replicated_merge_tree_fault_injection_probability}

类型：浮点数

默认值：0

创建表时在 ZooKeeper 中创建元数据后进行故障注入的概率。

## create_table_empty_primary_key_by_default {#create_table_empty_primary_key_by_default}

类型：布尔值

默认值：0

允许在没有指定 ORDER BY 和 PRIMARY KEY 时创建 *MergeTree 表的空主键。

## cross_join_min_bytes_to_compress {#cross_join_min_bytes_to_compress}

类型：UInt64

默认值：1073741824

在 CROSS JOIN 中进行压缩的块的最小大小。零值表示 - 禁用此阈值。当达到两个阈值之一（按行数或字节数）时压缩此块。

## cross_join_min_rows_to_compress {#cross_join_min_rows_to_compress}

类型：UInt64

默认值：10000000

在 CROSS JOIN 中压缩块的最小行数。零值表示 - 禁用此阈值。当达到两个阈值之一（按行数或字节数）时压缩此块。

## data_type_default_nullable {#data_type_default_nullable}

类型：布尔值

默认值：0

允许没有显式修饰符 [NULL 或 NOT NULL](/sql-reference/statements/create/table#null-or-not-null-modifiers) 的数据类型在列定义中是 [Nullable](/sql-reference/data-types/nullable)。

可能的值：

- 1 — 列定义中的数据类型默认设置为 `Nullable`。
- 0 — 列定义中的数据类型默认设置为不 `Nullable`。

## database_atomic_wait_for_drop_and_detach_synchronously {#database_atomic_wait_for_drop_and_detach_synchronously}

类型：布尔值

默认值：0

为所有 `DROP` 和 `DETACH` 查询添加修饰符 `SYNC`。

可能的值：

- 0 — 查询将延迟执行。
- 1 — 查询将立即执行。

## database_replicated_allow_explicit_uuid {#database_replicated_allow_explicit_uuid}

类型：UInt64

默认值：0

0 - 不允许显式指定告诉副本的 UUID，1 - 允许，2 - 允许，但忽略指定的 UUID，并生成一个随机的 UUID。

## database_replicated_allow_heavy_create {#database_replicated_allow_heavy_create}

类型：布尔值

默认值：0

允许在复制数据库引擎中使用长时间运行的 DDL 查询（CREATE AS SELECT 和 POPULATE）。注意，这可能会长时间阻塞 DDL 队列。

## database_replicated_allow_only_replicated_engine {#database_replicated_allow_only_replicated_engine}

类型：布尔值

默认值：0

允许在使用 Replicated 引擎的数据库中仅创建 Replicated 表。

## database_replicated_allow_replicated_engine_arguments {#database_replicated_allow_replicated_engine_arguments}

类型：UInt64

默认值：0

0 - 不允许显式指定 ZooKeeper 路径和副本名称在 Replicated 数据库中的 *MergeTree 表中。1 - 允许。2 - 允许，但忽略指定的路径并使用默认路径。3 - 允许且不记录警告。

## database_replicated_always_detach_permanently {#database_replicated_always_detach_permanently}

类型：布尔值

默认值：0

如果数据库引擎为 Replicated，则执行 DETACH TABLE 作为 DETACH TABLE PERMANENTLY。

## database_replicated_enforce_synchronous_settings {#database_replicated_enforce_synchronous_settings}

类型：布尔值

默认值：0

强制执行某些查询的同步等待（另请参阅 database_atomic_wait_for_drop_and_detach_synchronously、mutation_sync、alter_sync）。不建议启用这些设置。

## database_replicated_initial_query_timeout_sec {#database_replicated_initial_query_timeout_sec}

类型：UInt64

默认值：300

设置初始 DDL 查询等待 Replicated 数据库处理之前 DDL 队列条目的时间（以秒为单位）。

可能的值：

- 正整数。
- 0 — 无限制。

## decimal_check_overflow {#decimal_check_overflow}

类型：布尔值

默认值：1

检查十进制算术/比较操作的溢出。

## deduplicate_blocks_in_dependent_materialized_views {#deduplicate_blocks_in_dependent_materialized_views}

类型：布尔值

默认值：0

启用或禁用在接收数据的材料化视图中进行去重检查，数据来自 Replicated* 表。

可能的值：

      0 — 禁用。
      1 — 启用。

使用

默认情况下，不对材料化视图进行去重，但在源表中进行去重。
如果由于源表中的去重而跳过 INSERTed 块，则不会插入到附加的物化视图中。此行为的存在是为了使高聚合数据能够插入到材料化视图中，对于插入的块在材料化视图聚合后相同但源自于不同的对源表的 INSERT。
与此同时，此行为“破坏”了 `INSERT` 的幂等性。如果对主表的 `INSERT` 成功，并且对材料化视图的 `INSERT` 失败（例如由于与 ClickHouse Keeper 的通信故障），客户端将收到错误并可以重试操作。然而，材料化视图不会接收第二次插入，因为它将被主（源）表的去重丢弃。设置 `deduplicate_blocks_in_dependent_materialized_views` 允许更改此行为。在重试时，材料化视图将接收重复的插入，并将在其自身进行去重检查，
忽略源表的检查结果，并插入因第一次失败而丢失的行。

## default_materialized_view_sql_security {#default_materialized_view_sql_security}

类型：SQLSecurityType

默认值：DEFINER

允许在创建材料化视图时设置 SQL SECURITY 选项的默认值。[有关 SQL 安全性的更多信息](../../sql-reference/statements/create/view.md/#sql_security)。

默认值为 `DEFINER`。

## default_max_bytes_in_join {#default_max_bytes_in_join}

类型：UInt64

默认值：1000000000

如果需要 limit 但没有设置 max_bytes_in_join，则右侧表的最大大小。

## default_normal_view_sql_security {#default_normal_view_sql_security}

类型：SQLSecurityType

默认值：INVOKER

允许在创建常规视图时设置默认的 `SQL SECURITY` 选项。[有关 SQL 安全性的更多信息](../../sql-reference/statements/create/view.md/#sql_security)。

默认值为 `INVOKER`。

## default_table_engine {#default_table_engine}

类型：DefaultTableEngine

默认值：MergeTree

在 `CREATE` 语句中未设置 `ENGINE` 时使用的默认表引擎。

可能的值：

- 字符串，表示任何有效的表引擎名称。

云默认值：`SharedMergeTree`。

**示例**

查询：

```sql
SET default_table_engine = 'Log';

SELECT name, value, changed FROM system.settings WHERE name = 'default_table_engine';
```

结果：

```response
┌─name─────────────────┬─value─┬─changed─┐
│ default_table_engine │ Log   │       1 │
└──────────────────────┴───────┴─────────┘
```

在此示例中，任何不指定 `Engine` 的新表将使用 `Log` 表引擎：

查询：

```sql
CREATE TABLE my_table (
    x UInt32,
    y UInt32
);

SHOW CREATE TABLE my_table;
```

结果：

```response
┌─statement────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.my_table
(
    `x` UInt32,
    `y` UInt32
)
ENGINE = Log
└──────────────────────────────────────────────────────────────────────────┘
```
```

## default_temporary_table_engine {#default_temporary_table_engine}

Type: DefaultTableEngine

Default value: Memory

与 [default_table_engine](#default_table_engine) 相同，但用于临时表。

在此示例中，任何未指定 `Engine` 的新临时表将使用 `Log` 表引擎：

查询：

```sql
SET default_temporary_table_engine = 'Log';

CREATE TEMPORARY TABLE my_table (
    x UInt32,
    y UInt32
);

SHOW CREATE TEMPORARY TABLE my_table;
```

结果：

```response
┌─statement────────────────────────────────────────────────────────────────┐
│ CREATE TEMPORARY TABLE default.my_table
(
    `x` UInt32,
    `y` UInt32
)
ENGINE = Log
└──────────────────────────────────────────────────────────────────────────┘
```

## default_view_definer {#default_view_definer}

Type: String

Default value: CURRENT_USER

允许在创建视图时设置默认 `DEFINER` 选项。 [有关 SQL 安全的更多信息](../../sql-reference/statements/create/view.md/#sql_security)。

默认值为 `CURRENT_USER`。

## describe_compact_output {#describe_compact_output}

Type: Bool

Default value: 0

如果为真，则只在 DESCRIBE 查询结果中包含列名和类型。

## describe_extend_object_types {#describe_extend_object_types}

Type: Bool

Default value: 0

在 DESCRIBE 查询中推断 Object 类型的列的具体类型。

## describe_include_subcolumns {#describe_include_subcolumns}

Type: Bool

Default value: 0

启用在 [DESCRIBE](../../sql-reference/statements/describe-table.md) 查询中描述子列。例如，成员 [Tuple](../../sql-reference/data-types/tuple.md) 或 [Map](/sql-reference/data-types/map#reading-subcolumns-of-map)、[Nullable](../../sql-reference/data-types/nullable.md/#finding-null) 或 [Array](../../sql-reference/data-types/array.md/#array-size) 数据类型的子列。

可能的值：

- 0 — 子列不包含在 `DESCRIBE` 查询中。
- 1 — 子列包含在 `DESCRIBE` 查询中。

**示例**

参见 [DESCRIBE](../../sql-reference/statements/describe-table.md) 语句的示例。

## describe_include_virtual_columns {#describe_include_virtual_columns}

Type: Bool

Default value: 0

如果为真，表的虚拟列将包含在 DESCRIBE 查询的结果中。

## dialect {#dialect}

Type: Dialect

Default value: clickhouse

将用于解析查询的方言。

## dictionary_validate_primary_key_type {#dictionary_validate_primary_key_type}

Type: Bool

Default value: 0

验证字典的主键类型。默认情况下，简单布局的 ID 类型将隐式转换为 UInt64。

## distinct_overflow_mode {#distinct_overflow_mode}

Type: OverflowMode

Default value: throw

当限制被超过时该怎么办。

## distributed_aggregation_memory_efficient {#distributed_aggregation_memory_efficient}

Type: Bool

Default value: 1

启用分布式聚合的节省内存模式。

## distributed_background_insert_batch {#distributed_background_insert_batch}

Type: Bool

Default value: 0

启用/禁用以批量发送插入数据。

启用批量发送时， [Distributed](../../engines/table-engines/special/distributed.md) 表引擎尝试在一个操作中发送多个插入数据的文件，而不是单独发送。批量发送通过更好地利用服务器和网络资源来提高集群性能。

可能的值：

- 1 — 启用。
- 0 — 禁用。

## distributed_background_insert_max_sleep_time_ms {#distributed_background_insert_max_sleep_time_ms}

Type: Milliseconds

Default value: 30000

[Distributed](../../engines/table-engines/special/distributed.md) 表引擎发送数据的最大间隔。限制 [distributed_background_insert_sleep_time_ms](#distributed_background_insert_sleep_time_ms) 设置中指定的间隔的指数增长。

可能的值：

- 正整数，以毫秒为单位。

## distributed_background_insert_sleep_time_ms {#distributed_background_insert_sleep_time_ms}

Type: Milliseconds

Default value: 100

[Distributed](../../engines/table-engines/special/distributed.md) 表引擎发送数据的基本间隔。如发生错误，实际间隔以指数方式增长。

可能的值：

- 正整数，以毫秒为单位。

## distributed_background_insert_split_batch_on_failure {#distributed_background_insert_split_batch_on_failure}

Type: Bool

Default value: 0

启用/禁用在失败时拆分批次。

有时，将特定批次发送到远程分片可能会失败，因为某些复杂的管道（例如带有 `GROUP BY` 的 `MATERIALIZED VIEW`）导致 `Memory limit exceeded` 或类似错误。在这种情况下，重试不会有效（这会使得表的分布式发送卡住），但逐个发送该批次的文件可能成功插入。

因此，将此设置安装为 `1` 将禁用此类批次的批量发送（即临时禁用失败批次的 `distributed_background_insert_batch`）。

可能的值：

- 1 — 启用。
- 0 — 禁用。

:::note
此设置还会影响破损的批次（可能由于异常的服务器（机器）终止而导致，并且没有为 [Distributed](../../engines/table-engines/special/distributed.md) 表引擎设置 `fsync_after_insert`/`fsync_directories`）。
:::

:::note
您不应依赖于自动批次拆分，因为这可能会影响性能。
:::

## distributed_background_insert_timeout {#distributed_background_insert_timeout}

Type: UInt64

Default value: 0

用于分布式的插入查询的超时。仅在启用 insert_distributed_sync 时使用。零值表示没有超时。

## distributed_cache_bypass_connection_pool {#distributed_cache_bypass_connection_pool}

<CloudAvailableBadge/>

Type: Bool

Default value: 0

仅在 ClickHouse Cloud 中有效。允许绕过分布式缓存连接池。

## distributed_cache_connect_max_tries {#distributed_cache_connect_max_tries}

<CloudAvailableBadge/>

Type: UInt64

Default value: 20

仅在 ClickHouse Cloud 中有效。连接到分布式缓存失败时的最大尝试次数。

## distributed_cache_data_packet_ack_window {#distributed_cache_data_packet_ack_window}

<CloudAvailableBadge/>

Type: UInt64

Default value: 5

仅在 ClickHouse Cloud 中有效。一次分布式缓存读取请求中发送 ACK 的数据包序列窗口。

## distributed_cache_discard_connection_if_unread_data {#distributed_cache_discard_connection_if_unread_data}

<CloudAvailableBadge/>

Type: Bool

Default value: 1

仅在 ClickHouse Cloud 中有效。如果有未读取的数据，丢弃连接。

## distributed_cache_fetch_metrics_only_from_current_az {#distributed_cache_fetch_metrics_only_from_current_az}

<CloudAvailableBadge/>

Type: Bool

Default value: 1

仅在 ClickHouse Cloud 中有效。仅从系统中的当前可用区域获取指标。distributed_cache_metrics、system.distributed_cache_events。

## distributed_cache_log_mode {#distributed_cache_log_mode}

<CloudAvailableBadge/>

Type: DistributedCacheLogMode

Default value: on_error

仅在 ClickHouse Cloud 中有效。写入到系统的模式。distributed_cache_log。

## distributed_cache_max_unacked_inflight_packets {#distributed_cache_max_unacked_inflight_packets}

<CloudAvailableBadge/>

Type: UInt64

Default value: 10

仅在 ClickHouse Cloud 中有效。单次分布式缓存读取请求中最大未确认的在途数据包数。

## distributed_cache_min_bytes_for_seek {#distributed_cache_min_bytes_for_seek}

<CloudAvailableBadge/>

Type: Bool

Default value: 0

仅在 ClickHouse Cloud 中有效。进行分布式缓存查找的最小字节数。

## distributed_cache_pool_behaviour_on_limit {#distributed_cache_pool_behaviour_on_limit}

<CloudAvailableBadge/>

Type: DistributedCachePoolBehaviourOnLimit

Default value: wait

仅在 ClickHouse Cloud 中有效。识别达到池限制时分布式缓存连接的行为。

## distributed_cache_read_alignment {#distributed_cache_read_alignment}

<CloudAvailableBadge/>

Type: UInt64

Default value: 0

仅在 ClickHouse Cloud 中有效。测试用途的设置，请勿更改。

## distributed_cache_receive_response_wait_milliseconds {#distributed_cache_receive_response_wait_milliseconds}

<CloudAvailableBadge/>

Type: UInt64

Default value: 60000

仅在 ClickHouse Cloud 中有效。从分布式缓存接收请求数据的等待时间，以毫秒为单位。

## distributed_cache_receive_timeout_milliseconds {#distributed_cache_receive_timeout_milliseconds}

<CloudAvailableBadge/>

Type: UInt64

Default value: 10000

仅在 ClickHouse Cloud 中有效。等待从分布式缓存接收任何类型响应的时间，以毫秒为单位。

## distributed_cache_throw_on_error {#distributed_cache_throw_on_error}

<CloudAvailableBadge/>

Type: Bool

Default value: 0

仅在 ClickHouse Cloud 中有效。在与分布式缓存通信或从分布式缓存接收的异常发生时重新抛出异常。否则在出现错误时跳过分布式缓存。

## distributed_cache_wait_connection_from_pool_milliseconds {#distributed_cache_wait_connection_from_pool_milliseconds}

<CloudAvailableBadge/>

Type: UInt64

Default value: 100

仅在 ClickHouse Cloud 中有效。如果 distributed_cache_pool_behaviour_on_limit 为 wait，等待从连接池接收连接的时间，以毫秒为单位。

## distributed_connections_pool_size {#distributed_connections_pool_size}

Type: UInt64

Default value: 1024

用于分布式处理所有查询到单个 Distributed 表的远程服务器的最大并发连接数。我们建议设置一个不小于集群中服务器数量的值。

## distributed_ddl_entry_format_version {#distributed_ddl_entry_format_version}

Type: UInt64

Default value: 5

分布式 DDL (ON CLUSTER) 查询的兼容性版本。

## distributed_ddl_output_mode {#distributed_ddl_output_mode}

Type: DistributedDDLOutputMode

Default value: throw

设置分布式 DDL 查询结果的格式。

可能的值：

- `throw` — 为所有查询已完成的主机返回查询执行状态的结果集。如果查询在某些主机上失败，那么将重新抛出首个异常。如果查询在某些主机上仍未完成且 [distributed_ddl_task_timeout](#distributed_ddl_task_timeout) 超过了，那么会抛出 `TIMEOUT_EXCEEDED` 异常。
- `none` — 类似于 throw，但分布式 DDL 查询不返回结果集。
- `null_status_on_timeout` — 在结果集的某些行中返回 `NULL` 作为执行状态，而不是在查询未在相应主机上完成时抛出 `TIMEOUT_EXCEEDED`。
- `never_throw` — 不抛出 `TIMEOUT_EXCEEDED`，如果查询在某些主机上失败则不重新抛出异常。
- `none_only_active` — 同 `none`，但不等待 `Replicated` 数据库的非活动副本。注意：在这种模式下无法判断查询是否未在某个副本上执行，并将在后台执行。
- `null_status_on_timeout_only_active` — 类似于 `null_status_on_timeout`，但不等待 `Replicated` 数据库的非活动副本。
- `throw_only_active` — 类似于 `throw`，但不等待 `Replicated` 数据库的非活动副本。

云默认值：`none`。

## distributed_ddl_task_timeout {#distributed_ddl_task_timeout}

Type: Int64

Default value: 180

设置来自集群中所有主机的 DDL 查询响应的超时。如果 DDL 请求在所有主机上没有执行，则响应将包含超时错误，并且请求将以异步模式执行。负值表示无限制。

可能的值：

- 正整数。
- 0 — 异步模式。
- 负整数 — 无限超时。

## distributed_foreground_insert {#distributed_foreground_insert}

Type: Bool

Default value: 0

启用或禁用对 [Distributed](/engines/table-engines/special/distributed) 表的同步数据插入。

默认情况下，当将数据插入 `Distributed` 表时，ClickHouse 服务器以后台模式将数据发送到集群节点。当 `distributed_foreground_insert=1` 时，数据会被同步处理，`INSERT` 操作仅在所有数据都保存在所有分片上（每个分片至少一个副本，如果 `internal_replication` 为真）后才成功。

可能的值：

- 0 — 数据在后台模式下插入。
- 1 — 数据以同步模式插入。

云默认值：`1`。

**另请参见**

- [分布式表引擎](/engines/table-engines/special/distributed)
- [管理分布式表](/sql-reference/statements/system#managing-distributed-tables)

## distributed_group_by_no_merge {#distributed_group_by_no_merge}

Type: UInt64

Default value: 0

在分布式查询处理中不合并来自不同服务器的聚合状态，如果确定不同分片上有不同的键，则可以使用此选项。

可能的值：

- `0` — 禁用（最终查询处理在发起节点上完成）。
- `1` - 在分布式查询处理中不合并来自不同服务器的聚合状态（查询在分片上完全处理，发起者仅代理数据），可以在确定不同分片上有不同的键的情况下使用。
- `2` - 与 `1` 相同，但在发起者上应用 `ORDER BY` 和 `LIMIT`（在远程节点上查询完全处理的情况下无法实现，例如 `distributed_group_by_no_merge=1`）。

**示例**

```sql
SELECT *
FROM remote('127.0.0.{2,3}', system.one)
GROUP BY dummy
LIMIT 1
SETTINGS distributed_group_by_no_merge = 1
FORMAT PrettyCompactMonoBlock

┌─dummy─┐
│     0 │
│     0 │
└───────┘
```

```sql
SELECT *
FROM remote('127.0.0.{2,3}', system.one)
GROUP BY dummy
LIMIT 1
SETTINGS distributed_group_by_no_merge = 2
FORMAT PrettyCompactMonoBlock

┌─dummy─┐
│     0 │
└───────┘
```

## distributed_insert_skip_read_only_replicas {#distributed_insert_skip_read_only_replicas}

Type: Bool

Default value: 0

启用跳过只读副本的 INSERT 查询。

可能的值：

- 0 — INSERT 如常，如果将前往只读副本，将失败。
- 1 — 发起者在发送数据到分片之前将跳过只读副本。

## distributed_product_mode {#distributed_product_mode}

Type: DistributedProductMode

Default value: deny

更改 [分布式子查询](../../sql-reference/operators/in.md) 的行为。

ClickHouse 在查询包含分布式表的乘法，即查询分布式表时包含针对该分布式表的非 GLOBA  子查询时应用此设置。

限制：

- 仅适用于 IN 和 JOIN 子查询。
- 仅当 FROM 部分使用包含多个分片的分布式表时。
- 如果子查询涉及到包含多个分片的分布式表。
- 不用于表值 [remote](../../sql-reference/table-functions/remote.md) 函数。

可能的值：

- `deny` — 默认值。禁止使用这些类型的子查询（返回 “禁止双重分布式 IN/JOIN 子查询” 异常）。
- `local` — 在子查询中将数据库和表替换为目标服务器（分片）的本地对象（保留正常的 `IN`/`JOIN`）。
- `global` — 将 `IN`/`JOIN` 查询替换为 `GLOBAL IN`/`GLOBAL JOIN`。
- `allow` — 允许使用这些类型的子查询。

## distributed_push_down_limit {#distributed_push_down_limit}

Type: UInt64

Default value: 1

启用或禁用 [LIMIT](#limit) 在每个分片上单独应用。

这将避免：
- 通过网络发送多余的行；
- 在发起者上处理超过限制的行。

从 21.9 版本开始，您无法再获得不准确的结果，因为 `distributed_push_down_limit` 仅在满足至少一个条件时更改查询执行：
- [distributed_group_by_no_merge](#distributed_group_by_no_merge) > 0。
- 查询 **不具有** `GROUP BY`/`DISTINCT`/`LIMIT BY`，但具有 `ORDER BY`/`LIMIT`。
- 查询 **具有** `GROUP BY`/`DISTINCT`/`LIMIT BY` 与 `ORDER BY`/`LIMIT` 且：
    - [optimize_skip_unused_shards](#optimize_skip_unused_shards) 已启用。
    - [optimize_distributed_group_by_sharding_key](#optimize_distributed_group_by_sharding_key) 已启用。

可能的值：

- 0 — 禁用。
- 1 — 启用。

另见：

- [distributed_group_by_no_merge](#distributed_group_by_no_merge)
- [optimize_skip_unused_shards](#optimize_skip_unused_shards)
- [optimize_distributed_group_by_sharding_key](#optimize_distributed_group_by_sharding_key)

## distributed_replica_error_cap {#distributed_replica_error_cap}

Type: UInt64

Default value: 1000

- 类型：无符号整型
- 默认值：1000

每个副本的错误计数限制在此值上，防止单个副本积累过多错误。

另请参见：

- [load_balancing](#load_balancing-round_robin)
- [表引擎 Distributed](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_half_life](#distributed_replica_error_half_life)
- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)

## distributed_replica_error_half_life {#distributed_replica_error_half_life}

Type: Seconds

Default value: 60

- 类型：秒
- 默认值：60 秒

控制分布式表中错误归零的速度。如果某个副本在一段时间内不可用，累计 5 个错误，且 distributed_replica_error_half_life 设置为 1 秒，则该副本在最后一个错误后 3 秒被视为正常。

另请参见：

- [load_balancing](#load_balancing-round_robin)
- [表引擎 Distributed](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_cap](#distributed_replica_error_cap)
- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)

## distributed_replica_max_ignored_errors {#distributed_replica_max_ignored_errors}

Type: UInt64

Default value: 0

- 类型：无符号整型
- 默认值：0

在选择副本时将被忽略的错误数量（根据 `load_balancing` 算法）。

另请参见：

- [load_balancing](#load_balancing-round_robin)
- [表引擎 Distributed](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_cap](#distributed_replica_error_cap)
- [distributed_replica_error_half_life](#distributed_replica_error_half_life)

## do_not_merge_across_partitions_select_final {#do_not_merge_across_partitions_select_final}

Type: Bool

Default value: 0

仅在选择最终时合并同一分区中的片段。

## empty_result_for_aggregation_by_constant_keys_on_empty_set {#empty_result_for_aggregation_by_constant_keys_on_empty_set}

Type: Bool

Default value: 1

在对空集合按常量键进行聚合时返回空结果。

## empty_result_for_aggregation_by_empty_set {#empty_result_for_aggregation_by_empty_set}

Type: Bool

Default value: 0

在对空集合进行无键聚合时返回空结果。

## enable_adaptive_memory_spill_scheduler {#enable_adaptive_memory_spill_scheduler}
<ExperimentalBadge/>

Type: Bool

Default value: 0

触发处理器自适应地将数据溢出到外部存储。目前支持的范围连接。

## enable_blob_storage_log {#enable_blob_storage_log}

Type: Bool

Default value: 1

将有关 blob 存储操作的信息写入 system.blob_storage_log 表。

## enable_deflate_qpl_codec {#enable_deflate_qpl_codec}

Type: Bool

Default value: 0

如果开启，DEFLATE_QPL 编解码器可用于压缩列。

## enable_early_constant_folding {#enable_early_constant_folding}

Type: Bool

Default value: 1

启用查询优化，我们分析函数和子查询结果，如果存在常量则重写查询。

## enable_extended_results_for_datetime_functions {#enable_extended_results_for_datetime_functions}

Type: Bool

Default value: 0

启用或禁用返回类型结果：
- `Date32` 具有扩展范围（与类型 `Date` 相比）用于函数 [toStartOfYear](../../sql-reference/functions/date-time-functions.md/#tostartofyear)、 [toStartOfISOYear](../../sql-reference/functions/date-time-functions.md/#tostartofisoyear)、 [toStartOfQuarter](../../sql-reference/functions/date-time-functions.md/#tostartofquarter)、 [toStartOfMonth](../../sql-reference/functions/date-time-functions.md/#tostartofmonth)、 [toLastDayOfMonth](../../sql-reference/functions/date-time-functions.md/#tolastdayofmonth)、 [toStartOfWeek](../../sql-reference/functions/date-time-functions.md/#tostartofweek)、 [toLastDayOfWeek](../../sql-reference/functions/date-time-functions.md/#tolastdayofweek) 和 [toMonday](../../sql-reference/functions/date-time-functions.md/#tomonday)。
- `DateTime64` 具有扩展范围（与类型 `DateTime` 相比）用于函数 [toStartOfDay](../../sql-reference/functions/date-time-functions.md/#tostartofday)、 [toStartOfHour](../../sql-reference/functions/date-time-functions.md/#tostartofhour)、 [toStartOfMinute](../../sql-reference/functions/date-time-functions.md/#tostartofminute)、 [toStartOfFiveMinutes](../../sql-reference/functions/date-time-functions.md/#tostartoffiveminutes)、 [toStartOfTenMinutes](../../sql-reference/functions/date-time-functions.md/#tostartoftenminutes)、 [toStartOfFifteenMinutes](../../sql-reference/functions/date-time-functions.md/#tostartoffifteenminutes) 和 [timeSlot](../../sql-reference/functions/date-time-functions.md/#timeslot)。

可能的值：

- 0 — 函数为所有类型的参数返回 `Date` 或 `DateTime`。
- 1 — 函数对 `Date32` 或 `DateTime64` 参数返回 `Date32` 或 `DateTime64`，对其他参数返回 `Date` 或 `DateTime`。

## enable_filesystem_cache {#enable_filesystem_cache}

Type: Bool

Default value: 1

为远程文件系统使用缓存。此设置不会打开/关闭磁盘的缓存（必须通过磁盘配置完成），但允许在某些查询中绕过缓存，如果需要的话。

## enable_filesystem_cache_log {#enable_filesystem_cache_log}

Type: Bool

Default value: 0

允许记录每个查询的文件系统缓存日志。

## enable_filesystem_cache_on_write_operations {#enable_filesystem_cache_on_write_operations}

Type: Bool

Default value: 0

在写操作时写入缓存。要实际工作，此设置也需要添加到磁盘配置中。

## enable_filesystem_read_prefetches_log {#enable_filesystem_read_prefetches_log}

Type: Bool

Default value: 0

在查询期间记录到系统.filesystem 的预取日志。仅应用于测试或调试，不建议默认打开。

## enable_global_with_statement {#enable_global_with_statement}

Type: Bool

Default value: 1

将 WITH 语句传播至 UNION 查询和所有子查询。

## enable_http_compression {#enable_http_compression}

Type: Bool

Default value: 0

启用或禁用对 HTTP 请求的响应数据进行压缩。

有关更多信息，请阅读 [HTTP 接口描述](../../interfaces/http.md)。

可能的值：

- 0 — 禁用。
- 1 — 启用。

## enable_job_stack_trace {#enable_job_stack_trace}

Type: Bool

Default value: 1

在作业结果出现异常时输出作业创建者的堆栈追踪。

## enable_lightweight_delete {#enable_lightweight_delete}

Type: Bool

Default value: 1

为 mergetree 表启用轻量级 DELETE 变更。

## enable_memory_bound_merging_of_aggregation_results {#enable_memory_bound_merging_of_aggregation_results}

Type: Bool

Default value: 1

启用内存绑定的聚合结果合并策略。

## enable_multiple_prewhere_read_steps {#enable_multiple_prewhere_read_steps}

Type: Bool

Default value: 1

将更多条件从 WHERE 移动到 PREWHERE，并在存在多个条件与 AND 组合时进行多步从磁盘读取和过滤。

## enable_named_columns_in_function_tuple {#enable_named_columns_in_function_tuple}

Type: Bool

Default value: 0

在函数 tuple() 中生成命名元组，当所有名称唯一且可视为未加引号的标识符时。

## enable_optimize_predicate_expression {#enable_optimize_predicate_expression}

Type: Bool

Default value: 1

在 `SELECT` 查询中启用谓词下推。

谓词下推可能显著减少分布式查询的网络流量。

可能的值：

- 0 — 禁用。
- 1 — 启用。

使用

考虑以下查询：

1.  `SELECT count() FROM test_table WHERE date = '2018-10-10'`
2.  `SELECT count() FROM (SELECT * FROM test_table) WHERE date = '2018-10-10'`

如果 `enable_optimize_predicate_expression = 1`，那么这些查询的执行时间是相等的，因为 ClickHouse 在处理子查询时将 `WHERE` 应用于子查询。

如果 `enable_optimize_predicate_expression = 0`，那么第二个查询的执行时间会长得多，因为 `WHERE` 子句对所有数据应用在子查询完成之后。

## enable_optimize_predicate_expression_to_final_subquery {#enable_optimize_predicate_expression_to_final_subquery}

Type: Bool

Default value: 1

允许将谓词推送到最终子查询。

## enable_order_by_all {#enable_order_by_all}

Type: Bool

Default value: 1

启用或禁用使用 `ORDER BY ALL` 语法进行排序，参见 [ORDER BY](../../sql-reference/statements/select/order-by.md)。

可能的值：

- 0 — 禁用 ORDER BY ALL。
- 1 — 启用 ORDER BY ALL。

**示例**

查询：

```sql
CREATE TABLE TAB(C1 Int, C2 Int, ALL Int) ENGINE=Memory();

INSERT INTO TAB VALUES (10, 20, 30), (20, 20, 10), (30, 10, 20);

SELECT * FROM TAB ORDER BY ALL; -- returns an error that ALL is ambiguous

SELECT * FROM TAB ORDER BY ALL SETTINGS enable_order_by_all = 0;
```

结果：

```text
┌─C1─┬─C2─┬─ALL─┐
│ 20 │ 20 │  10 │
│ 30 │ 10 │  20 │
│ 10 │ 20 │  30 │
└────┴────┴─────┘
```

## enable_parsing_to_custom_serialization {#enable_parsing_to_custom_serialization}

Type: Bool

Default value: 1

如果为真，则数据可以根据从表格获得的序列化提示，直接解析到带有自定义序列化的列中（例如稀疏列）。

## enable_positional_arguments {#enable_positional_arguments}

Type: Bool

Default value: 1

启用或禁用对 [GROUP BY](/sql-reference/statements/select/group-by)、 [LIMIT BY](../../sql-reference/statements/select/limit-by.md)、 [ORDER BY](../../sql-reference/statements/select/order-by.md) 语句的支持。

可能的值：

- 0 — 不支持位置参数。
- 1 — 支持位置参数：可以使用列号代替列名。

**示例**

查询：

```sql
CREATE TABLE positional_arguments(one Int, two Int, three Int) ENGINE=Memory();

INSERT INTO positional_arguments VALUES (10, 20, 30), (20, 20, 10), (30, 10, 20);

SELECT * FROM positional_arguments ORDER BY 2,3;
```

结果：

```text
┌─one─┬─two─┬─three─┐
│  30 │  10 │   20  │
│  20 │  20 │   10  │
│  10 │  20 │   30  │
└─────┴─────┴───────┘
```

## enable_reads_from_query_cache {#enable_reads_from_query_cache}

Type: Bool

Default value: 1

如果启用，`SELECT` 查询的结果将从 [查询缓存](../query-cache.md) 中检索。

可能的值：

- 0 - 禁用
- 1 - 启用

## enable_s3_requests_logging {#enable_s3_requests_logging}

Type: Bool

Default value: 0

启用对 S3 请求的非常明确的日志记录。这仅适用于调试。

## enable_scalar_subquery_optimization {#enable_scalar_subquery_optimization}

Type: Bool

Default value: 1

如果设置为真，可以防止标量子查询（反序列化）大型标量值，并可能避免多次运行相同的子查询。

## enable_sharing_sets_for_mutations {#enable_sharing_sets_for_mutations}

Type: Bool

Default value: 1

允许在相同变更的不同任务之间共享为 IN 子查询构建的共享集对象。这减少了内存使用和 CPU 消耗。

## enable_software_prefetch_in_aggregation {#enable_software_prefetch_in_aggregation}

Type: Bool

Default value: 1

启用在聚合中使用软件预取。

## enable_unaligned_array_join {#enable_unaligned_array_join}

Type: Bool

Default value: 0

允许使用具有不同尺寸的多个数组进行 ARRAY JOIN。当启用此设置时，数组将调整为最长的一个。

## enable_url_encoding {#enable_url_encoding}

Type: Bool

Default value: 1

允许在 [URL](../../engines/table-engines/special/url.md) 引擎表中启用/禁用解码/编码路径。

默认情况下启用。

## enable_vertical_final {#enable_vertical_final}

Type: Bool

Default value: 1

如果启用，在 FINAL 期间通过将行标记为删除并稍后进行过滤，而不是合并行来去除重复行。

## enable_writes_to_query_cache {#enable_writes_to_query_cache}

Type: Bool

Default value: 1

如果启用，`SELECT` 查询的结果将在 [查询缓存](../query-cache.md) 中存储。

可能的值：

- 0 - 禁用
- 1 - 启用

## enable_zstd_qat_codec {#enable_zstd_qat_codec}

Type: Bool

Default value: 0

如果开启，ZSTD_QAT 编解码器可用于压缩列。

## enforce_strict_identifier_format {#enforce_strict_identifier_format}

Type: Bool

Default value: 0

如果启用，则仅允许包含字母数字字符和下划线的标识符。

## engine_file_allow_create_multiple_files {#engine_file_allow_create_multiple_files}

Type: Bool

Default value: 0

启用或禁用在文件引擎表中对于每次插入创建新文件（如果格式带有后缀（`JSON`、`ORC`、`Parquet` 等））。如果启用，每次插入都会创建一个新文件，其名称遵循以下模式：

`data.Parquet` -> `data.1.Parquet` -> `data.2.Parquet` 等。

可能的值：
- 0 — `INSERT` 查询将新数据追加到文件末尾。
- 1 — `INSERT` 查询将创建一个新文件。
```

## engine_file_empty_if_not_exists {#engine_file_empty_if_not_exists}

类型：布尔

默认值：0

允许从没有文件的文件引擎表中选择数据。

可能的值：
- 0 — `SELECT` 抛出异常。
- 1 — `SELECT` 返回空结果。
## engine_file_skip_empty_files {#engine_file_skip_empty_files}

类型：布尔

默认值：0

启用或禁用跳过 [File](../../engines/table-engines/special/file.md) 引擎表中的空文件。

可能的值：
- 0 — 如果空文件与请求的格式不兼容，则 `SELECT` 抛出异常。
- 1 — `SELECT` 返回空结果，针对空文件。
## engine_file_truncate_on_insert {#engine_file_truncate_on_insert}

类型：布尔

默认值：0

启用或禁用在 [File](../../engines/table-engines/special/file.md) 引擎表插入前的截断。

可能的值：
- 0 — `INSERT` 查询将新数据附加到文件末尾。
- 1 — `INSERT` 查询用新数据替换文件的现有内容。
## engine_url_skip_empty_files {#engine_url_skip_empty_files}

类型：布尔

默认值：0

启用或禁用跳过 [URL](../../engines/table-engines/special/url.md) 引擎表中的空文件。

可能的值：
- 0 — 如果空文件与请求的格式不兼容，则 `SELECT` 抛出异常。
- 1 — `SELECT` 返回空结果，针对空文件。
## except_default_mode {#except_default_mode}

类型：SetOperationMode

默认值：ALL

在 EXCEPT 查询中设置默认模式。可能的值：空字符串、'ALL'、'DISTINCT'。如果为空，则不带模式的查询将抛出异常。
## external_storage_connect_timeout_sec {#external_storage_connect_timeout_sec}

类型：UInt64

默认值：10

连接超时（秒）。目前仅支持 MySQL。
## external_storage_max_read_bytes {#external_storage_max_read_bytes}

类型：UInt64

默认值：0

限制在外部引擎表中刷新历史数据时的最大字节数。当前仅支持 MySQL 表引擎、数据库引擎和字典。如果等于 0，则此设置被禁用。
## external_storage_max_read_rows {#external_storage_max_read_rows}

类型：UInt64

默认值：0

限制在外部引擎表中刷新历史数据时的最大行数。当前仅支持 MySQL 表引擎、数据库引擎和字典。如果等于 0，则此设置被禁用。
## external_storage_rw_timeout_sec {#external_storage_rw_timeout_sec}

类型：UInt64

默认值：300

读/写超时（秒）。目前仅支持 MySQL。
## external_table_functions_use_nulls {#external_table_functions_use_nulls}

类型：布尔

默认值：1

定义如何在 [mysql](../../sql-reference/table-functions/mysql.md)、[postgresql](../../sql-reference/table-functions/postgresql.md) 和 [odbc](../../sql-reference/table-functions/odbc.md) 表函数中使用 Nullable 列。

可能的值：

- 0 — 表函数显式使用 Nullable 列。
- 1 — 表函数隐式使用 Nullable 列。

**用法**

如果设置为 `0`，则表函数不创建 Nullable 列，并用默认值替代 NULL。这也适用于数组内部的 NULL 值。
## external_table_strict_query {#external_table_strict_query}

类型：布尔

默认值：0

如果设置为 true，则禁止将表达式转换为外部表查询的本地过滤器。
## extract_key_value_pairs_max_pairs_per_row {#extract_key_value_pairs_max_pairs_per_row}

类型：UInt64

默认值：1000

`extractKeyValuePairs` 函数每行可以产生的最大对数。作为防止消耗过多内存的保护措施。
## extremes {#extremes}

类型：布尔

默认值：0

是否计数极值（查询结果列中的最小值和最大值）。接受 0 或 1。默认值为 0（禁用）。
有关更多信息，请参见“极值”部分。
## fallback_to_stale_replicas_for_distributed_queries {#fallback_to_stale_replicas_for_distributed_queries}

类型：布尔

默认值：1

如果没有可用的更新数据，则强制查询过时副本。请参见 [Replication](../../engines/table-engines/mergetree-family/replication.md)。

ClickHouse 从表的过时副本中选择最相关的。

在从指向已复制表的分布式表执行 `SELECT` 时使用。

默认情况下，1（启用）。
## filesystem_cache_boundary_alignment {#filesystem_cache_boundary_alignment}

类型：UInt64

默认值：0

文件系统缓存边界对齐。此设置仅适用于非磁盘读取（例如，远程表引擎 / 表函数的缓存，但不适用于 MergeTree 表的存储配置）。值 0 表示没有对齐。
## filesystem_cache_enable_background_download_during_fetch {#filesystem_cache_enable_background_download_during_fetch}

<CloudAvailableBadge/>

类型：布尔

默认值：1

仅在 ClickHouse Cloud 中有效。锁定缓存空间预留的等待时间。
## filesystem_cache_enable_background_download_for_metadata_files_in_packed_storage {#filesystem_cache_enable_background_download_for_metadata_files_in_packed_storage}

<CloudAvailableBadge/>

类型：布尔

默认值：1

仅在 ClickHouse Cloud 中有效。锁定缓存空间预留的等待时间。
## filesystem_cache_max_download_size {#filesystem_cache_max_download_size}

类型：UInt64

默认值：137438953472

单个查询可以下载的最大远程文件系统缓存大小。
## filesystem_cache_name {#filesystem_cache_name}

类型：字符串

默认值：

用于无状态表引擎或数据湖的文件系统缓存名称。
## filesystem_cache_prefer_bigger_buffer_size {#filesystem_cache_prefer_bigger_buffer_size}

类型：布尔

默认值：1

如果启用文件系统缓存，则优先选择更大的缓冲区尺寸，以避免写入小文件片段而恶化缓存性能。另一方面，启用此设置可能会增加内存使用。
## filesystem_cache_reserve_space_wait_lock_timeout_milliseconds {#filesystem_cache_reserve_space_wait_lock_timeout_milliseconds}

类型：UInt64

默认值：1000

在文件系统缓存中锁定缓存以进行空间预留的等待时间。
## filesystem_cache_segments_batch_size {#filesystem_cache_segments_batch_size}

类型：UInt64

默认值：20

读取缓冲区可以从缓存请求的单个文件片段的批处理大小限制。值过低将导致对缓存的请求过多，值过大可能会减慢从缓存中驱逐的速度。
## filesystem_cache_skip_download_if_exceeds_per_query_cache_write_limit {#filesystem_cache_skip_download_if_exceeds_per_query_cache_write_limit}

类型：布尔

默认值：1

如果超过查询缓存大小，则跳过从远程文件系统的下载。
## filesystem_prefetch_max_memory_usage {#filesystem_prefetch_max_memory_usage}

类型：UInt64

默认值：1073741824

预取的最大内存使用量。
## filesystem_prefetch_step_bytes {#filesystem_prefetch_step_bytes}

类型：UInt64

默认值：0

以字节为单位的预取步长。零表示 `auto` - 大约最佳的预取步长将自动推导，但可能不是 100% 最优。实际值可能会因设置 filesystem_prefetch_min_bytes_for_single_read_task 而有所不同。
## filesystem_prefetch_step_marks {#filesystem_prefetch_step_marks}

类型：UInt64

默认值：0

以标记为单位的预取步长。零表示 `auto` - 大约最佳的预取步长将自动推导，但可能不是 100% 最优。实际值可能会因设置 filesystem_prefetch_min_bytes_for_single_read_task 而有所不同。
## filesystem_prefetches_limit {#filesystem_prefetches_limit}

类型：UInt64

默认值：200

预取的最大数量。零表示无限制。如果您想限制预取数量，建议使用设置 `filesystem_prefetches_max_memory_usage`。
## final {#final}

类型：布尔

默认值：0

自动对查询中的所有表应用 [FINAL](../../sql-reference/statements/select/from.md/#final-modifier) 修饰符，适用于 [FINAL](../../sql-reference/statements/select/from.md/#final-modifier) 的表，包括连接表和子查询中的表，以及分布式表。

可能的值：

- 0 - 禁用
- 1 - 启用

示例：

```sql
CREATE TABLE test
(
    key Int64,
    some String
)
ENGINE = ReplacingMergeTree
ORDER BY key;

INSERT INTO test FORMAT Values (1, 'first');
INSERT INTO test FORMAT Values (1, 'second');

SELECT * FROM test;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘
┌─key─┬─some──┐
│   1 │ first │
└─────┴───────┘

SELECT * FROM test SETTINGS final = 1;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘

SET final = 1;
SELECT * FROM test;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘
```
## flatten_nested {#flatten_nested}

类型：布尔

默认值：1

设置 [nested](../../sql-reference/data-types/nested-data-structures/index.md) 列的数据格式。

可能的值：

- 1 — 嵌套列展平为单独的数组。
- 0 — 嵌套列保持为元组的单一数组。

**用法**

如果设置为 `0`，则可以使用任意层级的嵌套。

**示例**

查询：

``` sql
SET flatten_nested = 1;
CREATE TABLE t_nest (`n` Nested(a UInt32, b UInt32)) ENGINE = MergeTree ORDER BY tuple();

SHOW CREATE TABLE t_nest;
```

结果：

``` text
┌─statement───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.t_nest
(
    `n.a` Array(UInt32),
    `n.b` Array(UInt32)
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS index_granularity = 8192 │
└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

查询：

``` sql
SET flatten_nested = 0;

CREATE TABLE t_nest (`n` Nested(a UInt32, b UInt32)) ENGINE = MergeTree ORDER BY tuple();

SHOW CREATE TABLE t_nest;
```

结果：

``` text
┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.t_nest
(
    `n` Nested(a UInt32, b UInt32)
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS index_granularity = 8192 │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```
## force_aggregate_partitions_independently {#force_aggregate_partitions_independently}

类型：布尔

默认值：0

强制在适用时使用优化，但启发式决定不使用它。
## force_aggregation_in_order {#force_aggregation_in_order}

类型：布尔

默认值：0

该设置由服务器本身用于支持分布式查询。请勿手动更改，因为这将破坏正常操作。（强制在分布式聚合期间在远程节点上按顺序使用聚合）。
## force_data_skipping_indices {#force_data_skipping_indices}

类型：字符串

默认值：

如果未使用传递的数据跳过索引，则禁用查询执行。

考虑以下示例：

```sql
CREATE TABLE data
(
    key Int,
    d1 Int,
    d1_null Nullable(Int),
    INDEX d1_idx d1 TYPE minmax GRANULARITY 1,
    INDEX d1_null_idx assumeNotNull(d1_null) TYPE minmax GRANULARITY 1
)
Engine=MergeTree()
ORDER BY key;

SELECT * FROM data_01515;
SELECT * FROM data_01515 SETTINGS force_data_skipping_indices=''; -- 查询将产生 CANNOT_PARSE_TEXT 错误。
SELECT * FROM data_01515 SETTINGS force_data_skipping_indices='d1_idx'; -- 查询将产生 INDEX_NOT_USED 错误。
SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='d1_idx'; -- OK。
SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='`d1_idx`'; -- OK （全特征解析器的示例）。
SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='`d1_idx`, d1_null_idx'; -- 查询将产生 INDEX_NOT_USED 错误，因为未使用 d1_null_idx。
SELECT * FROM data_01515 WHERE d1 = 0 AND assumeNotNull(d1_null) = 0 SETTINGS force_data_skipping_indices='`d1_idx`, d1_null_idx'; -- OK。
```
## force_grouping_standard_compatibility {#force_grouping_standard_compatibility}

类型：布尔

默认值：1

使 GROUPING 函数在参数未用作聚合键时返回 1。
## force_index_by_date {#force_index_by_date}

类型：布尔

默认值：0

如果索引无法根据日期使用，则禁用查询执行。

适用于 MergeTree 家族的表。

如果 `force_index_by_date=1`，ClickHouse 将检查查询是否具有可以用来限制数据范围的日期键条件。如果没有合适的条件，则抛出异常。但是，它不会检查条件是否减少要读取的数据量。例如，条件 `Date != ' 2000-01-01 '` 是可接受的，即使它与表中的所有数据匹配（即，运行查询需要全表扫描）。有关 MergeTree 表中数据范围的更多信息，请参见 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)。
## force_optimize_projection {#force_optimize_projection}

类型：布尔

默认值：0

启用或禁用在 `SELECT` 查询中强制使用 [projections](../../engines/table-engines/mergetree-family/mergetree.md/#projections)，当启用投影优化时（请参见 [optimize_use_projections](#optimize_use_projections) 设置）。

可能的值：

- 0 — 投影优化不是强制的。
- 1 — 投影优化是强制的。
## force_optimize_projection_name {#force_optimize_projection_name}

类型：字符串

默认值：

如果设置为非空字符串，则检查此投影是否在查询中至少使用过一次。

可能的值：

- 字符串：在查询中使用的投影的名称。
## force_optimize_skip_unused_shards {#force_optimize_skip_unused_shards}

类型：UInt64

默认值：0

如果启用 [optimize_skip_unused_shards](#force_optimize_skip_unused_shards) 且无法跳过未使用的分片，则启用或禁用查询执行。如果无法跳过并且启用该设置，将引发异常。

可能的值：

- 0 — 禁用。ClickHouse 不会抛出异常。
- 1 — 启用。仅当表具有分片键时，查询执行被禁用。
- 2 — 启用。无论表是否已定义分片键，查询执行均被禁用。
## force_optimize_skip_unused_shards_nesting {#force_optimize_skip_unused_shards_nesting}

类型：UInt64

默认值：0

控制 [`force_optimize_skip_unused_shards`](#force_optimize_skip_unused_shards)（因此仍然需要 [`force_optimize_skip_unused_shards`](#force_optimize_skip_unused_shards)）取决于分布式查询的嵌套级别（当您有一个 `Distributed` 表指向另一个 `Distributed` 表时）。

可能的值：

- 0 - 禁用，`force_optimize_skip_unused_shards` 总是有效。
- 1 — 仅在第一级启用 `force_optimize_skip_unused_shards`。
- 2 — 允许 `force_optimize_skip_unused_shards` 直至第二级。
## force_primary_key {#force_primary_key}

类型：布尔

默认值：0

如果无法按主键索引，则禁用查询执行。

适用于 MergeTree 家族的表。

如果 `force_primary_key=1`，ClickHouse 将检查查询是否具有可以用来限制数据范围的主键条件。如果没有合适的条件，则抛出异常。但是，它不会检查条件是否减少要读取的数据量。有关 MergeTree 表中数据范围的更多信息，请参见 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)。
## force_remove_data_recursively_on_drop {#force_remove_data_recursively_on_drop}

类型：布尔

默认值：0

在 DROP 查询时递归删除数据。避免 “目录不为空” 的错误，但可能会静默删除已脱离的数据。
## formatdatetime_f_prints_scale_number_of_digits {#formatdatetime_f_prints_scale_number_of_digits}

类型：布尔

默认值：0

格式化程序 '%f' 在 'formatDateTime' 函数中仅打印 DateTime64 的精度数字，而不是固定的 6 位数字。
## formatdatetime_f_prints_single_zero {#formatdatetime_f_prints_single_zero}

类型：布尔

默认值：0

格式化程序 '%f' 在 'formatDateTime' 函数中打印单个零，而不是六个零，如果格式化值没有小数秒。
## formatdatetime_format_without_leading_zeros {#formatdatetime_format_without_leading_zeros}

类型：布尔

默认值：0

格式化程序 '%c'、'%l' 和 '%k' 在 'formatDateTime' 函数中打印没有前导零的月份和小时。
## formatdatetime_parsedatetime_m_is_month_name {#formatdatetime_parsedatetime_m_is_month_name}

类型：布尔

默认值：1

格式化程序 '%M' 在 'formatDateTime' 和 'parseDateTime' 函数中打印/解析月份名称而不是分钟。
## fsync_metadata {#fsync_metadata}

类型：布尔

默认值：1

启用或禁用在写入 `.sql` 文件时进行 [fsync](http://pubs.opengroup.org/onlinepubs/9699919799/functions/fsync.html)。默认启用。

如果服务器有数百万个不断创建和销毁的小表，禁用它是有意义的。
## function_implementation {#function_implementation}

类型：字符串

默认值：

选择特定目标或变体的函数实现（实验性）。如果为空，则启用所有实现。
## function_json_value_return_type_allow_complex {#function_json_value_return_type_allow_complex}

类型：布尔

默认值：0

控制是否允许 `json_value` 函数返回复杂类型（例如：结构、数组、映射）。

```sql
SELECT JSON_VALUE('{"hello":{"world":"!"}}', '$.hello') settings function_json_value_return_type_allow_complex=true

┌─JSON_VALUE('{"hello":{"world":"!"}}', '$.hello')─┐
│ {"world":"!"}                                    │
└──────────────────────────────────────────────────┘

1 行在集。耗时：0.001 秒。
```

可能的值：

- true — 允许。
- false — 不允许。
## function_json_value_return_type_allow_nullable {#function_json_value_return_type_allow_nullable}

类型：布尔

默认值：0

控制是否允许 `JSON_VALUE` 函数在值不存在时返回 `NULL`。

```sql
SELECT JSON_VALUE('{"hello":"world"}', '$.b') settings function_json_value_return_type_allow_nullable=true;

┌─JSON_VALUE('{"hello":"world"}', '$.b')─┐
│ ᴺᵁᴸᴸ                                   │
└────────────────────────────────────────┘

1 行在集。耗时：0.001 秒。
```

可能的值：

- true — 允许。
- false — 不允许。
## function_locate_has_mysql_compatible_argument_order {#function_locate_has_mysql_compatible_argument_order}

类型：布尔

默认值：1

控制函数 [locate](../../sql-reference/functions/string-search-functions.md/#locate) 中参数的顺序。

可能的值：

- 0 — 函数 `locate` 接受参数 `(haystack, needle[, start_pos])`。
- 1 — 函数 `locate` 接受参数 `(needle, haystack, [, start_pos])`（MySQL 兼容行为）。
## function_range_max_elements_in_block {#function_range_max_elements_in_block}

类型：UInt64

默认值：500000000

为函数 [range](/sql-reference/functions/array-functions#rangeend-rangestart--end--step) 生成的数据量设置安全阈值。定义函数每个数据块生成的最大值（每行的数组大小之和）。

可能的值：

- 正整数。

**另见**

- [max_block_size](#max_block_size)
- [min_insert_block_size_rows](#min_insert_block_size_rows)
## function_sleep_max_microseconds_per_block {#function_sleep_max_microseconds_per_block}

类型：UInt64

默认值：3000000

函数 `sleep` 允许在每个块中休眠的最大微秒数。如果用户调用它使用更大的值，将抛出异常。它是一个安全阈值。
## function_visible_width_behavior {#function_visible_width_behavior}

类型：UInt64

默认值：1

`visibleWidth` 行为的版本。0 - 仅计算代码点的数量；1 - 正确计算零宽度和合并字符，将全宽字符计为两个，估计制表符宽度，计算删除字符。
## geo_distance_returns_float64_on_float64_arguments {#geo_distance_returns_float64_on_float64_arguments}

类型：布尔

默认值：1

如果 `geoDistance`、`greatCircleDistance`、`greatCircleAngle` 函数的所有四个参数都是 Float64，则返回 Float64，并在内部计算中使用双精度。 在以前的 ClickHouse 版本中，这些函数始终返回 Float32。
## glob_expansion_max_elements {#glob_expansion_max_elements}

类型：UInt64

默认值：1000

允许的最大地址数量（用于外部存储、表函数等）。
## grace_hash_join_initial_buckets {#grace_hash_join_initial_buckets}
<ExperimentalBadge/>

类型：NonZeroUInt64

默认值：1

初始的 graceful hash join 桶数。
## grace_hash_join_max_buckets {#grace_hash_join_max_buckets}
<ExperimentalBadge/>

类型：NonZeroUInt64

默认值：1024

grace hash join 桶的数量限制。
## group_by_overflow_mode {#group_by_overflow_mode}

类型：OverflowModeGroupBy

默认值：throw

当限制被超出时该怎么做。
## group_by_two_level_threshold {#group_by_two_level_threshold}

类型：UInt64

默认值：100000

从多少个键开始，进行两级聚合。0 - 没有设置阈值。
## group_by_two_level_threshold_bytes {#group_by_two_level_threshold_bytes}

类型：UInt64

默认值：50000000

从多少字节的聚合状态开始使用两级聚合。0 - 没有设置阈值。当至少触发一个阈值时使用两级聚合。
## group_by_use_nulls {#group_by_use_nulls}

类型：布尔

默认值：0

更改 [GROUP BY 子句](/sql-reference/statements/select/group-by) 对聚合键类型的处理方式。
当使用 `ROLLUP`、`CUBE` 或 `GROUPING SETS` 指定时，一些聚合键可能不会用于生成某些结果行。
这些键对应的列根据此设置填充默认值或 `NULL`。

可能的值：

- 0 — 用于产生缺失值的聚合键的默认值被使用。
- 1 — ClickHouse 以 SQL 标准规定的方式执行 `GROUP BY`。聚合键的类型被转换为 [Nullable](/sql-reference/data-types/nullable)。对于没有使用的行，相应聚合键的列将填充 [NULL](/sql-reference/syntax#null)。

另见：

- [GROUP BY 子句](/sql-reference/statements/select/group-by)
## h3togeo_lon_lat_result_order {#h3togeo_lon_lat_result_order}

类型：布尔

默认值：0

函数 'h3ToGeo' 返回 (lon, lat) 如果为真，否则返回 (lat, lon)。
## handshake_timeout_ms {#handshake_timeout_ms}

类型：毫秒

默认值：10000

握手期间从副本接收 Hello 数据包的超时（毫秒）。
## hdfs_create_new_file_on_insert {#hdfs_create_new_file_on_insert}

类型：布尔

默认值：0

启用或禁用在 HDFS 引擎表中每次插入时创建新文件。如果启用，在每次插入时，将创建新 HDFS 文件，其名称类似于以下模式：

初始： `data.Parquet.gz` -> `data.1.Parquet.gz` -> `data.2.Parquet.gz`，依此类推。

可能的值：
- 0 — `INSERT` 查询将新数据附加到文件末尾。
- 1 — `INSERT` 查询将创建新文件。
## hdfs_ignore_file_doesnt_exist {#hdfs_ignore_file_doesnt_exist}

类型：布尔

默认值：0

如果在读取特定键时文件不存在，则忽略其缺失。

可能的值：
- 1 — `SELECT` 返回空结果。
- 0 — `SELECT` 抛出异常。
## hdfs_replication {#hdfs_replication}

类型：UInt64

默认值：0

实际的副本数可以在创建 hdfs 文件时指定。
## hdfs_skip_empty_files {#hdfs_skip_empty_files}

类型：布尔

默认值：0

启用或禁用跳过 [HDFS](../../engines/table-engines/integrations/hdfs.md) 引擎表中的空文件。

可能的值：
- 0 — `SELECT` 如果空文件与请求的格式不兼容，则抛出异常。
- 1 — `SELECT` 返回空结果，针对空文件。
## hdfs_throw_on_zero_files_match {#hdfs_throw_on_zero_files_match}

类型：布尔

默认值：0

如果根据 glob 扩展规则匹配零文件，则抛出错误。

可能的值：
- 1 — `SELECT` 抛出异常。
- 0 — `SELECT` 返回空结果。
## hdfs_truncate_on_insert {#hdfs_truncate_on_insert}

类型：布尔

默认值：0

启用或禁用在 hdfs 引擎表中插入前的截断。如果禁用，则在尝试插入时将抛出异常，前提是 HDFS 中已经存在文件。

可能的值：
- 0 — `INSERT` 查询将新数据附加到文件末尾。
- 1 — `INSERT` 查询用新数据替换文件的现有内容。
## hedged_connection_timeout_ms {#hedged_connection_timeout_ms}

类型：毫秒

默认值：50

为 Hedged 请求建立与副本连接的超时。
## hnsw_candidate_list_size_for_search {#hnsw_candidate_list_size_for_search}
<ExperimentalBadge/>

类型：UInt64

默认值：256

搜索向量相似性索引时动态候选列表的大小，也称为 'ef_search'。
## hsts_max_age {#hsts_max_age}

类型：UInt64

默认值：0

HSTS 的过期时间。0 表示禁用 HSTS。
## http_connection_timeout {#http_connection_timeout}

类型：秒

默认值：1

HTTP 连接超时（以秒为单位）。

可能的值：

- 任何正整数。
- 0 - 禁用（无限超时）。
## http_headers_progress_interval_ms {#http_headers_progress_interval_ms}

类型：UInt64

默认值：100

HTTP Headers X-ClickHouse-Progress 的发送频率不得低于每个指定的间隔。
## http_make_head_request {#http_make_head_request}

类型：布尔

默认值：1

`http_make_head_request` 设置允许在从 HTTP 读取数据时执行 `HEAD` 请求，以检索有关要读取的文件的信息，例如其大小。由于默认启用，因此在服务器不支持 `HEAD` 请求的情况下，可能希望禁用此设置。
## http_max_field_name_size {#http_max_field_name_size}

类型：UInt64

默认值：131072

HTTP 头字段名称的最大长度。
## http_max_field_value_size {#http_max_field_value_size}

类型：UInt64

默认值：131072

HTTP 头字段值的最大长度。
## http_max_fields {#http_max_fields}

类型：UInt64

默认值：1000000

HTTP 头中字段的最大数量。
## http_max_multipart_form_data_size {#http_max_multipart_form_data_size}

类型：UInt64

默认值：1073741824

对 multipart/form-data 内容大小的限制。此设置不能从 URL 参数解析，应在用户个人资料中设置。请注意，内容在查询执行开始之前被解析，外部表被创建在内存中。这是唯一在该阶段有效的限制（对最大内存使用和最大执行时间的限制在读取 HTTP 表单数据时没有效果）。
## http_max_request_param_data_size {#http_max_request_param_data_size}

类型：UInt64

默认值：10485760

在预定义 HTTP 请求中作为查询参数使用的请求数据的大小限制。
## http_max_tries {#http_max_tries}

类型：UInt64

默认值：10

通过 http 读取的最大尝试次数。
## http_max_uri_size {#http_max_uri_size}

类型：UInt64

默认值：1048576

设置 HTTP 请求的最大 URI 长度。

可能的值：

- 正整数。
## http_native_compression_disable_checksumming_on_decompress {#http_native_compression_disable_checksumming_on_decompress}

类型：布尔

默认值：0

启用或禁用在解压客户端的 HTTP POST 数据时进行校验和验证。仅用于 ClickHouse 原生压缩格式（不与 `gzip` 或 `deflate` 一起使用）。

有关更多信息，请阅读 [HTTP 接口描述](../../interfaces/http.md)。

可能的值：

- 0 — 禁用。
- 1 — 启用。
## http_receive_timeout {#http_receive_timeout}

类型：秒

默认值：30

HTTP 接收超时（以秒为单位）。

可能的值：

- 任何正整数。
- 0 - 禁用（无限超时）。
## http_response_buffer_size {#http_response_buffer_size}

类型：UInt64

默认值：0

在向客户端发送 HTTP 响应或刷新到磁盘之前，在服务器内存中缓存的字节数（当启用 http_wait_end_of_query 时）。
## http_response_headers {#http_response_headers}

类型：映射

默认值：{}

允许添加或覆盖服务器将在成功查询结果的响应中返回的 HTTP 头。这仅影响 HTTP 接口。

如果头已经被默认设置，提供的值将覆盖它。
如果头未默认设置，它将被添加到头列表中。
服务器默认设置而未通过此设置覆盖的头，将保持不变。

该设置允许您将头设置为常量值。目前没有方法将头设置为动态计算值。

名称或值中不能包含 ASCII 控制字符。

如果您实现了一个 UI 应用程序，允许用户修改设置，同时又根据返回的头做出决定，则建议将此设置限制为只读。

示例：`SET http_response_headers = '{"Content-Type": "image/png"}'`
## http_retry_initial_backoff_ms {#http_retry_initial_backoff_ms}

类型：UInt64

默认值：100

重试通过 http 读取时的回退的最小毫秒数。
```

## http_retry_max_backoff_ms {#http_retry_max_backoff_ms}

类型: UInt64

默认值: 10000

重试通过 HTTP 读取时的最大回退毫秒数。
## http_send_timeout {#http_send_timeout}

类型: 秒

默认值: 30

HTTP 发送超时（以秒为单位）。

可能的值：

- 任何正整数。
- 0 - 禁用（无限超时）。

:::note
只适用于默认配置文件。改动生效需要重启服务器。
:::
## http_skip_not_found_url_for_globs {#http_skip_not_found_url_for_globs}

类型: Bool

默认值: 1

跳过具有 HTTP_NOT_FOUND 错误的通配符 URLs。
## http_wait_end_of_query {#http_wait_end_of_query}

类型: Bool

默认值: 0

启用服务器端的 HTTP 响应缓冲。
## http_write_exception_in_output_format {#http_write_exception_in_output_format}

类型: Bool

默认值: 1

在输出格式中写入异常以产生有效的输出。与 JSON 和 XML 格式一起使用。
## http_zlib_compression_level {#http_zlib_compression_level}

类型: Int64

默认值: 3

设置 HTTP 请求响应中的数据压缩级别，如果 [enable_http_compression = 1](#enable_http_compression)。

可能的值：1 到 9 的数字。
## idle_connection_timeout {#idle_connection_timeout}

类型: UInt64

默认值: 3600

在指定的秒数后关闭空闲 TCP 连接的超时。

可能的值：

- 正整数（0 - 立即关闭，0 秒后）。
## ignore_cold_parts_seconds {#ignore_cold_parts_seconds}

<CloudAvailableBadge/>

类型: Int64

默认值: 0

仅在 ClickHouse Cloud 中生效。在 SELECT 查询中排除新数据部分，直到这些数据部分经过预热（见 [cache_populated_by_fetch](merge-tree-settings.md/#cache_populated_by_fetch)）或旧于指定的秒数。仅适用于 Replicated-/SharedMergeTree。
## ignore_data_skipping_indices {#ignore_data_skipping_indices}

类型: 字符串

默认值: 

如果查询使用了指定的跳过索引，则忽略它们。

考虑以下示例：

```sql
CREATE TABLE data
(
    key Int,
    x Int,
    y Int,
    INDEX x_idx x TYPE minmax GRANULARITY 1,
    INDEX y_idx y TYPE minmax GRANULARITY 1,
    INDEX xy_idx (x,y) TYPE minmax GRANULARITY 1
)
Engine=MergeTree()
ORDER BY key;

INSERT INTO data VALUES (1, 2, 3);

SELECT * FROM data;
SELECT * FROM data SETTINGS ignore_data_skipping_indices=''; -- 查询将产生 CANNOT_PARSE_TEXT 错误。
SELECT * FROM data SETTINGS ignore_data_skipping_indices='x_idx'; -- 正常。
SELECT * FROM data SETTINGS ignore_data_skipping_indices='na_idx'; -- 正常。

SELECT * FROM data WHERE x = 1 AND y = 1 SETTINGS ignore_data_skipping_indices='xy_idx',force_data_skipping_indices='xy_idx' ; -- 查询将产生 INDEX_NOT_USED 错误，因为 xy_idx 被明确忽略。
SELECT * FROM data WHERE x = 1 AND y = 2 SETTINGS ignore_data_skipping_indices='xy_idx';
```

没有忽略任何索引的查询：
```sql
EXPLAIN indexes = 1 SELECT * FROM data WHERE x = 1 AND y = 2;

Expression ((Projection + Before ORDER BY))
  Filter (WHERE)
    ReadFromMergeTree (default.data)
    Indexes:
      PrimaryKey
        Condition: true
        Parts: 1/1
        Granules: 1/1
      Skip
        Name: x_idx
        Description: minmax GRANULARITY 1
        Parts: 0/1
        Granules: 0/1
      Skip
        Name: y_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
      Skip
        Name: xy_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
```

忽略 `xy_idx` 索引：
```sql
EXPLAIN indexes = 1 SELECT * FROM data WHERE x = 1 AND y = 2 SETTINGS ignore_data_skipping_indices='xy_idx';

Expression ((Projection + Before ORDER BY))
  Filter (WHERE)
    ReadFromMergeTree (default.data)
    Indexes:
      PrimaryKey
        Condition: true
        Parts: 1/1
        Granules: 1/1
      Skip
        Name: x_idx
        Description: minmax GRANULARITY 1
        Parts: 0/1
        Granules: 0/1
      Skip
        Name: y_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
```

适用于 MergeTree 家族中的表。
## ignore_drop_queries_probability {#ignore_drop_queries_probability}

类型: Float

默认值: 0

如果启用，服务器将忽略所有 DROP 表查询的指定概率（对于 Memory 和 JOIN 引擎，它将将 DROP 替换为 TRUNCATE）。用于测试目的。
## ignore_materialized_views_with_dropped_target_table {#ignore_materialized_views_with_dropped_target_table}

类型: Bool

默认值: 0

在推送到视图时忽略已删除目标表的物化视图。
## ignore_on_cluster_for_replicated_access_entities_queries {#ignore_on_cluster_for_replicated_access_entities_queries}

类型: Bool

默认值: 0

忽略对复制访问实体管理查询的 ON CLUSTER 子句。
## ignore_on_cluster_for_replicated_named_collections_queries {#ignore_on_cluster_for_replicated_named_collections_queries}

类型: Bool

默认值: 0

忽略对复制命名集合管理查询的 ON CLUSTER 子句。
## ignore_on_cluster_for_replicated_udf_queries {#ignore_on_cluster_for_replicated_udf_queries}

类型: Bool

默认值: 0

忽略对复制用户定义函数管理查询的 ON CLUSTER 子句。
## implicit_select {#implicit_select}

类型: Bool

默认值: 0

允许在没有前导 SELECT 关键字的情况下编写简单的 SELECT 查询，这使得计算器风格的使用变得简单，例如 `1 + 2` 成为有效的查询。

在 `clickhouse-local` 中默认启用，并可以明确禁用。
## implicit_transaction {#implicit_transaction}
<ExperimentalBadge/>

类型: Bool

默认值: 0

如果启用且尚未在事务中，则将查询包装在一个完整事务中（开始 + 提交或回滚）。
## input_format_parallel_parsing {#input_format_parallel_parsing}

类型: Bool

默认值: 1

启用或禁用数据格式的有序并行解析。仅支持 [TSV](../../interfaces/formats.md/#tabseparated)、[TSKV](../../interfaces/formats.md/#tskv)、[CSV](../../interfaces/formats.md/#csv) 和 [JSONEachRow](../../interfaces/formats.md/#jsoneachrow) 格式。

可能的值：

- 1 — 启用。
- 0 — 禁用。
## insert_allow_materialized_columns {#insert_allow_materialized_columns}

类型: Bool

默认值: 0

如果设置启用，允许在 INSERT 中使用物化列。
## insert_deduplicate {#insert_deduplicate}

类型: Bool

默认值: 1

启用或禁用 `INSERT` 的区块去重（适用于 Replicated* 表）。

可能的值：

- 0 — 禁用。
- 1 — 启用。

默认情况下，通过 `INSERT` 语句插入到复制表的区块将被去重（见 [数据复制](../../engines/table-engines/mergetree-family/replication.md)）。
对于复制表，默认情况下每个分区仅去重最近的 100 个区块（见 [replicated_deduplication_window](merge-tree-settings.md/#replicated-deduplication-window)、 [replicated_deduplication_window_seconds](merge-tree-settings.md/#replicated-deduplication-window-seconds)）。
对于非复制表，请参见 [non_replicated_deduplication_window](merge-tree-settings.md/#non-replicated-deduplication-window)。
## insert_deduplication_token {#insert_deduplication_token}

类型: 字符串

默认值: 

该设置允许用户在 MergeTree/ReplicatedMergeTree 中提供自定义去重语义。
例如，通过在每个 INSERT 语句中提供唯一值来设置，可以避免相同插入的数据被去重。

可能的值：

- 任何字符串

`insert_deduplication_token` 仅在不为空时用于去重。

对于复制表，默认情况下每个分区仅去重最近的 100 次插入（见 [replicated_deduplication_window](merge-tree-settings.md/#replicated-deduplication-window)、 [replicated_deduplication_window_seconds](merge-tree-settings.md/#replicated-deduplication-window-seconds)）。
对于非复制表，请参见 [non_replicated_deduplication_window](merge-tree-settings.md/#non-replicated-deduplication-window)。

:::note
`insert_deduplication_token` 在分区级别上工作（同样适用于 `insert_deduplication` 校验和）。多个分区可以具有相同的 `insert_deduplication_token`。
:::

示例：

```sql
CREATE TABLE test_table
( A Int64 )
ENGINE = MergeTree
ORDER BY A
SETTINGS non_replicated_deduplication_window = 100;

INSERT INTO test_table SETTINGS insert_deduplication_token = 'test' VALUES (1);

-- 由于 insert_deduplication_token 不同，下一次插入不会被去重
INSERT INTO test_table SETTINGS insert_deduplication_token = 'test1' VALUES (1);

-- 由于 insert_deduplication_token 与之前的一个相同，下一次插入将被去重
INSERT INTO test_table SETTINGS insert_deduplication_token = 'test' VALUES (2);

SELECT * FROM test_table

┌─A─┐
│ 1 │
└───┘
┌─A─┐
│ 1 │
└───┘
```
## insert_keeper_fault_injection_probability {#insert_keeper_fault_injection_probability}

类型: Float

默认值: 0

在插入过程中 Keeper 请求的故障概率近似值。有效值范围在 [0.0f, 1.0f] 之间。
## insert_keeper_fault_injection_seed {#insert_keeper_fault_injection_seed}

类型: UInt64

默认值: 0

0 - 随机种子，否则为设置的值。
## insert_keeper_max_retries {#insert_keeper_max_retries}

类型: UInt64

默认值: 20

该设置设置 ClickHouse Keeper（或 ZooKeeper）请求在插入到复制 MergeTree 时的最大重试次数。仅考虑因网络错误、Keeper 会话超时或请求超时而失败的 Keeper 请求。

可能的值：

- 正整数。
- 0 — 禁用重试。

云的默认值：`20`。

Keeper 请求重试在经历了一些超时后进行。超时由以下设置控制：`insert_keeper_retry_initial_backoff_ms`、`insert_keeper_retry_max_backoff_ms`。
第一次重试在 `insert_keeper_retry_initial_backoff_ms` 超时后进行。后续的超时将按以下公式计算：
```
timeout = min(insert_keeper_retry_max_backoff_ms, latest_timeout * 2)
```

例如，如果 `insert_keeper_retry_initial_backoff_ms=100`、`insert_keeper_retry_max_backoff_ms=10000` 和 `insert_keeper_max_retries=8`，那么超时将为 `100, 200, 400, 800, 1600, 3200, 6400, 10000`。

除了容错能力，重试的目的是为了提供更好的用户体验——例如，在 Keeper 由于升级而重启时，允许避免在 INSERT 执行期间返回错误。
## insert_keeper_retry_initial_backoff_ms {#insert_keeper_retry_initial_backoff_ms}

类型: UInt64

默认值: 100

在 INSERT 查询执行期间重试失败的 Keeper 请求的初始超时（以毫秒为单位）。

可能的值：

- 正整数。
- 0 — 无超时。
## insert_keeper_retry_max_backoff_ms {#insert_keeper_retry_max_backoff_ms}

类型: UInt64

默认值: 10000

在 INSERT 查询执行期间重试失败的 Keeper 请求的最大超时（以毫秒为单位）。

可能的值：

- 正整数。
- 0 — 最大超时没有限制。
## insert_null_as_default {#insert_null_as_default}

类型: Bool

默认值: 1

启用或禁用向非 [nullable](/sql-reference/data-types/nullable) 数据类型的列插入 [默认值](/sql-reference/statements/create/table#default_values)，而不是 [NULL](/sql-reference/syntax#null)。
如果列类型不是可空的且此设置被禁用，则插入 `NULL` 将导致异常。若列类型是可空的，则 `NULL` 值将照样插入，无论此设置如何。

该设置适用于 [INSERT ... SELECT](../../sql-reference/statements/insert-into.md/#inserting-the-results-of-select) 查询。请注意，`SELECT` 子查询可以与 `UNION ALL` 子句连接。

可能的值：

- 0 — 向非可空列插入 `NULL` 将导致异常。
- 1 — 默认列值将代替 `NULL` 插入。
## insert_quorum {#insert_quorum}

类型: UInt64Auto

默认值: 0

:::note
该设置不适用于 SharedMergeTree，更多信息请参见 [SharedMergeTree 一致性](/cloud/reference/shared-merge-tree#consistency)。
:::

启用法定写入。

- 如果 `insert_quorum < 2`，则禁用法定写入。
- 如果 `insert_quorum >= 2`，则启用法定写入。
- 如果 `insert_quorum = 'auto'`，则使用多数数目（`number_of_replicas / 2 + 1`）作为法定数目。

法定写入

`INSERT` 仅在 ClickHouse 成功写入 `insert_quorum` 个副本的数据时才成功，时间限制为 `insert_quorum_timeout`。如果由于任何原因成功写入的副本数量未达到 `insert_quorum`，则写入视为失败，ClickHouse 将删除所有已写入数据的副本中的插入区块。

当 `insert_quorum_parallel` 被禁用时，法定副本上的所有副本是一致的，即它们包含所有先前 `INSERT` 查询的数据（`INSERT` 序列是线性化的）。在写入使用 `insert_quorum` 和禁用 `insert_quorum_parallel` 的数据时，可以通过使用 [select_sequential_consistency](#select_sequential_consistency) 启用 `SELECT` 查询的顺序一致性。

ClickHouse 生成异常：

- 如果查询时可用的副本数少于 `insert_quorum`。
- 当禁用 `insert_quorum_parallel` 时，如果在 `insert_quorum` 的副本中尚未插入前一个区块时尝试写入数据。此情况可能发生在用户尝试在前一个 `INSERT` 查询完成之前对同一表进行另一个 `INSERT` 查询时。

参见：

- [insert_quorum_timeout](#insert_quorum_timeout)
- [insert_quorum_parallel](#insert_quorum_parallel)
- [select_sequential_consistency](#select_sequential_consistency)
## insert_quorum_parallel {#insert_quorum_parallel}

类型: Bool

默认值: 1

:::note
该设置不适用于 SharedMergeTree，更多信息请参见 [SharedMergeTree 一致性](/cloud/reference/shared-merge-tree#consistency)。
:::

启用或禁用法定 `INSERT` 查询的并行性。如果启用，则可以在先前查询尚未完成时发送额外的 `INSERT` 查询。如果禁用，对同一表的额外写入将被拒绝。

可能的值：

- 0 — 禁用。
- 1 — 启用。

参见：

- [insert_quorum](#insert_quorum)
- [insert_quorum_timeout](#insert_quorum_timeout)
- [select_sequential_consistency](#select_sequential_consistency)
## insert_quorum_timeout {#insert_quorum_timeout}

类型: 毫秒

默认值: 600000

写入法定超时，以毫秒为单位。如果超时已过，而尚未进行写入，则 ClickHouse 将生成异常，客户端必须重复查询以将同一块写入同一或其他副本。

参见：

- [insert_quorum](#insert_quorum)
- [insert_quorum_parallel](#insert_quorum_parallel)
- [select_sequential_consistency](#select_sequential_consistency)
## insert_shard_id {#insert_shard_id}

类型: UInt64

默认值: 0

如果不为 `0`，则指定要将数据同步插入到的 [Distributed](/engines/table-engines/special/distributed) 表的分片。

如果 `insert_shard_id` 的值不正确，服务器将抛出异常。

要获取 `requested_cluster` 上的分片数，可以检查服务器配置或使用以下查询：

``` sql
SELECT uniq(shard_num) FROM system.clusters WHERE cluster = 'requested_cluster';
```

可能的值：

- 0 — 禁用。
- 从 `1` 到相应 [Distributed](/engines/table-engines/special/distributed) 表的 `shards_num` 的任何数字。

**示例**

查询：

```sql
CREATE TABLE x AS system.numbers ENGINE = MergeTree ORDER BY number;
CREATE TABLE x_dist AS x ENGINE = Distributed('test_cluster_two_shards_localhost', currentDatabase(), x);
INSERT INTO x_dist SELECT * FROM numbers(5) SETTINGS insert_shard_id = 1;
SELECT * FROM x_dist ORDER BY number ASC;
```

结果：

``` text
┌─number─┐
│      0 │
│      0 │
│      1 │
│      1 │
│      2 │
│      2 │
│      3 │
│      3 │
│      4 │
│      4 │
└────────┘
```
## interactive_delay {#interactive_delay}

类型: UInt64

默认值: 100000

检查请求执行是否已取消并发送进度的时间间隔（以微秒为单位）。
## intersect_default_mode {#intersect_default_mode}

类型: SetOperationMode

默认值: ALL

设置 INTERSECT 查询中的默认模式。可能的值：空字符串，'ALL'，'DISTINCT'。如果为空，则不带模式的查询将抛出异常。
## join_algorithm {#join_algorithm}

类型: JoinAlgorithm

默认值: direct,parallel_hash,hash

指定使用哪种 [JOIN](../../sql-reference/statements/select/join.md) 算法。

可以指定多种算法，并根据查询类型/严格性和表引擎为特定查询选择可用算法。

可能的值：

- grace_hash

 [Grace hash join](https://en.wikipedia.org/wiki/Hash_join#Grace_hash_join) 被使用。Grace hash 提供了一种性能较好的复杂连接的算法选项，同时限制内存使用。

 Grace join 的第一阶段读取右表并根据关键列的哈希值将其拆分为 N 个桶（初始时 N 为 `grace_hash_join_initial_buckets`）。这是以确保每个桶可以独立处理的方式完成的。第一桶中的行被添加到内存中的哈希表中，而其他则保存在磁盘上。如果哈希表超出了内存限制（例如，按照 [`max_bytes_in_join`](/operations/settings/query-complexity#settings-max_bytes_in_join) 的设置），则增大桶的数量并重新分配每行的桶。任何不属于当前桶的行将被刷新并重新分配。

 支持 `INNER/LEFT/RIGHT/FULL ALL/ANY JOIN`。

- hash

 使用 [Hash join algorithm](https://en.wikipedia.org/wiki/Hash_join)。最通用的实现，支持所有种类和严格性以及多个连接键，在 `JOIN ON` 部分中使用 `OR` 组合。

 使用 `hash` 算法时，JOIN 右侧被上传到内存。

- parallel_hash

 `hash` 连接的一种变体，将数据拆分为桶并并行构建多个哈希表以加快此过程。

 使用 `parallel_hash` 算法时，JOIN 右侧被上传到内存。

- partial_merge

 [sort-merge 算法](https://en.wikipedia.org/wiki/Sort-merge_join) 的一种变体，在这种变体中，仅完全排序右表。

 `RIGHT JOIN` 和 `FULL JOIN` 仅支持 `ALL` 严格性（不支持 `SEMI`、`ANTI`、`ANY` 和 `ASOF`）。

 使用 `partial_merge` 算法时，ClickHouse 将数据排序并将其转储到磁盘。ClickHouse 中的 `partial_merge` 算法与经典实现略有不同。首先，ClickHouse 按照连接键将右表排序为块，并为已排序块创建最小-最大索引。然后，它按 `join key` 对左表的部分进行排序，并将其与右表连接。最小-最大索引还用于跳过不需要的右表块。

- direct

 该算法可应用于支持键值请求的右表存储。

 `direct` 算法使用左表的行作为键在右表中进行查找。仅在特殊存储（例如 [Dictionary](/engines/table-engines/special/dictionary) 或 [EmbeddedRocksDB](../../engines/table-engines/integrations/embedded-rocksdb.md)）中支持，仅适用于 `LEFT` 和 `INNER` JOIN。

- auto

 当设置为 `auto` 时，首先尝试 `hash` 连接，如果违反内存限制，则动态切换到其他算法。

- full_sorting_merge

 [Sort-merge 算法](https://en.wikipedia.org/wiki/Sort-merge_join) 在连接之前对已连接的表进行完全排序。

- prefer_partial_merge

 如果可能，ClickHouse 始终尝试使用 `partial_merge` 连接，否则使用 `hash`。*已弃用*，与 `partial_merge,hash` 相同。

- default (已弃用)

 过时的值，请勿再使用。
 与 `direct,hash` 相同，即尝试使用直接连接和哈希连接（以此顺序）。
## join_any_take_last_row {#join_any_take_last_row}

类型: Bool

默认值: 0

更改具有 `ANY` 严格性连接操作的行为。

:::note
此设置仅适用于具有 [Join](../../engines/table-engines/special/join.md) 引擎表的 `JOIN` 操作。
:::

可能的值：

- 0 — 如果右表有多于一行匹配，则只连接找到的第一行。
- 1 — 如果右表有多于一行匹配，则只连接找到的最后一行。

参见：

- [JOIN 子句](/sql-reference/statements/select/join)
- [Join 表引擎](../../engines/table-engines/special/join.md)
- [join_default_strictness](#join_default_strictness)
## join_default_strictness {#join_default_strictness}

类型: JoinStrictness

默认值: ALL

为 [JOIN 子句](/sql-reference/statements/select/join) 设置默认严格性。

可能的值：

- `ALL` — 如果右表有多个匹配行，ClickHouse 会从匹配行创建一个 [笛卡尔积](https://en.wikipedia.org/wiki/Cartesian_product)。这是标准 SQL 中的正常 `JOIN` 行为。
- `ANY` — 如果右表有多个匹配行，则仅连接找到的第一行。如果右表只有一行匹配，则 `ANY` 和 `ALL` 的结果是相同的。
- `ASOF` — 用于连接具有不确定匹配的序列。
- `空字符串` — 如果查询中未指定 `ALL` 或 `ANY`，ClickHouse 将抛出异常。
## join_on_disk_max_files_to_merge {#join_on_disk_max_files_to_merge}

类型: UInt64

默认值: 64

限制在磁盘上执行的 MergeJoin 操作中允许的并行排序文件的数量。

设置的值越大，使用的内存越多，磁盘 I/O 请求的数量越少。

可能的值：

- 任何正整数，最小值为 2。
## join_output_by_rowlist_perkey_rows_threshold {#join_output_by_rowlist_perkey_rows_threshold}

类型: UInt64

默认值: 5

右表中每个键的行数平均值的下限，用于确定是否在哈希连接中按行列表输出。
## join_overflow_mode {#join_overflow_mode}

类型: OverflowMode

默认值: throw

当超出限制时应该采取什么措施。
## join_to_sort_maximum_table_rows {#join_to_sort_maximum_table_rows}
<ExperimentalBadge/>

类型: UInt64

默认值: 10000

右表中行的最大数量，以确定是否在左连接或内部连接中按键重新排列右表。
## join_to_sort_minimum_perkey_rows {#join_to_sort_minimum_perkey_rows}
<ExperimentalBadge/>

类型: UInt64

默认值: 40

右表中每个键的行数的下限，用于确定是否在左连接或内部连接中按键重新排列右表。此设置确保不对稀疏表键应用优化。
## join_use_nulls {#join_use_nulls}

类型: Bool

默认值: 0

设置 [JOIN](../../sql-reference/statements/select/join.md) 行为的类型。在合并表时，可能会出现空单元格。ClickHouse 根据此设置以不同方式填充它们。

可能的值：

- 0 — 空单元格用相应字段类型的默认值填充。
- 1 — `JOIN` 的行为与标准 SQL 相同。相应字段的类型将转换为 [Nullable](/sql-reference/data-types/nullable)，空单元格用 [NULL](/sql-reference/syntax) 填充。
## joined_subquery_requires_alias {#joined_subquery_requires_alias}

类型: Bool

默认值: 1

强制连接的子查询和表函数具有别名，以便正确的名称资格。
## kafka_disable_num_consumers_limit {#kafka_disable_num_consumers_limit}

类型: Bool

默认值: 0

禁用针对可用 CPU 核心数量的 kafka_num_consumers 的限制。
## kafka_max_wait_ms {#kafka_max_wait_ms}

类型: 毫秒

默认值: 5000

从 [Kafka](/engines/table-engines/integrations/kafka) 读取消息之前的等待时间（以毫秒为单位）。

可能的值：

- 正整数。
- 0 — 无限超时。

参见：

- [Apache Kafka](https://kafka.apache.org/)
## keeper_map_strict_mode {#keeper_map_strict_mode}

类型: Bool

默认值: 0

在对 KeeperMap 执行操作时强制执行额外检查。例如，在插入已存在的键时抛出异常。
## keeper_max_retries {#keeper_max_retries}

类型: UInt64

默认值: 10

一般 Keeper 操作的最大重试次数。
## keeper_retry_initial_backoff_ms {#keeper_retry_initial_backoff_ms}

类型: UInt64

默认值: 100

一般 Keeper 操作的初始回退超时。
## keeper_retry_max_backoff_ms {#keeper_retry_max_backoff_ms}

类型: UInt64

默认值: 5000

一般 Keeper 操作的最大回退超时。
## least_greatest_legacy_null_behavior {#least_greatest_legacy_null_behavior}

类型: Bool

默认值: 0

如果启用，函数 'least' 和 'greatest' 在其参数之一为 NULL 时返回 NULL。
## legacy_column_name_of_tuple_literal {#legacy_column_name_of_tuple_literal}

类型: Bool

默认值: 0

在大型元组文字的列名称中列出所有元素名称，而不是哈希。此设置仅由于兼容原因存在。在从低于 21.7 版本的集群进行滚动更新到更高版本时，设置为 'true' 是有意义的。
## lightweight_deletes_sync {#lightweight_deletes_sync}

类型: UInt64

默认值: 2

与 [`mutations_sync`](#mutations_sync) 相同，但仅控制轻量级删除的执行。

可能的值：

- 0 - 变更异步执行。
- 1 - 查询等待当前服务器上的轻量级删除完成。
- 2 - 查询等待所有副本（如果存在）的轻量级删除完成。

**另见**

- [ALTER 查询的同步性](../../sql-reference/statements/alter/index.md/#synchronicity-of-alter-queries)
- [变更](../../sql-reference/statements/alter/index.md/#mutations)
## limit {#limit}

类型: UInt64

默认值: 0

设置要从查询结果中获取的最大行数。它调整由 [LIMIT](/sql-reference/statements/select/limit) 子句设置的值，使得查询中指定的限制不能超过该设置。

可能的值：

- 0 — 行数无限制。
- 正整数。
## live_view_heartbeat_interval {#live_view_heartbeat_interval}
<ExperimentalBadge/>

类型: 秒

默认值: 15

用于指示实时查询存活的心跳间隔（以秒为单位）。
## load_balancing {#load_balancing}

类型: LoadBalancing

默认值: random

指定用于分布式查询处理的副本选择算法。

ClickHouse 支持以下选择副本的算法：

- [随机](#load_balancing-random) （默认）
- [最近主机名](#load_balancing-nearest_hostname)
- [主机名 Levenshtein 距离](#load_balancing-hostname_levenshtein_distance)
- [顺序](#load_balancing-in_order)
- [第一个或随机](#load_balancing-first_or_random)
- [轮询](#load_balancing-round_robin)

另见：

- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)
### 随机（默认） {#load_balancing-random}

``` sql
load_balancing = random
```

为每个副本计算错误的数量。查询被发送到具有最少错误的副本，如果有多个这样的副本，则发送到其中任何一个。
缺点：未考虑服务器的接近性；如果副本存在不同的数据，您也将获得不同的数据。
### 最近主机名 {#load_balancing-nearest_hostname}

``` sql
load_balancing = nearest_hostname
```

为每个副本计算错误的数量。每 5 分钟，将错误数量整体除以 2。因此，使用指数平滑计算最近一段时间的错误数量。如果有一个副本的错误数量最少（即最近在其他副本上发生了错误），查询将发送给它。如果有多个副本具有相同的最小错误数量，则查询将发送到其主机名与配置文件中服务器的主机名最相似的副本（在相同位置不同字符的数量，直到两个主机名的最小长度）。

例如，example01-01-1 和 example01-01-2 只在一个位置不同，而 example01-01-1 和 example01-02-2 则在两个位置不同。
这种方法看似简单，但不需要关于网络拓扑的外部数据，也不需要比较 IP 地址，这对我们的 IPv6 地址而言很复杂。

因此，如果有等效的副本，偏好按名称最接近的那个。
我们也可以假设，在发送查询到同一服务器时，如果没有故障，分布式查询也会转到同一服务器。因此，即使副本上的数据不同，查询也会基本返回相同的结果。
### 主机名 Levenshtein 距离 {#load_balancing-hostname_levenshtein_distance}

``` sql
load_balancing = hostname_levenshtein_distance
```

与 `nearest_hostname` 相同，但以 [Levenshtein 距离](https://en.wikipedia.org/wiki/Levenshtein_distance) 的方式比较主机名。例如：

``` text
example-clickhouse-0-0 ample-clickhouse-0-0
1

example-clickhouse-0-0 example-clickhouse-1-10
2

example-clickhouse-0-0 example-clickhouse-12-0
3
```
```
### 负载均衡方式 {#load_balancing-in_order}

``` sql
load_balancing = in_order
```

具有相同错误数量的副本将按照配置中指定的顺序访问。当您确切知道哪个副本更可取时，这种方法是合适的。
### 首选或随机 {#load_balancing-first_or_random}

``` sql
load_balancing = first_or_random
```

此算法选择集合中的第一个副本，如果第一个不可用，则选择一个随机副本。这在跨副本拓扑设置中有效，但在其他配置中无效。

`first_or_random` 算法解决了 `in_order` 算法的问题。当使用 `in_order` 时，如果一个副本宕机，下一个副本将承担双重负载，而其余副本处理通常的流量。使用 `first_or_random` 算法时，负载在仍可用的副本之间均匀分配。

可以通过使用设置 `load_balancing_first_offset` 明确定义第一个副本。这为在副本之间重新平衡查询工作负载提供了更多控制。
### 循环轮询 {#load_balancing-round_robin}

``` sql
load_balancing = round_robin
```

此算法在具有相同错误数量的副本之间采用循环轮询策略（仅计算带有 `round_robin` 策略的查询）。
## load_balancing_first_offset {#load_balancing_first_offset}

类型: UInt64

默认值: 0

在使用 FIRST_OR_RANDOM 负载均衡策略时，优先发送查询的副本。
## load_marks_asynchronously {#load_marks_asynchronously}

类型: Bool

默认值: 0

异步加载 MergeTree 标记。
## local_filesystem_read_method {#local_filesystem_read_method}

类型: String

默认值: pread_threadpool

从本地文件系统读取数据的方法，选项包括：read、pread、mmap、io_uring、pread_threadpool。`io_uring` 方法是实验性的，并且在存在并发读取和写入的情况下，不适用于 Log、TinyLog、StripeLog、File、Set 和 Join 以及其他可附加文件的表。
## local_filesystem_read_prefetch {#local_filesystem_read_prefetch}

类型: Bool

默认值: 0

在从本地文件系统读取数据时是否使用预读取。
## lock_acquire_timeout {#lock_acquire_timeout}

类型: Seconds

默认值: 120

定义一个锁请求在失败前等待的秒数。

锁超时用于保护在执行与表的读取/写入操作时避免死锁。当超时到期且锁请求失败时，ClickHouse 服务器抛出异常 "Locking attempt timed out! Possible deadlock avoided. Client should retry." 带有错误代码 `DEADLOCK_AVOIDED`。

可能的值：

- 正整数（以秒为单位）。
- 0 - 无锁超时。
## log_comment {#log_comment}

类型: String

默认值:

指定 [system.query_log](../system-tables/query_log.md) 表的 `log_comment` 字段的值和服务器日志的注释文本。

可以用来改善服务器日志的可读性。此外，有助于在运行 [clickhouse-test](../../development/tests.md) 后从 `system.query_log` 选择与测试相关的查询。

可能的值：

- 任何不超过 [max_query_size](#max_query_size) 的字符串。如果超过 max_query_size，服务器将抛出异常。

**示例**

查询：

``` sql
SET log_comment = 'log_comment test', log_queries = 1;
SELECT 1;
SYSTEM FLUSH LOGS;
SELECT type, query FROM system.query_log WHERE log_comment = 'log_comment test' AND event_date >= yesterday() ORDER BY event_time DESC LIMIT 2;
```

结果：

``` text
┌─type────────┬─query─────┐
│ QueryStart  │ SELECT 1; │
│ QueryFinish │ SELECT 1; │
└─────────────┴───────────┘
```
## log_formatted_queries {#log_formatted_queries}

类型: Bool

默认值: 0

允许将格式化查询记录到 [system.query_log](../../operations/system-tables/query_log.md) 系统表中（填充 `formatted_query` 列）。

可能的值：

- 0 — 系统表中不记录格式化查询。
- 1 — 系统表中记录格式化查询。
## log_processors_profiles {#log_processors_profiles}

类型: Bool

默认值: 1

写入处理器在执行/等待数据期间花费的时间到 `system.processors_profile_log` 表中。

另请参见：

- [`system.processors_profile_log`](../../operations/system-tables/processors_profile_log.md)
- [`EXPLAIN PIPELINE`](../../sql-reference/statements/explain.md/#explain-pipeline)
## log_profile_events {#log_profile_events}

类型: Bool

默认值: 1

将查询性能统计信息记录到 query_log、query_thread_log 和 query_views_log 中。
## log_queries {#log_queries}

类型: Bool

默认值: 1

设置查询日志记录。

使用此设置发送到 ClickHouse 的查询将根据 [query_log](../../operations/server-configuration-parameters/settings.md/#query-log) 服务器配置参数中的规则记录。

示例：

``` text
log_queries=1
```
## log_queries_cut_to_length {#log_queries_cut_to_length}

类型: UInt64

默认值: 100000

如果查询长度大于指定阈值（以字节为单位），则在写入查询日志时裁剪查询。还限制普通文本日志中打印的查询的长度。
## log_queries_min_query_duration_ms {#log_queries_min_query_duration_ms}

类型: Milliseconds

默认值: 0

如果启用（非零），则执行速度快于此设置值的查询将不会被记录（您可以将其视为 [MySQL Slow Query Log](https://dev.mysql.com/doc/refman/5.7/slow-query-log.html) 的 `long_query_time`），这基本上意味着您不会在以下表中找到它们：

- `system.query_log`
- `system.query_thread_log`

只有以下类型的查询会记录到日志中：

- `QUERY_FINISH`
- `EXCEPTION_WHILE_PROCESSING`

- 类型：毫秒
- 默认值：0（任何查询）
## log_queries_min_type {#log_queries_min_type}

类型: LogQueriesType

默认值: QUERY_START

记录的 `query_log` 最小类型。

可能的值：
- `QUERY_START` (`=1`)
- `QUERY_FINISH` (`=2`)
- `EXCEPTION_BEFORE_START` (`=3`)
- `EXCEPTION_WHILE_PROCESSING` (`=4`)

可以用来限制哪些实体将被记录到 `query_log`，例如，如果您只对错误感兴趣，则可以使用 `EXCEPTION_WHILE_PROCESSING`：

``` text
log_queries_min_type='EXCEPTION_WHILE_PROCESSING'
```
## log_queries_probability {#log_queries_probability}

类型: Float

默认值: 1

允许用户以指定概率向 [query_log](../../operations/system-tables/query_log.md)、[query_thread_log](../../operations/system-tables/query_thread_log.md) 和 [query_views_log](../../operations/system-tables/query_views_log.md) 系统表写入仅是随机选择的查询样本。这有助于减少每秒大量查询的负载。

可能的值：

- 0 — 查询未记录在系统表中。
- 介于 [0..1] 之间的正浮点数。例如，如果设置值为 `0.5`，则大约一半的查询记录在系统表中。
- 1 — 所有查询记录在系统表中。
## log_query_settings {#log_query_settings}

类型: Bool

默认值: 1

将查询设置记录到 query_log 和 OpenTelemetry span 日志中。
## log_query_threads {#log_query_threads}

类型: Bool

默认值: 0

设置查询线程日志记录。

查询线程日志记录到 [system.query_thread_log](../../operations/system-tables/query_thread_log.md) 表中。该设置仅在 [log_queries](#log_queries) 为真时生效。使用此设置由 ClickHouse 执行的查询线程将根据 [query_thread_log](/operations/server-configuration-parameters/settings#query_thread_log) 服务器配置参数中的规则记录。

可能的值：

- 0 — 禁用。
- 1 — 启用。

**示例**

``` text
log_query_threads=1
```
## log_query_views {#log_query_views}

类型: Bool

默认值: 1

设置查询视图日志记录。

当启用此设置的 ClickHouse 运行的查询具有相关视图（物化视图或实时视图）时，它们将记录在 [query_views_log](/operations/server-configuration-parameters/settings#query_views_log) 服务器配置参数中。

示例：

``` text
log_query_views=1
```
## low_cardinality_allow_in_native_format {#low_cardinality_allow_in_native_format}

类型: Bool

默认值: 1

允许或限制在 [Native](../../interfaces/formats.md/#native) 格式中使用 [LowCardinality](../../sql-reference/data-types/lowcardinality.md) 数据类型。

如果使用 `LowCardinality` 受到限制，ClickHouse 服务器将在 `SELECT` 查询中将 `LowCardinality` 列转换为普通列，并在 `INSERT` 查询中将普通列转换为 `LowCardinality` 列。

此设置主要是为了不支持 `LowCardinality` 数据类型的第三方客户端。

可能的值：

- 1 — 不限制使用 `LowCardinality`。
- 0 — 限制使用 `LowCardinality`。
## low_cardinality_max_dictionary_size {#low_cardinality_max_dictionary_size}

类型: UInt64

默认值: 8192

设置可以写入存储文件系统的 [LowCardinality](../../sql-reference/data-types/lowcardinality.md) 数据类型的共享全局字典的最大行数。此设置可防止字典无限增长而导致 RAM 问题。由于最大字典大小限制，ClickHouse 写入的所有无法编码的数据都将以普通方式写入。

可能的值：

- 任何正整数。
## low_cardinality_use_single_dictionary_for_part {#low_cardinality_use_single_dictionary_for_part}

类型: Bool

默认值: 0

开启或关闭对数据分片使用单一字典的选择。

默认情况下，ClickHouse 服务器会监控字典的大小，如果字典溢出，则服务器会开始写入下一个字典。要禁止创建多个字典，请设置 `low_cardinality_use_single_dictionary_for_part = 1`。

可能的值：

- 1 — 禁止为数据分片创建多个字典。
- 0 — 不禁止为数据分片创建多个字典。
## materialize_skip_indexes_on_insert {#materialize_skip_indexes_on_insert}

类型: Bool

默认值: 1

如果 INSERT 构建并存储跳过索引。如果禁用，跳过索引将在合并期间或通过显式 MATERIALIZE INDEX 构建和存储。
## materialize_statistics_on_insert {#materialize_statistics_on_insert}

类型: Bool

默认值: 1

如果 INSERT 构建并插入统计信息。如果禁用，统计信息将在合并期间或通过显式 MATERIALIZE STATISTICS 构建和存储。
## materialize_ttl_after_modify {#materialize_ttl_after_modify}

类型: Bool

默认值: 1

在 ALTER MODIFY TTL 查询后，应用 TTL 对旧数据。
## materialized_views_ignore_errors {#materialized_views_ignore_errors}

类型: Bool

默认值: 0

允许忽略物化视图中的错误，并将原始块传送到表中，而不考虑 MV。
## max_analyze_depth {#max_analyze_depth}

类型: UInt64

默认值: 5000

解释器执行的最大分析次数。
## max_ast_depth {#max_ast_depth}

类型: UInt64

默认值: 1000

查询语法树的最大深度。解析后检查。
## max_ast_elements {#max_ast_elements}

类型: UInt64

默认值: 50000

查询语法树的最大节点数。解析后检查。
## max_autoincrement_series {#max_autoincrement_series}

类型: UInt64

默认值: 1000

`generateSeriesID` 函数创建的系列的数量限制。

由于每个系列表示 Keeper 中的一个节点，因此建议不要超过几百万个。
## max_backup_bandwidth {#max_backup_bandwidth}

类型: UInt64

默认值: 0

特定备份在服务器上的最大读取速度（以字节/秒为单位）。零表示无限制。
## max_block_size {#max_block_size}

类型: UInt64

默认值: 65409

在 ClickHouse 中，数据通过块处理，块是一组列部分。单个块的内部处理周期高效，但每处理一个块时会产生显著的成本。

`max_block_size` 设置指示加载数据时单个块中建议的最大行数。块大小为 `max_block_size` 的数据并不总是从表中加载：如果 ClickHouse 确定需要检索更少的数据，则处理较小的块。

块大小不应过小，以避免每个块处理时产生明显的成本。也不应过大，以确保具有 LIMIT 子句的查询在处理第一个块后迅速执行。在设置 `max_block_size` 时，目标应避免在多个线程中提取大量列时消耗过多内存，并保持至少一些缓存局部性。
## max_bytes_before_external_group_by {#max_bytes_before_external_group_by}

类型: UInt64

默认值: 0

如果 GROUP BY 操作期间的内存使用超过此字节阈值，则激活“外部聚合”模式（将数据溢出到磁盘）。建议值是可用系统内存的一半。
## max_bytes_before_external_sort {#max_bytes_before_external_sort}

类型: UInt64

默认值: 0

如果 ORDER BY 操作期间的内存使用超过此字节阈值，则激活“外部排序”模式（将数据溢出到磁盘）。建议值是可用系统内存的一半。
## max_bytes_before_remerge_sort {#max_bytes_before_remerge_sort}

类型: UInt64

默认值: 1000000000

在带有 LIMIT 的 ORDER BY 的情况下，当内存使用超过指定阈值时，在最终合并之前执行额外的合并步骤，以仅保留顶部的 LIMIT 行。
## max_bytes_in_distinct {#max_bytes_in_distinct}

类型: UInt64

默认值: 0

执行 DISTINCT 时状态的最大总大小（以未压缩字节为单位）。
## max_bytes_in_join {#max_bytes_in_join}

类型: UInt64

默认值: 0

JOIN 的哈希表的最大大小（以内存中的字节数为单位）。
## max_bytes_in_set {#max_bytes_in_set}

类型: UInt64

默认值: 0

从执行 IN 部分中生成的集合的最大大小（以内存中的字节为单位）。
## max_bytes_ratio_before_external_group_by {#max_bytes_ratio_before_external_group_by}

类型: Double

默认值: 0.5

激活外部 GROUP BY 前使用的内存比率。如果将其设置为 0.6，当内存使用达到查询允许的内存的 60% 时，将使用外部 GROUP BY。
## max_bytes_ratio_before_external_sort {#max_bytes_ratio_before_external_sort}

类型: Double

默认值: 0.5

激活外部 ORDER BY 前使用的内存比率。如果将其设置为 0.6，当内存使用达到查询允许的内存的 60% 时，将使用外部 ORDER BY。
## max_bytes_to_read {#max_bytes_to_read}

类型: UInt64

默认值: 0

限制从最“深”源读取的字节数（解压后）。即，仅在最深的子查询中。当从远程服务器读取时，仅在远程服务器上检查。
## max_bytes_to_read_leaf {#max_bytes_to_read_leaf}

类型: UInt64

默认值: 0

限制在分布式查询的叶节点上读取的字节数（解压后）。限制仅适用于本地读取，不包括根节点上的最终合并阶段。请注意，此设置在 prefer_localhost_replica=1 时不稳定。
## max_bytes_to_sort {#max_bytes_to_sort}

类型: UInt64

默认值: 0

如果在 ORDER BY 操作中必须处理的（未压缩）字节数超过指定数量，则行为将由 `sort_overflow_mode` 确定，默认情况下是 - 抛出异常。
## max_bytes_to_transfer {#max_bytes_to_transfer}

类型: UInt64

默认值: 0

执行 GLOBAL IN/JOIN 部分时传输的外部表的最大大小（以未压缩字节为单位）。
## max_columns_to_read {#max_columns_to_read}

类型: UInt64

默认值: 0

如果查询需要读取超过指定列数，则抛出异常。零值表示无限制。此设置有助于防止过于复杂的查询。
## max_compress_block_size {#max_compress_block_size}

类型: UInt64

默认值: 1048576

在将未压缩数据压缩以写入表之前，块的最大大小。默认值为 1,048,576（1 MiB）。指定较小的块大小通常会略微降低压缩率，压缩和解压缩速度略有提高，因为缓存局部性得到改善，内存消耗也减少。

:::note
这是一个高级设置，如果您刚刚开始使用 ClickHouse，则不应更改此设置。
:::

请勿混淆压缩块（由字节组成的内存块）与查询处理块（来自表的一组行）。
## max_concurrent_queries_for_all_users {#max_concurrent_queries_for_all_users}

类型: UInt64

默认值: 0

如果此设置的值小于或等于当前同时处理的查询数量，则抛出异常。

示例：`max_concurrent_queries_for_all_users` 可以设置为 99，对于所有用户，数据库管理员可以将其设置为 100，以便在服务器过载时执行调查查询。

修改一个查询或用户的设置不会影响其他查询。

可能的值：

- 正整数。
- 0 — 不限制。

**示例**

```xml
<max_concurrent_queries_for_all_users>99</max_concurrent_queries_for_all_users>
```

**另请参见**

- [max_concurrent_queries](/operations/server-configuration-parameters/settings#max_concurrent_queries)
## max_concurrent_queries_for_user {#max_concurrent_queries_for_user}

类型: UInt64

默认值: 0

每个用户同时处理的最大查询数量。

可能的值：

- 正整数。
- 0 — 不限制。

**示例**

``` xml
<max_concurrent_queries_for_user>5</max_concurrent_queries_for_user>
```
## max_distributed_connections {#max_distributed_connections}

类型: UInt64

默认值: 1024

在向单个分布式表执行单个查询时，与远程服务器的最大同时连接数量。我们建议设置不小于集群中的服务器数量。

以下参数仅在创建分布式表（以及启动服务器）时使用，因此在运行时没有理由更改它们。
## max_distributed_depth {#max_distributed_depth}

类型: UInt64

默认值: 5

限制对 [Distributed](../../engines/table-engines/special/distributed.md) 表的递归查询的最大深度。

如果超出该值，服务器会抛出异常。

可能的值：

- 正整数。
- 0 — 无限制深度。
## max_download_buffer_size {#max_download_buffer_size}

类型: UInt64

默认值: 10485760

每个线程并行下载（例如，针对 URL 引擎）的最大缓冲区大小。
## max_download_threads {#max_download_threads}

类型: MaxThreads

默认值: 4

用于下载数据的最大线程数量（例如，针对 URL 引擎）。
## max_estimated_execution_time {#max_estimated_execution_time}

类型: Seconds

默认值: 0

最大查询估计执行时间（以秒为单位）。
## max_execution_speed {#max_execution_speed}

类型: UInt64

默认值: 0

每秒最大执行行数。
## max_execution_speed_bytes {#max_execution_speed_bytes}

类型: UInt64

默认值: 0

每秒最大执行字节数。
## max_execution_time {#max_execution_time}

类型: Seconds

默认值: 0

如果查询运行时间超过指定的秒数，则行为将由 `timeout_overflow_mode` 确定，默认情况下是 - 抛出异常。请注意，在数据处理过程中检查超时，查询仅在指定位置停止。此设置的实际运行时间将高于该值。
## max_execution_time_leaf {#max_execution_time_leaf}

类型: Seconds

默认值: 0

语义类似于 max_execution_time，但仅适用于分布式查询的叶节点，超时行为将由 `timeout_overflow_mode_leaf` 确定，默认情况下是 - 抛出异常。
## max_expanded_ast_elements {#max_expanded_ast_elements}

类型: UInt64

默认值: 500000

在扩展别名和星号后，查询语法树的最大节点数。
## max_fetch_partition_retries_count {#max_fetch_partition_retries_count}

类型: UInt64

默认值: 5

从另一个主机获取分区时的重试次数。
## max_final_threads {#max_final_threads}

类型: MaxThreads

默认值: 'auto(14)'

设置带有 [FINAL](/sql-reference/statements/select/from#final-modifier) 修饰符的 `SELECT` 查询数据读取阶段的最大并行线程数。

可能的值：

- 正整数。
- 0 或 1 - 禁用。`SELECT` 查询在单线程中执行。
## max_http_get_redirects {#max_http_get_redirects}

类型: UInt64

默认值: 0

允许的 HTTP GET 重定向跳数的最大数量。确保采取额外的安全措施，以防止恶意服务器将您的请求重定向到意外服务。\n\n当外部服务器重定向到另一个地址，但该地址似乎属于公司基础架构时，就会出现这种情况，通过向内部服务器发送 HTTP 请求，您可以从内部网络请求内部 API，绕过身份验证，甚至查询其他服务，例如 Redis 或 Memcached。当您没有内部基础架构（包括在您的 localhost 上运行的某些内容）或信任服务器时，允许重定向是安全的。不过请记住，如果 URL 使用 HTTP 而不是 HTTPS，您必须信任不仅是远程服务器，而且还有您的 ISP 以及中间的每个网络。
## max_hyperscan_regexp_length {#max_hyperscan_regexp_length}

类型: UInt64

默认值: 0

定义在 [hyperscan 多匹配函数](/sql-reference/functions/string-search-functions#multimatchany) 中每个正则表达式的最大长度。

可能的值：

- 正整数。
- 0 - 长度不限制。

**示例**

查询：

```sql
SELECT multiMatchAny('abcd', ['ab','bcd','c','d']) SETTINGS max_hyperscan_regexp_length = 3;
```

结果：

```text
┌─multiMatchAny('abcd', ['ab', 'bcd', 'c', 'd'])─┐
│                                              1 │
└────────────────────────────────────────────────┘
```

查询：

```sql
SELECT multiMatchAny('abcd', ['ab','bcd','c','d']) SETTINGS max_hyperscan_regexp_length = 2;
```

结果：

```text
Exception: Regexp length too large.
```

**另请参见**

- [max_hyperscan_regexp_total_length](#max_hyperscan_regexp_total_length)
## max_hyperscan_regexp_total_length {#max_hyperscan_regexp_total_length}

类型: UInt64

默认值: 0

设置每个 [hyperscan 多匹配函数](/sql-reference/functions/string-search-functions#multimatchany) 中所有正则表达式的最大总长度。

可能的值：

- 正整数。
- 0 - 长度不限制。

**示例**

查询：

```sql
SELECT multiMatchAny('abcd', ['a','b','c','d']) SETTINGS max_hyperscan_regexp_total_length = 5;
```

结果：

```text
┌─multiMatchAny('abcd', ['a', 'b', 'c', 'd'])─┐
│                                           1 │
└─────────────────────────────────────────────┘
```

查询：

```sql
SELECT multiMatchAny('abcd', ['ab','bc','c','d']) SETTINGS max_hyperscan_regexp_total_length = 5;
```

结果：

```text
Exception: Total regexp lengths too large.
```

**另请参见**

- [max_hyperscan_regexp_length](#max_hyperscan_regexp_length)
## max_insert_block_size {#max_insert_block_size}

类型: UInt64

默认值: 1048449

要形成插入到表中的数据块的行数大小。
此设置仅适用于服务器形成块的情况。
例如，针对 HTTP 接口的 INSERT，服务器解析数据格式并形成指定大小的块。
但在使用 clickhouse-client 时，客户端自己解析数据，服务器上的 `max_insert_block_size` 设置不会影响插入块的大小。
在使用 INSERT SELECT 时，该设置也没有意义，因为数据是通过 SELECT 后形成的相同块插入的。

默认值稍大于 `max_block_size`。这样做的原因是某些表引擎（`*MergeTree`）为每个插入的块在磁盘上形成数据部分，这是一个相当大的实体。同样，`*MergeTree` 表在插入期间对数据进行排序，足够大的块大小允许在 RAM 中对更多数据进行排序。
## max_insert_delayed_streams_for_parallel_write {#max_insert_delayed_streams_for_parallel_write}

类型: UInt64

默认值: 0

延迟最终部分刷新时的最大流（列）数。默认值 - 自动（1000，以防底层存储支持并行写入，否则为禁用）。
## max_insert_threads {#max_insert_threads}

类型: UInt64

默认值: 0

执行 `INSERT SELECT` 查询的最大线程数量。

可能的值：

- 0（或 1） — `INSERT SELECT` 不并行执行。
- 正整数，大于 1。

云默认值：从 `2` 到 `4`，具体取决于服务大小。

并行 `INSERT SELECT` 仅在 `SELECT` 部分并行执行时效果显著，请参阅 [max_threads](#max_threads) 设置。
较高的值会导致更高的内存使用。
## max_joined_block_size_rows {#max_joined_block_size_rows}

类型: UInt64

默认值: 65409

JOIN 结果的最大块大小（如果连接算法支持）。 0 表示无限制。
## max_limit_for_ann_queries {#max_limit_for_ann_queries}
<ExperimentalBadge/>

类型: UInt64

默认值: 1000000

LIMIT 大于此设置的 SELECT 查询无法使用向量相似性索引。帮助防止向量相似性索引中的内存溢出。
## max_live_view_insert_blocks_before_refresh {#max_live_view_insert_blocks_before_refresh}
<ExperimentalBadge/>

类型: UInt64

默认值: 64

限制插入块的最大数量，在此之后可合并的块将被丢弃并重新执行查询。
## max_local_read_bandwidth {#max_local_read_bandwidth}

类型: UInt64

默认值: 0

本地读取的最大速度（以字节/秒为单位）。
## max_local_write_bandwidth {#max_local_write_bandwidth}

类型: UInt64

默认值: 0

本地写入的最大速度（以字节/秒为单位）。
## max_memory_usage {#max_memory_usage}

类型: UInt64

默认值: 0

处理单个查询的最大内存使用量。零表示无限制。
## max_memory_usage_for_user {#max_memory_usage_for_user}

类型: UInt64

默认值: 0

处理用户所有并发运行的查询的最大内存使用量。零表示无限制。
## max_network_bandwidth {#max_network_bandwidth}

类型: UInt64

默认值: 0

限制以字节/秒为单位的网络数据交换速度。此设置适用于每个查询。

可能的值：

- 正整数。
- 0 — 禁用带宽控制。
## max_network_bandwidth_for_all_users {#max_network_bandwidth_for_all_users}

类型: UInt64

默认值: 0

限制以字节/秒为单位的网络数据交换速度。此设置适用于服务器上所有并发运行的查询。

可能的值：

- 正整数。
- 0 — 禁用数据速度控制。
## max_network_bandwidth_for_user {#max_network_bandwidth_for_user}

类型: UInt64

默认值: 0

限制以字节/秒为单位的网络数据交换速度。此设置适用于单个用户执行的所有并发查询。

可能的值：

- 正整数。
- 0 — 禁用数据速度控制。
## max_network_bytes {#max_network_bytes}

类型: UInt64

默认值: 0

限制在执行查询时通过网络接收或传输的数据量（以字节为单位）。此设置适用于每个单独查询。

可能的值：

- 正整数。
- 0 — 禁用数据量控制。
## max_number_of_partitions_for_independent_aggregation {#max_number_of_partitions_for_independent_aggregation}

类型: UInt64

默认值: 128

应用优化的表中的最大分区数量。
## max_parallel_replicas {#max_parallel_replicas}

类型: NonZeroUInt64

默认值: 1000

在执行查询时，每个分片的最大副本数量。

可能的值：

- 正整数。

**附加信息**

此选项将根据所使用的设置生成不同的结果。

:::note
当涉及连接或子查询时，此设置将生成不正确的结果，并且所有表不符合某些要求。有关详细信息，请参阅 [Distributed Subqueries and max_parallel_replicas](/operations/settings/settings#max_parallel_replicas)。
:::
### 使用 `SAMPLE` 键的并行处理

如果一个查询在多个服务器上并行执行，它可能会更快。但在以下情况下，查询性能可能会下降：

- 采样键在分区键中的位置不允许高效的范围扫描。
- 将采样键添加到表会降低基于其他列的过滤效率。
- 采样键是计算成本高昂的表达式。
- 集群延迟分布具有长尾，因此查询更多服务器会增加查询的整体延迟。
### 使用 [parallel_replicas_custom_key](#parallel_replicas_custom_key) 的并行处理

此设置适用于任何 replicated 表。

## max_parser_backtracks {#max_parser_backtracks}

类型: UInt64

默认值: 1000000

最大解析器回溯次数（它在递归下降解析过程中尝试不同替代方案的次数）。

## max_parser_depth {#max_parser_depth}

类型: UInt64

默认值: 1000

限制递归下降解析器的最大递归深度。允许控制栈大小。

可能的值:

- 正整数。
- 0 — 递归深度无限制。

## max_parsing_threads {#max_parsing_threads}

类型: MaxThreads

默认值: 'auto(14)'

在支持并行解析的输入格式中解析数据的最大线程数。默认情况下，它是自动确定的。

## max_partition_size_to_drop {#max_partition_size_to_drop}

类型: UInt64

默认值: 50000000000

查询时删除分区的限制。值为 0 意味着您可以不受限制地删除分区。

云默认值: 1 TB。

:::note
此查询设置覆盖其服务器设置的对应值，请参阅 [max_partition_size_to_drop](/operations/server-configuration-parameters/settings#max_partition_size_to_drop)
:::

## max_partitions_per_insert_block {#max_partitions_per_insert_block}

类型: UInt64

默认值: 100

限制单个 INSERT 的块中的最大分区数量。零表示无限。如果块包含过多分区，则抛出异常。此设置是安全阈值，因为使用大量分区是一种常见的误解。

## max_partitions_to_read {#max_partitions_to_read}

类型: Int64

默认值: -1

限制在一次查询中可以访问的最大分区数量。 &lt;= 0 意味着无限制。

## max_parts_to_move {#max_parts_to_move}

类型: UInt64

默认值: 1000

限制在一次查询中可以移动的部分数量。零表示无限制。

## max_query_size {#max_query_size}

类型: UInt64

默认值: 262144

SQL 解析器解析的查询字符串的最大字节数。INSERT 查询的 VALUES 子句中的数据通过单独的流解析器进行处理（消耗 O(1) RAM），不受此限制的影响。

:::note
`max_query_size` 不能在 SQL 查询中设置（例如，`SELECT now() SETTINGS max_query_size=10000`），因为 ClickHouse 需要分配一个缓冲区来解析查询，而该缓冲区的大小由 `max_query_size` 设置决定，必须在查询执行之前进行配置。
:::

## max_read_buffer_size {#max_read_buffer_size}

类型: UInt64

默认值: 1048576

从文件系统读取的缓冲区的最大大小。

## max_read_buffer_size_local_fs {#max_read_buffer_size_local_fs}

类型: UInt64

默认值: 131072

从本地文件系统读取的缓冲区的最大大小。如果设置为 0，则使用 max_read_buffer_size。

## max_read_buffer_size_remote_fs {#max_read_buffer_size_remote_fs}

类型: UInt64

默认值: 0

从远程文件系统读取的缓冲区的最大大小。如果设置为 0，则使用 max_read_buffer_size。

## max_recursive_cte_evaluation_depth {#max_recursive_cte_evaluation_depth}

类型: UInt64

默认值: 1000

递归 CTE 评估深度的最大限制。

## max_remote_read_network_bandwidth {#max_remote_read_network_bandwidth}

类型: UInt64

默认值: 0

读取时网络中数据交换的最大速度，单位为字节每秒。

## max_remote_write_network_bandwidth {#max_remote_write_network_bandwidth}

类型: UInt64

默认值: 0

写入时网络中数据交换的最大速度，单位为字节每秒。

## max_replica_delay_for_distributed_queries {#max_replica_delay_for_distributed_queries}

类型: UInt64

默认值: 300

禁用延迟副本的分布式查询。请参阅 [Replication](../../engines/table-engines/mergetree-family/replication.md)。

以秒为单位设置时间。如果副本的延迟大于或等于设定值，则不使用该副本。

可能的值:

- 正整数。
- 0 — 不检查副本延迟。

要防止使用具有非零延迟的任何副本，请将此参数设置为 1。

用于从指向复制表的分布式表执行 `SELECT` 时。

## max_result_bytes {#max_result_bytes}

类型: UInt64

默认值: 0

结果大小的限制，单位为字节（未压缩）。如果满足阈值，在处理数据块后查询将停止，但不会剪切结果的最后一个块，因此结果大小可能会超过阈值。注意：结果在内存中的大小被计算在此阈值内。即使结果大小很小，它可能会在内存中引用更大的数据结构，代表 LowCardinality 列的字典，以及 AggregateFunction 列的 Arena，因此尽管结果大小较小，阈值也可能会被超过。该设置相当低级，使用时需谨慎。

## max_result_rows {#max_result_rows}

类型: UInt64

默认值: 0

结果大小的限制，单位为行。若满足阈值，在处理数据块后查询将停止，但不会剪切结果的最后一个块，因此结果大小可能会超过阈值。

## max_rows_in_distinct {#max_rows_in_distinct}

类型: UInt64

默认值: 0

执行 DISTINCT 时的最大元素数量。

## max_rows_in_join {#max_rows_in_join}

类型: UInt64

默认值: 0

JOIN 的哈希表的最大大小（以行数计）。

## max_rows_in_set {#max_rows_in_set}

类型: UInt64

默认值: 0

执行 IN 部分后，得到的集合的最大大小（以元素数量计）。

## max_rows_in_set_to_optimize_join {#max_rows_in_set_to_optimize_join}

类型: UInt64

默认值: 0

在联合前通过彼此的行集合过滤的集合的最大大小。

可能的值:

- 0 — 禁用。
- 任何正整数。

## max_rows_to_group_by {#max_rows_to_group_by}

类型: UInt64

默认值: 0

如果在 GROUP BY 中的聚合生成超过指定数量的行（唯一的 GROUP BY 键），则行为将由 `group_by_overflow_mode` 决定，默认情况下为 - 抛出异常，但也可以切换为近似 GROUP BY 模式。

## max_rows_to_read {#max_rows_to_read}

类型: UInt64

默认值: 0

限制从最“深”的源读取的行数。即，仅在最深的子查询中检查。当从远程服务器读取时，仅在远程服务器上检查。

## max_rows_to_read_leaf {#max_rows_to_read_leaf}

类型: UInt64

默认值: 0

限制在分布式查询中在叶节点读取的行数。限制仅适用于本地读取，不包括根节点的最终合并阶段。注意，此设置在 prefer_localhost_replica=1 时不稳定。

## max_rows_to_sort {#max_rows_to_sort}

类型: UInt64

默认值: 0

如果必须处理超过指定数量的记录以进行 ORDER BY 操作，则行为将由 `sort_overflow_mode` 决定，默认情况下为 - 抛出异常。

## max_rows_to_transfer {#max_rows_to_transfer}

类型: UInt64

默认值: 0

在执行 GLOBAL IN/JOIN 部分时传输的外部表的最大大小（以行数计）。

## max_sessions_for_user {#max_sessions_for_user}

类型: UInt64

默认值: 0

用户的最大同时会话数。

## max_size_to_preallocate_for_aggregation {#max_size_to_preallocate_for_aggregation}

类型: UInt64

默认值: 1000000000000

在聚合之前允许在所有哈希表中总共预分配多少个元素的空间。

## max_size_to_preallocate_for_joins {#max_size_to_preallocate_for_joins}

类型: UInt64

默认值: 1000000000000

在连接之前允许在所有哈希表中总共预分配多少个元素的空间。

## max_streams_for_merge_tree_reading {#max_streams_for_merge_tree_reading}

类型: UInt64

默认值: 0

如果不为零，限制对 MergeTree 表的读取流的数量。

## max_streams_multiplier_for_merge_tables {#max_streams_multiplier_for_merge_tables}

类型: Float

默认值: 5

在从 Merge 表读取时请求更多的流。流将分布在 Merge 表所使用的多个表之间。这允许在多个线程之间更均匀地分配工作，特别是在合并表大小不同的情况下。

## max_streams_to_max_threads_ratio {#max_streams_to_max_threads_ratio}

类型: Float

默认值: 1

允许使用的源数量超过线程数 — 使工作在线程之间分布更均匀。假设这是一个临时解决方案，因为将来将有可能使源的数量等于线程的数量，而每个源可以动态选择可用的工作。

## max_subquery_depth {#max_subquery_depth}

类型: UInt64

默认值: 100

如果查询中嵌套子查询的数量超过指定数量，则抛出异常。这可以在一定程度上保护集群用户的查询合理性。

## max_table_size_to_drop {#max_table_size_to_drop}

类型: UInt64

默认值: 50000000000

查询时删除表的限制。值为 0 意味着您可以删除所有表而不受限制。

云默认值: 1 TB。

:::note
此查询设置覆盖其服务器设置的对应值，请参阅 [max_table_size_to_drop](/operations/server-configuration-parameters/settings#max_table_size_to_drop)
:::

## max_temporary_columns {#max_temporary_columns}

类型: UInt64

默认值: 0

如果查询因中间计算而在内存中生成的临时列数超过指定数量，则抛出异常。零值表示无限制。此设置有助于防止查询过于复杂。

## max_temporary_data_on_disk_size_for_query {#max_temporary_data_on_disk_size_for_query}

类型: UInt64

默认值: 0

所有并发运行的查询在磁盘上消耗的临时文件的最大数据量（以字节为单位）。零值表示无限制。

## max_temporary_data_on_disk_size_for_user {#max_temporary_data_on_disk_size_for_user}

类型: UInt64

默认值: 0

所有并发运行的用户查询在磁盘上消耗的临时文件的最大数据量（以字节为单位）。零值表示无限制。

## max_temporary_non_const_columns {#max_temporary_non_const_columns}

类型: UInt64

默认值: 0

类似于 'max_temporary_columns' 设置，但仅适用于非常量列。这是合理的，因为常量列便宜，允许更多。

## max_threads {#max_threads}

类型: MaxThreads

默认值: 'auto(14)'

查询处理线程的最大数量，不包括从远程服务器检索数据的线程（参见 'max_distributed_connections' 参数）。

此参数适用于并行执行查询处理管道各阶段的线程。例如，在从一个表读取时，如果可以使用至少 'max_threads' 数量的线程并行地评估带有函数的表达式、使用 WHERE 进行过滤并为 GROUP BY 进行预聚合，那么就会使用 'max_threads'。

对于由于 LIMIT 而快速完成的查询，可以设置较低的 'max_threads'。例如，如果所需的条目位于每个块中且 max_threads = 8，则将检索 8 个块，尽管只读取一个就足够。

`max_threads` 值越小，消耗的内存越少。

## max_threads_for_indexes {#max_threads_for_indexes}

类型: UInt64

默认值: 0

处理索引的最大线程数。

## max_untracked_memory {#max_untracked_memory}

类型: UInt64

默认值: 4194304

小的分配和释放在线程本地变量中进行分组，并仅在数量（绝对值）超过指定值时进行跟踪或分析。如果该值高于 'memory_profiler_step'，它将被有效地降低到 'memory_profiler_step'。

## memory_overcommit_ratio_denominator {#memory_overcommit_ratio_denominator}

类型: UInt64

默认值: 1073741824

这表示在全局层面达到硬限制时的软内存限制。此值用于计算查询的过量分配比例。零值表示跳过查询。有关 [内存过量分配](memory-overcommit.md) 的更多信息。

## memory_overcommit_ratio_denominator_for_user {#memory_overcommit_ratio_denominator_for_user}

类型: UInt64

默认值: 1073741824

这表示在用户级别达到硬限制时的软内存限制。此值用于计算查询的过量分配比例。零值表示跳过查询。有关 [内存过量分配](memory-overcommit.md) 的更多信息。

## memory_profiler_sample_max_allocation_size {#memory_profiler_sample_max_allocation_size}

类型: UInt64

默认值: 0

以与 `memory_profiler_sample_probability` 相等的概率收集小于或等于指定值的随机分配。值为 0 意味着禁用。您可能希望将 'max_untracked_memory' 设置为 0，以使此阈值按预期工作。

## memory_profiler_sample_min_allocation_size {#memory_profiler_sample_min_allocation_size}

类型: UInt64

默认值: 0

以与 `memory_profiler_sample_probability` 相等的概率收集大于或等于指定值的随机分配。值为 0 意味着禁用。您可能希望将 'max_untracked_memory' 设置为 0，以使此阈值按预期工作。

## memory_profiler_sample_probability {#memory_profiler_sample_probability}

类型: Float

默认值: 0

收集随机分配和释放，并将其写入 system.trace_log，带有 'MemorySample' trace_type。评分是针对每次分配/释放的，无论分配的大小如何（可以通过 `memory_profiler_sample_min_allocation_size` 和 `memory_profiler_sample_max_allocation_size` 进行更改）。注意，只有当未跟踪的内存量超过 'max_untracked_memory' 时，才会进行采样。您可能希望将 'max_untracked_memory' 设置为 0 以获得额外精细的采样。

## memory_profiler_step {#memory_profiler_step}

类型: UInt64

默认值: 4194304

设置内存分析师的步长。每当查询内存使用量超过每个字节的下一个步长时，内存分析师将收集分配的堆栈跟踪并将其写入 [trace_log](/operations/system-tables/trace_log)。

可能的值：

- 正整数。

- 0 以关闭内存分析师。

## memory_tracker_fault_probability {#memory_tracker_fault_probability}

类型: Float

默认值: 0

用于 `exception safety` 的测试 - 每次分配内存时都会抛出异常，概率为指定值。

## memory_usage_overcommit_max_wait_microseconds {#memory_usage_overcommit_max_wait_microseconds}

类型: UInt64

默认值: 5000000

在用户级别的内存过量分配的情况下，线程等待释放内存的最大时间。如果超时且内存尚未释放，则会抛出异常。有关 [内存过量分配](memory-overcommit.md) 的更多信息。

## merge_table_max_tables_to_look_for_schema_inference {#merge_table_max_tables_to_look_for_schema_inference}

类型: UInt64

默认值: 1000

在创建没有显式模式的 `Merge` 表时，或在使用 `merge` 表函数时，推导不超过指定数量匹配表的模式。如果表的数量较多，则仅从指定数量的表中推导模式。

## merge_tree_coarse_index_granularity {#merge_tree_coarse_index_granularity}

类型: UInt64

默认值: 8

在查找数据时，ClickHouse 检查索引文件中的数据标记。如果 ClickHouse 发现所需键位于某个范围内，则它会将此范围划分为 `merge_tree_coarse_index_granularity` 个子范围，并在那里递归查找所需键。

可能的值:

- 任何正偶数。

## merge_tree_compact_parts_min_granules_to_multibuffer_read {#merge_tree_compact_parts_min_granules_to_multibuffer_read}

<CloudAvailableBadge/>

类型: UInt64

默认值: 16

仅在 ClickHouse Cloud 中有效。合并树表的紧凑部分条带中的粒子数，以使用多缓冲读取器，该读取器支持并行读取和预读。在远程文件系统中读取时，使用多缓冲读取器将增加读取请求的数量。

## merge_tree_determine_task_size_by_prewhere_columns {#merge_tree_determine_task_size_by_prewhere_columns}

类型: Bool

默认值: 1

是否仅根据预处理列的大小来确定读取任务的大小。

## merge_tree_max_bytes_to_use_cache {#merge_tree_max_bytes_to_use_cache}

类型: UInt64

默认值: 2013265920

如果 ClickHouse 在一个查询中需要读取超过 `merge_tree_max_bytes_to_use_cache` 字节，则不会使用未压缩块的缓存。

未压缩块的缓存存储为查询提取的数据。ClickHouse 使用此缓存来加速对重复的小查询的响应。此设置可防止通过读取大量数据的查询对缓存进行垃圾处理。[uncompressed_cache_size](/operations/server-configuration-parameters/settings#uncompressed_cache_size) 服务器设置定义了未压缩块缓存的大小。

可能的值:

- 任何正整数。

## merge_tree_max_rows_to_use_cache {#merge_tree_max_rows_to_use_cache}

类型: UInt64

默认值: 1048576

如果 ClickHouse 在一个查询中需要读取超过 `merge_tree_max_rows_to_use_cache` 行，则不会使用未压缩块的缓存。

未压缩块的缓存存储为查询提取的数据。ClickHouse 使用此缓存来加速对重复的小查询的响应。此设置可防止通过读取大量数据的查询对缓存进行垃圾处理。[uncompressed_cache_size](/operations/server-configuration-parameters/settings#uncompressed_cache_size) 服务器设置定义了未压缩块缓存的大小。

可能的值:

- 任何正整数。

## merge_tree_min_bytes_for_concurrent_read {#merge_tree_min_bytes_for_concurrent_read}

类型: UInt64

默认值: 251658240

如果从 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 引擎表的单个文件中读取的字节数超过 `merge_tree_min_bytes_for_concurrent_read`，则 ClickHouse 尝试在多个线程中同时读取此文件。

可能的值:

- 正整数。

## merge_tree_min_bytes_for_concurrent_read_for_remote_filesystem {#merge_tree_min_bytes_for_concurrent_read_for_remote_filesystem}

类型: UInt64

默认值: 0

在从远程文件系统读取时，读取一个文件前的字节数的最小值，此时 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 引擎可以并行化读取。我们不建议使用此设置。

可能的值:

- 正整数。

## merge_tree_min_bytes_for_seek {#merge_tree_min_bytes_for_seek}

类型: UInt64

默认值: 0

如果在一个文件中要读取的两个数据块之间的距离小于 `merge_tree_min_bytes_for_seek` 字节，则 ClickHouse 会顺序读取包含两个块的文件范围，从而避免额外的寻址。

可能的值:

- 任何正整数。

## merge_tree_min_bytes_per_task_for_remote_reading {#merge_tree_min_bytes_per_task_for_remote_reading}

类型: UInt64

默认值: 2097152

每个任务的最小读取字节数。

## merge_tree_min_read_task_size {#merge_tree_min_read_task_size}

类型: UInt64

默认值: 8

任务大小的硬下限（即使颗粒数较少且可用线程数较多，也不会分配较小的任务）。

## merge_tree_min_rows_for_concurrent_read {#merge_tree_min_rows_for_concurrent_read}

类型: UInt64

默认值: 163840

如果从 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 表文件读取的行数超过 `merge_tree_min_rows_for_concurrent_read`，则 ClickHouse 尝试在多个线程中并发地读取此文件。

可能的值:

- 正整数。

## merge_tree_min_rows_for_concurrent_read_for_remote_filesystem {#merge_tree_min_rows_for_concurrent_read_for_remote_filesystem}

类型: UInt64

默认值: 0

在从远程文件系统读取时，读取一个文件前的行数的最小值，此时 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 引擎可以并行化读取。我们不建议使用此设置。

可能的值:

- 正整数。

## merge_tree_min_rows_for_seek {#merge_tree_min_rows_for_seek}

类型: UInt64

默认值: 0

如果在一个文件中要读取的两个数据块之间的距离小于 `merge_tree_min_rows_for_seek` 行，则 ClickHouse 不会在文件中寻址，而是顺序读取数据。

可能的值:

- 任何正整数。

## merge_tree_read_split_ranges_into_intersecting_and_non_intersecting_injection_probability {#merge_tree_read_split_ranges_into_intersecting_and_non_intersecting_injection_probability}

类型: Float

默认值: 0

用于 `PartsSplitter` 的测试 - 在每次从 MergeTree 读取时，以指定概率将读取范围分成相交和不相交。

## merge_tree_use_const_size_tasks_for_remote_reading {#merge_tree_use_const_size_tasks_for_remote_reading}

类型: Bool

默认值: 1

是否使用常量大小的任务从远程表中读取。

## merge_tree_use_deserialization_prefixes_cache {#merge_tree_use_deserialization_prefixes_cache}

类型: Bool

默认值: 1

在从 MergeTree 的 Wide 部分读取时启用从文件前缀缓存列元数据。

## merge_tree_use_prefixes_deserialization_thread_pool {#merge_tree_use_prefixes_deserialization_thread_pool}

类型: Bool

默认值: 1

启用在 MergeTree 的 Wide 部分中并行读取前缀的线程池。该线程池的大小由服务器设置 `max_prefixes_deserialization_thread_pool_size` 控制。

## merge_tree_use_v1_object_and_dynamic_serialization {#merge_tree_use_v1_object_and_dynamic_serialization}

类型: Bool

默认值: 0

启用后，MergeTree 中将使用 JSON 和 Dynamic 类型的 V1 序列化版本，而不是 V2。更改此设置仅在服务器重启后生效。

## metrics_perf_events_enabled {#metrics_perf_events_enabled}

类型: Bool

默认值: 0

如果启用，则在查询执行过程中将测量某些性能事件。

## metrics_perf_events_list {#metrics_perf_events_list}

类型: String

默认值: 

逗号分隔的性能度量列表，将在查询执行过程中测量。为空则表示所有事件。有关可用事件，请参阅源代码中的 PerfEventInfo。

## min_bytes_to_use_direct_io {#min_bytes_to_use_direct_io}

类型: UInt64

默认值: 0

使用直接 I/O 访问存储磁盘所需的最小数据量。

ClickHouse 在从表中读取数据时使用此设置。如果要读取的数据的总存储量超过 `min_bytes_to_use_direct_io` 字节，则 ClickHouse 将使用 `O_DIRECT` 选项从存储磁盘读取数据。

可能的值：

- 0 — 禁用直接 I/O。
- 正整数。

## min_bytes_to_use_mmap_io {#min_bytes_to_use_mmap_io}

类型: UInt64

默认值: 0

这是一个实验性设置。设置在不使用内核到用户空间的数据复制的情况下读取大文件所需的最小内存。建议的阈值约为 64 MB，因为 [mmap/munmap](https://en.wikipedia.org/wiki/Mmap) 速度较慢。仅在大文件上有意义，并且仅在数据位于页面缓存中时有帮助。

可能的值：

- 正整数。
- 0 — 大文件的读取仅靠内核到用户空间的数据复制。

## min_chunk_bytes_for_parallel_parsing {#min_chunk_bytes_for_parallel_parsing}

类型: NonZeroUInt64

默认值: 10485760

- 类型: 无符号整数
- 默认值: 1 MiB

每个线程并行解析的最小块大小（以字节为单位）。

## min_compress_block_size {#min_compress_block_size}

类型: UInt64

默认值: 65536

对于 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 表。为了在处理查询时减少延迟，如果在写入下一个标记时块的大小至少为 `min_compress_block_size`，则会压缩块。默认为 65,536。

如果未压缩数据的实际大小小于 `max_compress_block_size`，则块的实际大小不小于此值，并且不小于一个标记的数据量。

让我们看一个示例。假设在创建表期间将 `index_granularity` 设置为 8192。

我们写入一个 UInt32 类型的列（每个值 4 字节）。写入 8192 行时，总共将是 32 KB 的数据。由于 min_compress_block_size = 65,536，因此每两个标记形成一个压缩块。

我们写入一个 URL 列，其类型为 String（每个值平均大小为 60 字节）。写入 8192 行时，平均将少于 500 KB 的数据。由于这超过了 65,536，因此每个标记形成一个压缩块。在这种情况下，从磁盘中读取单个标记的范围时，不会解压额外数据。

:::note
这是一个专家级别的设置，如果您刚开始使用 ClickHouse，不应更改它。
:::

## min_count_to_compile_aggregate_expression {#min_count_to_compile_aggregate_expression}

类型: UInt64

默认值: 3

开始 JIT 编译所需的相同聚合表达式的最小数量。仅在启用 [compile_aggregate_expressions](#compile_aggregate_expressions) 设置时有效。

可能的值：

- 正整数。
- 0 — 相同的聚合表达式始终进行 JIT 编译。

## min_count_to_compile_expression {#min_count_to_compile_expression}

类型: UInt64

默认值: 3

在编译之前执行相同表达式所需的最小数量。

## min_count_to_compile_sort_description {#min_count_to_compile_sort_description}

类型: UInt64

默认值: 3

在进行 JIT 编译之前所需的相同排序描述的数量。

## min_execution_speed {#min_execution_speed}

类型: UInt64

默认值: 0

每秒的最小执行行数。

## min_execution_speed_bytes {#min_execution_speed_bytes}

类型: UInt64

默认值: 0

每秒的最小执行字节数。

## min_external_sort_block_bytes {#min_external_sort_block_bytes}

类型: UInt64

默认值: 104857600

外部排序中转储到磁盘的最小块大小，以避免生成过多文件。

## min_external_table_block_size_bytes {#min_external_table_block_size_bytes}

类型: UInt64

默认值: 268402944

如果块不够大，将外部表传递给指定大小的块。

## min_external_table_block_size_rows {#min_external_table_block_size_rows}

类型: UInt64

默认值: 1048449

如果块不够大，将外部表传递给指定大小的行数。

## min_free_disk_bytes_to_perform_insert {#min_free_disk_bytes_to_perform_insert}

类型: UInt64

默认值: 0

执行插入所需的最小可用磁盘空间字节数。

## min_free_disk_ratio_to_perform_insert {#min_free_disk_ratio_to_perform_insert}

类型: Float

默认值: 0

执行插入所需的最小可用磁盘空间比例。

## min_free_disk_space_for_temporary_data {#min_free_disk_space_for_temporary_data}

类型: UInt64

默认值: 0

在进行外部排序和聚合时，写入临时数据时应保留的最小磁盘空间。

## min_hit_rate_to_use_consecutive_keys_optimization {#min_hit_rate_to_use_consecutive_keys_optimization}

类型: Float

默认值: 0.5

在聚合中使用连续键优化的缓存的最小命中率，以将其保持启用状态。

## min_insert_block_size_bytes {#min_insert_block_size_bytes}

类型: UInt64

默认值: 268402944

为 `INSERT` 查询可以插入到表中的块设置的最小字节数。较小的块将压缩成更大的块。

可能的值：

- 正整数。
- 0 — 禁用压缩。

## min_insert_block_size_bytes_for_materialized_views {#min_insert_block_size_bytes_for_materialized_views}

类型: UInt64

默认值: 0

为 `INSERT` 查询可以插入到表中的块设置的最小字节数。较小的块将压缩成更大的块。该设置仅适用于插入到 [物化视图](../../sql-reference/statements/create/view.md) 的块。通过调整该设置，您可以控制插入到物化视图时的块压缩，并避免过度使用内存。

可能的值：

- 任何正整数。
- 0 — 禁用压缩。

**另请参见**

- [min_insert_block_size_bytes](#min_insert_block_size_bytes)

## min_insert_block_size_rows {#min_insert_block_size_rows}

类型: UInt64

默认值: 1048449

为 `INSERT` 查询可以插入到表中的块设置的最小行数。较小的块将压缩成更大的块。

可能的值：

- 正整数。
- 0 — 禁用压缩。
```

## min_insert_block_size_rows_for_materialized_views {#min_insert_block_size_rows_for_materialized_views}

Type: UInt64

Default value: 0

设置可以通过 `INSERT` 查询插入到表中的块的最小行数。较小的块会被压缩成更大的块。此设置仅应用于插入到 [物化视图](../../sql-reference/statements/create/view.md) 的块。通过调整此设置，您可以控制在推送到物化视图时块的压缩，并避免过度占用内存。

Possible values:

- 任何正整数。
- 0 — 禁用压缩。

**See Also**

- [min_insert_block_size_rows](#min_insert_block_size_rows)
## min_joined_block_size_bytes {#min_joined_block_size_bytes}

Type: UInt64

Default value: 524288

JOIN 结果的最小块大小（如果连接算法支持）。0 表示无限制。
## mongodb_throw_on_unsupported_query {#mongodb_throw_on_unsupported_query}

Type: Bool

Default value: 1

如果启用，当无法构建 MongoDB 查询时，MongoDB 表将返回错误。否则，ClickHouse 将读取整个表并在本地处理。此选项不适用于遗留实现或当 'allow_experimental_analyzer=0' 时。
## move_all_conditions_to_prewhere {#move_all_conditions_to_prewhere}

Type: Bool

Default value: 1

将所有可行的条件从 WHERE 移动到 PREWHERE。
## move_primary_key_columns_to_end_of_prewhere {#move_primary_key_columns_to_end_of_prewhere}

Type: Bool

Default value: 1

将包含主键列的 PREWHERE 条件移动到 AND 链的末尾。这些条件可能在主键分析中被考虑，因此不会对 PREWHERE 过滤贡献太大。
## multiple_joins_try_to_keep_original_names {#multiple_joins_try_to_keep_original_names}

Type: Bool

Default value: 0

在多重连接重写时不在顶层表达式列表中添加别名。
## mutations_execute_nondeterministic_on_initiator {#mutations_execute_nondeterministic_on_initiator}

Type: Bool

Default value: 0

如果为真，常量非确定性函数（例如，函数 `now()`）将在发起者上执行，并在 `UPDATE` 和 `DELETE` 查询中替换为文字。这有助于在执行带有常量非确定性函数的变更时保持副本间数据同步。默认值: `false`。
## mutations_execute_subqueries_on_initiator {#mutations_execute_subqueries_on_initiator}

Type: Bool

Default value: 0

如果为真，标量子查询将在发起者上执行，并在 `UPDATE` 和 `DELETE` 查询中替换为文字。默认值: `false`。
## mutations_max_literal_size_to_replace {#mutations_max_literal_size_to_replace}

Type: UInt64

Default value: 16384

在 `UPDATE` 和 `DELETE` 查询中替换的序列化文字的最大大小（以字节为单位）。仅在启用上述两个设置中的至少一个时生效。默认值: 16384（16 KiB）。
## mutations_sync {#mutations_sync}

Type: UInt64

Default value: 0

允许以同步方式执行 `ALTER TABLE ... UPDATE|DELETE|MATERIALIZE INDEX|MATERIALIZE PROJECTION|MATERIALIZE COLUMN|MATERIALIZE STATISTICS` 查询（[变更](../../sql-reference/statements/alter/index.md/#mutations)）。

Possible values:

- 0 - 变更异步执行。
- 1 - 查询等待当前服务器上的所有变更完成。
- 2 - 查询等待所有副本（如果存在）的所有变更完成。
## mysql_datatypes_support_level {#mysql_datatypes_support_level}

Type: MySQLDataTypesSupport

Default value: 

定义如何将 MySQL 类型转换为相应的 ClickHouse 类型。以逗号分隔的列表，任意组合 `decimal`、`datetime64`、`date2Date32` 或 `date2String`。
- `decimal`: 在精度允许时，将 `NUMERIC` 和 `DECIMAL` 类型转换为 `Decimal`。
- `datetime64`: 在精度不为 `0` 时，将 `DATETIME` 和 `TIMESTAMP` 类型转换为 `DateTime64` 而不是 `DateTime`。
- `date2Date32`: 将 `DATE` 转换为 `Date32` 而不是 `Date`。优先于 `date2String`。
- `date2String`: 将 `DATE` 转换为 `String` 而不是 `Date`。被 `datetime64` 覆盖。
## mysql_map_fixed_string_to_text_in_show_columns {#mysql_map_fixed_string_to_text_in_show_columns}

Type: Bool

Default value: 1

当启用时，ClickHouse 数据类型 [FixedString](../../sql-reference/data-types/fixedstring.md) 将在 [SHOW COLUMNS](../../sql-reference/statements/show.md/#show_columns) 中显示为 `TEXT`。

仅当通过 MySQL 线协议建立连接时有效。

- 0 - 使用 `BLOB`。
- 1 - 使用 `TEXT`。
## mysql_map_string_to_text_in_show_columns {#mysql_map_string_to_text_in_show_columns}

Type: Bool

Default value: 1

当启用时，ClickHouse 数据类型 [String](../../sql-reference/data-types/string.md) 将在 [SHOW COLUMNS](../../sql-reference/statements/show.md/#show_columns) 中显示为 `TEXT`。

仅当通过 MySQL 线协议建立连接时有效。

- 0 - 使用 `BLOB`。
- 1 - 使用 `TEXT`。
## mysql_max_rows_to_insert {#mysql_max_rows_to_insert}

Type: UInt64

Default value: 65536

MySQL 存储引擎中 MySQL 批量插入的最大行数。
## network_compression_method {#network_compression_method}

Type: String

Default value: LZ4

设置用于服务器之间以及服务器与 [clickhouse-client](../../interfaces/cli.md) 之间通信的数据压缩方法。

Possible values:

- `LZ4` — 设置 LZ4 压缩方法。
- `ZSTD` — 设置 ZSTD 压缩方法。

**See Also**

- [network_zstd_compression_level](#network_zstd_compression_level)
## network_zstd_compression_level {#network_zstd_compression_level}

Type: Int64

Default value: 1

调整 ZSTD 压缩的级别。仅在 [network_compression_method](#network_compression_method) 设置为 `ZSTD` 时使用。

Possible values:

- 从 1 到 15 的正整数。
## normalize_function_names {#normalize_function_names}

Type: Bool

Default value: 1

将函数名称标准化为其规范名称。
## number_of_mutations_to_delay {#number_of_mutations_to_delay}

Type: UInt64

Default value: 0

如果变更表至少包含这么多未完成的变更，则人工减慢表的变更。0 - 禁用。
## number_of_mutations_to_throw {#number_of_mutations_to_throw}

Type: UInt64

Default value: 0

如果变更表至少包含这么多未完成的变更，则抛出 '变更过多 ...' 异常。0 - 禁用。
## odbc_bridge_connection_pool_size {#odbc_bridge_connection_pool_size}

Type: UInt64

Default value: 16

ODBC 桥中每个连接设置字符串的连接池大小。
## odbc_bridge_use_connection_pooling {#odbc_bridge_use_connection_pooling}

Type: Bool

Default value: 1

在 ODBC 桥中使用连接池。如果设置为 false，则每次都创建一个新连接。
## offset {#offset}

Type: UInt64

Default value: 0

设置在开始返回查询的行之前要跳过的行数。它调整由 [OFFSET](/sql-reference/statements/select/offset) 子句设置的偏移量，以便这两个值相加。

Possible values:

- 0 — 不跳过行。
- 正整数。

**Example**

输入表：

``` sql
CREATE TABLE test (i UInt64) ENGINE = MergeTree() ORDER BY i;
INSERT INTO test SELECT number FROM numbers(500);
```

查询：

``` sql
SET limit = 5;
SET offset = 7;
SELECT * FROM test LIMIT 10 OFFSET 100;
```
结果：

``` text
┌───i─┐
│ 107 │
│ 108 │
│ 109 │
└─────┘
```
## opentelemetry_start_trace_probability {#opentelemetry_start_trace_probability}

Type: Float

Default value: 0

设置 ClickHouse 可以为执行的查询启动跟踪的概率（如果没有提供父 [trace context](https://www.w3.org/TR/trace-context/)）。

Possible values:

- 0 — 禁用所有执行查询的跟踪（如果没有提供父跟踪上下文）。
- 介于 [0..1] 之间的正浮点数。例如，如果设置值为 `0.5`，则 ClickHouse 平均可以为一半的查询启动跟踪。
- 1 — 启用所有执行查询的跟踪。
## opentelemetry_trace_processors {#opentelemetry_trace_processors}

Type: Bool

Default value: 0

收集 OpenTelemetry 处理器的跨度。
## optimize_aggregation_in_order {#optimize_aggregation_in_order}

Type: Bool

Default value: 0

启用在 [SELECT](../../sql-reference/statements/select/index.md) 查询中对 [GROUP BY](/sql-reference/statements/select/group-by) 的优化，以便在 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 表中按相应顺序聚合数据。

Possible values:

- 0 — 禁用 `GROUP BY` 优化。
- 1 — 启用 `GROUP BY` 优化。

**See Also**

- [GROUP BY optimization](/sql-reference/statements/select/group-by#group-by-optimization-depending-on-table-sorting-key)
## optimize_aggregators_of_group_by_keys {#optimize_aggregators_of_group_by_keys}

Type: Bool

Default value: 1

消除 SELECT 部分中 GROUP BY 键的 min/max/any/anyLast 聚合器。
## optimize_and_compare_chain {#optimize_and_compare_chain}

Type: Bool

Default value: 1

在 AND 链中填充常量比较，以增强过滤能力。支持操作符 `<`、`<=`、`>`、`>=`、`=` 及其混合。例如，`(a < b) AND (b < c) AND (c < 5)` 将变为 `(a < b) AND (b < c) AND (c < 5) AND (b < 5) AND (a < 5)`。
## optimize_append_index {#optimize_append_index}

Type: Bool

Default value: 0

使用 [约束](../../sql-reference/statements/create/table.md/#constraints) 以追加索引条件。默认值为 `false`。

Possible values:

- true, false
## optimize_arithmetic_operations_in_aggregate_functions {#optimize_arithmetic_operations_in_aggregate_functions}

Type: Bool

Default value: 1

将算术运算移出聚合函数。
## optimize_count_from_files {#optimize_count_from_files}

Type: Bool

Default value: 1

启用或禁用从不同输入格式的文件中计数行数的优化。适用于 `file` / `s3` / `url` / `hdfs` / `azureBlobStorage` 的表函数/引擎。

Possible values:

- 0 — 禁用优化。
- 1 — 启用优化。
## optimize_distinct_in_order {#optimize_distinct_in_order}

Type: Bool

Default value: 1

如果某些列在 DISTINCT 中形成了排序的前缀，则启用 DISTINCT 优化。例如，合并树中排序键的前缀或 ORDER BY 声明。
## optimize_distributed_group_by_sharding_key {#optimize_distributed_group_by_sharding_key}

Type: Bool

Default value: 1

优化 `GROUP BY sharding_key` 查询，通过避免在发起者服务器上进行昂贵的聚合（这样可以减少发起者服务器上的查询内存使用）。

支持以下类型的查询（以及它们的所有组合）：

- `SELECT DISTINCT [..., ]sharding_key[, ...] FROM dist`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...]`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] ORDER BY x`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] LIMIT 1`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] LIMIT 1 BY x`

不支持以下类型的查询（某些查询的支持可能会稍后添加）：

- `SELECT ... GROUP BY sharding_key[, ...] WITH TOTALS`
- `SELECT ... GROUP BY sharding_key[, ...] WITH ROLLUP`
- `SELECT ... GROUP BY sharding_key[, ...] WITH CUBE`
- `SELECT ... GROUP BY sharding_key[, ...] SETTINGS extremes=1`

Possible values:

- 0 — 禁用。
- 1 — 启用。

另请参阅：

- [distributed_group_by_no_merge](#distributed_group_by_no_merge)
- [distributed_push_down_limit](#distributed_push_down_limit)
- [optimize_skip_unused_shards](#optimize_skip_unused_shards)

:::note
目前它需要 `optimize_skip_unused_shards` （原因是将来可能会默认启用，并且只有在通过分布式表插入数据时，即根据 sharding_key 分配数据时，它才能正常工作）。
:::
## optimize_extract_common_expressions {#optimize_extract_common_expressions}

Type: Bool

Default value: 1

允许从 WHERE、PREWHERE、ON、HAVING 和 QUALIFY 表达式中的析取中提取公共表达式。类似于 `(A AND B) OR (A AND C)` 的逻辑表达式可以重写为 `A AND (B OR C)`，这可能有助于利用:
- 简单过滤表达式中的索引
- 交叉到内连接优化
## optimize_functions_to_subcolumns {#optimize_functions_to_subcolumns}

Type: Bool

Default value: 1

启用或禁用通过将某些函数转换为读取子列进行优化。这减少了待读取的数据量。

可以转换的函数包括：

- [length](/sql-reference/functions/array-functions#length) 读取 [size0](../../sql-reference/data-types/array.md/#array-size) 子列。
- [empty](/sql-reference/functions/array-functions#empty) 读取 [size0](../../sql-reference/data-types/array.md/#array-size) 子列。
- [notEmpty](/sql-reference/functions/array-functions#notempty) 读取 [size0](../../sql-reference/data-types/array.md/#array-size) 子列。
- [isNull](/sql-reference/functions/functions-for-nulls#isnull) 读取 [null](../../sql-reference/data-types/nullable.md/#finding-null) 子列。
- [isNotNull](/sql-reference/functions/functions-for-nulls#isnotnull) 读取 [null](../../sql-reference/data-types/nullable.md/#finding-null) 子列。
- [count](/sql-reference/aggregate-functions/reference/count) 读取 [null](../../sql-reference/data-types/nullable.md/#finding-null) 子列。
- [mapKeys](/sql-reference/functions/tuple-map-functions#mapkeys) 读取 [keys](/sql-reference/data-types/map#reading-subcolumns-of-map) 子列。
- [mapValues](/sql-reference/functions/tuple-map-functions#mapvalues) 读取 [values](/sql-reference/data-types/map#reading-subcolumns-of-map) 子列。

Possible values:

- 0 — 禁用优化。
- 1 — 启用优化。
## optimize_group_by_constant_keys {#optimize_group_by_constant_keys}

Type: Bool

Default value: 1

当块中的所有键都是常量时优化 GROUP BY。
## optimize_group_by_function_keys {#optimize_group_by_function_keys}

Type: Bool

Default value: 1

消除 GROUP BY 部分中其他键的函数。
## optimize_if_chain_to_multiif {#optimize_if_chain_to_multiif}

Type: Bool

Default value: 0

将 if(cond1, then1, if(cond2, ...)) 链替换为 multiIf。目前对于数字类型没有好处。
## optimize_if_transform_strings_to_enum {#optimize_if_transform_strings_to_enum}

Type: Bool

Default value: 0

将 If 和 Transform 中的字符串类型参数替换为枚举。默认情况下禁用，因为这会在分布式查询中造成不一致的更改，从而导致查询失败。
## optimize_injective_functions_in_group_by {#optimize_injective_functions_in_group_by}

Type: Bool

Default value: 1

在 GROUP BY 部分用其参数替换可注入函数。
## optimize_injective_functions_inside_uniq {#optimize_injective_functions_inside_uniq}

Type: Bool

Default value: 1

在 uniq*() 函数内部删除一个参数的可注入函数。
## optimize_min_equality_disjunction_chain_length {#optimize_min_equality_disjunction_chain_length}

Type: UInt64

Default value: 3

表达式 `expr = x1 OR ... expr = xN` 的最小长度以进行优化。
## optimize_min_inequality_conjunction_chain_length {#optimize_min_inequality_conjunction_chain_length}

Type: UInt64

Default value: 3

表达式 `expr <> x1 AND ... expr <> xN` 的最小长度以进行优化。
## optimize_move_to_prewhere {#optimize_move_to_prewhere}

Type: Bool

Default value: 1

在 [SELECT](../../sql-reference/statements/select/index.md) 查询中启用或禁用自动 [PREWHERE](../../sql-reference/statements/select/prewhere.md) 优化。

仅在 [*MergeTree](../../engines/table-engines/mergetree-family/index.md) 表上有效。

Possible values:

- 0 — 禁用自动 `PREWHERE` 优化。
- 1 — 启用自动 `PREWHERE` 优化。
## optimize_move_to_prewhere_if_final {#optimize_move_to_prewhere_if_final}

Type: Bool

Default value: 0

在 [SELECT](../../sql-reference/statements/select/index.md) 查询中启用或禁用带有 [FINAL](/sql-reference/statements/select/from#final-modifier) 修饰符的自动 [PREWHERE](../../sql-reference/statements/select/prewhere.md) 优化。

仅在 [*MergeTree](../../engines/table-engines/mergetree-family/index.md) 表上有效。

Possible values:

- 0 — 禁用在带有 `FINAL` 修饰符的 `SELECT` 查询中的自动 `PREWHERE` 优化。
- 1 — 启用在带有 `FINAL` 修饰符的 `SELECT` 查询中的自动 `PREWHERE` 优化。

**See Also**

- [optimize_move_to_prewhere](#optimize_move_to_prewhere) 设置。
## optimize_multiif_to_if {#optimize_multiif_to_if}

Type: Bool

Default value: 1

将仅有一个条件的 'multiIf' 替换为 'if'。
## optimize_normalize_count_variants {#optimize_normalize_count_variants}

Type: Bool

Default value: 1

将在语义上等于 count() 的聚合函数重写为 count()。
## optimize_on_insert {#optimize_on_insert}

Type: Bool

Default value: 1

在插入之前启用或禁用数据转换，仿佛在该块上进行过合并（根据表引擎）。

Possible values:

- 0 — 禁用。
- 1 — 启用。

**Example**

启用和禁用之间的差异：

查询：

```sql
SET optimize_on_insert = 1;

CREATE TABLE test1 (`FirstTable` UInt32) ENGINE = ReplacingMergeTree ORDER BY FirstTable;

INSERT INTO test1 SELECT number % 2 FROM numbers(5);

SELECT * FROM test1;

SET optimize_on_insert = 0;

CREATE TABLE test2 (`SecondTable` UInt32) ENGINE = ReplacingMergeTree ORDER BY SecondTable;

INSERT INTO test2 SELECT number % 2 FROM numbers(5);

SELECT * FROM test2;
```

结果：

``` text
┌─FirstTable─┐
│          0 │
│          1 │
└────────────┘

┌─SecondTable─┐
│           0 │
│           0 │
│           0 │
│           1 │
│           1 │
└─────────────┘
```

注意，此设置影响 [物化视图](/sql-reference/statements/create/view#materialized-view) 的行为。
## optimize_or_like_chain {#optimize_or_like_chain}

Type: Bool

Default value: 0

优化多个 OR LIKE 为 multiMatchAny。此优化不应默认启用，因为在某些情况下会无视索引分析。
## optimize_read_in_order {#optimize_read_in_order}

Type: Bool

Default value: 1

在 [SELECT](../../sql-reference/statements/select/index.md) 查询中启用 [ORDER BY](/sql-reference/statements/select/order-by#optimization-of-data-reading) 优化，以便从 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 表中读取数据。

Possible values:

- 0 — 禁用 `ORDER BY` 优化。
- 1 — 启用 `ORDER BY` 优化。

**See Also**

- [ORDER BY Clause](/sql-reference/statements/select/order-by#optimization-of-data-reading)
## optimize_read_in_window_order {#optimize_read_in_window_order}

Type: Bool

Default value: 1

启用窗口子句中的 ORDER BY 优化，以按相应顺序读取 MergeTree 表中的数据。
## optimize_redundant_functions_in_order_by {#optimize_redundant_functions_in_order_by}

Type: Bool

Default value: 1

如果其参数也在 ORDER BY 中，则从 ORDER BY 中删除函数。
## optimize_respect_aliases {#optimize_respect_aliases}

Type: Bool

Default value: 1

如果设置为 true，将在 WHERE/GROUP BY/ORDER BY 中尊重别名，这将有助于分区修剪/副索引/optimize_aggregation_in_order/optimize_read_in_order/optimize_trivial_count。
## optimize_rewrite_aggregate_function_with_if {#optimize_rewrite_aggregate_function_with_if}

Type: Bool

Default value: 1

当逻辑上等同时，将具有 if 表达式的聚合函数重写为参数。例如，`avg(if(cond, col, null))` 可以重写为 `avgOrNullIf(cond, col)`。这可能会提高性能。

:::note
仅支持分析器（`enable_analyzer = 1`）。
:::
## optimize_rewrite_array_exists_to_has {#optimize_rewrite_array_exists_to_has}

Type: Bool

Default value: 0

在逻辑上等同时将 arrayExists() 函数重写为 has()。例如，arrayExists(x -> x = 1, arr) 可以重写为 has(arr, 1)。
## optimize_rewrite_sum_if_to_count_if {#optimize_rewrite_sum_if_to_count_if}

Type: Bool

Default value: 1

在逻辑上等同时，将 sumIf() 和 sum(if()) 函数重写为 countIf() 函数。
## optimize_skip_merged_partitions {#optimize_skip_merged_partitions}

Type: Bool

Default value: 0

启用或禁用优化 [OPTIMIZE TABLE ... FINAL](../../sql-reference/statements/optimize.md) 查询，如果只有一个分区，其级别 > 0 并且没有过期的 TTL。

- `OPTIMIZE TABLE ... FINAL SETTINGS optimize_skip_merged_partitions=1`

默认情况下，`OPTIMIZE TABLE ... FINAL` 查询即使没有任何操作，也会重写一个分区。

Possible values:

- 1 - 启用优化。
- 0 - 禁用优化。
## optimize_skip_unused_shards {#optimize_skip_unused_shards}

Type: Bool

Default value: 0

启用或禁用对具有分片密钥条件的 [SELECT](../../sql-reference/statements/select/index.md) 查询的未使用分片跳过（假设数据是通过分片密钥分布的，否则查询将产生不正确的结果）。

Possible values:

- 0 — 禁用。
- 1 — 启用。
## optimize_skip_unused_shards_limit {#optimize_skip_unused_shards_limit}

Type: UInt64

Default value: 1000

分片密钥值的数量限制，当达到限制时，将关闭 `optimize_skip_unused_shards`。

过多的值可能需要大量处理，而收益值得怀疑，因为如果在 `IN (...)` 中有大量值，则查询很可能会发送到所有分片。
## optimize_skip_unused_shards_nesting {#optimize_skip_unused_shards_nesting}

Type: UInt64

Default value: 0

控制 [`optimize_skip_unused_shards`](#optimize_skip_unused_shards) （因此仍然要求 [`optimize_skip_unused_shards`](#optimize_skip_unused_shards)），根据分布式查询的嵌套级别（当您拥有 `Distributed` 表检查另一个 `Distributed` 表时）。

Possible values:

- 0 — 禁用，`optimize_skip_unused_shards` 始终有效。
- 1 — 只在第一层启用 `optimize_skip_unused_shards`。
- 2 — 启用 `optimize_skip_unused_shards` 直到第二层。
## optimize_skip_unused_shards_rewrite_in {#optimize_skip_unused_shards_rewrite_in}

Type: Bool

Default value: 1

在查询中重写远程分片的 IN，以排除不属于该分片的值（需要 optimize_skip_unused_shards）。

Possible values:

- 0 — 禁用。
- 1 — 启用。
## optimize_sorting_by_input_stream_properties {#optimize_sorting_by_input_stream_properties}

Type: Bool

Default value: 1

通过输入流的排序属性优化排序。
## optimize_substitute_columns {#optimize_substitute_columns}

Type: Bool

Default value: 0

使用 [约束](../../sql-reference/statements/create/table.md/#constraints) 进行列替换。默认值为 `false`。

Possible values:

- true, false
## optimize_syntax_fuse_functions {#optimize_syntax_fuse_functions}

Type: Bool

Default value: 0

启用通过相同参数融合聚合函数。将包含至少两个相同参数的 [sum](/sql-reference/aggregate-functions/reference/sum)、[count](/sql-reference/aggregate-functions/reference/count) 或 [avg](/sql-reference/aggregate-functions/reference/avg) 的查询重写为 [sumCount](/sql-reference/aggregate-functions/reference/sumcount)。

Possible values:

- 0 — 不融合具有相同参数的函数。
- 1 — 融合具有相同参数的函数。

**Example**

查询：

``` sql
CREATE TABLE fuse_tbl(a Int8, b Int8) Engine = Log;
SET optimize_syntax_fuse_functions = 1;
EXPLAIN SYNTAX SELECT sum(a), sum(b), count(b), avg(b) from fuse_tbl FORMAT TSV;
```

结果：

``` text
SELECT
    sum(a),
    sumCount(b).1,
    sumCount(b).2,
    (sumCount(b).1) / (sumCount(b).2)
FROM fuse_tbl
```
## optimize_throw_if_noop {#optimize_throw_if_noop}

Type: Bool

Default value: 0

启用或禁用当 [OPTIMIZE](../../sql-reference/statements/optimize.md) 查询未执行任何合并时抛出异常。

默认情况下，如果没有任何操作，`OPTIMIZE` 返回成功。此设置可让您区分这些情况并在异常消息中获取原因。

Possible values:

- 1 — 启用抛出异常。
- 0 — 禁用抛出异常。
## optimize_time_filter_with_preimage {#optimize_time_filter_with_preimage}

Type: Bool

Default value: 1

通过将函数转换为等效比较而优化日期和日期时间谓词，无需转换（例如，`toYear(col) = 2023 -> col >= '2023-01-01' AND col <= '2023-12-31'`）。
## optimize_trivial_approximate_count_query {#optimize_trivial_approximate_count_query}

Type: Bool

Default value: 0

对支持此估计的存储进行简单计数优化时使用近似值，例如，EmbeddedRocksDB。

Possible values:

   - 0 — 禁用优化。
   - 1 — 启用优化。
## optimize_trivial_count_query {#optimize_trivial_count_query}

Type: Bool

Default value: 1

启用或禁用对简单查询 `SELECT count() FROM table` 的优化，使用来自 MergeTree 的元数据。如果您需要使用行级安全性，请禁用此设置。

Possible values:

   - 0 — 禁用优化。
   - 1 — 启用优化。

另请参阅：

- [optimize_functions_to_subcolumns](#optimize_functions_to_subcolumns)
## optimize_trivial_insert_select {#optimize_trivial_insert_select}

Type: Bool

Default value: 0

优化简单 'INSERT INTO table SELECT ... FROM TABLES' 查询。
## optimize_uniq_to_count {#optimize_uniq_to_count}

Type: Bool

Default value: 1

在子查询具有 distinct 或 group by 子句时，将 uniq 及其变体（除了 uniqUpTo）重写为 count。
## optimize_use_implicit_projections {#optimize_use_implicit_projections}

Type: Bool

Default value: 1

自动选择隐式投影以执行 SELECT 查询。
## optimize_use_projections {#optimize_use_projections}

Type: Bool

Default value: 1

在处理 `SELECT` 查询时启用或禁用 [投影](../../engines/table-engines/mergetree-family/mergetree.md/#projections) 优化。

Possible values:

- 0 — 禁用投影优化。
- 1 — 启用投影优化。
## optimize_using_constraints {#optimize_using_constraints}

Type: Bool

Default value: 0

用于查询优化的 [约束](../../sql-reference/statements/create/table.md/#constraints)。默认值为 `false`。

Possible values:

- true, false
## os_thread_priority {#os_thread_priority}

Type: Int64

Default value: 0

设置执行查询的线程的优先级（[nice](https://en.wikipedia.org/wiki/Nice_(Unix)))。操作系统调度程序在选择每个可用 CPU 核心上运行的下一个线程时会考虑此优先级。

:::note
要使用此设置，您需要设置 `CAP_SYS_NICE` 权限。`clickhouse-server` 包在安装时会设置它。一些虚拟环境不允许您设置 `CAP_SYS_NICE` 权限。在这种情况下，`clickhouse-server` 会在启动时显示一条消息。
:::

Possible values:

- 您可以在 `[-20, 19]` 范围内设置值。

较低的值意味着更高的优先级。具有较低 `nice` 优先级值的线程比具有高值的线程执行频率更高。对于长时间运行的非交互式查询，较高的值是可取的，因为它允许这些查询在到达时快速放弃资源，以便为短时交互式查询腾出空间。
## output_format_compression_level {#output_format_compression_level}

Type: UInt64

Default value: 3

如果查询输出被压缩，则默认压缩级别。当 `SELECT` 查询具有 `INTO OUTFILE` 时应用此设置，或在写入表函数 `file`、`url`、`hdfs`、`s3` 或 `azureBlobStorage` 时。

Possible values: 从 `1` 到 `22`。
## output_format_compression_zstd_window_log {#output_format_compression_zstd_window_log}

Type: UInt64

Default value: 0

当输出压缩方法为 `zstd` 时可以使用。如果大于 `0`，则该设置显式设置压缩窗口大小（2 的幂），并为 zstd 压缩启用长范围模式。这可以帮助实现更好的压缩比。

Possible values: 非负数。注意，如果值太小或太大，`zstdlib` 将抛出异常。典型值从 `20`（窗口大小 = `1MB`）到 `30`（窗口大小 = `1GB`）。
## output_format_parallel_formatting {#output_format_parallel_formatting}

Type: Bool

Default value: 1

启用或禁用数据格式的并行格式化。仅支持 [TSV](../../interfaces/formats.md/#tabseparated)、[TSKV](../../interfaces/formats.md/#tskv)、[CSV](../../interfaces/formats.md/#csv) 和 [JSONEachRow](../../interfaces/formats.md/#jsoneachrow) 格式。

Possible values:

- 1 — 启用。
- 0 — 禁用。
## page_cache_inject_eviction {#page_cache_inject_eviction}

Type: Bool

Default value: 0

用户空间页面缓存将随机无效化某些页面。用于测试。
```

## parallel_distributed_insert_select {#parallel_distributed_insert_select}

类型: UInt64

默认值: 0

启用并行分布式 `INSERT ... SELECT` 查询。

如果我们执行 `INSERT INTO distributed_table_a SELECT ... FROM distributed_table_b` 查询，并且两个表都使用相同的集群，且两个表都是 [replicated](../../engines/table-engines/mergetree-family/replication.md) 或非复制的，则此查询将在每个分片上本地处理。

可能的值：

- 0 — 禁用。
- 1 — `SELECT`将在分布式引擎的基础表的每个分片上执行。
- 2 — `SELECT`和`INSERT`将在分布式引擎的基础表的每个分片上执行。

## parallel_replica_offset {#parallel_replica_offset}
<BetaBadge/>

类型: UInt64

默认值: 0

这是一个内部设置，不应直接使用，表示“并行副本”模式的实现细节。此设置将由发起服务器自动设置，以进行分布式查询到参与查询处理的副本的索引。

## parallel_replicas_allow_in_with_subquery {#parallel_replicas_allow_in_with_subquery}
<BetaBadge/>

类型: Bool

默认值: 1

如果为真，则IN子查询将在每个跟随副本上执行。

## parallel_replicas_count {#parallel_replicas_count}
<BetaBadge/>

类型: UInt64

默认值: 0

这是一个内部设置，不应直接使用，表示“并行副本”模式的实现细节。此设置将由发起服务器自动设置，以进行分布式查询到参与查询处理的并行副本的数量。

## parallel_replicas_custom_key {#parallel_replicas_custom_key}
<BetaBadge/>

类型: String

默认值: 

一个任意整数表达式，可用于为特定表在副本之间分配工作。该值可以是任何整数表达式。

建议使用主键的简单表达式。

如果此设置用于由一个分片和多个副本组成的集群，这些副本将转换为虚拟分片。否则，其行为将与`SAMPLE`键相同，将使用每个分片的多个副本。

## parallel_replicas_custom_key_range_lower {#parallel_replicas_custom_key_range_lower}
<BetaBadge/>

类型: UInt64

默认值: 0

允许过滤器类型 `range` 基于自定义范围 `[parallel_replicas_custom_key_range_lower, INT_MAX]` 在副本之间均匀地分配工作。

与 [parallel_replicas_custom_key_range_upper](#parallel_replicas_custom_key_range_upper) 一起使用时，它允许过滤器在 `[parallel_replicas_custom_key_range_lower, parallel_replicas_custom_key_range_upper]` 范围内均匀地分配工作。

注意：此设置不会导致在查询处理期间筛选任何额外数据，而是改变了在并行处理过程中范围过滤器中断范围 `[0, INT_MAX]` 的点。

## parallel_replicas_custom_key_range_upper {#parallel_replicas_custom_key_range_upper}
<BetaBadge/>

类型: UInt64

默认值: 0

允许过滤器类型 `range` 基于自定义范围 `[0, parallel_replicas_custom_key_range_upper]` 在副本之间均匀地分配工作。值为 0 会禁用上限，将其设置为自定义键表达式的最大值。

与 [parallel_replicas_custom_key_range_lower](#parallel_replicas_custom_key_range_lower) 一起使用时，它允许过滤器在 `[parallel_replicas_custom_key_range_lower, parallel_replicas_custom_key_range_upper]` 范围内均匀地分配工作。

注意：此设置不会导致在查询处理期间筛选任何额外数据，而是改变了在并行处理过程中范围过滤器中断范围 `[0, INT_MAX]` 的点。

## parallel_replicas_for_non_replicated_merge_tree {#parallel_replicas_for_non_replicated_merge_tree}
<BetaBadge/>

类型: Bool

默认值: 0

如果为真，ClickHouse 将对非复制的 MergeTree 表也使用并行副本算法。

## parallel_replicas_index_analysis_only_on_coordinator {#parallel_replicas_index_analysis_only_on_coordinator}
<BetaBadge/>

类型: Bool

默认值: 1

仅在副本协调者上进行索引分析，并跳过其他副本。仅在启用 parallel_replicas_local_plan 时有效。

## parallel_replicas_local_plan {#parallel_replicas_local_plan}
<BetaBadge/>

类型: Bool

默认值: 1

为本地副本构建本地计划。

## parallel_replicas_mark_segment_size {#parallel_replicas_mark_segment_size}
<BetaBadge/>

类型: UInt64

默认值: 0

将分区虚拟划分为段，以便在副本之间进行并行读取。此设置控制这些段的大小。未建议更改，除非您绝对确定自己在做什么。值应在 [128; 16384] 范围内。

## parallel_replicas_min_number_of_rows_per_replica {#parallel_replicas_min_number_of_rows_per_replica}
<BetaBadge/>

类型: UInt64

默认值: 0

限制查询中使用的副本数量为（估计读取的行 / min_number_of_rows_per_replica）。最大仍受 'max_parallel_replicas' 的限制。

## parallel_replicas_mode {#parallel_replicas_mode}
<BetaBadge/>

类型: ParallelReplicasMode

默认值: read_tasks

用于自定义键的并行副本的过滤器类型。默认 - 在自定义键上使用模运算，范围 - 在自定义键上的范围过滤器，使用自定义键值类型的所有可能值。

## parallel_replicas_only_with_analyzer {#parallel_replicas_only_with_analyzer}
<BetaBadge/>

类型: Bool

默认值: 1

启用分析器以使用并行副本。如果未启用分析器，查询执行将回退到本地执行，即使启用了从副本的并行读取。未启用分析器的情况下使用并行副本不受支持。

## parallel_replicas_prefer_local_join {#parallel_replicas_prefer_local_join}
<BetaBadge/>

类型: Bool

默认值: 1

如果为真，并且 JOIN 可以使用并行副本算法执行，并且右侧 JOIN 部分的所有存储都是 *MergeTree，将使用本地 JOIN 而不是 GLOBAL JOIN。

## parallel_view_processing {#parallel_view_processing}

类型: Bool

默认值: 0

允许同时推送到附加视图，而不是顺序推送。

## parallelize_output_from_storages {#parallelize_output_from_storages}

类型: Bool

默认值: 1

并行化从存储的读取步骤的输出。它允许在从存储读取后立即并行处理查询（如果可能）。

## parsedatetime_parse_without_leading_zeros {#parsedatetime_parse_without_leading_zeros}

类型: Bool

默认值: 1

格式化器 '%c'、'%l' 和 '%k' 在函数 'parseDateTime' 中解析月份和小时时不带前导零。

## partial_merge_join_left_table_buffer_bytes {#partial_merge_join_left_table_buffer_bytes}

类型: UInt64

默认值: 0

如果不为 0，则在部分合并连接中将左表块分组为更大的块。使用每个连接线程最多 2 倍的指定内存。

## partial_merge_join_rows_in_right_blocks {#partial_merge_join_rows_in_right_blocks}

类型: UInt64

默认值: 65536

限制部分合并 JOIN 算法中右侧连接数据块的大小，以用于 [JOIN](../../sql-reference/statements/select/join.md) 查询。

ClickHouse 服务器：

1. 将右侧连接数据拆分为行数不超过指定数量的块。
2. 为每个块索引其最小和最大值。
3. 如果可行，将准备好的块卸载到磁盘。

可能的值：

- 任何正整数。推荐的值范围: [1000, 100000]。

## partial_result_on_first_cancel {#partial_result_on_first_cancel}

类型: Bool

默认值: 0

允许查询在取消后返回部分结果。

## parts_to_delay_insert {#parts_to_delay_insert}

类型: UInt64

默认值: 0

如果目标表在单个分区中至少包含这么多活动部件，则人为减慢插入到表中。

## parts_to_throw_insert {#parts_to_throw_insert}

类型: UInt64

默认值: 0

如果在目标表的单个分区中活动部件数量超过此数字，则抛出“部件过多...”异常。

## periodic_live_view_refresh {#periodic_live_view_refresh}

类型: 秒

默认值: 60

强制周期性刷新的实时视图的间隔。

## poll_interval {#poll_interval}

类型: UInt64

默认值: 10

在服务器的查询等待循环中阻塞指定的秒数。

## postgresql_connection_attempt_timeout {#postgresql_connection_attempt_timeout}

类型: UInt64

默认值: 2

单次尝试连接 PostgreSQL 端点的连接超时（秒）。该值作为连接 URL 的 `connect_timeout` 参数传递。

## postgresql_connection_pool_auto_close_connection {#postgresql_connection_pool_auto_close_connection}

类型: Bool

默认值: 0

在将连接返回到池之前关闭连接。

## postgresql_connection_pool_retries {#postgresql_connection_pool_retries}

类型: UInt64

默认值: 2

PostgreSQL 表引擎和数据库引擎的连接池推送/弹出重试次数。

## postgresql_connection_pool_size {#postgresql_connection_pool_size}

类型: UInt64

默认值: 16

PostgreSQL 表引擎和数据库引擎的连接池大小。

## postgresql_connection_pool_wait_timeout {#postgresql_connection_pool_wait_timeout}

类型: UInt64

默认值: 5000

PostgreSQL 表引擎和数据库引擎在空池时的连接池推送/弹出超时。默认情况下将在空池时阻塞。

## postgresql_fault_injection_probability {#postgresql_fault_injection_probability}

类型: Float

默认值: 0

内部（用于复制） PostgreSQL 查询失败的近似概率。有效值在 [0.0f, 1.0f] 范围内。

## prefer_column_name_to_alias {#prefer_column_name_to_alias}

类型: Bool

默认值: 0

启用或禁用在查询表达式和子句中使用原始列名而不是别名。当别名与列名相同时，这一点尤其重要，见 [Expression Aliases](/sql-reference/syntax#notes-on-usage)。启用此设置，可以使 ClickHouse 中的别名语法规则与大多数其他数据库引擎更兼容。

可能的值：

- 0 — 列名替换为别名。
- 1 — 列名不替换为别名。

**示例**

启用和禁用之间的区别：

查询：

```sql
SET prefer_column_name_to_alias = 0;
SELECT avg(number) AS number, max(number) FROM numbers(10);
```

结果：

```text
从服务器收到异常（版本 21.5.1）：
代码: 184. DB::Exception: 从 localhost:9000 收到。DB::Exception: 聚合函数 avg(number) 在查询中位于另一个聚合函数内部: 在处理 avg(number) AS number 时。
```

查询：

```sql
SET prefer_column_name_to_alias = 1;
SELECT avg(number) AS number, max(number) FROM numbers(10);
```

结果：

```text
┌─number─┬─max(number)─┐
│    4.5 │           9 │
└────────┴─────────────┘
```

## prefer_external_sort_block_bytes {#prefer_external_sort_block_bytes}

类型: UInt64

默认值: 16744704

优先最大块字节数用于外部排序，从而减少合并时的内存使用。

## prefer_global_in_and_join {#prefer_global_in_and_join}

类型: Bool

默认值: 0

启用使用 `GLOBAL IN` / `GLOBAL JOIN` 替换 `IN` / `JOIN` 操作符。

可能的值：

- 0 — 禁用。`IN` / `JOIN` 操作符不被替换为 `GLOBAL IN` / `GLOBAL JOIN`。
- 1 — 启用。`IN` / `JOIN` 操作符被替换为 `GLOBAL IN` / `GLOBAL JOIN`。

**用法**

尽管 `SET distributed_product_mode=global` 可以改变分布式表的查询行为，但它不适用于本地表或来自外部资源的表。这是 `prefer_global_in_and_join` 设置发挥作用的时候。

例如，我们有服务节点的查询，里面包含不适合分布的本地表。我们需要在分布处理期间动态地分散它们的数据，使用 `GLOBAL` 关键字 — `GLOBAL IN` / `GLOBAL JOIN`。

`prefer_global_in_and_join` 的另一个用例是访问由外部引擎创建的表。此设置帮助减少在联接这些表时对外部源的调用次数：每个查询只需一次调用。

**另见：**

- [Distributed subqueries](/sql-reference/operators/in#distributed-subqueries) 以获取有关如何使用 `GLOBAL IN` / `GLOBAL JOIN` 的更多信息。

## prefer_localhost_replica {#prefer_localhost_replica}

类型: Bool

默认值: 1

启用/禁用处理分布式查询时首选使用 localhost 副本。

可能的值：

- 1 — 如果存在，ClickHouse 始终将查询发送到 localhost 副本。
- 0 — ClickHouse 使用 [load_balancing](#load_balancing) 设置指定的平衡策略。

:::note
如果您使用 [max_parallel_replicas](#max_parallel_replicas) 而没有 [parallel_replicas_custom_key](#parallel_replicas_custom_key)，则应禁用此设置。
如果设置了 [parallel_replicas_custom_key](#parallel_replicas_custom_key)，仅当在包含多个副本的多个分片的集群上使用时才禁用此设置。
如果在单个分片和多个副本的集群上使用，则禁用此设置会产生负面影响。
:::

## prefer_warmed_unmerged_parts_seconds {#prefer_warmed_unmerged_parts_seconds}

<CloudAvailableBadge/>

类型: Int64

默认值: 0

仅在 ClickHouse Cloud 中有效。如果合并部分的年龄小于规定的秒数且尚未预热（见 [cache_populated_by_fetch](merge-tree-settings.md/#cache_populated_by_fetch)），但其所有源部分都是可用的并且已预热，则 SELECT 查询将优先读取那些部分。仅适用于 Replicated-/SharedMergeTree。请注意，这仅检查 CacheWarmer 是否处理了该部分；如果部分是由其他内容获取到缓存中的，它仍将被视为冷部分，直到 CacheWarmer 处理它；如果它被加热，然后从缓存中驱逐，它仍将被视为热部分。

## preferred_block_size_bytes {#preferred_block_size_bytes}

类型: UInt64

默认值: 1000000

此设置调整查询处理的数据块大小，并代表对粗略的 `max_block_size` 设置的额外微调。如果列较大，并且使用 ‘max_block_size’ 行，则块大小可能会大于指定的字节数，则将其大小降低以改善 CPU 缓存的局部性。

## preferred_max_column_in_block_size_bytes {#preferred_max_column_in_block_size_bytes}

类型: UInt64

默认值: 0

读取时块中的最大列大小的限制。帮助减少缓存未命中的数量。应接近 L2 缓存的大小。

## preferred_optimize_projection_name {#preferred_optimize_projection_name}

类型: String

默认值: 

如果设置为非空字符串，ClickHouse 将尝试在查询中应用指定的投影。

可能的值：

- 字符串：首选投影的名称。

## prefetch_buffer_size {#prefetch_buffer_size}

类型: UInt64

默认值: 1048576

从文件系统读取的预取缓冲区的最大大小。

## print_pretty_type_names {#print_pretty_type_names}

类型: Bool

默认值: 1

允许以美观的方式打印嵌套类型名称，在 `DESCRIBE` 查询和 `toTypeName()` 函数中缩进。

示例：

```sql
CREATE TABLE test (a Tuple(b String, c Tuple(d Nullable(UInt64), e Array(UInt32), f Array(Tuple(g String, h Map(String, Array(Tuple(i String, j UInt64))))), k Date), l Nullable(String))) ENGINE=Memory;
DESCRIBE TABLE test FORMAT TSVRaw SETTINGS print_pretty_type_names=1;
```

```
a   Tuple(
    b String,
    c Tuple(
        d Nullable(UInt64),
        e Array(UInt32),
        f Array(Tuple(
            g String,
            h Map(
                String,
                Array(Tuple(
                    i String,
                    j UInt64
                ))
            )
        )),
        k Date
    ),
    l Nullable(String)
)
```

## priority {#priority}

类型: UInt64

默认值: 0

查询的优先级。1 - 最高，值越高 - 优先级越低；0 - 不使用优先级。

## push_external_roles_in_interserver_queries {#push_external_roles_in_interserver_queries}

类型: Bool

默认值: 1

启用在执行查询时将用户角色从发起者推送到其他节点。

## query_cache_compress_entries {#query_cache_compress_entries}

类型: Bool

默认值: 1

压缩 [query cache](../query-cache.md) 中的条目。减少查询缓存的内存消耗，但增加写入到/从中读取的速度。

可能的值：

- 0 - 禁用
- 1 - 启用

## query_cache_max_entries {#query_cache_max_entries}

类型: UInt64

默认值: 0

当前用户可以存储在 [query cache](../query-cache.md) 中的查询结果的最大数量。0 表示无限制。

可能的值：

- 正整数 >= 0。

## query_cache_max_size_in_bytes {#query_cache_max_size_in_bytes}

类型: UInt64

默认值: 0

当前用户可以在 [query cache](../query-cache.md) 中分配的最大内存（以字节为单位）。0 表示无限制。

可能的值：

- 正整数 >= 0。

## query_cache_min_query_duration {#query_cache_min_query_duration}

类型: 毫秒

默认值: 0

查询存储其结果到 [query cache](../query-cache.md) 的最小持续时间（毫秒）。

可能的值：

- 正整数 >= 0。

## query_cache_min_query_runs {#query_cache_min_query_runs}

类型: UInt64

默认值: 0

`SELECT` 查询必须运行的最小次数，在此之后其结果将存储在 [query cache](../query-cache.md) 中。

可能的值：

- 正整数 >= 0。

## query_cache_nondeterministic_function_handling {#query_cache_nondeterministic_function_handling}

类型: QueryResultCacheNondeterministicFunctionHandling

默认值: throw

控制 [query cache](../query-cache.md) 如何处理带有非确定性函数的 `SELECT` 查询，例如 `rand()` 或 `now()`。

可能的值：

- `'throw'` - 抛出异常并不缓存查询结果。
- `'save'` - 缓存查询结果。
- `'ignore'` - 不缓存查询结果且不抛出异常。

## query_cache_share_between_users {#query_cache_share_between_users}

类型: Bool

默认值: 0

如果启用，则在 [query cache](../query-cache.md) 中缓存的 `SELECT` 查询的结果可被其他用户读取。
出于安全原因，不建议启用此设置。

可能的值：

- 0 - 禁用
- 1 - 启用

## query_cache_squash_partial_results {#query_cache_squash_partial_results}

类型: Bool

默认值: 1

将部分结果块压缩为 [max_block_size](#max_block_size) 大小的块。降低查询缓存的插入性能，但提高缓存条目的压缩性（见 [query_cache_compress-entries](#query_cache_compress_entries)）。

可能的值：

- 0 - 禁用
- 1 - 启用

## query_cache_system_table_handling {#query_cache_system_table_handling}

类型: QueryResultCacheSystemTableHandling

默认值: throw

控制 [query cache](../query-cache.md) 如何处理针对系统表的 `SELECT` 查询，即数据库 `system.*` 和 `information_schema.*` 中的表。

可能的值：

- `'throw'` - 抛出异常并不缓存查询结果。
- `'save'` - 缓存查询结果。
- `'ignore'` - 不缓存查询结果且不抛出异常。

## query_cache_tag {#query_cache_tag}

类型: String

默认值: 

一个字符串，用作 [query cache](../query-cache.md) 条目的标签。
相同的查询如果有不同的标签，会被查询缓存视为不同的查询。

可能的值：

- 任意字符串。

## query_cache_ttl {#query_cache_ttl}

类型: 秒

默认值: 60

此时间到期后，以秒为单位的 [query cache](../query-cache.md) 条目将变得过期。

可能的值：

- 正整数 >= 0。

## query_metric_log_interval {#query_metric_log_interval}

类型: Int64

默认值: -1

个别查询的 [query_metric_log](../../operations/system-tables/query_metric_log.md) 收集的间隔（毫秒）。

如果设置为任何负值，它将采用 [query_metric_log setting](/operations/server-configuration-parameters/settings#query_metric_log) 中的 `collect_interval_milliseconds` 值，或者如果未提供则默认为 1000。

要禁用单个查询的收集，设置 `query_metric_log_interval` 为 0。

默认值: -1。

## query_plan_aggregation_in_order {#query_plan_aggregation_in_order}

类型: Bool

默认值: 1

切换查询计划级优化中的顺序聚合。

仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅应供开发人员调试使用。该设置可能在未来以向后不兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_convert_outer_join_to_inner_join {#query_plan_convert_outer_join_to_inner_join}

类型: Bool

默认值: 1

如果 JOIN 后的过滤器始终过滤默认值，则允许将外部 JOIN 转换为内部 JOIN。

## query_plan_enable_multithreading_after_window_functions {#query_plan_enable_multithreading_after_window_functions}

类型: Bool

默认值: 1

enable 在计算窗口函数后允许多线程处理，以实现并行流处理。

## query_plan_enable_optimizations {#query_plan_enable_optimizations}

类型: Bool

默认值: 1

切换查询计划级的优化。

:::note
这是一个专家级设置，仅应供开发人员调试使用。该设置可能在未来以向后不兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用所有查询计划级的优化。
- 1 - 启用查询计划级的优化（但个别优化仍然可以通过其个别设置禁用）。

## query_plan_execute_functions_after_sorting {#query_plan_execute_functions_after_sorting}

类型: Bool

默认值: 1

切换查询计划级的优化，将表达式移动到排序步骤之后。

仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅应供开发人员调试使用。该设置可能在未来以向后不兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_filter_push_down {#query_plan_filter_push_down}

类型: Bool

默认值: 1

切换查询计划级优化，向下移动执行计划中的过滤器。

仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅应供开发人员调试使用。该设置可能在未来以向后不兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_join_swap_table {#query_plan_join_swap_table}

类型: BoolAuto

默认值: auto

决定 JOIN 中哪个侧应为构建表（也称为内部表，即插入哈希表的表）在查询计划中。此设置仅支持 `ALL` JOIN 严格性与 `JOIN ON` 子句。可能的值有：
- 'auto': 让规划者决定使用哪个表作为构建表。
- 'false': 永不交换表（右表为构建表）。
- 'true': 始终交换表（左表为构建表）。

## query_plan_lift_up_array_join {#query_plan_lift_up_array_join}

类型: Bool

默认值: 1

切换查询计划级优化，将 ARRAY JOIN 向上移动到执行计划中。

仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅应供开发人员调试使用。该设置可能在未来以向后不兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_lift_up_union {#query_plan_lift_up_union}

类型: Bool

默认值: 1

切换查询计划级优化，将更大的 query plan 子树移入 union，以启用进一步的优化。

仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅应供开发人员调试使用。该设置可能在未来以向后不兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_max_optimizations_to_apply {#query_plan_max_optimizations_to_apply}

类型: UInt64

默认值: 10000

限制应用于查询计划的总优化数量，见设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations)。
有助于避免复杂查询的优化时间过长。
在 EXPLAIN PLAN 查询中，在达到该限制后停止应用优化，并按原样返回计划。
对于常规查询执行，如果实际应用的优化数量超过此设置，将抛出异常。

:::note
这是一个专家级设置，仅应供开发人员调试使用。该设置可能在未来以向后不兼容的方式更改或被移除。
:::

## query_plan_merge_expressions {#query_plan_merge_expressions}

类型: Bool

默认值: 1

切换查询计划级优化，合并连续的过滤器。

仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅应供开发人员调试使用。该设置可能在未来以向后不兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_merge_filters {#query_plan_merge_filters}

类型: Bool

默认值: 1

允许在查询计划中合并过滤器。

## query_plan_optimize_prewhere {#query_plan_optimize_prewhere}

类型: Bool

默认值: 1

允许将过滤器推送到支持的存储的 PREWHERE 表达式中。

## query_plan_push_down_limit {#query_plan_push_down_limit}

类型: Bool

默认值: 1

切换查询计划级优化，向下移动执行计划中的 LIMIT。

仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅应供开发人员调试使用。该设置可能在未来以向后不兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_read_in_order {#query_plan_read_in_order}

类型: Bool

默认值: 1

切换按照顺序读取的查询计划级优化。

仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅应供开发人员调试使用。该设置可能在未来以向后不兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_remove_redundant_distinct {#query_plan_remove_redundant_distinct}

类型: Bool

默认值: 1

切换查询计划级优化，删除冗余的 DISTINCT 步骤。

仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅应供开发人员调试使用。该设置可能在未来以向后不兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_remove_redundant_sorting {#query_plan_remove_redundant_sorting}

类型: Bool

默认值: 1

切换查询计划级优化，删除冗余的排序步骤，例如在子查询中。

仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅应供开发人员调试使用。该设置可能在未来以向后不兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_reuse_storage_ordering_for_window_functions {#query_plan_reuse_storage_ordering_for_window_functions}

类型: Bool

默认值: 1

切换查询计划级优化，使用存储排序时用于窗口函数的排序。

仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅应供开发人员调试使用。该设置可能在未来以向后不兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用

## query_plan_split_filter {#query_plan_split_filter}

类型: Bool

默认值: 1

:::note
这是一个专家级设置，仅应供开发人员调试使用。该设置可能在未来以向后不兼容的方式更改或被移除。
:::

切换查询计划级优化，将过滤器分割为表达式。

仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

可能的值：

- 0 - 禁用
- 1 - 启用
```

## query_plan_try_use_vector_search {#query_plan_try_use_vector_search}

类型: Bool

默认值: 1

切换一个查询计划级别的优化，尝试使用向量相似性索引。
仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 的值为 1 时生效。

:::note
这是一个专家级设置，仅应由开发人员用于调试。该设置可能在未来以不向后兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用
## query_plan_use_new_logical_join_step {#query_plan_use_new_logical_join_step}

类型: Bool

默认值: 1

在查询计划中使用新的逻辑连接步骤
## query_profiler_cpu_time_period_ns {#query_profiler_cpu_time_period_ns}

类型: UInt64

默认值: 1000000000

设置 [查询分析器](../../operations/optimizing-performance/sampling-query-profiler.md) 的 CPU 时钟定时器周期。该定时器只计算 CPU 时间。

可能的值：

- 正整数的纳秒数。

    推荐值：

            - 10000000（每秒 100 次）纳秒及以上用于单个查询。
            - 1000000000（每秒一次）用于集群级分析。

- 0 表示关闭该定时器。

**在 ClickHouse Cloud 中暂时禁用。**

另请参见：

- 系统表 [trace_log](/operations/system-tables/trace_log)
## query_profiler_real_time_period_ns {#query_profiler_real_time_period_ns}

类型: UInt64

默认值: 1000000000

设置 [查询分析器](../../operations/optimizing-performance/sampling-query-profiler.md) 的实时时钟定时器周期。实时时钟定时器计算墙钟时间。

可能的值：

- 正整数，单位为纳秒。

    推荐值：

            - 10000000（每秒 100 次）纳秒及以下用于单个查询。
            - 1000000000（每秒一次）用于集群级分析。

- 0 表示关闭该定时器。

**在 ClickHouse Cloud 中暂时禁用。**

另请参见：

- 系统表 [trace_log](/operations/system-tables/trace_log)
## queue_max_wait_ms {#queue_max_wait_ms}

类型: 毫秒

默认值: 0

请求队列中的等待时间，如果并发请求的数量超过最大值。
## rabbitmq_max_wait_ms {#rabbitmq_max_wait_ms}

类型: 毫秒

默认值: 5000

在重试之前，从 RabbitMQ 读取的等待时间。
## read_backoff_max_throughput {#read_backoff_max_throughput}

类型: UInt64

默认值: 1048576

在读取缓慢的情况下减少线程数的设置。当读取带宽低于每秒多少字节时计算事件。
## read_backoff_min_concurrency {#read_backoff_min_concurrency}

类型: UInt64

默认值: 1

在读取缓慢的情况下尽量保持最小线程数的设置。
## read_backoff_min_events {#read_backoff_min_events}

类型: UInt64

默认值: 2

在读取缓慢的情况下减少线程数的设置。线程数减少的事件数量。
## read_backoff_min_interval_between_events_ms {#read_backoff_min_interval_between_events_ms}

类型: 毫秒

默认值: 1000

在读取缓慢的情况下减少线程数的设置。如果前一个事件经过的时间少于一定量，则不关注该事件。
## read_backoff_min_latency_ms {#read_backoff_min_latency_ms}

类型: 毫秒

默认值: 1000

在读取缓慢的情况下减少线程数的设置。仅关注耗时至少如此的读取。
## read_from_filesystem_cache_if_exists_otherwise_bypass_cache {#read_from_filesystem_cache_if_exists_otherwise_bypass_cache}

类型: Bool

默认值: 0

允许以被动模式使用文件系统缓存 - 利用现有的缓存条目，但不向缓存中添加更多条目。如果将此设置用于重查询并将其在实时短查询中禁用，这将避免因为重查询而造成的缓存冲突，从而提高整体系统效率。
## read_from_page_cache_if_exists_otherwise_bypass_cache {#read_from_page_cache_if_exists_otherwise_bypass_cache}

类型: Bool

默认值: 0

以被动模式使用用户空间页面缓存，类似于 `read_from_filesystem_cache_if_exists_otherwise_bypass_cache`。
## read_in_order_two_level_merge_threshold {#read_in_order_two_level_merge_threshold}

类型: UInt64

默认值: 100

在按主键顺序读取时运行初步合并步骤所需读取的最小分片数量。
## read_in_order_use_buffering {#read_in_order_use_buffering}

类型: Bool

默认值: 1

在按主键顺序读取时合并前使用缓冲。这增加了查询执行的并行性。
## read_in_order_use_virtual_row {#read_in_order_use_virtual_row}

类型: Bool

默认值: 0

在按主键顺序读取时使用虚拟行或其单调函数。它在多个分片中查找时很有用，因为只涉及相关的分片。
## read_overflow_mode {#read_overflow_mode}

类型: OverflowMode

默认值: throw

超过限制时应采取的措施。
## read_overflow_mode_leaf {#read_overflow_mode_leaf}

类型: OverflowMode

默认值: throw

超过叶子限制时应采取的措施。
## read_priority {#read_priority}

类型: Int64

默认值: 0

从本地文件系统或远程文件系统读取数据的优先级。仅支持本地文件系统的 `pread_threadpool` 方法以及远程文件系统的 `threadpool` 方法。
## read_through_distributed_cache {#read_through_distributed_cache}

<CloudAvailableBadge/>

类型: Bool

默认值: 0

仅在 ClickHouse Cloud 中生效。允许从分布式缓存中读取。
## readonly {#readonly}

类型: UInt64

默认值: 0

0 - 无只读限制。1 - 仅允许读取请求，以及更改显式允许的设置。2 - 仅允许读取请求，以及更改设置，除了 'readonly' 设置之外。
## receive_data_timeout_ms {#receive_data_timeout_ms}

类型: 毫秒

默认值: 2000

接收第一个数据包的连接超时，或者从副本接收的正进展数据包。
## receive_timeout {#receive_timeout}

类型: 秒

默认值: 300

从网络接收数据的超时（以秒为单位）。如果在此时间段内没有接收到字节，则抛出异常。如果在客户端上设置此设置，套接字的 'send_timeout' 也将在服务器的相应连接端设置。
## regexp_max_matches_per_row {#regexp_max_matches_per_row}

类型: UInt64

默认值: 1000

设置每行单个正则表达式的最大匹配次数。使用它来防止在使用贪婪的正则表达式时出现内存超载，特别是在 [extractAllGroupsHorizontal](/sql-reference/functions/string-search-functions#extractallgroupshorizontal) 函数中。

可能的值：

- 正整数。
## reject_expensive_hyperscan_regexps {#reject_expensive_hyperscan_regexps}

类型: Bool

默认值: 1

拒绝与 hyperscan 评估时可能会昂贵的模式（由于 NFA 状态爆炸）。
## remerge_sort_lowered_memory_bytes_ratio {#remerge_sort_lowered_memory_bytes_ratio}

类型: Float

默认值: 2

如果重新合并后内存使用没有减少到此比例，将禁用重新合并。
## remote_filesystem_read_method {#remote_filesystem_read_method}

类型: 字符串

默认值: threadpool

从远程文件系统读取数据的方法之一：read，threadpool。
## remote_filesystem_read_prefetch {#remote_filesystem_read_prefetch}

类型: Bool

默认值: 1

在从远程文件系统读取数据时是否应使用预取。
## remote_fs_read_backoff_max_tries {#remote_fs_read_backoff_max_tries}

类型: UInt64

默认值: 5

最大尝试读取次数，带有退避机制。
## remote_fs_read_max_backoff_ms {#remote_fs_read_max_backoff_ms}

类型: UInt64

默认值: 10000

尝试从远程磁盘读取数据时的最大等待时间。
## remote_read_min_bytes_for_seek {#remote_read_min_bytes_for_seek}

类型: UInt64

默认值: 4194304

远程读取（url，s3）进行查找所需的最小字节数，而不是忽略并读取。
## rename_files_after_processing {#rename_files_after_processing}

类型: 字符串

默认值: 

- **类型:** 字符串

- **默认值:** 空字符串

此设置允许为 `file` 表函数处理的文件指定重命名模式。当选项被设置时，所有由 `file` 表函数读取的文件将在处理成功的情况下根据指定的模式和占位符重命名。
### 占位符

- `%a` — 完整的原始文件名（例如，"sample.csv"）。
- `%f` — 不带扩展名的原始文件名（例如，"sample"）。
- `%e` — 带点的原始文件扩展名（例如，".csv"）。
- `%t` — 时间戳（以微秒为单位）。
- `%%` — 百分号 ("%")。
### 例子
- 选项: `--rename_files_after_processing="processed_%f_%t%e"`

- 查询: `SELECT * FROM file('sample.csv')`

如果读取 `sample.csv` 成功，文件将重命名为 `processed_sample_1683473210851438.csv`
## replace_running_query {#replace_running_query}

类型: Bool

默认值: 0

使用 HTTP 接口时，可以传递 'query_id' 参数。这是作为查询标识符的任意字符串。
如果此时同一用户的同一 'query_id' 查询已经存在，则行为取决于 'replace_running_query' 参数。

`0`（默认值）– 抛出异常（如果已经有相同 'query_id' 的查询在运行，则不允许运行新查询）。

`1` – 取消旧查询并开始运行新查询。

将此参数设置为 1 以实现分段条件的建议。在输入下一个字符后，如果旧查询尚未完成，则应取消该查询。
## replace_running_query_max_wait_ms {#replace_running_query_max_wait_ms}

类型: 毫秒

默认值: 5000

当 [replace_running_query](#replace_running_query) 设置处于激活状态时，运行与同一 `query_id` 的查询以完成的等待时间。

可能的值：

- 正整数。
- 0 — 抛出异常，不允许运行新的查询（如果服务器已经在执行与同一 `query_id` 的查询）。
## replication_wait_for_inactive_replica_timeout {#replication_wait_for_inactive_replica_timeout}

类型: Int64

默认值: 120

指定等待不活动副本执行的 [ALTER](../../sql-reference/statements/alter/index.md)、[OPTIMIZE](../../sql-reference/statements/optimize.md) 或 [TRUNCATE](../../sql-reference/statements/truncate.md) 查询的时间（以秒为单位）。

可能的值：

- 0 — 不等待。
- 负整数 — 无限期等待。
- 正整数 — 等待的秒数。
## restore_replace_external_dictionary_source_to_null {#restore_replace_external_dictionary_source_to_null}

类型: Bool

默认值: 0

在恢复时将外部字典源替换为 Null。用于测试目的。
## restore_replace_external_engines_to_null {#restore_replace_external_engines_to_null}

类型: Bool

默认值: 0

用于测试目的。将所有外部引擎替换为 Null，以不启动外部连接。
## restore_replace_external_table_functions_to_null {#restore_replace_external_table_functions_to_null}

类型: Bool

默认值: 0

用于测试目的。将所有外部表函数替换为 Null，以不启动外部连接。
## restore_replicated_merge_tree_to_shared_merge_tree {#restore_replicated_merge_tree_to_shared_merge_tree}

类型: Bool

默认值: 0

在恢复期间将表引擎从 Replicated*MergeTree 更改为 Shared*MergeTree。
## result_overflow_mode {#result_overflow_mode}

类型: OverflowMode

默认值: throw

超过限制时应采取的措施。
## rewrite_count_distinct_if_with_count_distinct_implementation {#rewrite_count_distinct_if_with_count_distinct_implementation}

类型: Bool

默认值: 0

允许将 `countDistcintIf` 重写为 [count_distinct_implementation](#count_distinct_implementation) 设置。

可能的值：

- true — 允许。
- false — 不允许。
## s3_allow_multipart_copy {#s3_allow_multipart_copy}

类型: Bool

默认值: 1

允许在 S3 中进行分块复制。
## s3_allow_parallel_part_upload {#s3_allow_parallel_part_upload}

类型: Bool

默认值: 1

使用多个线程进行 S3 分块上传。这可能导致稍微更高的内存使用。
## s3_check_objects_after_upload {#s3_check_objects_after_upload}

类型: Bool

默认值: 0

检查每个上传对象与 S3 的头请求，以确保上传成功。
## s3_connect_timeout_ms {#s3_connect_timeout_ms}

类型: UInt64

默认值: 1000

s3 磁盘的主机连接超时。
## s3_create_new_file_on_insert {#s3_create_new_file_on_insert}

类型: Bool

默认值: 0

启用或禁用在每次插入S3引擎表中创建新文件。如果启用，每次插入都会创建一个新的 S3 对象，键类似于以下模式：

初始: `data.Parquet.gz` -> `data.1.Parquet.gz` -> `data.2.Parquet.gz`，依此类推。

可能的值：
- 0 — `INSERT` 查询将新数据附加到文件末尾。
- 1 — `INSERT` 查询将创建一个新文件。
## s3_disable_checksum {#s3_disable_checksum}

类型: Bool

默认值: 0

向 S3 发送文件时不计算校验和。这通过避免对文件进行过多的处理来加快写入。由于 MergeTree 表的数据由 ClickHouse 进行校验和，因此在使用 HTTPS 访问 S3 时，TLS 层在通过网络传输时已提供完整性。因此，在 S3 上进行额外的校验和提供了深度防御。
## s3_ignore_file_doesnt_exist {#s3_ignore_file_doesnt_exist}

类型: Bool

默认值: 0

在读取某些键时，如果不存在文件则忽略文件的缺失。

可能的值：
- 1 — `SELECT` 函数返回空结果。
- 0 — `SELECT` 函数抛出异常。
## s3_list_object_keys_size {#s3_list_object_keys_size}

类型: UInt64

默认值: 1000

ListObject 请求中可以批量返回的最大文件数量。
## s3_max_connections {#s3_max_connections}

类型: UInt64

默认值: 1024

每个服务器的最大连接数。
## s3_max_get_burst {#s3_max_get_burst}

类型: UInt64

默认值: 0

在达到每秒请求限制之前可以同时发出的最大请求数。默认值 (0) 等于 `s3_max_get_rps`。
## s3_max_get_rps {#s3_max_get_rps}

类型: UInt64

默认值: 0

达到节流的 S3 GET 请求每秒限制。零表示无限制。
## s3_max_inflight_parts_for_one_file {#s3_max_inflight_parts_for_one_file}

类型: UInt64

默认值: 20

在多部分上传请求中并发加载的部分的最大数量。0 表示无限制。
## s3_max_part_number {#s3_max_part_number}

类型: UInt64

默认值: 10000

S3 上传部分的最大部分编号。
## s3_max_put_burst {#s3_max_put_burst}

类型: UInt64

默认值: 0

在达到每秒请求限制之前可以同时发出的最大请求数。默认值 (0) 等于 `s3_max_put_rps`。
## s3_max_put_rps {#s3_max_put_rps}

类型: UInt64

默认值: 0

达到节流的 S3 PUT 请求每秒限制。零表示无限制。
## s3_max_redirects {#s3_max_redirects}

类型: UInt64

默认值: 10

允许的最大 S3 重定向跳数。
## s3_max_single_operation_copy_size {#s3_max_single_operation_copy_size}

类型: UInt64

默认值: 33554432

S3 中单次操作复制的最大大小。只有在 s3_allow_multipart_copy 为真时才使用此设置。
## s3_max_single_part_upload_size {#s3_max_single_part_upload_size}

类型: UInt64

默认值: 33554432

使用单部分上传到 S3 的对象的最大大小。
## s3_max_single_read_retries {#s3_max_single_read_retries}

类型: UInt64

默认值: 4

单次 S3 读取期间的最大重试次数。
## s3_max_unexpected_write_error_retries {#s3_max_unexpected_write_error_retries}

类型: UInt64

默认值: 4

在 S3 写入期间发生意外错误时的最大重试次数。
## s3_max_upload_part_size {#s3_max_upload_part_size}

类型: UInt64

默认值: 5368709120

在 S3 中进行多部分上传期间，要上传的部分的最大大小。
## s3_min_upload_part_size {#s3_min_upload_part_size}

类型: UInt64

默认值: 16777216

在 S3 中进行多部分上传期间，要上传的部分的最小大小。
## s3_request_timeout_ms {#s3_request_timeout_ms}

类型: UInt64

默认值: 30000

向 S3 发送和接收数据的空闲超时。如果单个 TCP 读取或写入调用阻塞这么长时间，则失败。
## s3_retry_attempts {#s3_retry_attempts}

类型: UInt64

默认值: 100

Aws::Client::RetryStrategy 的设置，Aws::Client 自行重试，0 表示不重试。
## s3_skip_empty_files {#s3_skip_empty_files}

类型: Bool

默认值: 1

启用或禁用在 [S3](../../engines/table-engines/integrations/s3.md) 引擎表中跳过空文件。

可能的值：
- 0 — `SELECT` 如果空文件不兼容请求格式则抛出异常。
- 1 — `SELECT` 对于空文件返回空结果。
## s3_strict_upload_part_size {#s3_strict_upload_part_size}

类型: UInt64

默认值: 0

在 S3 进行多部分上传期间上传部分的确切大小（某些实现不支持可变大小的部分）。
## s3_throw_on_zero_files_match {#s3_throw_on_zero_files_match}

类型: Bool

默认值: 0

当 ListObjects 请求无法匹配任何文件时抛出错误。
## s3_truncate_on_insert {#s3_truncate_on_insert}

类型: Bool

默认值: 0

启用或禁用在 S3 引擎表中进行插入前截断。如果禁用，如果 S3 对象已经存在，插入尝试将抛出异常。

可能的值：
- 0 — `INSERT` 查询将新数据附加到文件末尾。
- 1 — `INSERT` 查询用新数据替换文件的现有内容。
## s3_upload_part_size_multiply_factor {#s3_upload_part_size_multiply_factor}

类型: UInt64

默认值: 2

在从单个写入到 S3 上传 `s3_multiply_parts_count_threshold` 部分时，每次将 `s3_min_upload_part_size` 乘以此因子。
## s3_upload_part_size_multiply_parts_count_threshold {#s3_upload_part_size_multiply_parts_count_threshold}

类型: UInt64

默认值: 500

每次将此数量的部分上传到 S3 时，将 `s3_min_upload_part_size` 乘以 `s3_upload_part_size_multiply_factor`。
## s3_use_adaptive_timeouts {#s3_use_adaptive_timeouts}

类型: Bool

默认值: 1

设置为 `true` 时，所有 S3 请求的前两次尝试使用较低的发送和接收超时。
设置为 `false` 时，所有尝试的超时均相同。
## s3_validate_request_settings {#s3_validate_request_settings}

类型: Bool

默认值: 1

启用 S3 请求设置验证。

可能的值：
- 1 — 验证设置。
- 0 — 不验证设置。
## s3queue_default_zookeeper_path {#s3queue_default_zookeeper_path}

类型: 字符串

默认值: /clickhouse/s3queue/

S3Queue 引擎的默认 zookeeper 路径前缀。
## s3queue_enable_logging_to_s3queue_log {#s3queue_enable_logging_to_s3queue_log}

类型: Bool

默认值: 0

启用写入 system.s3queue_log。该值可以通过表设置单独重写。
## s3queue_migrate_old_metadata_to_buckets {#s3queue_migrate_old_metadata_to_buckets}

类型: Bool

默认值: 0

将 S3Queue 表的旧元数据结构迁移到新结构。
## schema_inference_cache_require_modification_time_for_url {#schema_inference_cache_require_modification_time_for_url}

类型: Bool

默认值: 1

对于具有 Last-Modified 头的 URL，使用缓存中的模式并进行最后修改时间验证。
## schema_inference_use_cache_for_azure {#schema_inference_use_cache_for_azure}

类型: Bool

默认值: 1

在使用 Azure 表函数时，在模式推断中使用缓存。
## schema_inference_use_cache_for_file {#schema_inference_use_cache_for_file}

类型: Bool

默认值: 1

在使用文件表函数时，在模式推断中使用缓存。
## schema_inference_use_cache_for_hdfs {#schema_inference_use_cache_for_hdfs}

类型: Bool

默认值: 1

在使用 HDFS 表函数时，在模式推断中使用缓存。
## schema_inference_use_cache_for_s3 {#schema_inference_use_cache_for_s3}

类型: Bool

默认值: 1

在使用 S3 表函数时，在模式推断中使用缓存。
## schema_inference_use_cache_for_url {#schema_inference_use_cache_for_url}

类型: Bool

默认值: 1

在使用 URL 表函数时，在模式推断中使用缓存。
## select_sequential_consistency {#select_sequential_consistency}

类型: UInt64

默认值: 0

:::note
此设置在 SharedMergeTree 和 ReplicatedMergeTree 之间的行为不同，请参阅 [SharedMergeTree 一致性](/cloud/reference/shared-merge-tree#consistency) 以获取有关 SharedMergeTree 中 `select_sequential_consistency` 行为的更多信息。
:::

启用或禁用 `SELECT` 查询的顺序一致性。需要禁用 `insert_quorum_parallel`（默认启用）。

可能的值：

- 0 — 禁用。
- 1 — 启用。

使用方式

当启用顺序一致性时，ClickHouse 允许客户端仅对包含所有先前执行的 `INSERT` 查询数据的副本执行 `SELECT` 查询。如果客户端引用了部分副本，ClickHouse 将生成异常。SELECT 查询将不包括尚未写入副本的 quorum 的数据。

当 `insert_quorum_parallel` 被启用（默认值），则 `select_sequential_consistency` 不起作用。这是因为并行的 `INSERT` 查询可以写入不同的 quorum 副本集合，因此无法保证单个副本会接收到所有写入。

另请参见：

- [insert_quorum](#insert_quorum)
- [insert_quorum_timeout](#insert_quorum_timeout)
- [insert_quorum_parallel](#insert_quorum_parallel)
## send_logs_level {#send_logs_level}

类型: LogsLevel

默认值: fatal

以指定的最低级别将服务器文本日志发送到客户端。有效值：'trace'，'debug'，'information'，'warning'，'error'，'fatal'，'none'
## send_logs_source_regexp {#send_logs_source_regexp}

类型: 字符串

默认值: 

带有指定正则表达式的服务器文本日志来匹配日志源名称。空表示所有源。
## send_progress_in_http_headers {#send_progress_in_http_headers}

类型: Bool

默认值: 0

启用或禁用 `clickhouse-server` 响应中的 `X-ClickHouse-Progress` HTTP 响应头。

有关更多信息，请阅读 [HTTP 接口描述](../../interfaces/http.md)。

可能的值：

- 0 — 禁用。
- 1 — 启用。
## send_timeout {#send_timeout}

类型: 秒

默认值: 300

向网络发送数据的超时（以秒为单位）。如果客户端需要发送某些数据但在此时间段内无法发送任何字节，则抛出异常。如果在客户端上设置此设置，则套接字的 'receive_timeout' 也将在服务器的相应连接端设置。
## session_timezone {#session_timezone}
<BetaBadge/>

类型: 时区

默认值: 

设置当前会话或查询的隐式时区。
隐式时区是应用于没有明确指定时区的 DateTime/DateTime64 类型值的时区。
该设置优先于全局配置的（服务器级别）隐式时区。
值 ''（空字符串）表示当前会话或查询的隐式时区等于 [服务器时区](../server-configuration-parameters/settings.md/#timezone)。

您可以使用 `timeZone()` 和 `serverTimeZone()` 函数获取会话时区和服务器时区。

可能的值：

- 任何来自 `system.time_zones` 的时区名称，例如 `Europe/Berlin`、`UTC` 或 `Zulu`

示例：

```sql
SELECT timeZone(), serverTimeZone() FORMAT CSV

"Europe/Berlin","Europe/Berlin"
```

```sql
SELECT timeZone(), serverTimeZone() SETTINGS session_timezone = 'Asia/Novosibirsk' FORMAT CSV

"Asia/Novosibirsk","Europe/Berlin"
```

将会话时区 'America/Denver' 分配给没有明确指定时区的内部 DateTime：

```sql
SELECT toDateTime64(toDateTime64('1999-12-12 23:23:23.123', 3), 3, 'Europe/Zurich') SETTINGS session_timezone = 'America/Denver' FORMAT TSV

1999-12-13 07:23:23.123
```

:::warning
并非所有解析 DateTime/DateTime64 的函数都遵循 `session_timezone`。这可能导致微妙的错误。
请参阅以下示例和解释。
:::

```sql
CREATE TABLE test_tz (`d` DateTime('UTC')) ENGINE = Memory AS SELECT toDateTime('2000-01-01 00:00:00', 'UTC');

SELECT *, timeZone() FROM test_tz WHERE d = toDateTime('2000-01-01 00:00:00') SETTINGS session_timezone = 'Asia/Novosibirsk'
0 rows in set.

SELECT *, timeZone() FROM test_tz WHERE d = '2000-01-01 00:00:00' SETTINGS session_timezone = 'Asia/Novosibirsk'
┌───────────────────d─┬─timeZone()───────┐
│ 2000-01-01 00:00:00 │ Asia/Novosibirsk │
└─────────────────────┴──────────────────┘
```

这发生是因为不同的解析管道：

- 第一个 `SELECT` 查询中未明确给定时区的 `toDateTime()` 函数遵循了 `session_timezone` 和全局时区设置。
- 在第二个查询中，从字符串解析的 DateTime 继承了已存在列 `d` 的类型和时区。因此，`session_timezone` 和全局时区设置未被遵循。

**另请参见**

- [timezone](../server-configuration-parameters/settings.md/#timezone)
## set_overflow_mode {#set_overflow_mode}

类型: OverflowMode

默认值: throw

超过限制时应采取的措施。
## shared_merge_tree_sync_parts_on_partition_operations {#shared_merge_tree_sync_parts_on_partition_operations}

类型: Bool

默认值: 1

在 SMT 表中执行 MOVE|REPLACE|ATTACH 分区操作后自动同步数据部分集。仅限云端。
## short_circuit_function_evaluation {#short_circuit_function_evaluation}

类型: ShortCircuitFunctionEvaluation

默认值: enable

允许根据 [短路方案](https://en.wikipedia.org/wiki/Short-circuit_evaluation) 计算 [if](../../sql-reference/functions/conditional-functions.md/#if)、[multiIf](../../sql-reference/functions/conditional-functions.md/#multiif)、[and](/sql-reference/functions/logical-functions#and) 和 [or](/sql-reference/functions/logical-functions#or) 函数。这有助于优化这些函数中复杂表达式的执行并防止可能的异常（例如，预期外的零除法）。

可能的值：

- `enable` — 启用适用于此的短路函数评估（可能抛出异常或计算密集型）。
- `force_enable` — 为所有函数启用短路函数评估。
- `disable` — 禁用短路函数评估。
## short_circuit_function_evaluation_for_nulls {#short_circuit_function_evaluation_for_nulls}

类型: Bool

默认值: 1

优化评估返回 NULL 的函数，当任何参数为 NULL 时。当函数参数中 NULL 值的百分比超过 `short_circuit_function_evaluation_for_nulls_threshold` 时，系统跳过逐行评估该函数。相反，它立即为所有行返回 NULL，避免不必要的计算。
## short_circuit_function_evaluation_for_nulls_threshold {#short_circuit_function_evaluation_for_nulls_threshold}

类型: Double

默认值: 1

在仅对所有参数均为非 NULL 的行执行具有 Nullable 参数的函数的行的 NULL 值的比例阈值。当包含 NULL 值的行的比例超过此阈值时，这些包含 NULL 值的行将不会被评估。
## show_table_uuid_in_table_create_query_if_not_nil {#show_table_uuid_in_table_create_query_if_not_nil}

类型: Bool

默认值: 0

设置 `SHOW TABLE` 查询显示。

可能的值：

- 0 — 查询将不显示表 UUID。
- 1 — 查询将显示表 UUID。
## single_join_prefer_left_table {#single_join_prefer_left_table}

类型: Bool

默认值: 1

对于单个 JOIN，在标识符模糊的情况下，优先考虑左表。
## skip_redundant_aliases_in_udf {#skip_redundant_aliases_in_udf}

类型: Bool

默认值: 0

在用户定义的函数中不使用（替代）冗余别名以简化其使用。

可能的值：

- 1 — 在 UDF 中跳过（替代）别名。
- 0 — 在 UDF 中不跳过（替代）别名。

**示例**

启用和禁用之间的区别：

查询：

```sql
SET skip_redundant_aliases_in_udf = 0;
CREATE FUNCTION IF NOT EXISTS test_03274 AS ( x ) -> ((x + 1 as y, y + 2));

EXPLAIN SYNTAX SELECT test_03274(4 + 2);
```

结果：

```text
SELECT ((4 + 2) + 1 AS y, y + 2)
```

查询：

```sql
SET skip_redundant_aliases_in_udf = 1;
CREATE FUNCTION IF NOT EXISTS test_03274 AS ( x ) -> ((x + 1 as y, y + 2));

EXPLAIN SYNTAX SELECT test_03274(4 + 2);
```

结果：

```text
SELECT ((4 + 2) + 1, ((4 + 2) + 1) + 2)
```
```
```yaml
title: '跳过不可用分片'
sidebar_label: '跳过不可用分片'
keywords: ['跳过', '不可用', '分片']
description: '启用或禁用默默跳过不可用分片的功能。'
```

## skip_unavailable_shards {#skip_unavailable_shards}



类型: Bool

默认值: 0

启用或禁用默默跳过不可用分片。

如果所有副本不可用，则认为分片不可用。副本在以下情况下不可用：

- ClickHouse 出于任何原因无法连接到副本。

    连接到副本时，ClickHouse 会进行几次尝试。如果所有这些尝试都失败，则该副本被视为不可用。

- 副本无法通过 DNS 解析。

    如果副本的主机名无法通过 DNS 解析，这可能表示以下情况：

    - 副本的主机没有 DNS 记录。这在具有动态 DNS 的系统中可能会发生，例如 [Kubernetes](https://kubernetes.io)，其中节点在停机期间可能无法解析，这并不算一个错误。

    - 配置错误。ClickHouse 配置文件包含错误的主机名。

可能的值：

- 1 — 启用跳过。

    如果分片不可用，ClickHouse 将基于部分数据返回结果，并且不会报告节点可用性问题。

- 0 — 禁用跳过。

    如果分片不可用，ClickHouse 将抛出异常。
## sleep_after_receiving_query_ms {#sleep_after_receiving_query_ms}



类型: 毫秒

默认值: 0

在 TCPHandler 中接收查询后休眠的时间。
## sleep_in_send_data_ms {#sleep_in_send_data_ms}



类型: 毫秒

默认值: 0

在 TCPHandler 中发送数据时的休眠时间。
## sleep_in_send_tables_status_ms {#sleep_in_send_tables_status_ms}



类型: 毫秒

默认值: 0

在 TCPHandler 中发送表状态响应时的休眠时间。
## sort_overflow_mode {#sort_overflow_mode}



类型: OverflowMode

默认值: throw

当超过限制时，该如何处理。
## split_intersecting_parts_ranges_into_layers_final {#split_intersecting_parts_ranges_into_layers_final}



类型: Bool

默认值: 1

在最终优化期间将相交的部分范围拆分成层。
## split_parts_ranges_into_intersecting_and_non_intersecting_final {#split_parts_ranges_into_intersecting_and_non_intersecting_final}



类型: Bool

默认值: 1

在最终优化期间将部分范围拆分为相交和不相交。
## splitby_max_substrings_includes_remaining_string {#splitby_max_substrings_includes_remaining_string}



类型: Bool

默认值: 0

控制当参数 `max_substrings` > 0 的函数 [splitBy*()](../../sql-reference/functions/splitting-merging-functions.md) 是否将在结果数组的最后一个元素中包含剩余字符串。

可能的值：

- `0` - 剩余字符串不会包含在结果数组的最后一个元素中。
- `1` - 剩余字符串将包含在结果数组的最后一个元素中。这是 Spark 的 [`split()`](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.split.html) 函数和 Python 的 ['string.split()'](https://docs.python.org/3/library/stdtypes.html#str.split) 方法的行为。
## stop_refreshable_materialized_views_on_startup {#stop_refreshable_materialized_views_on_startup}
<ExperimentalBadge/>


类型: Bool

默认值: 0

在服务器启动时，防止调度可刷新的物化视图，就好像使用了 SYSTEM STOP VIEWS。您可以在之后手动使用 `SYSTEM START VIEWS` 或 `SYSTEM START VIEW <name>` 启动它们。也适用于新创建的视图。对不可刷新的物化视图没有影响。
## storage_file_read_method {#storage_file_read_method}



类型: LocalFSReadMethod

默认值: pread

从存储文件读取数据的方法，选项之一：`read`，`pread`，`mmap`。mmap 方法不适用于 clickhouse-server（它用于 clickhouse-local）。
## storage_system_stack_trace_pipe_read_timeout_ms {#storage_system_stack_trace_pipe_read_timeout_ms}



类型: 毫秒

默认值: 100

从管道读取以接收来自线程的信息的最大时间，当查询 `system.stack_trace` 表时。此设置用于测试目的，不应由用户更改。
## stream_flush_interval_ms {#stream_flush_interval_ms}



类型: 毫秒

默认值: 7500

在超时的情况下或当一个线程生成 [max_insert_block_size](#max_insert_block_size) 行时适用于流式表。

默认值为 7500。

值越小，数据被冲洗到表中的频率越高。将值设置得过低会导致性能下降。
## stream_like_engine_allow_direct_select {#stream_like_engine_allow_direct_select}



类型: Bool

默认值: 0

允许对 Kafka、RabbitMQ、FileLog、Redis Streams 和 NATS 引擎进行直接 SELECT 查询。如果有附加的物化视图，即使启用此设置，也不允许进行 SELECT 查询。
## stream_like_engine_insert_queue {#stream_like_engine_insert_queue}



类型: String

默认值: 

当流式引擎从多个队列读取时，用户在写入时需要选择一个队列进行插入。被 Redis Streams 和 NATS 使用。
## stream_poll_timeout_ms {#stream_poll_timeout_ms}



类型: 毫秒

默认值: 500

从/到流存储轮询数据的超时。
## system_events_show_zero_values {#system_events_show_zero_values}



类型: Bool

默认值: 0

允许从 [`system.events`](../../operations/system-tables/events.md) 选择零值事件。

一些监控系统要求将每个检查点的所有指标值传递给它们，即使指标值为零。

可能的值：

- 0 — 禁用。
- 1 — 启用。

**示例**

查询

```sql
SELECT * FROM system.events WHERE event='QueryMemoryLimitExceeded';
```

结果

```text
Ok.
```

查询
```sql
SET system_events_show_zero_values = 1;
SELECT * FROM system.events WHERE event='QueryMemoryLimitExceeded';
```

结果

```text
┌─event────────────────────┬─value─┬─description───────────────────────────────────────────┐
│ QueryMemoryLimitExceeded │     0 │ 查询超出内存限制的次数。                             │
└──────────────────────────┴───────┴───────────────────────────────────────────────────────┘
```
## table_function_remote_max_addresses {#table_function_remote_max_addresses}



类型: UInt64

默认值: 1000

设置从模式生成的 [remote](../../sql-reference/table-functions/remote.md) 函数的最大地址数。

可能的值：

- 正整数。
## tcp_keep_alive_timeout {#tcp_keep_alive_timeout}



类型: 秒

默认值: 290

在 TCP 开始发送保持活动探测之前，连接需要保持空闲的时间（秒）。
## temporary_data_in_cache_reserve_space_wait_lock_timeout_milliseconds {#temporary_data_in_cache_reserve_space_wait_lock_timeout_milliseconds}



类型: UInt64

默认值: 600000

锁定缓存以进行临时数据在文件系统缓存中的空间保留的等待时间。
## temporary_files_codec {#temporary_files_codec}



类型: String

默认值: LZ4

设置在磁盘上用于排序和连接操作的临时文件的压缩编解码器。

可能的值：

- LZ4 — 应用 [LZ4](https://en.wikipedia.org/wiki/LZ4_(compression_algorithm)) 压缩。
- NONE — 不应用压缩。
## throw_if_deduplication_in_dependent_materialized_views_enabled_with_async_insert {#throw_if_deduplication_in_dependent_materialized_views_enabled_with_async_insert}



类型: Bool

默认值: 1

当设置 `deduplicate_blocks_in_dependent_materialized_views` 与 `async_insert` 一起启用时，对 INSERT 查询抛出异常。它确保了正确性，因为这些功能不能一起工作。
## throw_if_no_data_to_insert {#throw_if_no_data_to_insert}



类型: Bool

默认值: 1

允许或禁止空 INSERT，默认启用（对空插入抛出错误）。仅适用于使用 [`clickhouse-client`](/interfaces/cli) 或使用 [gRPC 接口](/interfaces/grpc) 的 INSERT。
## throw_on_error_from_cache_on_write_operations {#throw_on_error_from_cache_on_write_operations}



类型: Bool

默认值: 0

在写操作（INSERT，合并）时忽略来自缓存的错误。
## throw_on_max_partitions_per_insert_block {#throw_on_max_partitions_per_insert_block}



类型: Bool

默认值: 1

与 max_partitions_per_insert_block 一起使用。如果为 true（默认），则在达到 max_partitions_per_insert_block 时将抛出异常。如果为 false，将记录达到此限制的插入查询的详细信息以及分区数量。如果您试图了解更改 max_partitions_per_insert_block 对用户的影响，这可能很有用。
## throw_on_unsupported_query_inside_transaction {#throw_on_unsupported_query_inside_transaction}
<ExperimentalBadge/>


类型: Bool

默认值: 1

如果在事务中使用不支持的查询则抛出异常。
## timeout_before_checking_execution_speed {#timeout_before_checking_execution_speed}



类型: 秒

默认值: 10

在经过指定时间后检查速度是否过低。
## timeout_overflow_mode {#timeout_overflow_mode}



类型: OverflowMode

默认值: throw

当超出限制时，该如何处理。
## timeout_overflow_mode_leaf {#timeout_overflow_mode_leaf}



类型: OverflowMode

默认值: throw

当叶限超出限制时，该如何处理。
## totals_auto_threshold {#totals_auto_threshold}



类型: Float

默认值: 0.5

`totals_mode = 'auto'` 的阈值。
请参见 "WITH TOTALS 修饰符" 部分。
## totals_mode {#totals_mode}



类型: TotalsMode

默认值: after_having_exclusive

当 HAVING 存在以及当 max_rows_to_group_by 和 group_by_overflow_mode = 'any' 存在时，计算 TOTALS 的方式。
请参见 "WITH TOTALS 修饰符" 部分。
## trace_profile_events {#trace_profile_events}



类型: Bool

默认值: 0

启用或禁用在每次更新配置文件事件时收集堆栈跟踪，以及配置文件事件的名称和递增的值，并将其发送到 [trace_log](/operations/system-tables/trace_log)。

可能的值：

- 1 — 启用配置文件事件跟踪。
- 0 — 禁用配置文件事件跟踪。
## transfer_overflow_mode {#transfer_overflow_mode}



类型: OverflowMode

默认值: throw

当超出限制时，该如何处理。
## transform_null_in {#transform_null_in}



类型: Bool

默认值: 0

启用 [NULL](/sql-reference/syntax#null) 值在 [IN](../../sql-reference/operators/in.md) 操作符中的相等性。

默认情况下，`NULL` 值无法进行比较，因为 `NULL` 意味着未定义的值。因此，比较 `expr = NULL` 必须始终返回 `false`。使用此设置时，`NULL = NULL` 对于 `IN` 操作符返回 `true`。

可能的值：

- 0 — 在 `IN` 操作符中比较 `NULL` 值返回 `false`。
- 1 — 在 `IN` 操作符中比较 `NULL` 值返回 `true`。

**示例**

考虑 `null_in` 表：

``` text
┌──idx─┬─────i─┐
│    1 │     1 │
│    2 │  NULL │
│    3 │     3 │
└──────┴───────┘
```

查询：

``` sql
SELECT idx, i FROM null_in WHERE i IN (1, NULL) SETTINGS transform_null_in = 0;
```

结果：

``` text
┌──idx─┬────i─┐
│    1 │    1 │
└──────┴──────┘
```

查询：

``` sql
SELECT idx, i FROM null_in WHERE i IN (1, NULL) SETTINGS transform_null_in = 1;
```

结果：

``` text
┌──idx─┬─────i─┐
│    1 │     1 │
│    2 │  NULL │
└──────┴───────┘
```

**另见**

- [IN 操作符中的 NULL 处理](/sql-reference/operators/in#null-processing)
## traverse_shadow_remote_data_paths {#traverse_shadow_remote_data_paths}



类型: Bool

默认值: 0

在查询 system.remote_data_paths 时，除了实际的表数据之外，还遍历冻结数据（影子目录）。
## union_default_mode {#union_default_mode}



类型: SetOperationMode

默认值: 

设置组合 `SELECT` 查询结果的模式。该设置仅在与 [UNION](../../sql-reference/statements/select/union.md) 共享且未显式指定 `UNION ALL` 或 `UNION DISTINCT` 时使用。

可能的值：

- `'DISTINCT'` — ClickHouse 输出合并查询结果时去除重复行。
- `'ALL'` — ClickHouse 输出合并查询结果时包括重复行。
- `''` — ClickHouse 在使用 `UNION` 时生成异常。

参见 [UNION](../../sql-reference/statements/select/union.md) 中的示例。
## unknown_packet_in_send_data {#unknown_packet_in_send_data}



类型: UInt64

默认值: 0

在第 N 个数据包中发送未知数据包，而不是数据。
## use_async_executor_for_materialized_views {#use_async_executor_for_materialized_views}



类型: Bool

默认值: 0

使用异步且可能是多线程执行的物化视图查询，可以加速在 INSERT 期间的视图处理，但也会消耗更多内存。
## use_cache_for_count_from_files {#use_cache_for_count_from_files}



类型: Bool

默认值: 1

启用在从表函数 `file`/`s3`/`url`/`hdfs`/`azureBlobStorage` 计数时缓存行数。

默认启用。
## use_client_time_zone {#use_client_time_zone}



类型: Bool

默认值: 0

使用客户端时区来解释 DateTime 字符串值，而不是采用服务器时区。
## use_compact_format_in_distributed_parts_names {#use_compact_format_in_distributed_parts_names}



类型: Bool

默认值: 1

对背景 INSERT 到使用 `Distributed` 引擎的表存储块使用紧凑格式。

可能的值：

- 0 — 使用 `user[:password]@host:port#default_database` 目录格式。
- 1 — 使用 `[shard{shard_index}[_replica{replica_index}]]` 目录格式。

:::note
- 如果 `use_compact_format_in_distributed_parts_names=0`，对集群定义的更改将不会应用于后台 INSERT。
- 如果 `use_compact_format_in_distributed_parts_names=1`，更改集群定义中节点的顺序将更改 `shard_index`/`replica_index`，因此要注意。
:::
## use_concurrency_control {#use_concurrency_control}



类型: Bool

默认值: 1

遵循服务器的并发控制（请参见 `concurrent_threads_soft_limit_num` 和 `concurrent_threads_soft_limit_ratio_to_cores` 全局服务器设置）。如果禁用，它允许使用更多的线程，即使服务器过载（不推荐正常使用，主要用于测试）。
## use_hedged_requests {#use_hedged_requests}



类型: Bool

默认值: 1

启用远程查询的边缘请求逻辑。它允许通过不同的副本建立多个连接以进行查询。
如果在 `hedged_connection_timeout` 内未建立与副本的现有连接，或在 `receive_data_timeout` 内未接收到数据，则启用新连接。查询使用发送非空进度包（或数据包的第一个连接，如果 `allow_changing_replica_until_first_data_packet`）；其他连接将被取消。支持使用 `max_parallel_replicas > 1` 的查询。

默认启用。

在 Cloud 中默认禁用。
## use_hive_partitioning {#use_hive_partitioning}



类型: Bool

默认值: 1

启用时，ClickHouse 将在文件类表引擎 [File](/sql-reference/table-functions/file#hive-style-partitioning)/[S3](/sql-reference/table-functions/s3#hive-style-partitioning)/[URL](/sql-reference/table-functions/url#hive-style-partitioning)/[HDFS](/sql-reference/table-functions/hdfs#hive-style-partitioning)/[AzureBlobStorage](/sql-reference/table-functions/azureBlobStorage#hive-style-partitioning) 的路径中检测 Hive 风格的分区 (`/name=value/`)，并允许在查询中将分区列作为虚拟列使用。这些虚拟列将与分区路径中的名称相同，但以 `_` 开头。
## use_iceberg_partition_pruning {#use_iceberg_partition_pruning}



类型: Bool

默认值: 0

对 Iceberg 表使用 Iceberg 分区修剪。
## use_index_for_in_with_subqueries {#use_index_for_in_with_subqueries}



类型: Bool

默认值: 1

尝试在 IN 操作符的右侧有子查询或表表达式时使用索引进行过滤。
## use_index_for_in_with_subqueries_max_values {#use_index_for_in_with_subqueries_max_values}



类型: UInt64

默认值: 0

IN 操作符右侧集合要使用表索引进行过滤的最大大小。这可以避免由于为大查询准备额外数据结构而导致的性能下降和更高的内存使用。零表示无限制。
## use_json_alias_for_old_object_type {#use_json_alias_for_old_object_type}



类型: Bool

默认值: 0

启用时，`JSON` 数据类型别名将用于创建旧的 [Object('json')](../../sql-reference/data-types/json.md) 类型，而不是新的 [JSON](../../sql-reference/data-types/newjson.md) 类型。
## use_local_cache_for_remote_storage {#use_local_cache_for_remote_storage}



类型: Bool

默认值: 1

对远程存储（例如 HDFS 或 S3）使用本地缓存，仅用于远程表引擎。
## use_page_cache_for_disks_without_file_cache {#use_page_cache_for_disks_without_file_cache}



类型: Bool

默认值: 0

对于没有启用文件系统缓存的远程磁盘使用用户空间页面缓存。
## use_page_cache_with_distributed_cache {#use_page_cache_with_distributed_cache}



类型: Bool

默认值: 0

当使用分布式缓存时使用用户空间页面缓存。
## use_query_cache {#use_query_cache}



类型: Bool

默认值: 0

如果启用，`SELECT` 查询可能会利用 [查询缓存](../query-cache.md)。参数 [enable_reads_from_query_cache](#enable_reads_from_query_cache) 和 [enable_writes_to_query_cache](#enable_writes_to_query_cache) 更详细地控制缓存的使用。

可能的值：

- 0 - 禁用
- 1 - 启用
## use_query_condition_cache {#use_query_condition_cache}



类型: Bool

默认值: 0

启用查询条件缓存。

可能的值：

- 0 - 禁用
- 1 - 启用
## use_skip_indexes {#use_skip_indexes}



类型: Bool

默认值: 1

在查询执行期间使用数据跳过索引。

可能的值：

- 0 — 禁用。
- 1 — 启用。
## use_skip_indexes_if_final {#use_skip_indexes_if_final}



类型: Bool

默认值: 0

控制在执行带有 FINAL 修饰符的查询时是否使用跳过索引。

默认情况下，此设置被禁用，因为跳过索引可能会排除包含最新数据的行（粒度），这可能导致不正确的结果。当启用时，即使有 FINAL 修饰符也应用跳过索引，这可能会提高性能，但存在遗漏最近更新的风险。

可能的值：

- 0 — 禁用。
- 1 — 启用。
## use_structure_from_insertion_table_in_table_functions {#use_structure_from_insertion_table_in_table_functions}



类型: UInt64

默认值: 2

使用插入表的结构，而不是从数据推断模式。可能的值：0 - 禁用，1 - 启用，2 - 自动选择。
## use_uncompressed_cache {#use_uncompressed_cache}



类型: Bool

默认值: 0

是否使用未压缩块的缓存。接受 0 或 1。默认值为 0（禁用）。
使用未压缩缓存（仅适用于 MergeTree 系列的表）可以显著降低延迟并提高在大量短查询时的吞吐量。对于频繁发送短请求的用户，请启用此设置。同时注意 [uncompressed_cache_size](/operations/server-configuration-parameters/settings#uncompressed_cache_size) 配置参数（仅在配置文件中设置） - 未压缩缓存块的大小。默认值为 8 GiB。未压缩缓存在需要时填充，最少使用的数据会自动删除。

对于读取至少一定量数据（百万行或以上）的查询，将自动禁用未压缩缓存，以为真正的小查询腾出空间。这意味着您可以始终将 'use_uncompressed_cache' 设置为 1。
## use_variant_as_common_type {#use_variant_as_common_type}



类型: Bool

默认值: 0

允许在没有公共类型的参数类型时，将 `Variant` 类型用作 [if](../../sql-reference/functions/conditional-functions.md/#if)/[multiIf](../../sql-reference/functions/conditional-functions.md/#multiif)/[array](../../sql-reference/functions/array-functions.md)/[map](../../sql-reference/functions/tuple-map-functions.md) 函数的结果类型。

示例：

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(if(number % 2, number, range(number))) as variant_type FROM numbers(1);
SELECT if(number % 2, number, range(number)) as variant FROM numbers(5);
```

```text
┌─variant_type───────────────────┐
│ Variant(Array(UInt64), UInt64) │
└────────────────────────────────┘
┌─variant───┐
│ []        │
│ 1         │
│ [0,1]     │
│ 3         │
│ [0,1,2,3] │
└───────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(multiIf((number % 4) = 0, 42, (number % 4) = 1, [1, 2, 3], (number % 4) = 2, 'Hello, World!', NULL)) AS variant_type FROM numbers(1);
SELECT multiIf((number % 4) = 0, 42, (number % 4) = 1, [1, 2, 3], (number % 4) = 2, 'Hello, World!', NULL) AS variant FROM numbers(4);
```

```text
─variant_type─────────────────────────┐
│ Variant(Array(UInt8), String, UInt8) │
└──────────────────────────────────────┘

┌─variant───────┐
│ 42            │
│ [1,2,3]       │
│ Hello, World! │
│ ᴺᵁᴸᴸ          │
└───────────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(array(range(number), number, 'str_' || toString(number))) as array_of_variants_type from numbers(1);
SELECT array(range(number), number, 'str_' || toString(number)) as array_of_variants FROM numbers(3);
```

```text
┌─array_of_variants_type────────────────────────┐
│ Array(Variant(Array(UInt64), String, UInt64)) │
└───────────────────────────────────────────────┘

┌─array_of_variants─┐
│ [[],0,'str_0']    │
│ [[0],1,'str_1']   │
│ [[0,1],2,'str_2'] │
└───────────────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(map('a', range(number), 'b', number, 'c', 'str_' || toString(number))) as map_of_variants_type from numbers(1);
SELECT map('a', range(number), 'b', number, 'c', 'str_' || toString(number)) as map_of_variants FROM numbers(3);
```

```text
┌─map_of_variants_type────────────────────────────────┐
│ Map(String, Variant(Array(UInt64), String, UInt64)) │
└─────────────────────────────────────────────────────┘

┌─map_of_variants───────────────┐
│ {'a':[],'b':0,'c':'str_0'}    │
│ {'a':[0],'b':1,'c':'str_1'}   │
│ {'a':[0,1],'b':2,'c':'str_2'} │
└───────────────────────────────┘
```
## use_with_fill_by_sorting_prefix {#use_with_fill_by_sorting_prefix}



类型: Bool

默认值: 1

ORDER BY 子句中将填充列排列在 WITH FILL 之前的列形成排序前缀。具有不同排序前缀值的行将独立地被填充。
## validate_enum_literals_in_operators {#validate_enum_literals_in_operators}



类型: Bool

默认值: 0

如果启用，验证操作符（如 `IN`，`NOT IN`，`==`，`!=` 中的枚举文字，并在文字不是有效的枚举值时抛出异常。
## validate_mutation_query {#validate_mutation_query}



类型: Bool

默认值: 1

在接受突变查询之前验证突变查询。突变在后台执行，运行无效查询将导致突变卡住，需要手动干预。

仅在遇到向后不兼容的错误时更改此设置。
## validate_polygons {#validate_polygons}



类型: Bool

默认值: 1

启用或禁用在 [pointInPolygon](/sql-reference/functions/geo/coordinates#pointinpolygon) 函数中抛出异常，如果多边形自相交或自切线。

可能的值：

- 0 — 禁用抛出异常。`pointInPolygon` 接受无效多边形并为其返回可能不正确的结果。
- 1 — 启用抛出异常。
## wait_changes_become_visible_after_commit_mode {#wait_changes_become_visible_after_commit_mode}
<ExperimentalBadge/>


类型: TransactionsWaitCSNMode

默认值: wait_unknown

等待提交的更改在最新快照中实际可见。
## wait_for_async_insert {#wait_for_async_insert}



类型: Bool

默认值: 1

如果为 true，等待异步插入处理。
## wait_for_async_insert_timeout {#wait_for_async_insert_timeout}



类型: 秒

默认值: 120

等待异步插入处理的超时时间。
## wait_for_window_view_fire_signal_timeout {#wait_for_window_view_fire_signal_timeout}
<ExperimentalBadge/>


类型: 秒

默认值: 10

等待事件时间处理中窗口视图触发信号的超时时间。
## window_view_clean_interval {#window_view_clean_interval}
<ExperimentalBadge/>


类型: 秒

默认值: 60

窗口视图清理的间隔时间（以秒为单位），以释放过时数据。
## window_view_heartbeat_interval {#window_view_heartbeat_interval}
<ExperimentalBadge/>


类型: 秒

默认值: 15

心跳间隔（以秒为单位），表示监视查询仍然处于活动状态。
## workload {#workload}



类型: String

默认值: default

可用于访问资源的工作负载名称。
## write_through_distributed_cache {#write_through_distributed_cache}


<CloudAvailableBadge/>

类型: Bool

默认值: 0

仅在 ClickHouse Cloud 中有效。允许写入到分布式缓存（写入到 s3 也将通过分布式缓存进行）。 
## zstd_window_log_max {#zstd_window_log_max}



类型: Int64

默认值: 0

允许选择 ZSTD 的最大窗口日志（不适用于 MergeTree 系列）。
