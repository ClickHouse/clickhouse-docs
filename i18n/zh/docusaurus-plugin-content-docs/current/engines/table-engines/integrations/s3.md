---
description: "此引擎用于与 Amazon S3 生态系统集成。类似 HDFS 引擎，但提供 S3 特有的功能。"
sidebar_label: "S3"
sidebar_position: 180
slug: /engines/table-engines/integrations/s3
title: "S3 表引擎"
doc_type: "reference"
---



# S3 表引擎

此引擎提供与 [Amazon S3](https://aws.amazon.com/s3/) 生态系统的集成能力。该引擎类似于 [HDFS](/engines/table-engines/integrations/hdfs) 引擎，但提供了 S3 特有的功能。



## 示例 {#example}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE=S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'gzip')
    SETTINGS input_format_with_names_use_header = 0;

INSERT INTO s3_engine_table VALUES ('one', 1), ('two', 2), ('three', 3);

SELECT * FROM s3_engine_table LIMIT 2;
```

```text
┌─name─┬─value─┐
│ one  │     1 │
│ two  │     2 │
└──────┴───────┘
```


## 创建表 {#creating-a-table}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE = S3(path [, NOSIGN | aws_access_key_id, aws_secret_access_key,] format, [compression], [partition_strategy], [partition_columns_in_data_file])
    [PARTITION BY expr]
    [SETTINGS ...]
```

### 引擎参数 {#parameters}

- `path` — 存储桶 URL 及文件路径。在只读模式下支持以下通配符：`*`、`**`、`?`、`{abc,def}` 和 `{N..M}`，其中 `N`、`M` 为数字,`'abc'`、`'def'` 为字符串。更多信息请参见[下文](#wildcards-in-path)。
- `NOSIGN` - 如果提供此关键字代替凭证,则所有请求将不会被签名。
- `format` — 文件的[格式](/sql-reference/formats#formats-overview)。
- `aws_access_key_id`、`aws_secret_access_key` - [AWS](https://aws.amazon.com/) 账户用户的长期凭证。您可以使用这些凭证对请求进行身份验证。此参数为可选项。如果未指定凭证,将从配置文件中读取。更多信息请参见[使用 S3 进行数据存储](../mergetree-family/mergetree.md#table_engine-mergetree-s3)。
- `compression` — 压缩类型。支持的值：`none`、`gzip/gz`、`brotli/br`、`xz/LZMA`、`zstd/zst`。此参数为可选项。默认情况下,将根据文件扩展名自动检测压缩类型。
- `partition_strategy` – 选项：`WILDCARD` 或 `HIVE`。`WILDCARD` 要求路径中包含 `{_partition_id}`,该占位符将被分区键替换。`HIVE` 不允许使用通配符,假定路径为表根目录,并生成 Hive 风格的分区目录,使用 Snowflake ID 作为文件名,文件格式作为扩展名。默认值为 `WILDCARD`。
- `partition_columns_in_data_file` - 仅与 `HIVE` 分区策略一起使用。告知 ClickHouse 是否期望分区列写入数据文件中。默认值为 `false`。
- `storage_class_name` - 选项：`STANDARD` 或 `INTELLIGENT_TIERING`,允许指定 [AWS S3 Intelligent Tiering](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/)。

### 数据缓存 {#data-cache}

`S3` 表引擎支持在本地磁盘上缓存数据。
有关文件系统缓存配置选项和使用方法,请参见此[章节](/operations/storing-data.md/#using-local-cache)。
缓存基于存储对象的路径和 ETag 进行,因此 ClickHouse 不会读取过期的缓存版本。

要启用缓存,请使用设置 `filesystem_cache_name = '<name>'` 和 `enable_filesystem_cache = 1`。

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
SETTINGS filesystem_cache_name = 'cache_for_s3', enable_filesystem_cache = 1;
```

在配置文件中定义缓存有两种方式。

1. 在 ClickHouse 配置文件中添加以下部分：

```xml
<clickhouse>
    <filesystem_caches>
        <cache_for_s3>
            <path>path to cache directory</path>
            <max_size>10Gi</max_size>
        </cache_for_s3>
    </filesystem_caches>
</clickhouse>
```

2. 重用 ClickHouse `storage_configuration` 部分的缓存配置（以及缓存存储）,[此处有详细说明](/operations/storing-data.md/#using-local-cache)

### PARTITION BY {#partition-by}

`PARTITION BY` — 可选项。在大多数情况下,您不需要分区键,即使需要,通常也不需要比按月更细粒度的分区键。分区不会加速查询（与 ORDER BY 表达式相反）。您不应使用过于细粒度的分区。不要按客户端标识符或名称对数据进行分区（而应将客户端标识符或名称作为 ORDER BY 表达式中的第一列）。

对于按月分区,请使用 `toYYYYMM(date_column)` 表达式,其中 `date_column` 是类型为 [Date](/sql-reference/data-types/date.md) 的日期列。此处的分区名称采用 `"YYYYMM"` 格式。

#### 分区策略 {#partition-strategy}

`WILDCARD`（默认）：将文件路径中的 `{_partition_id}` 通配符替换为实际的分区键。不支持读取。


`HIVE` 实现了 Hive 风格的读写分区。读取通过递归 glob 模式实现,等同于 `SELECT * FROM s3('table_root/**.parquet')`。
写入按以下格式生成文件:`<prefix>/<key1=val1/key2=val2...>/<snowflakeid>.<toLower(file_format)>`。

注意:使用 `HIVE` 分区策略时,`use_hive_partitioning` 设置无效。

`HIVE` 分区策略示例:

```sql
arthur :) CREATE TABLE t_03363_parquet (year UInt16, country String, counter UInt8)
ENGINE = S3(s3_conn, filename = 't_03363_parquet', format = Parquet, partition_strategy='hive')
PARTITION BY (year, country);

arthur :) INSERT INTO t_03363_parquet VALUES
    (2022, 'USA', 1),
    (2022, 'Canada', 2),
    (2023, 'USA', 3),
    (2023, 'Mexico', 4),
    (2024, 'France', 5),
    (2024, 'Germany', 6),
    (2024, 'Germany', 7),
    (1999, 'Brazil', 8),
    (2100, 'Japan', 9),
    (2024, 'CN', 10),
    (2025, '', 11);

arthur :) select _path, * from t_03363_parquet;

    ┌─_path──────────────────────────────────────────────────────────────────────┬─year─┬─country─┬─counter─┐
 1. │ test/t_03363_parquet/year=2100/country=Japan/7329604473272971264.parquet   │ 2100 │ Japan   │       9 │
 2. │ test/t_03363_parquet/year=2024/country=France/7329604473323302912.parquet  │ 2024 │ France  │       5 │
 3. │ test/t_03363_parquet/year=2022/country=Canada/7329604473314914304.parquet  │ 2022 │ Canada  │       2 │
 4. │ test/t_03363_parquet/year=1999/country=Brazil/7329604473289748480.parquet  │ 1999 │ Brazil  │       8 │
 5. │ test/t_03363_parquet/year=2023/country=Mexico/7329604473293942784.parquet  │ 2023 │ Mexico  │       4 │
 6. │ test/t_03363_parquet/year=2023/country=USA/7329604473319108608.parquet     │ 2023 │ USA     │       3 │
 7. │ test/t_03363_parquet/year=2025/country=/7329604473327497216.parquet        │ 2025 │         │      11 │
 8. │ test/t_03363_parquet/year=2024/country=CN/7329604473310720000.parquet      │ 2024 │ CN      │      10 │
 9. │ test/t_03363_parquet/year=2022/country=USA/7329604473298137088.parquet     │ 2022 │ USA     │       1 │
10. │ test/t_03363_parquet/year=2024/country=Germany/7329604473306525696.parquet │ 2024 │ Germany │       6 │
11. │ test/t_03363_parquet/year=2024/country=Germany/7329604473306525696.parquet │ 2024 │ Germany │       7 │
    └────────────────────────────────────────────────────────────────────────────┴──────┴─────────┴─────────┘
```

### 查询分区数据 {#querying-partitioned-data}

此示例使用 [docker compose 配方](https://github.com/ClickHouse/examples/tree/5fdc6ff72f4e5137e23ea075c88d3f44b0202490/docker-compose-recipes/recipes/ch-and-minio-S3),该配方集成了 ClickHouse 和 MinIO。您可以通过替换端点和身份验证值,使用 S3 重现相同的查询。

请注意,`ENGINE` 配置中的 S3 端点使用参数标记 `{_partition_id}` 作为 S3 对象(文件名)的一部分,SELECT 查询会针对这些生成的对象名称进行查询(例如 `test_3.csv`)。


:::note
如示例所示,目前尚不直接支持从已分区的 S3 表进行查询,但可以通过使用 S3 表函数查询各个分区来实现。

在 S3 中写入分区数据的主要用途是将数据传输到另一个 ClickHouse 系统(例如,从本地部署系统迁移到 ClickHouse Cloud)。由于 ClickHouse 数据集通常非常庞大,而网络可靠性有时并不完美,因此分批传输数据集是合理的做法,这也是分区写入的意义所在。
:::

#### 创建表 {#create-the-table}

```sql
CREATE TABLE p
(
    `column1` UInt32,
    `column2` UInt32,
    `column3` UInt32
)
ENGINE = S3(
-- highlight-next-line
           'http://minio:10000/clickhouse//test_{_partition_id}.csv',
           'minioadmin',
           'minioadminpassword',
           'CSV')
PARTITION BY column3
```

#### 插入数据 {#insert-data}

```sql
INSERT INTO p VALUES (1, 2, 3), (3, 2, 1), (78, 43, 45)
```

#### 从分区 3 查询 {#select-from-partition-3}

:::tip
此查询使用 s3 表函数
:::

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│  1 │  2 │  3 │
└────┴────┴────┘
```

#### 从分区 1 查询 {#select-from-partition-1}

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_1.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│  3 │  2 │  1 │
└────┴────┴────┘
```

#### 从分区 45 查询 {#select-from-partition-45}

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_45.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│ 78 │ 43 │ 45 │
└────┴────┴────┘
```

#### 限制 {#limitation}

您可能会自然地尝试执行 `Select * from p`,但如上所述,此查询将失败;请使用前述查询方式。

```sql
SELECT * FROM p
```

```response
Received exception from server (version 23.4.1):
Code: 48. DB::Exception: Received from localhost:9000. DB::Exception: Reading from a partitioned S3 storage is not implemented yet. (NOT_IMPLEMENTED)
```


## 插入数据 {#inserting-data}

注意:数据行只能插入到新文件中。不支持合并周期或文件拆分操作。文件写入后,后续的插入操作将失败。为避免此问题,可以使用 `s3_truncate_on_insert` 和 `s3_create_new_file_on_insert` 设置。更多详细信息请参见[此处](/integrations/s3#inserting-data)。


## 虚拟列 {#virtual-columns}

- `_path` — 文件路径。类型:`LowCardinality(String)`。
- `_file` — 文件名。类型:`LowCardinality(String)`。
- `_size` — 文件大小(以字节为单位)。类型:`Nullable(UInt64)`。如果大小未知,则值为 `NULL`。
- `_time` — 文件最后修改时间。类型:`Nullable(DateTime)`。如果时间未知,则值为 `NULL`。
- `_etag` — 文件的 ETag。类型:`LowCardinality(String)`。如果 ETag 未知,则值为 `NULL`。
- `_tags` — 文件标签。类型:`Map(String, String)`。如果不存在标签,则值为空映射 `{}'。

有关虚拟列的更多信息,请参阅[此处](../../../engines/table-engines/index.md#table_engines-virtual_columns)。


## 实现细节 {#implementation-details}

- 读取和写入可以并行执行
- 不支持以下功能:
  - `ALTER` 和 `SELECT...SAMPLE` 操作。
  - 索引。
  - [零拷贝](../../../operations/storing-data.md#zero-copy)复制技术上可行,但不受支持。

  :::note 零拷贝复制尚未准备好用于生产环境
  在 ClickHouse 22.8 及更高版本中,零拷贝复制默认处于禁用状态。不建议在生产环境中使用此功能。
  :::


## 路径中的通配符 {#wildcards-in-path}

`path` 参数可以使用类似 bash 的通配符指定多个文件。要处理的文件必须存在且匹配完整的路径模式。文件列表在执行 `SELECT` 时确定(而非 `CREATE` 时)。

- `*` — 匹配除 `/` 之外的任意数量字符,包括空字符串。
- `**` — 匹配任意数量字符,包括 `/`,也包括空字符串。
- `?` — 匹配任意单个字符。
- `{some_string,another_string,yet_another_one}` — 匹配 `'some_string'`、`'another_string'`、`'yet_another_one'` 中的任意一个字符串。
- `{N..M}` — 匹配从 N 到 M 范围内的任意数字,包括两端边界。N 和 M 可以带前导零,例如 `000..078`。

使用 `{}` 的构造方式类似于 [remote](../../../sql-reference/table-functions/remote.md) 表函数。

:::note
如果文件列表包含带前导零的数字范围,请对每位数字分别使用大括号构造,或使用 `?`。
:::

**通配符示例 1**

创建包含名为 `file-000.csv`、`file-001.csv`、...、`file-999.csv` 文件的表:

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/my_folder/file-{000..999}.csv', 'CSV');
```

**通配符示例 2**

假设我们在 S3 上有多个 CSV 格式文件,URI 如下:

- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_3.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_3.csv'

有几种方法可以创建包含全部六个文件的表:

1. 指定文件后缀范围:

```sql
CREATE TABLE table_with_range (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_{1..3}', 'CSV');
```

2. 获取所有带 `some_file_` 前缀的文件(两个文件夹中不应存在其他带此前缀的文件):

```sql
CREATE TABLE table_with_question_mark (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_?', 'CSV');
```

3. 获取两个文件夹中的所有文件(所有文件都应符合查询中描述的格式和架构):

```sql
CREATE TABLE table_with_asterisk (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/*', 'CSV');
```


## 存储设置 {#storage-settings}

- [s3_truncate_on_insert](/operations/settings/settings.md#s3_truncate_on_insert) - 允许在插入数据前截断文件。默认禁用。
- [s3_create_new_file_on_insert](/operations/settings/settings.md#s3_create_new_file_on_insert) - 当格式包含后缀时,允许每次插入时创建新文件。默认禁用。
- [s3_skip_empty_files](/operations/settings/settings.md#s3_skip_empty_files) - 允许在读取时跳过空文件。默认启用。


## S3 相关设置 {#settings}

以下设置可以在查询执行前设置,或放置在配置文件中。

- `s3_max_single_part_upload_size` — 使用单部分上传方式上传到 S3 的对象的最大大小。默认值为 `32Mb`。
- `s3_min_upload_part_size` — 在多部分上传到 [S3 Multipart upload](https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html) 时上传分块的最小大小。默认值为 `16Mb`。
- `s3_max_redirects` — 允许的 S3 重定向跳转的最大次数。默认值为 `10`。
- `s3_single_read_retries` — 单次读取的最大重试次数。默认值为 `4`。
- `s3_max_put_rps` — 触发限流前每秒最大 PUT 请求速率。默认值为 `0`(无限制)。
- `s3_max_put_burst` — 在达到每秒请求限制之前可以同时发出的最大请求数。默认情况下(`0` 值)等于 `s3_max_put_rps`。
- `s3_max_get_rps` — 触发限流前每秒最大 GET 请求速率。默认值为 `0`(无限制)。
- `s3_max_get_burst` — 在达到每秒请求限制之前可以同时发出的最大请求数。默认情况下(`0` 值)等于 `s3_max_get_rps`。
- `s3_upload_part_size_multiply_factor` - 每当单次写入到 S3 上传了 `s3_multiply_parts_count_threshold` 个分块时,将 `s3_min_upload_part_size` 乘以此因子。默认值为 `2`。
- `s3_upload_part_size_multiply_parts_count_threshold` - 每当上传到 S3 的分块数量达到此值时,`s3_min_upload_part_size` 将乘以 `s3_upload_part_size_multiply_factor`。默认值为 `500`。
- `s3_max_inflight_parts_for_one_file` - 限制单个对象可以并发运行的 put 请求数量。该数量应该受到限制。值 `0` 表示无限制。默认值为 `20`。每个正在传输的分块对于前 `s3_upload_part_size_multiply_factor` 个分块具有大小为 `s3_min_upload_part_size` 的缓冲区,当文件足够大时会更大,请参阅 `upload_part_size_multiply_factor`。使用默认设置,对于小于 `8G` 的文件,单个上传文件消耗不超过 `320Mb`。对于更大的文件,消耗会更大。

安全注意事项:如果恶意用户可以指定任意 S3 URL,则必须将 `s3_max_redirects` 设置为零以避免 [SSRF](https://en.wikipedia.org/wiki/Server-side_request_forgery) 攻击;或者,必须在服务器配置中指定 `remote_host_filter`。


## 基于端点的设置 {#endpoint-settings}

以下设置可在配置文件中为指定端点配置(将通过 URL 的精确前缀进行匹配):

- `endpoint` — 指定端点的前缀。必填。
- `access_key_id` 和 `secret_access_key` — 指定用于该端点的访问凭证。可选。
- `use_environment_credentials` — 如果设置为 `true`,S3 客户端将尝试从环境变量和 [Amazon EC2](https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud) 元数据中获取该端点的凭证。可选,默认值为 `false`。
- `region` — 指定 S3 区域名称。可选。
- `use_insecure_imds_request` — 如果设置为 `true`,S3 客户端在从 Amazon EC2 元数据获取凭证时将使用不安全的 IMDS 请求。可选,默认值为 `false`。
- `expiration_window_seconds` — 检查基于过期时间的凭证是否已过期的宽限期(秒)。可选,默认值为 `120`。
- `no_sign_request` - 忽略所有凭证,使请求不进行签名。适用于访问公共存储桶。
- `header` — 向该端点的请求添加指定的 HTTP 标头。可选,可多次指定。
- `access_header` - 在没有其他来源提供凭证的情况下,向该端点的请求添加指定的 HTTP 标头。
- `server_side_encryption_customer_key_base64` — 如果指定,将设置访问使用 SSE-C 加密的 S3 对象所需的标头。可选。
- `server_side_encryption_kms_key_id` - 如果指定,将设置访问使用 [SSE-KMS 加密](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html)的 S3 对象所需的标头。如果指定空字符串,将使用 AWS 托管的 S3 密钥。可选。
- `server_side_encryption_kms_encryption_context` - 如果与 `server_side_encryption_kms_key_id` 一起指定,将设置 SSE-KMS 的加密上下文标头。可选。
- `server_side_encryption_kms_bucket_key_enabled` - 如果与 `server_side_encryption_kms_key_id` 一起指定,将设置用于启用 SSE-KMS 的 S3 存储桶密钥的标头。可选,可设置为 `true` 或 `false`,默认为空(与存储桶级别设置保持一致)。
- `max_single_read_retries` — 单次读取的最大重试次数。默认值为 `4`。可选。
- `max_put_rps`、`max_put_burst`、`max_get_rps` 和 `max_get_burst` - 用于特定端点的限流设置(参见上述说明),而非按查询限流。可选。

**示例:**

```xml
<s3>
    <endpoint-name>
        <endpoint>https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/</endpoint>
        <!-- <access_key_id>ACCESS_KEY_ID</access_key_id> -->
        <!-- <secret_access_key>SECRET_ACCESS_KEY</secret_access_key> -->
        <!-- <region>us-west-1</region> -->
        <!-- <use_environment_credentials>false</use_environment_credentials> -->
        <!-- <use_insecure_imds_request>false</use_insecure_imds_request> -->
        <!-- <expiration_window_seconds>120</expiration_window_seconds> -->
        <!-- <no_sign_request>false</no_sign_request> -->
        <!-- <header>Authorization: Bearer SOME-TOKEN</header> -->
        <!-- <server_side_encryption_customer_key_base64>BASE64-ENCODED-KEY</server_side_encryption_customer_key_base64> -->
        <!-- <server_side_encryption_kms_key_id>KMS_KEY_ID</server_side_encryption_kms_key_id> -->
        <!-- <server_side_encryption_kms_encryption_context>KMS_ENCRYPTION_CONTEXT</server_side_encryption_kms_encryption_context> -->
        <!-- <server_side_encryption_kms_bucket_key_enabled>true</server_side_encryption_kms_bucket_key_enabled> -->
        <!-- <max_single_read_retries>4</max_single_read_retries> -->
    </endpoint-name>
</s3>
```


## 处理归档文件 {#working-with-archives}

假设我们在 S3 上有以下 URI 的多个归档文件:

- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip'

可以使用 :: 从这些归档文件中提取数据。通配符既可以用于 URL 部分,也可以用于 :: 之后的部分(用于指定归档文件内部的文件名)。

```sql
SELECT *
FROM s3(
   'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-1{0..2}.csv.zip :: *.csv'
);
```

:::note
ClickHouse 支持三种归档格式:
ZIP
TAR
7Z
ZIP 和 TAR 归档文件可以从任何受支持的存储位置访问,而 7Z 归档文件只能从 ClickHouse 安装所在的本地文件系统读取。
:::


## 访问公共存储桶 {#accessing-public-buckets}

ClickHouse 会尝试从多种不同类型的来源获取凭证。
有时,在访问某些公共存储桶时可能会出现问题,导致客户端返回 `403` 错误代码。
可以通过使用 `NOSIGN` 关键字来避免此问题,强制客户端忽略所有凭证并且不对请求进行签名。

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv', NOSIGN, 'CSVWithNames');
```


## 优化性能 {#optimizing-performance}

有关优化 s3 函数性能的详细信息,请参阅[详细指南](/integrations/s3/performance)。


## 另请参阅 {#see-also}

- [s3 表函数](../../../sql-reference/table-functions/s3.md)
- [将 S3 与 ClickHouse 集成](/integrations/s3)
