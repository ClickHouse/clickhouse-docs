---
slug: /engines/table-engines/integrations/s3
sidebar_position: 180
sidebar_label: S3
title: 'S3 表引擎'
description: '该引擎提供与亚马逊 S3 生态系统的集成。类似于 HDFS 引擎，但提供 S3 特定的功能。'
---


# S3 表引擎

该引擎提供与 [Amazon S3](https://aws.amazon.com/s3/) 生态系统的集成。该引擎类似于 [HDFS](/engines/table-engines/integrations/hdfs) 引擎，但提供 S3 特定的功能。

## 示例 {#example}

``` sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE=S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'gzip')
    SETTINGS input_format_with_names_use_header = 0;

INSERT INTO s3_engine_table VALUES ('one', 1), ('two', 2), ('three', 3);

SELECT * FROM s3_engine_table LIMIT 2;
```

```text
┌─name─┬─value─┐
│ one  │     1 │
│ two  │     2 │
└──────┴───────┘
```

## 创建表 {#creating-a-table}

``` sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE = S3(path [, NOSIGN | aws_access_key_id, aws_secret_access_key,] format, [compression])
    [PARTITION BY expr]
    [SETTINGS ...]
```

### 引擎参数 {#parameters}

- `path` — 存储桶的 URL 和文件路径。支持只读模式下的以下通配符：`*`、`**`、`?`、`{abc,def}` 和 `{N..M}`，其中 `N`、`M` 为数字，`'abc'`、`'def'` 为字符串。更多信息见 [下面](#wildcards-in-path)。
- `NOSIGN` - 如果在凭据的位置提供了此关键字，则所有请求将不会被签名。
- `format` — 文件的 [格式](/sql-reference/formats#formats-overview)。
- `aws_access_key_id`、`aws_secret_access_key` - 用于 [AWS](https://aws.amazon.com/) 账户用户的长期凭据。您可以使用这些凭据来验证请求。该参数是可选的。如果未指定凭据，则将从配置文件中使用。更多信息见 [使用 S3 进行数据存储](../mergetree-family/mergetree.md#table_engine-mergetree-s3)。
- `compression` — 压缩类型。支持的值有：`none`、`gzip/gz`、`brotli/br`、`xz/LZMA`、`zstd/zst`。该参数是可选的。默认情况下，它将根据文件扩展名自动检测压缩。

### 数据缓存 {#data-cache}

`S3` 表引擎支持在本地磁盘上进行数据缓存。
有关文件系统缓存配置选项和用法，请参见此 [部分](/operations/storing-data.md/#using-local-cache)。
缓存是根据路径和存储对象的 ETag 进行的，因此 ClickHouse 不会读取过期的缓存版本。

要启用缓存，请使用设置 `filesystem_cache_name = '<name>'` 和 `enable_filesystem_cache = 1`。

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
SETTINGS filesystem_cache_name = 'cache_for_s3', enable_filesystem_cache = 1;
```

有两种方法可以在配置文件中定义缓存。

1. 向 ClickHouse 配置文件添加以下部分：

``` xml
<clickhouse>
    <filesystem_caches>
        <cache_for_s3>
            <path>缓存目录的路径</path>
            <max_size>10Gi</max_size>
        </cache_for_s3>
    </filesystem_caches>
</clickhouse>
```

2. 重用 ClickHouse `storage_configuration` 部分中的缓存配置（因此缓存存储），[在这里描述](/operations/storing-data.md/#using-local-cache)。

### PARTITION BY {#partition-by}

`PARTITION BY` — 可选。在大多数情况下，您不需要分区键，如果需要，通常不需要比按月更细化的分区键。分区不会加速查询（与 ORDER BY 表达式相反）。您永远不应使用过于细化的分区。不要根据客户标识符或名称对数据进行分区（相反，将客户标识符或名称作为 ORDER BY 表达式中的第一列）。

对于按月分区，使用 `toYYYYMM(date_column)` 表达式，其中 `date_column` 是日期类型 [Date](/sql-reference/data-types/date.md) 的列。此处分区名称具有 `"YYYYMM"` 格式。

### 查询分区数据 {#querying-partitioned-data}

此示例使用 [docker compose recipe](https://github.com/ClickHouse/examples/tree/5fdc6ff72f4e5137e23ea075c88d3f44b0202490/docker-compose-recipes/recipes/ch-and-minio-S3)，它集成了 ClickHouse 和 MinIO。您应该能够通过替换端点和身份验证值来重复相同的查询。

请注意，`ENGINE` 配置中的 S3 端点使用参数 `{_partition_id}` 作为 S3 对象（文件名）的一部分，并且 SELECT 查询针对这些结果对象名称进行选择（例如，`test_3.csv`）。

:::note
正如示例所示，目前不直接支持从分区 S3 表中查询，但可以通过使用 S3 表函数查询各个分区来实现。

将分区数据写入 S3 的主要用例是将这些数据传输到另一个 ClickHouse 系统（例如，从本地系统迁移到 ClickHouse Cloud）。由于 ClickHouse 数据集通常非常大，并且网络可靠性有时不够完美，因此逐个子集传输数据集是有意义的，因此需要分区写入。
:::

#### 创建表 {#create-the-table}
```sql
CREATE TABLE p
(
    `column1` UInt32,
    `column2` UInt32,
    `column3` UInt32
)
ENGINE = S3(

# highlight-next-line
           'http://minio:10000/clickhouse//test_{_partition_id}.csv',
           'minioadmin',
           'minioadminpassword',
           'CSV')
PARTITION BY column3
```

#### 插入数据 {#insert-data}
```sql
insert into p values (1, 2, 3), (3, 2, 1), (78, 43, 45)
```

#### 从分区 3 中选择 {#select-from-partition-3}

:::tip
此查询使用 s3 表函数
:::

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│  1 │  2 │  3 │
└────┴────┴────┘
```

#### 从分区 1 中选择 {#select-from-partition-1}
```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_1.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│  3 │  2 │  1 │
└────┴────┴────┘
```

#### 从分区 45 中选择 {#select-from-partition-45}
```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_45.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│ 78 │ 43 │ 45 │
└────┴────┴────┘
```

#### 限制 {#limitation}

您可能会尝试 `Select * from p`，但正如上面所述，此查询会失败；请使用前面的查询。

```sql
SELECT * FROM p
```
```response
从服务器接收到异常 (版本 23.4.1):
代码: 48. DB::Exception: 从 localhost:9000 接收。DB::Exception: 从分区化 S3 存储读取尚未实现。 (NOT_IMPLEMENTED)
```

## 虚拟列 {#virtual-columns}

- `_path` — 文件的路径。类型：`LowCardinality(String)`。
- `_file` — 文件名。类型：`LowCardinality(String)`。
- `_size` — 文件的字节大小。类型：`Nullable(UInt64)`。如果大小未知，值为 `NULL`。
- `_time` — 文件的最后修改时间。类型：`Nullable(DateTime)`。如果时间未知，值为 `NULL`。
- `_etag` — 文件的 ETag。类型：`LowCardinality(String)`。如果 etag 未知，值为 `NULL`。

有关虚拟列的更多信息，请参见 [这里](../../../engines/table-engines/index.md#table_engines-virtual_columns)。

## 实现细节 {#implementation-details}

- 读取和写入可以并行进行。
- 不支持：
    - `ALTER` 和 `SELECT...SAMPLE` 操作。
    - 索引。
    - [零拷贝](../../../operations/storing-data.md#zero-copy) 复制是可能的，但不受支持。

  :::note 零拷贝复制尚未准备好用于生产
  在 ClickHouse 版本 22.8 及更高版本中，默认情况下禁用零拷贝复制。此功能不推荐用于生产环境。
  :::

## 路径中的通配符 {#wildcards-in-path}

`path` 参数可以使用类 bash 通配符指定多个文件。要处理的文件必须存在并且与整个路径模式匹配。文件列表在 `SELECT` 时确定（而不是在 `CREATE` 时）。

- `*` — 替代任何数量的任何字符（不包括 `/`），包括空字符串。
- `**` — 替代任何数量的任何字符（包括 `/`），包括空字符串。
- `?` — 替代任何单个字符。
- `{some_string,another_string,yet_another_one}` — 替代字符串 `'some_string', 'another_string', 'yet_another_one'` 中的任意一个字符串。
- `{N..M}` — 替代从 N 到 M 的任何数字，包括两个边界。N 和 M 可以带有前导零，例如 `000..078`。

带有 `{}` 的构造与 [远程](../../../sql-reference/table-functions/remote.md) 表函数类似。

:::note
如果文件列表包含带有前导零的数字范围，请分别使用大括号构造每个数字，或使用 `?`。
:::

**通配符示例 1**

创建名为 `file-000.csv`、`file-001.csv` …、`file-999.csv` 的文件的表：

``` sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/my_folder/file-{000..999}.csv', 'CSV');
```

**通配符示例 2**

假设我们在 S3 上有几个 CSV 格式的文件，其 URI 如下：

- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_3.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_3.csv'


有几种方法可以创建包含所有六个文件的表：

1. 指定文件后缀的范围：

``` sql
CREATE TABLE table_with_range (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_{1..3}', 'CSV');
```

2. 获取所有以 `some_file_` 为前缀的文件（这两个文件夹中不应存在具有此前缀的其他文件）：

``` sql
CREATE TABLE table_with_question_mark (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_?', 'CSV');
```

3. 获取两个文件夹中的所有文件（所有文件应满足查询中描述的格式和模式）：

``` sql
CREATE TABLE table_with_asterisk (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/*', 'CSV');
```

## 存储设置 {#storage-settings}

- [s3_truncate_on_insert](/operations/settings/settings.md#s3_truncate_on_insert) - 允许在插入时截断文件。默认情况下禁用。
- [s3_create_new_file_on_insert](/operations/settings/settings.md#s3_create_new_file_on_insert) - 允许在每次插入时创建一个新文件（如果格式具有后缀）。默认情况下禁用。
- [s3_skip_empty_files](/operations/settings/settings.md#s3_skip_empty_files) - 允许在读取时跳过空文件。默认情况下启用。

## 与 S3 相关的设置 {#settings}

可以在查询执行之前设置以下设置，或将其放入配置文件中。

- `s3_max_single_part_upload_size` — 使用单部分上传到 S3 的最大对象大小。默认值为 `32Mb`。
- `s3_min_upload_part_size` — 在分块上传到 [S3 分块上传](https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html) 时，部分的最小大小。默认值为 `16Mb`。
- `s3_max_redirects` — 允许的最大 S3 重定向跳数。默认值为 `10`。
- `s3_single_read_retries` — 单次读取期间的最大尝试次数。默认值为 `4`。
- `s3_max_put_rps` — 限制前的最大 PUT 请求每秒率。默认值为 `0`（无限制）。
- `s3_max_put_burst` — 可以同时发出的最大请求数，超出请求每秒限制。默认值为 `0`（等于 `s3_max_put_rps`）。
- `s3_max_get_rps` — 限制前的最大 GET 请求每秒率。默认值为 `0`（无限制）。
- `s3_max_get_burst` — 可以同时发出的最大请求数，超出请求每秒限制。默认值为 `0`（等于 `s3_max_get_rps`）。
- `s3_upload_part_size_multiply_factor` - 每次从单次写入 S3 上传的 `s3_min_upload_part_size` 乘以此因子。默认值为 `2`。
- `s3_upload_part_size_multiply_parts_count_threshold` - 每次上传到 S3 的此部分数量时，`s3_min_upload_part_size` 乘以 `s3_upload_part_size_multiply_factor`。默认值为 `500`。
- `s3_max_inflight_parts_for_one_file` - 限制可以同时运行的对单个对象的 PUT 请求数量。此数字应受到限制。值为 `0` 表示无限制。默认值为 `20`。每个正在进行的部分都具有大小为 `s3_min_upload_part_size` 的缓冲区，对于前 `s3_upload_part_size_multiply_factor` 部分以及文件足够大的情况下更多，请参见 `upload_part_size_multiply_factor`。在默认设置下，上传的文件消耗不超过 `320Mb`，对于小于 `8G` 的文件而言。对于较大的文件，消耗则更高。

安全考虑：如果恶意用户可以指定任意 S3 URL，则必须将 `s3_max_redirects` 设置为零，以避免 [SSRF](https://en.wikipedia.org/wiki/Server-side_request_forgery) 攻击；或者，另外，可以在服务器配置中指定 `remote_host_filter`。

## 基于端点的设置 {#endpoint-settings}

可以在配置文件中为给定端点指定以下设置（将按 URL 的确切前缀进行匹配）：

- `endpoint` — 指定端点的前缀。必填。
- `access_key_id` 和 `secret_access_key` — 指定与给定端点一起使用的凭据。可选。
- `use_environment_credentials` — 如果设置为 `true`，S3 客户端将在获取给定端点的凭据时尝试从环境变量和 [Amazon EC2](https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud) 元数据中获取。可选，默认值为 `false`。
- `region` — 指定 S3 区域名称。可选。
- `use_insecure_imds_request` — 如果设置为 `true`，S3 客户端将在从 Amazon EC2 元数据中获取凭据时使用不安全的 IMDS 请求。可选，默认值为 `false`。
- `expiration_window_seconds` — 检查基于过期的凭据是否已过期的宽限期。可选，默认值为 `120`。
- `no_sign_request` - 忽略所有凭据，因此请求不会被签名。适用于访问公共存储桶。
- `header` — 向对给定端点的请求添加指定的 HTTP 头。可选，可以指定多次。
- `access_header` - 在没有来自其他来源的其他凭据的情况下，向对给定端点的请求添加指定的 HTTP 头。
- `server_side_encryption_customer_key_base64` — 如果指定，则将设置访问具有 SSE-C 加密的 S3 对象所需的标头。可选。
- `server_side_encryption_kms_key_id` - 如果指定，则将设置访问具有 [SSE-KMS 加密](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html) 的 S3 对象所需的标头。如果指定了空字符串，则将使用 AWS 管理的 S3 密钥。可选。
- `server_side_encryption_kms_encryption_context` - 如果与 `server_side_encryption_kms_key_id` 一起指定，则将为 SSE-KMS 设置指定的加密上下文标头。可选。
- `server_side_encryption_kms_bucket_key_enabled` - 如果与 `server_side_encryption_kms_key_id` 一起指定，将设置启用 SSE-KMS 的 S3 存储桶密钥的标头。可选，可以为 `true` 或 `false`，默认设置为空（与存储桶级别设置匹配）。
- `max_single_read_retries` — 单次读取期间的最大尝试次数。默认值为 `4`。可选。
- `max_put_rps`、`max_put_burst`、`max_get_rps` 和 `max_get_burst` - 用于特定端点的限流设置（见上文描述），而不是每个查询的设置。可选。

**示例：**

``` xml
<s3>
    <endpoint-name>
        <endpoint>https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/</endpoint>
        <!-- <access_key_id>ACCESS_KEY_ID</access_key_id> -->
        <!-- <secret_access_key>SECRET_ACCESS_KEY</secret_access_key> -->
        <!-- <region>us-west-1</region> -->
        <!-- <use_environment_credentials>false</use_environment_credentials> -->
        <!-- <use_insecure_imds_request>false</use_insecure_imds_request> -->
        <!-- <expiration_window_seconds>120</expiration_window_seconds> -->
        <!-- <no_sign_request>false</no_sign_request> -->
        <!-- <header>Authorization: Bearer SOME-TOKEN</header> -->
        <!-- <server_side_encryption_customer_key_base64>BASE64-ENCODED-KEY</server_side_encryption_customer_key_base64> -->
        <!-- <server_side_encryption_kms_key_id>KMS_KEY_ID</server_side_encryption_kms_key_id> -->
        <!-- <server_side_encryption_kms_encryption_context>KMS_ENCRYPTION_CONTEXT</server_side_encryption_kms_encryption_context> -->
        <!-- <server_side_encryption_kms_bucket_key_enabled>true</server_side_encryption_kms_bucket_key_enabled> -->
        <!-- <max_single_read_retries>4</max_single_read_retries> -->
    </endpoint-name>
</s3>
```

## 处理归档 {#working-with-archives}

假设我们在 S3 上有几个归档文件，其 URI 如下：

- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip'

使用 `::` 从这些归档中提取数据是可能的。可以在 URL 部分和 `::` 后的部分（负责归档内文件的名称）中使用通配符。

``` sql
SELECT *
FROM s3(
   'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-1{0..2}.csv.zip :: *.csv'
);
```

:::note 
ClickHouse 支持三种归档格式：
ZIP
TAR
7Z
虽然可以从任何受支持存储位置访问 ZIP 和 TAR 归档，但 7Z 归档只可以从安装 ClickHouse 的本地文件系统读取。  
:::


## 访问公共存储桶 {#accessing-public-buckets}

ClickHouse 尝试从多种不同类型的来源获取凭据。
有时，在访问某些公共存储桶时，这可能会导致客户返回 `403` 错误代码。
可以通过使用 `NOSIGN` 关键字避免此问题，强制客户端忽略所有凭据，并不对请求进行签名。

``` sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv', NOSIGN, 'CSVWithNames');
```

## 优化性能 {#optimizing-performance}

有关优化 s3 函数性能的详细信息，请参见 [我们详细的指南](/integrations/s3/performance)。

## 另请参阅 {#see-also}

- [s3 表函数](../../../sql-reference/table-functions/s3.md)
