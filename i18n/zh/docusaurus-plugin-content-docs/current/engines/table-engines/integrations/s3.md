---
description: '此引擎提供与 Amazon S3 生态系统的集成能力。类似于 HDFS 引擎，但提供 S3 专有功能。'
sidebar_label: 'S3'
sidebar_position: 180
slug: /engines/table-engines/integrations/s3
title: 'S3 表引擎'
doc_type: 'reference'
---



# S3 表引擎 {#s3-table-engine}

该引擎提供与 [Amazon S3](https://aws.amazon.com/s3/) 生态系统的集成。该引擎类似于 [HDFS](/engines/table-engines/integrations/hdfs) 引擎，但提供 S3 特有的功能。



## 示例 {#example}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE=S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'gzip')
    SETTINGS input_format_with_names_use_header = 0;

INSERT INTO s3_engine_table VALUES ('one', 1), ('two', 2), ('three', 3);

SELECT * FROM s3_engine_table LIMIT 2;
```

```text
┌─name─┬─value─┐
│ one  │     1 │
│ two  │     2 │
└──────┴───────┘
```


## 创建表 {#creating-a-table}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE = S3(path [, NOSIGN | aws_access_key_id, aws_secret_access_key,] format, [compression], [partition_strategy], [partition_columns_in_data_file])
    [PARTITION BY expr]
    [SETTINGS ...]
```

### 引擎参数 {#parameters}

* `path` — 带有文件路径的 bucket URL。只读模式下支持以下通配符：`*`、`**`、`?`、`{abc,def}` 和 `{N..M}`，其中 `N`、`M` 为数字，`'abc'`、`'def'` 为字符串。更多信息参见[下文](#wildcards-in-path)。
* `NOSIGN` - 如果在凭证位置提供此关键字，所有请求都不会被签名。
* `format` — 文件的[格式](/sql-reference/formats#formats-overview)。
* `aws_access_key_id`、`aws_secret_access_key` - [AWS](https://aws.amazon.com/) 账号用户的长期凭证。可使用这些凭证对请求进行身份验证。该参数是可选的。如果未指定凭证，将从配置文件中读取。更多信息参见 [Using S3 for Data Storage](../mergetree-family/mergetree.md#table_engine-mergetree-s3)。
* `compression` — 压缩类型。支持的值：`none`、`gzip/gz`、`brotli/br`、`xz/LZMA`、`zstd/zst`。该参数是可选的。默认情况下，将根据文件扩展名自动检测压缩类型。
* `partition_strategy` – 选项：`WILDCARD` 或 `HIVE`。`WILDCARD` 要求在路径中包含 `{_partition_id}`，该占位符将被分区键替换。`HIVE` 不允许使用通配符，假定路径是表的根路径，并生成 Hive 风格的分区目录，使用 Snowflake ID 作为文件名，并将文件格式作为扩展名。默认值为 `WILDCARD`。
* `partition_columns_in_data_file` - 仅在使用 `HIVE` 分区策略时生效。用于告知 ClickHouse 是否应在数据文件中写入分区列。默认值为 `false`。
* `storage_class_name` - 选项：`STANDARD` 或 `INTELLIGENT_TIERING`，用于指定 [AWS S3 Intelligent Tiering](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/)。

### 数据缓存 {#data-cache}

`S3` 表引擎支持在本地磁盘上进行数据缓存。
有关文件系统缓存的配置选项和使用方法，请参见本[节](/operations/storing-data.md/#using-local-cache)。
缓存是基于路径和存储对象的 ETag 进行的，因此 ClickHouse 不会读取过期的缓存版本。

要启用缓存，请使用设置 `filesystem_cache_name = '<name>'` 和 `enable_filesystem_cache = 1`。

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
SETTINGS filesystem_cache_name = 'cache_for_s3', enable_filesystem_cache = 1;
```

在配置文件中定义缓存有两种方法。

1. 在 ClickHouse 配置文件中添加以下部分：

```xml
<clickhouse>
    <filesystem_caches>
        <cache_for_s3>
            <path>path to cache directory</path>
            <max_size>10Gi</max_size>
        </cache_for_s3>
    </filesystem_caches>
</clickhouse>
```

2. 复用 ClickHouse `storage_configuration` 部分的缓存配置（以及相应的缓存存储），[详见此处](/operations/storing-data.md/#using-local-cache)

### PARTITION BY {#partition-by}

`PARTITION BY` — 可选。在大多数情况下无需分区键；即使需要，通常也不必细化到按月以下的粒度。分区不会提升查询性能（与 ORDER BY 表达式不同）。切勿使用粒度过细的分区。不要按照客户端标识符或名称对数据进行分区（相反，应将客户端标识符或名称作为 ORDER BY 表达式中的第一列）。

对于按月分区，使用 `toYYYYMM(date_column)` 表达式，其中 `date_column` 是类型为 [Date](/sql-reference/data-types/date.md) 的日期列。此处的分区名采用 `"YYYYMM"` 格式。

#### 分区策略 {#partition-strategy}

`WILDCARD`（默认）：将文件路径中的 `{_partition_id}` 通配符替换为实际的分区键值。不支持读取。


`HIVE` 在读写时实现了 Hive 风格的分区。读取通过递归 glob 通配模式实现，等价于 `SELECT * FROM s3('table_root/**.parquet')`。
写入生成的文件格式如下：`<prefix>/<key1=val1/key2=val2...>/<snowflakeid>.<toLower(file_format)>`。

注意：使用 `HIVE` 分区策略时，`use_hive_partitioning` 设置将不会生效。

`HIVE` 分区策略示例：

```sql
arthur :) CREATE TABLE t_03363_parquet (year UInt16, country String, counter UInt8)
ENGINE = S3(s3_conn, filename = 't_03363_parquet', format = Parquet, partition_strategy='hive')
PARTITION BY (year, country);

arthur :) INSERT INTO t_03363_parquet VALUES
    (2022, 'USA', 1),
    (2022, 'Canada', 2),
    (2023, 'USA', 3),
    (2023, 'Mexico', 4),
    (2024, 'France', 5),
    (2024, 'Germany', 6),
    (2024, 'Germany', 7),
    (1999, 'Brazil', 8),
    (2100, 'Japan', 9),
    (2024, 'CN', 10),
    (2025, '', 11);

arthur :) select _path, * from t_03363_parquet;

    ┌─_path──────────────────────────────────────────────────────────────────────┬─year─┬─country─┬─counter─┐
 1. │ test/t_03363_parquet/year=2100/country=Japan/7329604473272971264.parquet   │ 2100 │ Japan   │       9 │
 2. │ test/t_03363_parquet/year=2024/country=France/7329604473323302912.parquet  │ 2024 │ France  │       5 │
 3. │ test/t_03363_parquet/year=2022/country=Canada/7329604473314914304.parquet  │ 2022 │ Canada  │       2 │
 4. │ test/t_03363_parquet/year=1999/country=Brazil/7329604473289748480.parquet  │ 1999 │ Brazil  │       8 │
 5. │ test/t_03363_parquet/year=2023/country=Mexico/7329604473293942784.parquet  │ 2023 │ Mexico  │       4 │
 6. │ test/t_03363_parquet/year=2023/country=USA/7329604473319108608.parquet     │ 2023 │ USA     │       3 │
 7. │ test/t_03363_parquet/year=2025/country=/7329604473327497216.parquet        │ 2025 │         │      11 │
 8. │ test/t_03363_parquet/year=2024/country=CN/7329604473310720000.parquet      │ 2024 │ CN      │      10 │
 9. │ test/t_03363_parquet/year=2022/country=USA/7329604473298137088.parquet     │ 2022 │ USA     │       1 │
10. │ test/t_03363_parquet/year=2024/country=Germany/7329604473306525696.parquet │ 2024 │ Germany │       6 │
11. │ test/t_03363_parquet/year=2024/country=Germany/7329604473306525696.parquet │ 2024 │ Germany │       7 │
    └────────────────────────────────────────────────────────────────────────────┴──────┴─────────┴─────────┘
```

### 查询分区数据 {#querying-partitioned-data}

此示例使用了集成 ClickHouse 和 MinIO 的 [docker compose 配置示例](https://github.com/ClickHouse/examples/tree/5fdc6ff72f4e5137e23ea075c88d3f44b0202490/docker-compose-recipes/recipes/ch-and-minio-S3)。你可以通过替换 endpoint 和认证信息，改为使用 S3 来复现相同的查询。

请注意，在 `ENGINE` 配置中，S3 endpoint 使用参数令牌 `{_partition_id}` 作为 S3 对象（文件名）的一部分，并且 SELECT 查询会在这些生成的对象名称上执行查询（例如，`test_3.csv`）。


:::note
如示例所示，目前尚不支持直接从已分区的 S3 表中进行查询，
但可以通过使用 S3 表函数分别查询各个分区来实现。

在 S3 中写入分区数据的主要用例，是为了便于将这些数据迁移到另一套
ClickHouse 系统（例如，从本地部署系统迁移到 ClickHouse
Cloud）。由于 ClickHouse 数据集通常非常庞大，且网络
可靠性有时难以保证，因此以子集方式传输数据集是合理的做法，这也就是进行分区写入的原因。
:::

#### 创建表 {#create-the-table}

```sql
CREATE TABLE p
(
    `column1` UInt32,
    `column2` UInt32,
    `column3` UInt32
)
ENGINE = S3(
-- highlight-next-line
           'http://minio:10000/clickhouse//test_{_partition_id}.csv',
           'minioadmin',
           'minioadminpassword',
           'CSV')
PARTITION BY column3
```

#### 插入数据 {#insert-data}

```sql
INSERT INTO p VALUES (1, 2, 3), (3, 2, 1), (78, 43, 45)
```

#### 从分区 3 查询 {#select-from-partition-3}

:::tip
此查询使用 s3 表函数。
:::

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│  1 │  2 │  3 │
└────┴────┴────┘
```

#### 从分区 1 查询 {#select-from-partition-1}

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_1.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│  3 │  2 │  1 │
└────┴────┴────┘
```

#### 查询分区 45 {#select-from-partition-45}

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_45.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│ 78 │ 43 │ 45 │
└────┴────┴────┘
```

#### 限制 {#limitation}

你可能很自然地会尝试执行 `Select * from p`，但如上所述，该查询会失败；请改用前面的查询。

```sql
SELECT * FROM p
```

```response
Received exception from server (version 23.4.1):
Code: 48. DB::Exception: Received from localhost:9000. DB::Exception: Reading from a partitioned S3 storage is not implemented yet. (NOT_IMPLEMENTED)
```


## 插入数据 {#inserting-data}

请注意，数据行只能插入到新文件中。不存在合并周期或文件拆分操作。一旦文件写入完成，后续插入将会失败。为避免这种情况，可以使用 `s3_truncate_on_insert` 和 `s3_create_new_file_on_insert` 设置。更多详情参见[此处](/integrations/s3#inserting-data)。



## 虚拟列 {#virtual-columns}

- `_path` — 文件路径。类型：`LowCardinality(String)`。
- `_file` — 文件名。类型：`LowCardinality(String)`。
- `_size` — 文件大小（字节数）。类型：`Nullable(UInt64)`。如果大小未知，则值为 `NULL`。
- `_time` — 文件的最后修改时间。类型：`Nullable(DateTime)`。如果时间未知，则值为 `NULL`。
- `_etag` — 文件的 ETag。类型：`LowCardinality(String)`。如果 ETag 未知，则值为 `NULL`。
- `_tags` — 文件的标签。类型：`Map(String, String)`。如果不存在标签，则值为空映射 `{}'`{""}

有关虚拟列的更多信息，请参阅[此处](../../../engines/table-engines/index.md#table_engines-virtual_columns)。



## 实现细节 {#implementation-details}

- 读写可以并行进行
- 不支持：
  - `ALTER` 和 `SELECT...SAMPLE` 操作。
  - 索引。
  - 可以实现 [Zero-copy](../../../operations/storing-data.md#zero-copy) 复制，但目前不受支持。

  :::note Zero-copy 复制尚未准备好用于生产环境
  在 ClickHouse 22.8 及更高版本中，Zero-copy 复制默认是禁用的。不建议在生产环境中使用该功能。
  :::



## 路径中的通配符 {#wildcards-in-path}

`path` 参数可以使用类似 bash 的通配符来指定多个文件。要被处理的文件必须存在，并且与整个路径模式完全匹配。文件列表在执行 `SELECT` 时确定（而不是在 `CREATE` 时）。

* `*` — 匹配除 `/` 之外的任意数量的任意字符，包括空字符串。
* `**` — 匹配包括 `/` 在内的任意数量的任意字符，包括空字符串。
* `?` — 匹配任意单个字符。
* `{some_string,another_string,yet_another_one}` — 匹配字符串 `'some_string'`、`'another_string'`、`'yet_another_one'` 中的任意一个。
* `{N..M}` — 匹配从 N 到 M 范围内（包含两端）的任意数字。N 和 M 可以带有前导零，例如 `000..078`。

使用 `{}` 的语法形式类似于 [remote](../../../sql-reference/table-functions/remote.md) 表函数。

:::note
如果文件列表中包含带前导零的数字范围，请为每一位数字分别使用带花括号的语法形式，或者使用 `?`。
:::

**带通配符的示例 1**

创建使用文件名为 `file-000.csv`、`file-001.csv`、...、`file-999.csv` 的表：

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/my_folder/file-{000..999}.csv', 'CSV');
```

**带通配符的示例 2**

假设我们在 S3 上有若干 CSV 格式的文件，其 URI 如下所示：

* &#39;[https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some&#95;folder/some&#95;file&#95;1.csv](https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_1.csv)&#39;
* &#39;[https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some&#95;folder/some&#95;file&#95;2.csv](https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_2.csv)&#39;
* &#39;[https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some&#95;folder/some&#95;file&#95;3.csv](https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_3.csv)&#39;
* &#39;[https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another&#95;folder/some&#95;file&#95;1.csv](https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_1.csv)&#39;
* &#39;[https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another&#95;folder/some&#95;file&#95;2.csv](https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_2.csv)&#39;
* &#39;[https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another&#95;folder/some&#95;file&#95;3.csv](https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_3.csv)&#39;

有多种方式可以创建一个包含这六个文件的表：

1. 指定文件名后缀的范围：

```sql
CREATE TABLE table_with_range (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_{1..3}', 'CSV');
```

2. 取出所有以 `some_file_` 为前缀的文件（这两个文件夹中都不应包含带有该前缀的多余文件）：

```sql
CREATE TABLE table_with_question_mark (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_?', 'CSV');
```

3. 将两个文件夹中的所有文件都纳入（所有文件都应符合查询中描述的格式和 schema）：

```sql
CREATE TABLE table_with_asterisk (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/*', 'CSV');
```


## 存储设置 {#storage-settings}

- [s3_truncate_on_insert](/operations/settings/settings.md#s3_truncate_on_insert) - 允许在插入前截断目标文件。默认情况下为禁用。
- [s3_create_new_file_on_insert](/operations/settings/settings.md#s3_create_new_file_on_insert) - 如果格式带有后缀，允许在每次插入时创建一个新文件。默认情况下为禁用。
- [s3_skip_empty_files](/operations/settings/settings.md#s3_skip_empty_files) - 允许在读取时跳过空文件。默认情况下为启用。



## 与 S3 相关的设置 {#settings}

以下设置可以在查询执行前配置，或者写入配置文件中。

- `s3_max_single_part_upload_size` — 使用单部分上传到 S3 时，单个对象允许上传的最大大小。默认值为 `32Mb`。
- `s3_min_upload_part_size` — 使用多部分上传到 [S3 Multipart upload](https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html) 时，每个分片上传的最小大小。默认值为 `16Mb`。
- `s3_max_redirects` — 允许的 S3 重定向跳转的最大次数。默认值为 `10`。
- `s3_single_read_retries` — 单次读取操作的最大重试次数。默认值为 `4`。
- `s3_max_put_rps` — 在开始限流之前，每秒允许的最大 PUT 请求速率。默认值为 `0`（无限制）。
- `s3_max_put_burst` — 在触发每秒请求数限制之前，允许同时发起的最大请求数。默认（值为 `0`）时等于 `s3_max_put_rps`。
- `s3_max_get_rps` — 在开始限流之前，每秒允许的最大 GET 请求速率。默认值为 `0`（无限制）。
- `s3_max_get_burst` — 在触发每秒请求数限制之前，允许同时发起的最大请求数。默认（值为 `0`）时等于 `s3_max_get_rps`。
- `s3_upload_part_size_multiply_factor` - 每当从一次写入到 S3 上传了 `s3_multiply_parts_count_threshold` 个分片时，将 `s3_min_upload_part_size` 乘以该因子。默认值为 `2`。
- `s3_upload_part_size_multiply_parts_count_threshold` - 每当向 S3 上传的分片数量达到该值时，`s3_min_upload_part_size` 会乘以 `s3_upload_part_size_multiply_factor`。默认值为 `500`。
- `s3_max_inflight_parts_for_one_file` - 限制针对同一对象可并发运行的 PUT 请求数量。该数值应被限制。值为 `0` 表示无限制。默认值为 `20`。每个进行中的分片都会有一个大小为 `s3_min_upload_part_size` 的缓冲区，适用于前 `s3_upload_part_size_multiply_factor` 个分片，当文件足够大时缓冲区会更大，参见 `upload_part_size_multiply_factor`。在默认设置下，对于小于 `8GB` 的文件，每个上传文件占用的内存不会超过 `320Mb`。对于更大的文件，占用会更高。

安全注意事项：如果恶意用户可以指定任意 S3 URL，则必须将 `s3_max_redirects` 设置为 0 以避免 [SSRF](https://en.wikipedia.org/wiki/Server-side_request_forgery) 攻击；或者在服务器配置中指定 `remote_host_filter`。



## 基于 endpoint 的设置 {#endpoint-settings}

可以在配置文件中为指定的 endpoint 配置以下设置（将通过 URL 的精确前缀进行匹配）：

* `endpoint` — 指定 endpoint 的前缀。必填。
* `access_key_id` 和 `secret_access_key` — 指定用于该 endpoint 的凭证。可选。
* `use_environment_credentials` — 如果设置为 `true`，S3 客户端将尝试从环境变量和该 endpoint 的 [Amazon EC2](https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud) 元数据中获取凭证。可选，默认值为 `false`。
* `region` — 指定 S3 区域名称。可选。
* `use_insecure_imds_request` — 如果设置为 `true`，S3 客户端在从 Amazon EC2 元数据获取凭证时将使用不安全的 IMDS 请求。可选，默认值为 `false`。
* `expiration_window_seconds` — 用于检查基于过期时间的凭证是否已过期的宽限期（秒）。可选，默认值为 `120`。
* `no_sign_request` - 忽略所有凭证，使请求不被签名。适用于访问公共 bucket。
* `header` — 为发往指定 endpoint 的请求添加给定的 HTTP 头。可选，可多次指定。
* `access_header` - 为发往指定 endpoint 的请求添加给定的 HTTP 头，用于不存在来自其他来源的凭证时的情况。
* `server_side_encryption_customer_key_base64` — 如果指定，则会为访问使用 SSE-C 加密的 S3 对象设置所需的头。可选。
* `server_side_encryption_kms_key_id` - 如果指定，则会为访问使用 [SSE-KMS 加密](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html) 的 S3 对象设置所需的头。如果指定为空字符串，则会使用 AWS 托管的 S3 密钥。可选。
* `server_side_encryption_kms_encryption_context` - 如果与 `server_side_encryption_kms_key_id` 一起指定，则会设置给定的 SSE-KMS 加密上下文头。可选。
* `server_side_encryption_kms_bucket_key_enabled` - 如果与 `server_side_encryption_kms_key_id` 一起指定，则会设置启用 SSE-KMS 的 S3 bucket keys 的头。可选，可为 `true` 或 `false`，默认不设置（遵从 bucket 级别设置）。
* `max_single_read_retries` — 单次读取期间的最大重试次数。默认值为 `4`。可选。
* `max_put_rps`、`max_put_burst`、`max_get_rps` 和 `max_get_burst` - 针对特定 endpoint 使用的限流设置（参见上文描述），而不是按查询级别设置。可选。

**示例：**

```xml
<s3>
    <endpoint-name>
        <endpoint>https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/</endpoint>
        <!-- <access_key_id>ACCESS_KEY_ID</access_key_id> -->
        <!-- <secret_access_key>SECRET_ACCESS_KEY</secret_access_key> -->
        <!-- <region>us-west-1</region> -->
        <!-- <use_environment_credentials>false</use_environment_credentials> -->
        <!-- <use_insecure_imds_request>false</use_insecure_imds_request> -->
        <!-- <expiration_window_seconds>120</expiration_window_seconds> -->
        <!-- <no_sign_request>false</no_sign_request> -->
        <!-- <header>Authorization: Bearer SOME-TOKEN</header> -->
        <!-- <server_side_encryption_customer_key_base64>BASE64-ENCODED-KEY</server_side_encryption_customer_key_base64> -->
        <!-- <server_side_encryption_kms_key_id>KMS_KEY_ID</server_side_encryption_kms_key_id> -->
        <!-- <server_side_encryption_kms_encryption_context>KMS_ENCRYPTION_CONTEXT</server_side_encryption_kms_encryption_context> -->
        <!-- <server_side_encryption_kms_bucket_key_enabled>true</server_side_encryption_kms_bucket_key_enabled> -->
        <!-- <max_single_read_retries>4</max_single_read_retries> -->
    </endpoint-name>
</s3>
```


## 使用归档文件 {#working-with-archives}

假设我们在 S3 上有若干归档文件，其 URI 如下：

* &#39;[https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip](https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip)&#39;
* &#39;[https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip](https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip)&#39;
* &#39;[https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip](https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip)&#39;

可以使用 :: 从这些归档文件中提取数据。通配符（globs）既可以用于 URL 部分，也可以用于 :: 之后的部分（用于指定归档内部文件的名称）。

```sql
SELECT *
FROM s3(
   'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-1{0..2}.csv.zip :: *.csv'
);
```

:::note
ClickHouse 支持三种归档格式：
ZIP
TAR
7Z
虽然可以从任何受支持的存储位置访问 ZIP 和 TAR 归档文件，但 7Z 归档文件只能从安装了 ClickHouse 的本地文件系统读取。
:::


## 访问公共 bucket {#accessing-public-buckets}

ClickHouse 会尝试从多种不同类型的来源获取凭证。
有时，在访问某些公共 bucket 时，这可能会导致问题，使客户端返回 `403` 错误码。
可以通过使用 `NOSIGN` 关键字来避免此问题，该关键字会强制客户端忽略所有凭证，请求时不进行签名。

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv', NOSIGN, 'CSVWithNames');
```


## 优化性能 {#optimizing-performance}

有关如何优化 S3 FUNCTION 性能的详细信息，请参阅 [我们的详细指南](/integrations/s3/performance)。



## 另请参阅 {#see-also}

- [S3 表函数](../../../sql-reference/table-functions/s3.md)
- [将 S3 与 ClickHouse 集成](/integrations/s3)
