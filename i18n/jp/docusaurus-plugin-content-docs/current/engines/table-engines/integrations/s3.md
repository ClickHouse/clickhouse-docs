---
description: 'このエンジンは Amazon S3 エコシステムとの統合機能を提供します。HDFS エンジンと類似していますが、S3 固有の機能を備えています。'
sidebar_label: 'S3'
sidebar_position: 180
slug: /engines/table-engines/integrations/s3
title: 'S3 テーブルエンジン'
doc_type: 'reference'
---



# S3 テーブルエンジン

このエンジンは [Amazon S3](https://aws.amazon.com/s3/) エコシステムとの統合機能を提供します。このエンジンは [HDFS](/engines/table-engines/integrations/hdfs) エンジンと類似していますが、S3 固有の機能も備えています。



## 例 {#example}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE=S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'gzip')
    SETTINGS input_format_with_names_use_header = 0;

INSERT INTO s3_engine_table VALUES ('one', 1), ('two', 2), ('three', 3);

SELECT * FROM s3_engine_table LIMIT 2;
```

```text
┌─name─┬─value─┐
│ one  │     1 │
│ two  │     2 │
└──────┴───────┘
```


## テーブルの作成 {#creating-a-table}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE = S3(path [, NOSIGN | aws_access_key_id, aws_secret_access_key,] format, [compression], [partition_strategy], [partition_columns_in_data_file])
    [PARTITION BY expr]
    [SETTINGS ...]
```

### エンジンパラメータ {#parameters}

- `path` — ファイルへのパスを含むバケットURL。読み取り専用モードで以下のワイルドカードをサポートします：`*`、`**`、`?`、`{abc,def}`、および`{N..M}`（`N`、`M`は数値、`'abc'`、`'def'`は文字列）。詳細については[下記](#wildcards-in-path)を参照してください。
- `NOSIGN` - 認証情報の代わりにこのキーワードを指定すると、すべてのリクエストが署名されません。
- `format` — ファイルの[フォーマット](/sql-reference/formats#formats-overview)。
- `aws_access_key_id`、`aws_secret_access_key` - [AWS](https://aws.amazon.com/)アカウントユーザーの長期認証情報。リクエストの認証に使用できます。このパラメータは省略可能です。認証情報が指定されていない場合は、設定ファイルから読み込まれます。詳細については[S3をデータストレージに使用する](../mergetree-family/mergetree.md#table_engine-mergetree-s3)を参照してください。
- `compression` — 圧縮タイプ。サポートされる値：`none`、`gzip/gz`、`brotli/br`、`xz/LZMA`、`zstd/zst`。このパラメータは省略可能です。デフォルトでは、ファイル拡張子から圧縮形式を自動検出します。
- `partition_strategy` – オプション：`WILDCARD`または`HIVE`。`WILDCARD`はパス内に`{_partition_id}`が必要で、これが実際のパーティションキーに置き換えられます。`HIVE`はワイルドカードを許可せず、パスをテーブルルートとみなし、Snowflake IDをファイル名、ファイルフォーマットを拡張子としたHive形式のパーティションディレクトリを生成します。デフォルトは`WILDCARD`です。
- `partition_columns_in_data_file` - `HIVE`パーティション戦略でのみ使用されます。パーティションカラムがデータファイルに書き込まれることを想定するかどうかをClickHouseに指示します。デフォルトは`false`です。
- `storage_class_name` - オプション：`STANDARD`または`INTELLIGENT_TIERING`。[AWS S3 Intelligent Tiering](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/)を指定できます。

### データキャッシュ {#data-cache}

`S3`テーブルエンジンは、ローカルディスク上のデータキャッシュをサポートしています。
ファイルシステムキャッシュの設定オプションと使用方法については、この[セクション](/operations/storing-data.md/#using-local-cache)を参照してください。
キャッシュはストレージオブジェクトのパスとETagに基づいて作成されるため、ClickHouseは古いキャッシュバージョンを読み取りません。

キャッシュを有効にするには、設定`filesystem_cache_name = '<name>'`と`enable_filesystem_cache = 1`を使用します。

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
SETTINGS filesystem_cache_name = 'cache_for_s3', enable_filesystem_cache = 1;
```

設定ファイルでキャッシュを定義する方法は2つあります。

1. ClickHouse設定ファイルに以下のセクションを追加します：

```xml
<clickhouse>
    <filesystem_caches>
        <cache_for_s3>
            <path>path to cache directory</path>
            <max_size>10Gi</max_size>
        </cache_for_s3>
    </filesystem_caches>
</clickhouse>
```

2. ClickHouseの`storage_configuration`セクションからキャッシュ設定（およびキャッシュストレージ）を再利用します。[こちらで説明](/operations/storing-data.md/#using-local-cache)

### PARTITION BY {#partition-by}

`PARTITION BY` — 省略可能。ほとんどの場合、パーティションキーは不要であり、必要な場合でも通常は月単位より細かいパーティションキーは必要ありません。パーティショニングはクエリを高速化しません（ORDER BY式とは対照的です）。過度に細かいパーティショニングは使用すべきではありません。クライアント識別子や名前でデータをパーティション化しないでください（代わりに、クライアント識別子や名前をORDER BY式の最初のカラムにしてください）。

月単位でパーティション化するには、`toYYYYMM(date_column)`式を使用します。ここで`date_column`は[Date](/sql-reference/data-types/date.md)型の日付を持つカラムです。パーティション名は`"YYYYMM"`形式になります。

#### パーティション戦略 {#partition-strategy}

`WILDCARD`（デフォルト）：ファイルパス内の`{_partition_id}`ワイルドカードを実際のパーティションキーに置き換えます。読み取りはサポートされていません。


`HIVE`は、読み取りと書き込みにHiveスタイルのパーティショニングを実装します。読み取りは再帰的なglobパターンを使用して実装されており、`SELECT * FROM s3('table_root/**.parquet')`と同等です。
書き込みは次の形式でファイルを生成します:`<prefix>/<key1=val1/key2=val2...>/<snowflakeid>.<toLower(file_format)>`。

注:`HIVE`パーティション戦略を使用する場合、`use_hive_partitioning`設定は効果がありません。

`HIVE`パーティション戦略の例:

```sql
arthur :) CREATE TABLE t_03363_parquet (year UInt16, country String, counter UInt8)
ENGINE = S3(s3_conn, filename = 't_03363_parquet', format = Parquet, partition_strategy='hive')
PARTITION BY (year, country);

arthur :) INSERT INTO t_03363_parquet VALUES
    (2022, 'USA', 1),
    (2022, 'Canada', 2),
    (2023, 'USA', 3),
    (2023, 'Mexico', 4),
    (2024, 'France', 5),
    (2024, 'Germany', 6),
    (2024, 'Germany', 7),
    (1999, 'Brazil', 8),
    (2100, 'Japan', 9),
    (2024, 'CN', 10),
    (2025, '', 11);

arthur :) select _path, * from t_03363_parquet;

    ┌─_path──────────────────────────────────────────────────────────────────────┬─year─┬─country─┬─counter─┐
 1. │ test/t_03363_parquet/year=2100/country=Japan/7329604473272971264.parquet   │ 2100 │ Japan   │       9 │
 2. │ test/t_03363_parquet/year=2024/country=France/7329604473323302912.parquet  │ 2024 │ France  │       5 │
 3. │ test/t_03363_parquet/year=2022/country=Canada/7329604473314914304.parquet  │ 2022 │ Canada  │       2 │
 4. │ test/t_03363_parquet/year=1999/country=Brazil/7329604473289748480.parquet  │ 1999 │ Brazil  │       8 │
 5. │ test/t_03363_parquet/year=2023/country=Mexico/7329604473293942784.parquet  │ 2023 │ Mexico  │       4 │
 6. │ test/t_03363_parquet/year=2023/country=USA/7329604473319108608.parquet     │ 2023 │ USA     │       3 │
 7. │ test/t_03363_parquet/year=2025/country=/7329604473327497216.parquet        │ 2025 │         │      11 │
 8. │ test/t_03363_parquet/year=2024/country=CN/7329604473310720000.parquet      │ 2024 │ CN      │      10 │
 9. │ test/t_03363_parquet/year=2022/country=USA/7329604473298137088.parquet     │ 2022 │ USA     │       1 │
10. │ test/t_03363_parquet/year=2024/country=Germany/7329604473306525696.parquet │ 2024 │ Germany │       6 │
11. │ test/t_03363_parquet/year=2024/country=Germany/7329604473306525696.parquet │ 2024 │ Germany │       7 │
    └────────────────────────────────────────────────────────────────────────────┴──────┴─────────┴─────────┘
```

### パーティション化されたデータのクエリ {#querying-partitioned-data}

この例では、ClickHouseとMinIOを統合する[docker composeレシピ](https://github.com/ClickHouse/examples/tree/5fdc6ff72f4e5137e23ea075c88d3f44b0202490/docker-compose-recipes/recipes/ch-and-minio-S3)を使用しています。エンドポイントと認証値を置き換えることで、S3を使用して同じクエリを再現できます。

`ENGINE`設定のS3エンドポイントは、S3オブジェクト(ファイル名)の一部としてパラメータトークン`{_partition_id}`を使用しており、SELECTクエリはそれらの結果オブジェクト名(例:`test_3.csv`)に対して実行されることに注意してください。


:::note
例に示すように、パーティション化されたS3テーブルからのクエリは現時点では直接サポートされていませんが、S3テーブル関数を使用して個々のパーティションをクエリすることで実現できます。

S3にパーティション化されたデータを書き込む主な用途は、そのデータを別のClickHouseシステムに転送できるようにすることです(例:オンプレミスシステムからClickHouse Cloudへの移行)。ClickHouseのデータセットは非常に大きいことが多く、ネットワークの信頼性が完璧でない場合もあるため、データセットをサブセット単位で転送することが合理的です。そのため、パーティション化された書き込みが使用されます。
:::

#### テーブルの作成 {#create-the-table}

```sql
CREATE TABLE p
(
    `column1` UInt32,
    `column2` UInt32,
    `column3` UInt32
)
ENGINE = S3(
-- highlight-next-line
           'http://minio:10000/clickhouse//test_{_partition_id}.csv',
           'minioadmin',
           'minioadminpassword',
           'CSV')
PARTITION BY column3
```

#### データの挿入 {#insert-data}

```sql
INSERT INTO p VALUES (1, 2, 3), (3, 2, 1), (78, 43, 45)
```

#### パーティション3からの選択 {#select-from-partition-3}

:::tip
このクエリはs3テーブル関数を使用します
:::

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│  1 │  2 │  3 │
└────┴────┴────┘
```

#### パーティション1からの選択 {#select-from-partition-1}

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_1.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│  3 │  2 │  1 │
└────┴────┴────┘
```

#### パーティション45からの選択 {#select-from-partition-45}

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_45.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│ 78 │ 43 │ 45 │
└────┴────┴────┘
```

#### 制限事項 {#limitation}

`Select * from p`を試みたくなるかもしれませんが、上記のように、このクエリは失敗します。前述のクエリを使用してください。

```sql
SELECT * FROM p
```

```response
Received exception from server (version 23.4.1):
Code: 48. DB::Exception: Received from localhost:9000. DB::Exception: Reading from a partitioned S3 storage is not implemented yet. (NOT_IMPLEMENTED)
```


## データの挿入 {#inserting-data}

行は新しいファイルにのみ挿入可能です。マージサイクルやファイル分割操作は行われません。ファイルが一度書き込まれると、以降の挿入は失敗します。これを回避するには、`s3_truncate_on_insert`および`s3_create_new_file_on_insert`設定を使用してください。詳細は[こちら](/integrations/s3#inserting-data)を参照してください。


## 仮想カラム {#virtual-columns}

- `_path` — ファイルへのパス。型: `LowCardinality(String)`。
- `_file` — ファイル名。型: `LowCardinality(String)`。
- `_size` — ファイルのサイズ(バイト単位)。型: `Nullable(UInt64)`。サイズが不明な場合、値は `NULL` になります。
- `_time` — ファイルの最終更新時刻。型: `Nullable(DateTime)`。時刻が不明な場合、値は `NULL` になります。
- `_etag` — ファイルのETag。型: `LowCardinality(String)`。ETagが不明な場合、値は `NULL` になります。
- `_tags` — ファイルのタグ。型: `Map(String, String)`。タグが存在しない場合、値は空のマップ `{}` になります。

仮想カラムの詳細については、[こちら](../../../engines/table-engines/index.md#table_engines-virtual_columns)を参照してください。


## 実装の詳細 {#implementation-details}

- 読み取りと書き込みは並列実行可能です
- サポートされていない機能:
  - `ALTER`および`SELECT...SAMPLE`操作
  - インデックス
  - [ゼロコピー](../../../operations/storing-data.md#zero-copy)レプリケーションは技術的に可能ですが、サポートされていません。

  :::note ゼロコピーレプリケーションは本番環境での使用準備が整っていません
  ゼロコピーレプリケーションは、ClickHouseバージョン22.8以降ではデフォルトで無効になっています。この機能は本番環境での使用を推奨しません。
  :::


## パス内のワイルドカード {#wildcards-in-path}

`path` 引数では、bashライクなワイルドカードを使用して複数のファイルを指定できます。処理対象となるファイルは存在し、パスパターン全体に一致する必要があります。ファイルのリストは `SELECT` 実行時に決定されます(`CREATE` 実行時ではありません)。

- `*` — `/` を除く任意の文字を任意の数だけ置換します(空文字列を含む)。
- `**` — `/` を含む任意の文字を任意の数だけ置換します(空文字列を含む)。
- `?` — 任意の1文字を置換します。
- `{some_string,another_string,yet_another_one}` — `'some_string'`、`'another_string'`、`'yet_another_one'` のいずれかの文字列を置換します。
- `{N..M}` — N から M までの範囲内の任意の数値を置換します(両端を含む)。N と M は先頭にゼロを含むことができます(例: `000..078`)。

`{}` を使用した構文は、[remote](../../../sql-reference/table-functions/remote.md) テーブル関数と同様です。

:::note
ファイルのリストに先頭ゼロ付きの数値範囲が含まれる場合は、各桁に対して個別に中括弧を使用した構文を使用するか、`?` を使用してください。
:::

**ワイルドカードの例 1**

`file-000.csv`、`file-001.csv`、...、`file-999.csv` という名前のファイルでテーブルを作成します:

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/my_folder/file-{000..999}.csv', 'CSV');
```

**ワイルドカードの例 2**

S3上に以下のURIを持つCSV形式の複数のファイルがあるとします:

- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_3.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_3.csv'

6つのファイルすべてで構成されるテーブルを作成する方法はいくつかあります:

1. ファイルの接尾辞の範囲を指定する:

```sql
CREATE TABLE table_with_range (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_{1..3}', 'CSV');
```

2. `some_file_` 接頭辞を持つすべてのファイルを取得する(両方のフォルダにこの接頭辞を持つ余分なファイルが存在しないこと):

```sql
CREATE TABLE table_with_question_mark (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_?', 'CSV');
```

3. 両方のフォルダ内のすべてのファイルを取得する(すべてのファイルはクエリで記述された形式とスキーマを満たす必要があります):

```sql
CREATE TABLE table_with_asterisk (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/*', 'CSV');
```


## ストレージ設定 {#storage-settings}

- [s3_truncate_on_insert](/operations/settings/settings.md#s3_truncate_on_insert) - 挿入前にファイルを切り詰めます。デフォルトでは無効です。
- [s3_create_new_file_on_insert](/operations/settings/settings.md#s3_create_new_file_on_insert) - フォーマットにサフィックスがある場合、挿入ごとに新しいファイルを作成します。デフォルトでは無効です。
- [s3_skip_empty_files](/operations/settings/settings.md#s3_skip_empty_files) - 読み取り時に空のファイルをスキップします。デフォルトでは有効です。


## S3関連の設定 {#settings}

以下の設定は、クエリ実行前に設定するか、設定ファイルに記述することができます。

- `s3_max_single_part_upload_size` — S3へのシングルパートアップロードでアップロードするオブジェクトの最大サイズ。デフォルト値は`32Mb`です。
- `s3_min_upload_part_size` — [S3マルチパートアップロード](https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html)時にアップロードするパートの最小サイズ。デフォルト値は`16Mb`です。
- `s3_max_redirects` — 許可されるS3リダイレクトホップの最大数。デフォルト値は`10`です。
- `s3_single_read_retries` — 単一読み取り時の最大再試行回数。デフォルト値は`4`です。
- `s3_max_put_rps` — スロットリング前の1秒あたりの最大PUTリクエスト数。デフォルト値は`0`(無制限)です。
- `s3_max_put_burst` — 1秒あたりのリクエスト制限に達する前に同時に発行できるリクエストの最大数。デフォルト(`0`の値)では`s3_max_put_rps`と同じ値になります。
- `s3_max_get_rps` — スロットリング前の1秒あたりの最大GETリクエスト数。デフォルト値は`0`(無制限)です。
- `s3_max_get_burst` — 1秒あたりのリクエスト制限に達する前に同時に発行できるリクエストの最大数。デフォルト(`0`の値)では`s3_max_get_rps`と同じ値になります。
- `s3_upload_part_size_multiply_factor` - S3への単一書き込みから`s3_multiply_parts_count_threshold`個のパートがアップロードされるたびに、`s3_min_upload_part_size`にこの係数を乗算します。デフォルト値は`2`です。
- `s3_upload_part_size_multiply_parts_count_threshold` - この数のパートがS3にアップロードされるたびに、`s3_min_upload_part_size`が`s3_upload_part_size_multiply_factor`で乗算されます。デフォルト値は`500`です。
- `s3_max_inflight_parts_for_one_file` - 1つのオブジェクトに対して同時に実行できるputリクエストの数を制限します。この数は制限する必要があります。値`0`は無制限を意味します。デフォルト値は`20`です。各処理中のパートは、最初の`s3_upload_part_size_multiply_factor`個のパートに対して`s3_min_upload_part_size`のサイズのバッファを持ち、ファイルが十分に大きい場合はさらに大きくなります。`upload_part_size_multiply_factor`を参照してください。デフォルト設定では、`8G`未満のファイルの場合、1つのアップロードファイルは最大`320Mb`を消費します。より大きなファイルの場合、消費量はさらに増加します。

セキュリティ上の考慮事項: 悪意のあるユーザーが任意のS3 URLを指定できる場合、[SSRF](https://en.wikipedia.org/wiki/Server-side_request_forgery)攻撃を回避するために`s3_max_redirects`をゼロに設定する必要があります。または、サーバー設定で`remote_host_filter`を指定する必要があります。


## エンドポイントベースの設定 {#endpoint-settings}

以下の設定は、指定されたエンドポイント(URLの完全一致プレフィックスによってマッチングされます)の設定ファイルで指定できます:

- `endpoint` — エンドポイントのプレフィックスを指定します。必須です。
- `access_key_id` および `secret_access_key` — 指定されたエンドポイントで使用する認証情報を指定します。オプションです。
- `use_environment_credentials` — `true` に設定すると、S3クライアントは指定されたエンドポイントの環境変数および[Amazon EC2](https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud)メタデータから認証情報の取得を試みます。オプションで、デフォルト値は `false` です。
- `region` — S3リージョン名を指定します。オプションです。
- `use_insecure_imds_request` — `true` に設定すると、S3クライアントはAmazon EC2メタデータから認証情報を取得する際に安全でないIMDSリクエストを使用します。オプションで、デフォルト値は `false` です。
- `expiration_window_seconds` — 有効期限ベースの認証情報が期限切れかどうかを確認するための猶予期間です。オプションで、デフォルト値は `120` です。
- `no_sign_request` - すべての認証情報を無視し、リクエストに署名しません。パブリックバケットへのアクセスに有用です。
- `header` — 指定されたエンドポイントへのリクエストに指定されたHTTPヘッダーを追加します。オプションで、複数回指定できます。
- `access_header` - 他のソースからの認証情報がない場合に、指定されたエンドポイントへのリクエストに指定されたHTTPヘッダーを追加します。
- `server_side_encryption_customer_key_base64` — 指定された場合、SSE-C暗号化を使用したS3オブジェクトへのアクセスに必要なヘッダーが設定されます。オプションです。
- `server_side_encryption_kms_key_id` - 指定された場合、[SSE-KMS暗号化](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html)を使用したS3オブジェクトへのアクセスに必要なヘッダーが設定されます。空文字列が指定された場合、AWS管理のS3キーが使用されます。オプションです。
- `server_side_encryption_kms_encryption_context` - `server_side_encryption_kms_key_id` と共に指定された場合、SSE-KMSの指定された暗号化コンテキストヘッダーが設定されます。オプションです。
- `server_side_encryption_kms_bucket_key_enabled` - `server_side_encryption_kms_key_id` と共に指定された場合、SSE-KMSのS3バケットキーを有効にするヘッダーが設定されます。オプションで、`true` または `false` を指定でき、デフォルトは何も設定されません(バケットレベルの設定に従います)。
- `max_single_read_retries` — 単一読み取り中の最大試行回数です。デフォルト値は `4` です。オプションです。
- `max_put_rps`、`max_put_burst`、`max_get_rps`、および `max_get_burst` - クエリごとではなく特定のエンドポイントに使用するスロットリング設定(上記の説明を参照)です。オプションです。

**例:**

```xml
<s3>
    <endpoint-name>
        <endpoint>https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/</endpoint>
        <!-- <access_key_id>ACCESS_KEY_ID</access_key_id> -->
        <!-- <secret_access_key>SECRET_ACCESS_KEY</secret_access_key> -->
        <!-- <region>us-west-1</region> -->
        <!-- <use_environment_credentials>false</use_environment_credentials> -->
        <!-- <use_insecure_imds_request>false</use_insecure_imds_request> -->
        <!-- <expiration_window_seconds>120</expiration_window_seconds> -->
        <!-- <no_sign_request>false</no_sign_request> -->
        <!-- <header>Authorization: Bearer SOME-TOKEN</header> -->
        <!-- <server_side_encryption_customer_key_base64>BASE64-ENCODED-KEY</server_side_encryption_customer_key_base64> -->
        <!-- <server_side_encryption_kms_key_id>KMS_KEY_ID</server_side_encryption_kms_key_id> -->
        <!-- <server_side_encryption_kms_encryption_context>KMS_ENCRYPTION_CONTEXT</server_side_encryption_kms_encryption_context> -->
        <!-- <server_side_encryption_kms_bucket_key_enabled>true</server_side_encryption_kms_bucket_key_enabled> -->
        <!-- <max_single_read_retries>4</max_single_read_retries> -->
    </endpoint-name>
</s3>
```


## アーカイブの操作 {#working-with-archives}

S3上に以下のURIを持つ複数のアーカイブファイルがあるとします:

- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip'

これらのアーカイブからのデータ抽出は `::` を使用して行うことができます。Globパターンは、URL部分とアーカイブ内のファイル名を指定する `::` の後の部分の両方で使用できます。

```sql
SELECT *
FROM s3(
   'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-1{0..2}.csv.zip :: *.csv'
);
```

:::note
ClickHouseは3つのアーカイブ形式をサポートしています:
ZIP
TAR
7Z
ZIPおよびTARアーカイブはサポートされている任意のストレージロケーションからアクセスできますが、7Zアーカイブはローカルファイルシステムからのみ読み取り可能です。
:::


## パブリックバケットへのアクセス {#accessing-public-buckets}

ClickHouseは、さまざまな種類のソースから認証情報の取得を試みます。
場合によっては、パブリックバケットへのアクセス時に問題が発生し、クライアントが`403`エラーコードを返すことがあります。
この問題は、`NOSIGN`キーワードを使用することで回避できます。このキーワードにより、クライアントはすべての認証情報を無視し、リクエストに署名を行わなくなります。

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv', NOSIGN, 'CSVWithNames');
```


## パフォーマンスの最適化 {#optimizing-performance}

s3関数のパフォーマンス最適化の詳細については、[詳細ガイド](/integrations/s3/performance)をご参照ください。


## 関連項目 {#see-also}

- [s3テーブル関数](../../../sql-reference/table-functions/s3.md)
- [ClickHouseとS3の統合](/integrations/s3)
