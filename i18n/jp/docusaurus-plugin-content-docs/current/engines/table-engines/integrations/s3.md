---
description: 'このエンジンは、Amazon S3 エコシステムとの統合を提供します。HDFS エンジンに似ていますが、S3 専用の機能を提供します。'
sidebar_label: 'S3'
sidebar_position: 180
slug: /engines/table-engines/integrations/s3
title: 'S3 テーブルエンジン'
---


# S3 テーブルエンジン

このエンジンは、[Amazon S3](https://aws.amazon.com/s3/) エコシステムとの統合を提供します。このエンジンは、[HDFS](/engines/table-engines/integrations/hdfs) エンジンに似ていますが、S3 専用の機能を提供します。

## 例 {#example}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE=S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'gzip')
    SETTINGS input_format_with_names_use_header = 0;

INSERT INTO s3_engine_table VALUES ('one', 1), ('two', 2), ('three', 3);

SELECT * FROM s3_engine_table LIMIT 2;
```

```text
┌─name─┬─value─┐
│ one  │     1 │
│ two  │     2 │
└──────┴───────┘
```

## テーブルの作成 {#creating-a-table}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE = S3(path [, NOSIGN | aws_access_key_id, aws_secret_access_key,] format, [compression])
    [PARTITION BY expr]
    [SETTINGS ...]
```

### エンジンパラメータ {#parameters}

- `path` — ファイルへのパスを持つバケットの URL。読み取り専用モードで次のワイルドカードをサポートします: `*`, `**`, `?`, `{abc,def}` および `{N..M}` ここで `N`, `M` は数字、`'abc'`, `'def'` は文字列です。詳細については [以下](#wildcards-in-path) を参照してください。
- `NOSIGN` - このキーワードが認証情報の代わりに提供された場合、すべてのリクエストは署名されません。
- `format` — ファイルの[形式](/sql-reference/formats#formats-overview)。
- `aws_access_key_id`, `aws_secret_access_key` - [AWS](https://aws.amazon.com/) アカウントユーザーの長期的な認証情報。これらを使用してリクエストを認証できます。このパラメータはオプションです。認証情報が指定されていない場合、構成ファイルから使用されます。詳細については、[S3をデータストレージとして使用する](../mergetree-family/mergetree.md#table_engine-mergetree-s3)を参照してください。
- `compression` — 圧縮タイプ。サポートされている値: `none`, `gzip/gz`, `brotli/br`, `xz/LZMA`, `zstd/zst`。このパラメータはオプションです。デフォルトでは、ファイル拡張子によって圧縮が自動検出されます。

### データキャッシュ {#data-cache}

`S3` テーブルエンジンは、ローカルディスク上でのデータキャッシュをサポートします。
この[セクション](/operations/storing-data.md/#using-local-cache)でファイルシステムキャッシュの構成オプションと使用法を参照してください。
キャッシングは、ストレージオブジェクトのパスとETagに基づいて行われるため、ClickHouseは古いキャッシュバージョンを読みません。

キャッシングを有効にするには、`filesystem_cache_name = '<name>'` および `enable_filesystem_cache = 1` の設定を使用します。

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
SETTINGS filesystem_cache_name = 'cache_for_s3', enable_filesystem_cache = 1;
```

構成ファイルでキャッシュを定義する方法は 2 つあります。

1. ClickHouse 構成ファイルに次のセクションを追加します:

```xml
<clickhouse>
    <filesystem_caches>
        <cache_for_s3>
            <path>キャッシュディレクトリへのパス</path>
            <max_size>10Gi</max_size>
        </cache_for_s3>
    </filesystem_caches>
</clickhouse>
```

2. ClickHouse の `storage_configuration` セクションからキャッシュ構成（およびそれによるキャッシュストレージ）を再利用します。 [ここで説明されている](../operations/storing-data.md/#using-local-cache)。

### PARTITION BY {#partition-by}

`PARTITION BY` — オプションです。ほとんどの場合、パーティションキーは必要ありません。そして必要な場合でも、月単位よりも細かいパーティションキーは必要ありません。パーティショニングはクエリの速度を上げません（ORDER BY 表現とは対照的に）。あまりにも細かいパーティショニングを使用してはいけません。クライアント識別子や名前でデータをパーティショニングしないでください（代わりに、クライアント識別子や名前を ORDER BY 表現の最初のカラムにしてください）。

月単位でのパーティショニングには、`toYYYYMM(date_column)` 表現を使用します。ここで `date_column` は [Date](/sql-reference/data-types/date.md) タイプの日付を持つカラムです。ここでのパーティション名は `"YYYYMM"` 形式です。

### パーティションデータのクエリ {#querying-partitioned-data}

この例では、ClickHouse と MinIO を統合する [docker compose レシピ](https://github.com/ClickHouse/examples/tree/5fdc6ff72f4e5137e23ea075c88d3f44b0202490/docker-compose-recipes/recipes/ch-and-minio-S3) を使用しています。同じクエリを S3 を使用して再現できるはずです。エンドポイントと認証値を置き換えることで実現します。

`ENGINE` 構成内の S3 エンドポイントが、S3 オブジェクト（ファイル名）の一部としてパラメータトークン `{_partition_id}` を使用していることに注意してください。また、SELECT クエリがこれらの結果オブジェクト名（例: `test_3.csv`）に対して選択することに注意してください。

:::note
例に示すように、パーティショニングされた S3 テーブルからのクエリは
現時点では直接サポートされていませんが、S3 テーブル関数を使用して個々のパーティションをクエリすることで達成できます。

S3 にパーティショニングデータを書くことの主なユースケースは、そのデータを別の
ClickHouse システム（例えば、オンプレミスシステムから ClickHouse
Cloud への移行）に転送できるようにすることです。ClickHouse データセットは非常に大きいことが多いため、ネットワークの信頼性が時には完璧ではないため、データセットをサブセットで転送することが理にかなっています。したがって、パーティショニングされた書き込みが必要です。
:::

#### テーブルの作成 {#create-the-table}
```sql
CREATE TABLE p
(
    `column1` UInt32,
    `column2` UInt32,
    `column3` UInt32
)
ENGINE = S3(
-- highlight-next-line
           'http://minio:10000/clickhouse//test_{_partition_id}.csv',
           'minioadmin',
           'minioadminpassword',
           'CSV')
PARTITION BY column3
```

#### データを挿入する {#insert-data}
```sql
insert into p values (1, 2, 3), (3, 2, 1), (78, 43, 45)
```

#### パーティション 3 から選択 {#select-from-partition-3}

:::tip
このクエリは S3 テーブル関数を使用します
:::

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│  1 │  2 │  3 │
└────┴────┴────┘
```

#### パーティション 1 から選択 {#select-from-partition-1}
```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_1.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│  3 │  2 │  1 │
└────┴────┴────┘
```

#### パーティション 45 から選択 {#select-from-partition-45}
```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_45.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│ 78 │ 43 │ 45 │
└────┴────┴────┘
```

#### 制限 {#limitation}

自然に `Select * from p` を試みるかもしれませんが、上で述べたように、このクエリは失敗します。前のクエリを使用してください。

```sql
SELECT * FROM p
```
```response
Received exception from server (version 23.4.1):
Code: 48. DB::Exception: Received from localhost:9000. DB::Exception: Reading from a partitioned S3 storage is not implemented yet. (NOT_IMPLEMENTED)
```

## データの挿入 {#inserting-data}

行は新しいファイルにのみ挿入できます。マージサイクルやファイル分割操作はありません。ファイルが書き込まれると、その後の挿入は失敗します。これを避けるために、`s3_truncate_on_insert` および `s3_create_new_file_on_insert` 設定を使用できます。詳細は [こちら](../integrations/s3#inserting-data) を参照してください。

## 仮想カラム {#virtual-columns}

- `_path` — ファイルへのパス。タイプ: `LowCardinality(String)`。
- `_file` — ファイル名。タイプ: `LowCardinality(String)`。
- `_size` — ファイルのサイズ（バイト単位）。タイプ: `Nullable(UInt64)`。サイズが不明な場合、値は `NULL` になります。
- `_time` — ファイルの最終修正時間。タイプ: `Nullable(DateTime)`。時間が不明な場合、値は `NULL` になります。
- `_etag` — ファイルのETag。タイプ: `LowCardinality(String)`。etag が不明な場合、値は `NULL` になります。

仮想カラムに関する詳細は [こちら](../../../engines/table-engines/index.md#table_engines-virtual_columns) を参照してください。

## 実装の詳細 {#implementation-details}

- 読み取りと書き込みは並行して行うことができます
- サポートされていない:
    - `ALTER` および `SELECT...SAMPLE` 操作。
    - インデックス。
    - [ゼロコピー](../../../operations/storing-data.md#zero-copy) レプリケーションは可能ですが、サポートされていません。

  :::note ゼロコピー レプリケーションは製品版の準備ができていません
  ゼロコピー レプリケーションは ClickHouse バージョン 22.8 以降でデフォルトで無効になっています。この機能は製品での使用は推奨されていません。
  :::

## パス内のワイルドカード {#wildcards-in-path}

`path` 引数は、bash のようなワイルドカードを使用して複数のファイルを指定できます。処理されるファイルは、存在し、完全なパスパターンに一致する必要があります。ファイルのリストは `SELECT` 時に決定されます（`CREATE` 時ではありません）。

- `*` — `/` を除く任意の数の任意の文字を空文字列を含めて置き換えます。
- `**` — `/` を含む任意の数の任意の文字を空文字列を含めて置き換えます。
- `?` — 任意の単一文字を置き換えます。
- `{some_string,another_string,yet_another_one}` — `'some_string', 'another_string', 'yet_another_one'` のいずれかの文字列を置き換えます。
- `{N..M}` — N から M の範囲内の任意の数を置き換えます（両端を含む）。 N と M は先頭ゼロを持つことができます（例: `000..078`）。

`{}` の構造は [remote](../../../sql-reference/table-functions/remote.md) テーブル関数に似ています。

:::note
ファイルのリストに先頭ゼロの数値範囲が含まれている場合、各桁ごとにブレースを使用するか、`?` を使用してください。
:::

**ワイルドカード 1 の例**

`file-000.csv`, `file-001.csv`, ... , `file-999.csv` という名前のファイルでテーブルを作成します:

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/my_folder/file-{000..999}.csv', 'CSV');
```

**ワイルドカード 2 の例**

次の URIs を持つ CSV 形式のファイルがいくつかあるとします:

- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_3.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_3.csv'

これらの 6 つのファイルすべてで構成されるテーブルを作成する方法は いくつかあります:

1. ファイルの接尾辞の範囲を指定します:

```sql
CREATE TABLE table_with_range (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_{1..3}', 'CSV');
```

2. `some_file_` プレフィックスを持つすべてのファイルを取得します（両方のフォルダーにそのような接頭辞を持つ余分なファイルがない必要があります）:

```sql
CREATE TABLE table_with_question_mark (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_?', 'CSV');
```

3. 両方のフォルダー内のすべてのファイルを取得します（すべてのファイルは、クエリで説明した形式とスキーマを満たす必要があります）:

```sql
CREATE TABLE table_with_asterisk (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/*', 'CSV');
```

## ストレージ設定 {#storage-settings}

- [s3_truncate_on_insert](/operations/settings/settings.md#s3_truncate_on_insert) - 挿入する前にファイルを切り捨てることができます。デフォルトでは無効です。
- [s3_create_new_file_on_insert](/operations/settings/settings.md#s3_create_new_file_on_insert) - 形式にサフィックスがある場合、各挿入時に新しいファイルを作成します。デフォルトでは無効です。
- [s3_skip_empty_files](/operations/settings/settings.md#s3_skip_empty_files) - 読み取り中に空のファイルをスキップできます。デフォルトで有効です。

## S3関連の設定 {#settings}

次の設定は、クエリ実行前に設定するか、構成ファイルに配置できます。

- `s3_max_single_part_upload_size` — 単一パートアップロードを使用して S3 にアップロードするオブジェクトの最大サイズ。デフォルト値は `32Mb`。
- `s3_min_upload_part_size` — [S3 マルチパートアップロード](https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html)中にアップロードするパートの最小サイズ。デフォルト値は `16Mb`。
- `s3_max_redirects` — 許可される S3 リダイレクトホップの最大数。デフォルト値は `10`。
- `s3_single_read_retries` — 単一の読み取り中の最大試行回数。デフォルト値は `4`。
- `s3_max_put_rps` — スロットリング前の最大 PUT リクエスト毎秒率。デフォルト値は `0`（無制限）。
- `s3_max_put_burst` — 要求毎秒制限に達する前に同時に発行できる要求の最大数。デフォルト値（`0`値）で `s3_max_put_rps` に等しい。
- `s3_max_get_rps` — スロットリング前の最大 GET リクエスト毎秒率。デフォルト値は `0`（無制限）。
- `s3_max_get_burst` — 要求毎秒制限に達する前に同時に発行できる要求の最大数。デフォルト値（`0` 値）で `s3_max_get_rps` に等しい。
- `s3_upload_part_size_multiply_factor` - この係数で `s3_min_upload_part_size` を乗算します。各回で `s3_multiply_parts_count_threshold` パーツが S3 にアップロードされます。デフォルト値は `2`です。
- `s3_upload_part_size_multiply_parts_count_threshold` - この数のパーツが S3 にアップロードされるたびに、`s3_min_upload_part_size` が `s3_upload_part_size_multiply_factor` に乗算されます。デフォルト値は `500`。
- `s3_max_inflight_parts_for_one_file` - 一つのオブジェクトに対して同時に実行できる PUT リクエストの数を制限します。この数は制限されるべきです。値 `0` は無制限を意味します。デフォルト値は `20`。各インフライトパーツには最初の `s3_upload_part_size_multiply_factor` パーツのための `s3_min_upload_part_size` サイズのバッファがあります。ファイルが大きい場合は、さらに多く必要とされます。デフォルト設定では、1 つのアップロードファイルは 8G より小さいファイルで最大 320Mb を消費します。大きなファイルでは消費が増えます。

セキュリティ上の考慮事項: 悪意のあるユーザーが任意の S3 URL を指定できる場合、`s3_max_redirects` は SSRF（[Server-side request forgery](https://en.wikipedia.org/wiki/Server-side_request_forgery)）攻撃を避けるためにゼロに設定する必要があります。あるいは、サーバー構成で `remote_host_filter` を指定することができます。

## エンドポイントベースの設定 {#endpoint-settings}

次の設定は、構成ファイルで指定のエンドポイントについて具体的なプレフィックスを持つ URL に一致させるために設定できます:

- `endpoint` — エンドポイントのプレフィックスを指定します。必須です。
- `access_key_id` および `secret_access_key` — 指定されたエンドポイントで使用する認証情報を指定します。オプションです。
- `use_environment_credentials` — `true` に設定されている場合、S3 クライアントは指定されたエンドポイントに対して環境変数および[Amazon EC2](https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud) メタデータから認証情報を取得しようとします。オプションで、デフォルト値は `false`です。
- `region` — S3 リージョン名を指定します。オプションです。
- `use_insecure_imds_request` — `true` に設定されている場合、S3 クライアントは Amazon EC2 メタデータからの認証情報を取得する際に不安全な IMDS リクエストを使用します。オプションで、デフォルト値は `false`です。
- `expiration_window_seconds` — 有効期限に基づく認証情報が期限切れかどうかを確認するための猶予期間。オプションで、デフォルト値は `120`です。
- `no_sign_request` - すべての認証情報を無視し、リクエストに署名しないようにします。公開バケットにアクセスするために便利です。
- `header` — 特定のエンドポイントへのリクエストに指定された HTTP ヘッダーを追加します。オプションで、複数回指定可能です。
- `access_header` - 追加の認証情報がない場合、特定のエンドポイントへのリクエストに指定された HTTP ヘッダーを追加します。
- `server_side_encryption_customer_key_base64` — 指定されると、SSE-C 暗号化で S3 オブジェクトにアクセスするための必須ヘッダーが設定されます。オプションです。
- `server_side_encryption_kms_key_id` - 指定されると、[SSE-KMS 暗号化](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html)で S3 オブジェクトにアクセスするための必須ヘッダーが設定されます。空文字列が指定されると、AWS 管理 S3 キーが使用されます。オプションです。
- `server_side_encryption_kms_encryption_context` - `server_side_encryption_kms_key_id` とともに指定されると、SSE-KMS 用の指定された暗号化コンテキストヘッダーが設定されます。オプションです。
- `server_side_encryption_kms_bucket_key_enabled` - `server_side_encryption_kms_key_id` とともに指定されると、SSE-KMS 用の S3 バケットキーを有効にするためのヘッダーが設定されます。オプションで、`true` または `false` が指定可能で、デフォルトでは何も設定されていません（バケットレベルの設定に一致します）。
- `max_single_read_retries` — 単一の読み取り中の最大試行回数。デフォルト値は `4`。オプションです。
- `max_put_rps`, `max_put_burst`, `max_get_rps` および `max_get_burst` - 特定のエンドポイントで使用するためのスロットリング設定（上記の説明を参照）。オプションです。

**例:**

```xml
<s3>
    <endpoint-name>
        <endpoint>https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/</endpoint>
        <!-- <access_key_id>ACCESS_KEY_ID</access_key_id> -->
        <!-- <secret_access_key>SECRET_ACCESS_KEY</secret_access_key> -->
        <!-- <region>us-west-1</region> -->
        <!-- <use_environment_credentials>false</use_environment_credentials> -->
        <!-- <use_insecure_imds_request>false</use_insecure_imds_request> -->
        <!-- <expiration_window_seconds>120</expiration_window_seconds> -->
        <!-- <no_sign_request>false</no_sign_request> -->
        <!-- <header>Authorization: Bearer SOME-TOKEN</header> -->
        <!-- <server_side_encryption_customer_key_base64>BASE64-ENCODED-KEY</server_side_encryption_customer_key_base64> -->
        <!-- <server_side_encryption_kms_key_id>KMS_KEY_ID</server_side_encryption_kms_key_id> -->
        <!-- <server_side_encryption_kms_encryption_context>KMS_ENCRYPTION_CONTEXT</server_side_encryption_kms_encryption_context> -->
        <!-- <server_side_encryption_kms_bucket_key_enabled>true</server_side_encryption_kms_bucket_key_enabled> -->
        <!-- <max_single_read_retries>4</max_single_read_retries> -->
    </endpoint-name>
</s3>
```

## アーカイブの操作 {#working-with-archives}

次のような URI を持つアーカイブファイルがいくつかあるとします:

- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip'

これらのアーカイブからデータを抽出することは、次のようにして可能です。`::` を使用します。グロブは URL 部分とアーカイブ内のファイル名部分の両方で使用できます。

```sql
SELECT *
FROM s3(
   'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-1{0..2}.csv.zip :: *.csv'
);
```

:::note
ClickHouse は 3 つのアーカイブ形式をサポートします:
ZIP
TAR
7Z
ZIP と TAR アーカイブは、サポートされているストレージロケーションからアクセスできますが、7Z アーカイブは ClickHouse がインストールされたローカル ファイルシステムからのみ読み取ることができます。
:::


## 公開バケットへのアクセス {#accessing-public-buckets}

ClickHouse は、多くの異なるタイプのソースから認証情報を取得しようとします。
時々、公開されているいくつかのバケットにアクセスする際に `403` エラーコードを返す問題を引き起こすことがあります。
この問題は、`NOSIGN` キーワードを使用することで回避でき、クライアントがすべての認証情報を無視し、リクエストに署名しないように強制します。

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv', NOSIGN, 'CSVWithNames');
```

## パフォーマンスの最適化 {#optimizing-performance}

S3 関数のパフォーマンスを最適化する詳細については、[こちらの詳細ガイド](/integrations/s3/performance)を参照してください。

## 参照 {#see-also}

- [s3 テーブル関数](../../../sql-reference/table-functions/s3.md)
- [S3 と ClickHouse の統合](/integrations/s3)
