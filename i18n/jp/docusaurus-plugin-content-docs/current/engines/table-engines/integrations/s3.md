---
slug: /engines/table-engines/integrations/s3
sidebar_position: 180
sidebar_label: S3
title: "S3テーブルエンジン"
description: "このエンジンはAmazon S3エコシステムとの統合を提供します。HDFSエンジンに似ていますが、S3特有の機能を提供します。"
---


# S3テーブルエンジン

このエンジンは、[Amazon S3](https://aws.amazon.com/s3/)エコシステムとの統合を提供します。このエンジンは、[HDFS](/engines/table-engines/integrations/hdfs)エンジンに似ていますが、S3特有の機能を提供します。

## 例 {#example}

``` sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE=S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'gzip')
    SETTINGS input_format_with_names_use_header = 0;

INSERT INTO s3_engine_table VALUES ('one', 1), ('two', 2), ('three', 3);

SELECT * FROM s3_engine_table LIMIT 2;
```

```text
┌─name─┬─value─┐
│ one  │     1 │
│ two  │     2 │
└──────┴───────┘
```

## テーブルの作成 {#creating-a-table}

``` sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE = S3(path [, NOSIGN | aws_access_key_id, aws_secret_access_key,] format, [compression])
    [PARTITION BY expr]
    [SETTINGS ...]
```

### エンジンパラメータ {#parameters}

- `path` — ファイルへのパスを持つバケットのURL。読み取り専用モードで次のワイルドカードをサポートします: `*`, `**`, `?`, `{abc,def}` および `{N..M}` （ここで `N`, `M` は数字、`'abc'`, `'def'` は文字列）。詳細については、[下記](#wildcards-in-path)を参照してください。
- `NOSIGN` - このキーワードが認証情報の代わりに指定されると、すべてのリクエストは署名されません。
- `format` — ファイルの[フォーマット](/sql-reference/formats#formats-overview)。
- `aws_access_key_id`, `aws_secret_access_key` - [AWS](https://aws.amazon.com/)アカウントユーザーの長期的な認証情報。これらを使用してリクエストを認証できます。パラメータはオプションです。認証情報が指定されていない場合、それらは構成ファイルから使用されます。詳細については、[S3をデータストレージとして使用する](../mergetree-family/mergetree.md#table_engine-mergetree-s3)を参照してください。
- `compression` — 圧縮タイプ。サポートされている値: `none`, `gzip/gz`, `brotli/br`, `xz/LZMA`, `zstd/zst`。パラメータはオプションです。デフォルトでは、ファイル拡張子によって圧縮が自動的に検出されます。

### データキャッシュ {#data-cache}

`S3`テーブルエンジンは、ローカルディスク上でデータキャッシングをサポートします。ファイルシステムキャッシュの構成オプションと使用法については、この[セクション](/operations/storing-data.md/#using-local-cache)を参照してください。キャッシングは、ストレージオブジェクトのパスとETagに依存して行われるため、ClickHouseは古いキャッシュバージョンを読み込みません。

キャッシングを有効にするには、設定 `filesystem_cache_name = '<name>'` と `enable_filesystem_cache = 1` を使用します。

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
SETTINGS filesystem_cache_name = 'cache_for_s3', enable_filesystem_cache = 1;
```

キャッシュ定義には、構成ファイルで次の2つの方法があります。

1. ClickHouse構成ファイルに次のセクションを追加します:

``` xml
<clickhouse>
    <filesystem_caches>
        <cache_for_s3>
            <path>キャッシュディレクトリへのパス</path>
            <max_size>10Gi</max_size>
        </cache_for_s3>
    </filesystem_caches>
</clickhouse>
```

2. ClickHouseの `storage_configuration` セクションからキャッシュ構成（およびそれに伴うキャッシュストレージ）を再利用します。[こちらで説明されています](/operations/storing-data.md/#using-local-cache)

### PARTITION BY {#partition-by}

`PARTITION BY` — オプションです。ほとんどの場合、パーティションキーは必要なく、必要な場合でも、通常は月単位よりも詳細なパーティションキーは必要ありません。パーティショニングはクエリの速度を上げるものではありません（ORDER BY式とは対照的に）。あまり細かいパーティショニングは使用しないでください。クライアント識別子や名前でデータをパーティショニングしないでください（代わりに、クライアント識別子または名前をORDER BY式の最初のカラムにします）。

月単位でのパーティショニングには、`toYYYYMM(date_column)`式を使用します。ここで、`date_column`は[Date](/sql-reference/data-types/date.md)型の日付を持つカラムです。パーティション名は「YYYYMM」形式になります。

### パーティショニングデータのクエリ {#querying-partitioned-data}

この例では、[docker composeレシピ](https://github.com/ClickHouse/examples/tree/5fdc6ff72f4e5137e23ea075c88d3f44b0202490/docker-compose-recipes/recipes/ch-and-minio-S3)を使用して、ClickHouseとMinIOを統合しています。同様のクエリをS3で再現するには、エンドポイントと認証値を置き換えるだけで済みます。

`ENGINE`構成内のS3エンドポイントは、S3オブジェクト（ファイル名）の一部としてパラメータトークン`{_partition_id}`を使用しており、SELECTクエリはそれらの結果となるオブジェクト名（例: `test_3.csv`）に対して選択します。

:::note
例のように、パーティショニングされたS3テーブルからのクエリは
現時点では直接サポートされていませんが、S3テーブル関数を使用して
個々のパーティションをクエリすることで実現できます。

S3にパーティション化されたデータを書く主なユースケースは、
そのデータを別のClickHouseシステムに転送できるようにすることです
（例えば、オンプレミスシステムからClickHouse Cloudへの移行）。  
ClickHouseデータセットは非常に大きいことが多く、ネットワークの信頼性が
時々完璧ではないため、データセットをサブセットに分けて転送することが理にかなっています、したがってパーティショニングされた書き込みが必要です。
:::

#### テーブルの作成 {#create-the-table}
```sql
CREATE TABLE p
(
    `column1` UInt32,
    `column2` UInt32,
    `column3` UInt32
)
ENGINE = S3(

# highlight-next-line
           'http://minio:10000/clickhouse//test_{_partition_id}.csv',
           'minioadmin',
           'minioadminpassword',
           'CSV')
PARTITION BY column3
```

#### データを挿入 {#insert-data}
```sql
insert into p values (1, 2, 3), (3, 2, 1), (78, 43, 45)
```

#### パーティション3から選択 {#select-from-partition-3}

:::tip
このクエリはs3テーブル関数を使用しています。
:::

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│  1 │  2 │  3 │
└────┴────┴────┘
```

#### パーティション1から選択 {#select-from-partition-1}
```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_1.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│  3 │  2 │  1 │
└────┴────┴────┘
```

#### パーティション45から選択 {#select-from-partition-45}
```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_45.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│ 78 │ 43 │ 45 │
└────┴────┴────┘
```

#### 制限 {#limitation}

あなたは自然に `Select * from p` を試みるかもしれませんが、先に述べたように、このクエリは失敗します; 前述のクエリを使用してください。

```sql
SELECT * FROM p
```
```response
Received exception from server (version 23.4.1):
Code: 48. DB::Exception: Received from localhost:9000. DB::Exception: Reading from a partitioned S3 storage is not implemented yet. (NOT_IMPLEMENTED)
```

## 仮想カラム {#virtual-columns}

- `_path` — ファイルへのパス。型: `LowCardinality(String)`。
- `_file` — ファイルの名前。型: `LowCardinality(String)`。
- `_size` — ファイルのサイズ（バイト単位）。型: `Nullable(UInt64)`。サイズが不明な場合、値は`NULL`になります。
- `_time` — ファイルの最終更新時刻。型: `Nullable(DateTime)`。時刻が不明な場合、値は`NULL`になります。
- `_etag` — ファイルのETag。型: `LowCardinality(String)`。etagが不明な場合、値は`NULL`になります。

仮想カラムの詳細については、[こちら](../../../engines/table-engines/index.md#table_engines-virtual_columns)を参照してください。

## 実装の詳細 {#implementation-details}

- 読み取りと書き込みは並行して行うことができます。
- サポートされていないもの:
    - `ALTER`および`SELECT...SAMPLE`操作。
    - インデックス。
    - [ゼロコピー](../../../operations/storing-data.md#zero-copy)レプリケーションは可能ですが、サポートされていません。

  :::note ゼロコピーのレプリケーションは本番環境には適していません
  ゼロコピーのレプリケーションは、ClickHouseバージョン22.8以降でデフォルトで無効になっています。この機能は本番環境での使用は推奨されません。
  :::

## パス内のワイルドカード {#wildcards-in-path}

`path`引数は、bashスタイルのワイルドカードを使用して複数のファイルを指定できます。処理されるファイルは存在し、全パスパターンに一致する必要があります。ファイルのリストは`SELECT`中に決定されます（`CREATE`時ではありません）。

- `*` — 任意の数の任意の文字（`/`を除く）を空文字列も含めて代替します。
- `**` — 任意の数のあらゆる文字（`/`を含む）を空文字列も含めて代替します。
- `?` — 任意の単一文字を代替します。
- `{some_string,another_string,yet_another_one}` — 文字列のいずれかを代替します `'some_string', 'another_string', 'yet_another_one'`。
- `{N..M}` — NからMまでの範囲内の任意の数字を代替します。NとMは先頭ゼロを含むことができます。例: `000..078`。

`{}`を使用した構文は、[リモート](../../../sql-reference/table-functions/remote.md)テーブル関数に似ています。

:::note
ファイルのリストに先頭ゼロ付きの数字範囲が含まれている場合、各数字ごとに波括弧付きの構文を使用するか、`?`を使用してください。
:::

**ワイルドカードを使用した例1**

`file-000.csv`、`file-001.csv`、...、`file-999.csv`という名前のファイルでテーブルを作成します:

``` sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/my_folder/file-{000..999}.csv', 'CSV');
```

**ワイルドカードを使用した例2**

次のURIでCSV形式のいくつかのファイルがS3にあるとします:

- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_3.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_3.csv'


次の方法で6つのファイルからなるテーブルを作成できます:

1. ファイルの接尾辞範囲を指定します:

``` sql
CREATE TABLE table_with_range (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_{1..3}', 'CSV');
```

2. `some_file_`という接頭辞を持つすべてのファイルを取り込みます（両方のフォルダにその接頭辞を持つ余計なファイルがないべきです）:

``` sql
CREATE TABLE table_with_question_mark (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_?', 'CSV');
```

3. 両方のフォルダ内のすべてのファイルを取り込みます（すべてのファイルは、クエリで記述された形式とスキーマに一致するべきです）:

``` sql
CREATE TABLE table_with_asterisk (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/*', 'CSV');
```

## ストレージ設定 {#storage-settings}

- [s3_truncate_on_insert](/operations/settings/settings.md#s3_truncate_on_insert) - 挿入前にファイルを切り捨てることを許可します。デフォルトでは無効です。
- [s3_create_new_file_on_insert](/operations/settings/settings.md#s3_create_new_file_on_insert) - 各挿入時に新しいファイルを作成することを許可します。デフォルトでは無効です。
- [s3_skip_empty_files](/operations/settings/settings.md#s3_skip_empty_files) - 読み取り時に空のファイルをスキップすることを許可します。デフォルトでは有効です。

## S3関連設定 {#settings}

以下の設定は、クエリ実行前に設定するか、構成ファイルに配置できます。

- `s3_max_single_part_upload_size` — S3に単一パートアップロードを使用してアップロードするオブジェクトの最大サイズ。デフォルト値は`32Mb`です。
- `s3_min_upload_part_size` — [S3マルチパートアップロード](https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html)中にアップロードされる部分の最小サイズ。デフォルト値は`16Mb`です。
- `s3_max_redirects` — 許可される最大S3リダイレクトホップ数。デフォルト値は`10`です。
- `s3_single_read_retries` — 単一読み取り中の最大試行回数。デフォルト値は`4`です。
- `s3_max_put_rps` — スロットリングの前に許可される最大PUTリクエストの毎秒レート。デフォルト値は`0`（無制限）です。
- `s3_max_put_burst` — リクエスト毎秒制限に達する前に、同時に発行できるリクエストの最大数。デフォルト値（`0`値）は`s3_max_put_rps`に等しいです。
- `s3_max_get_rps` — スロットリングの前に許可される最大GETリクエストの毎秒レート。デフォルト値は`0`（無制限）です。
- `s3_max_get_burst` — リクエスト毎秒制限に達する前に、同時に発行できるリクエストの最大数。デフォルト値（`0`値）は`s3_max_get_rps`に等しいです。
- `s3_upload_part_size_multiply_factor` - `s3_min_upload_part_size`を、この係数で毎回`s3_multiply_parts_count_threshold`パーツがS3にアップロードされるごとに乗算します。デフォルト値は`2`です。
- `s3_upload_part_size_multiply_parts_count_threshold` - この数のパーツがS3にアップロードされるたびに、`s3_min_upload_part_size`が`s3_upload_part_size_multiply_factor`で乗算されます。デフォルト値は`500`です。
- `s3_max_inflight_parts_for_one_file` - 1つのオブジェクトに対して同時に実行できるPUTリクエストの数を制限します。この数は制限する必要があります。値`0`は無制限を意味します。デフォルト値は`20`です。各進行中部分は、最初の`s3_upload_part_size_multiply_factor`パーツについて`s3_min_upload_part_size`のサイズを持つバッファを持ち、ファイルが十分に大きければ追加のサイズを持ちます。デフォルト設定で、アップロードされたファイルは8G未満の場合、最大320Mbを消費します。大きいファイルの場合、消費は大きくなります。

セキュリティ考慮: 悪意のあるユーザーが任意のS3 URLを指定できる場合、`s3_max_redirects`はゼロに設定する必要があり、[SSRF](https://en.wikipedia.org/wiki/Server-side_request_forgery)攻撃を避けなければなりません。あるいは、サーバー構成に`remote_host_filter`を指定する必要があります。

## エンドポイントに基づく設定 {#endpoint-settings}

以下の設定は、特定のエンドポイントの構成ファイルに指定できます（エンドポイントの正確な接頭辞で一致します）:

- `endpoint` — エンドポイントの接頭辞を指定します。必須です。
- `access_key_id` と `secret_access_key` — 与えられたエンドポイントに使用する認証情報を指定します。オプションです。
- `use_environment_credentials` — `true`に設定すると、S3クライアントは環境変数および指定されたエンドポイントの[Amazon EC2](https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud)メタデータから認証情報を取得します。オプションで、デフォルト値は`false`です。
- `region` — S3リージョン名を指定します。オプションです。
- `use_insecure_imds_request` — `true`に設定すると、S3クライアントは、Amazon EC2メタデータから認証情報を取得する際に安全でないIMDSリクエストを使用します。オプションで、デフォルト値は`false`です。
- `expiration_window_seconds` — 有効期限ベースの認証情報が期限切れかどうかを確認するための猶予時間。オプションで、デフォルト値は`120`です。
- `no_sign_request` - すべての認証情報を無視し、リクエストに署名しないようにします。公開バケットにアクセスする場合に便利です。
- `header` — 指定されたHTTPヘッダーを指定されたエンドポイントへのリクエストに追加します。オプションで、複数回指定できます。
- `access_header` - 他のソースからの他の認証情報がない場合に、指定されたHTTPヘッダーを指定されたエンドポイントへのリクエストに追加します。
- `server_side_encryption_customer_key_base64` — 指定された場合、SSE-C暗号化されたS3オブジェクトへのアクセスに必要なヘッダーが設定されます。オプションです。
- `server_side_encryption_kms_key_id` - 指定された場合、[SSE-KMS暗号化](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html)されたS3オブジェクトへのアクセスに必要なヘッダーが設定されます。空の文字列が指定された場合、AWS管理のS3キーが使用されます。オプションです。
- `server_side_encryption_kms_encryption_context` - `server_side_encryption_kms_key_id`と一緒に指定された場合、SSE-KMS用の指定された暗号化コンテキストヘッダーが設定されます。オプションです。
- `server_side_encryption_kms_bucket_key_enabled` - `server_side_encryption_kms_key_id`と一緒に指定された場合、SSE-KMS用のS3バケットキーを有効にするためのヘッダーが設定されます。オプションで、`true`または`false`で設定でき、デフォルトでは何も設定されていません（バケットレベルの設定に一致します）。
- `max_single_read_retries` — 単一読み取り中の最大試行回数。デフォルト値は`4`です。オプションです。
- `max_put_rps`, `max_put_burst`, `max_get_rps`および `max_get_burst` - 特定のエンドポイントに対してクエリごとの代わりに使用するスロットリング設定（上記の説明を参照）です。オプションです。

**例:**

``` xml
<s3>
    <endpoint-name>
        <endpoint>https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/</endpoint>
        <!-- <access_key_id>ACCESS_KEY_ID</access_key_id> -->
        <!-- <secret_access_key>SECRET_ACCESS_KEY</secret_access_key> -->
        <!-- <region>us-west-1</region> -->
        <!-- <use_environment_credentials>false</use_environment_credentials> -->
        <!-- <use_insecure_imds_request>false</use_insecure_imds_request> -->
        <!-- <expiration_window_seconds>120</expiration_window_seconds> -->
        <!-- <no_sign_request>false</no_sign_request> -->
        <!-- <header>Authorization: Bearer SOME-TOKEN</header> -->
        <!-- <server_side_encryption_customer_key_base64>BASE64-ENCODED-KEY</server_side_encryption_customer_key_base64> -->
        <!-- <server_side_encryption_kms_key_id>KMS_KEY_ID</server_side_encryption_kms_key_id> -->
        <!-- <server_side_encryption_kms_encryption_context>KMS_ENCRYPTION_CONTEXT</server_side_encryption_kms_encryption_context> -->
        <!-- <server_side_encryption_kms_bucket_key_enabled>true</server_side_encryption_kms_bucket_key_enabled> -->
        <!-- <max_single_read_retries>4</max_single_read_retries> -->
    </endpoint-name>
</s3>
```

## アーカイブの操作 {#working-with-archives}

次のURIでS3にいくつかのアーカイブファイルがあるとします:

- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip'

これらのアーカイブからデータを抽出するには、::を使用します。グロブはURL部分およびアーカイブ内のファイル名部分の両方で使用できます。

``` sql
SELECT *
FROM s3(
   'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-1{0..2}.csv.zip :: *.csv'
);
```

:::note 
ClickHouseは3つのアーカイブ形式をサポートしています：
ZIP
TAR
7Z
ZIPおよびTARアーカイブは、サポートされている任意のストレージロケーションからアクセスできますが、7ZアーカイブはClickHouseがインストールされているローカルファイルシステムからのみ読み取ることができます。  
:::


## 公開バケットへのアクセス {#accessing-public-buckets}

ClickHouseはさまざまなソースから認証情報を取得しようとします。時には、公開バケットにアクセスする際に、クライアントが`403`エラーコードを返す問題が発生する場合があります。この問題は、`NOSIGN`キーワードを使用することで回避できます。これにより、クライアントはすべての認証情報を無視し、リクエストを署名しなくなります。

``` sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv', NOSIGN, 'CSVWithNames');
```

## パフォーマンスの最適化 {#optimizing-performance}

s3関数のパフォーマンスを最適化する詳細については、[当社の詳細ガイド](/integrations/s3/performance)を参照してください。

## 参照 {#see-also}

- [s3テーブル関数](../../../sql-reference/table-functions/s3.md)
