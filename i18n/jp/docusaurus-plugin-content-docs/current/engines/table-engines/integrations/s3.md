---
description: 'このエンジンは Amazon S3 エコシステムと統合します。HDFS エンジンに似ていますが、S3 固有の機能も備えています。'
sidebar_label: 'S3'
sidebar_position: 180
slug: /engines/table-engines/integrations/s3
title: 'S3 テーブルエンジン'
doc_type: 'reference'
---



# S3 テーブルエンジン {#s3-table-engine}

このエンジンは、[Amazon S3](https://aws.amazon.com/s3/) エコシステムとの連携機能を提供します。このエンジンは [HDFS](/engines/table-engines/integrations/hdfs) エンジンと似ていますが、S3 固有の機能を備えています。



## 例 {#example}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE=S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'gzip')
    SETTINGS input_format_with_names_use_header = 0;

INSERT INTO s3_engine_table VALUES ('one', 1), ('two', 2), ('three', 3);

SELECT * FROM s3_engine_table LIMIT 2;
```

```text
┌─name─┬─value─┐
│ one  │     1 │
│ two  │     2 │
└──────┴───────┘
```


## テーブルの作成 {#creating-a-table}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE = S3(path [, NOSIGN | aws_access_key_id, aws_secret_access_key,] format, [compression], [partition_strategy], [partition_columns_in_data_file])
    [PARTITION BY expr]
    [SETTINGS ...]
```

### エンジンパラメータ {#parameters}

* `path` — ファイルへのパスを含むバケットの URL。読み取り専用モードでは、次のワイルドカードをサポートします: `*`, `**`, `?`, `{abc,def}`, `{N..M}`。ここで `N`, `M` は数値、`'abc'`, `'def'` は文字列です。詳細は[下記](#wildcards-in-path)を参照してください。
* `NOSIGN` — このキーワードが認証情報の代わりに指定された場合、すべてのリクエストは署名されません。
* `format` — ファイルの[フォーマット](/sql-reference/formats#formats-overview)。
* `aws_access_key_id`, `aws_secret_access_key` — [AWS](https://aws.amazon.com/) アカウントユーザーの長期認証情報。これらを使用してリクエストの認証を行うことができます。パラメータはオプションです。認証情報が指定されていない場合は、設定ファイルの値が使用されます。詳細は [Using S3 for Data Storage](../mergetree-family/mergetree.md#table_engine-mergetree-s3) を参照してください。
* `compression` — 圧縮タイプ。サポートされる値: `none`, `gzip/gz`, `brotli/br`, `xz/LZMA`, `zstd/zst`。パラメータはオプションです。デフォルトでは、ファイル拡張子に基づいて圧縮形式を自動検出します。
* `partition_strategy` — オプション: `WILDCARD` または `HIVE`。`WILDCARD` では、パスに `{_partition_id}` を含める必要があり、これはパーティションキーに置き換えられます。`HIVE` はワイルドカードを許可せず、パスをテーブルルートとみなし、Snowflake ID をファイル名、ファイルフォーマットを拡張子として Hive スタイルのパーティションディレクトリを生成します。デフォルトは `WILDCARD` です。
* `partition_columns_in_data_file` — `HIVE` パーティション戦略でのみ使用されます。ClickHouse がデータファイル内にパーティションカラムが書き込まれていることを想定すべきかどうかを指定します。デフォルトは `false` です。
* `storage_class_name` — オプション: `STANDARD` または `INTELLIGENT_TIERING`。 [AWS S3 Intelligent Tiering](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/) を指定できます。

### データキャッシュ {#data-cache}

`S3` テーブルエンジンはローカルディスク上でのデータキャッシュをサポートします。
ファイルシステムキャッシュの設定オプションと使用方法は、この[セクション](/operations/storing-data.md/#using-local-cache)を参照してください。
キャッシュはストレージオブジェクトのパスと ETag に基づいて行われるため、ClickHouse は古いキャッシュバージョンを読み取りません。

キャッシュを有効にするには、`filesystem_cache_name = '<name>'` と `enable_filesystem_cache = 1` という設定を使用します。

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
SETTINGS filesystem_cache_name = 'cache_for_s3', enable_filesystem_cache = 1;
```

設定ファイルでキャッシュを定義する方法は 2 つあります。

1. ClickHouse の設定ファイルに以下のセクションを追加します：

```xml
<clickhouse>
    <filesystem_caches>
        <cache_for_s3>
            <path>path to cache directory</path>
            <max_size>10Gi</max_size>
        </cache_for_s3>
    </filesystem_caches>
</clickhouse>
```

2. ClickHouse の `storage_configuration` セクション（[こちら](/operations/storing-data.md/#using-local-cache)で説明）からキャッシュ設定（およびそれに伴うキャッシュストレージ）を再利用します。

### PARTITION BY {#partition-by}

`PARTITION BY` — オプションです。ほとんどの場合、パーティションキーは不要であり、必要な場合でも月単位より細かいパーティションキーが必要になることは、一般的にはありません。パーティション分割はクエリを高速化しません（ORDER BY 式とは対照的です）。細かすぎるパーティション分割は決して行うべきではありません。クライアント識別子や名前でデータをパーティション分割しないでください（その代わり、クライアント識別子または名前を ORDER BY 式の先頭のカラムにします）。

月単位でパーティション分割を行うには、`toYYYYMM(date_column)` 式を使用します。ここで、`date_column` は型が [Date](/sql-reference/data-types/date.md) の日付を格納するカラムです。この場合のパーティション名は `"YYYYMM"` 形式になります。

#### パーティション戦略 {#partition-strategy}

`WILDCARD`（デフォルト）：ファイルパス内の `{_partition_id}` ワイルドカードを実際のパーティションキーに置き換えます。読み取りはサポートされていません。


`HIVE` は読み取りおよび書き込みに対して Hive スタイルのパーティショニングを実装します。読み取りは再帰的な glob パターンを用いて実装されており、`SELECT * FROM s3('table_root/**.parquet')` と同等です。
書き込みでは、次の形式でファイルを生成します: `<prefix>/<key1=val1/key2=val2...>/<snowflakeid>.<toLower(file_format)>`。

注意: `HIVE` パーティショニング戦略を使用している場合、`use_hive_partitioning` 設定は効果を持ちません。

`HIVE` パーティショニング戦略の例:

```sql
arthur :) CREATE TABLE t_03363_parquet (year UInt16, country String, counter UInt8)
ENGINE = S3(s3_conn, filename = 't_03363_parquet', format = Parquet, partition_strategy='hive')
PARTITION BY (year, country);

arthur :) INSERT INTO t_03363_parquet VALUES
    (2022, 'USA', 1),
    (2022, 'Canada', 2),
    (2023, 'USA', 3),
    (2023, 'Mexico', 4),
    (2024, 'France', 5),
    (2024, 'Germany', 6),
    (2024, 'Germany', 7),
    (1999, 'Brazil', 8),
    (2100, 'Japan', 9),
    (2024, 'CN', 10),
    (2025, '', 11);

arthur :) select _path, * from t_03363_parquet;

    ┌─_path──────────────────────────────────────────────────────────────────────┬─year─┬─country─┬─counter─┐
 1. │ test/t_03363_parquet/year=2100/country=Japan/7329604473272971264.parquet   │ 2100 │ Japan   │       9 │
 2. │ test/t_03363_parquet/year=2024/country=France/7329604473323302912.parquet  │ 2024 │ France  │       5 │
 3. │ test/t_03363_parquet/year=2022/country=Canada/7329604473314914304.parquet  │ 2022 │ Canada  │       2 │
 4. │ test/t_03363_parquet/year=1999/country=Brazil/7329604473289748480.parquet  │ 1999 │ Brazil  │       8 │
 5. │ test/t_03363_parquet/year=2023/country=Mexico/7329604473293942784.parquet  │ 2023 │ Mexico  │       4 │
 6. │ test/t_03363_parquet/year=2023/country=USA/7329604473319108608.parquet     │ 2023 │ USA     │       3 │
 7. │ test/t_03363_parquet/year=2025/country=/7329604473327497216.parquet        │ 2025 │         │      11 │
 8. │ test/t_03363_parquet/year=2024/country=CN/7329604473310720000.parquet      │ 2024 │ CN      │      10 │
 9. │ test/t_03363_parquet/year=2022/country=USA/7329604473298137088.parquet     │ 2022 │ USA     │       1 │
10. │ test/t_03363_parquet/year=2024/country=Germany/7329604473306525696.parquet │ 2024 │ Germany │       6 │
11. │ test/t_03363_parquet/year=2024/country=Germany/7329604473306525696.parquet │ 2024 │ Germany │       7 │
    └────────────────────────────────────────────────────────────────────────────┴──────┴─────────┴─────────┘
```

### パーティション化されたデータのクエリ実行 {#querying-partitioned-data}

この例では、ClickHouse と MinIO を統合した [docker compose レシピ](https://github.com/ClickHouse/examples/tree/5fdc6ff72f4e5137e23ea075c88d3f44b0202490/docker-compose-recipes/recipes/ch-and-minio-S3) を使用します。エンドポイントと認証情報の値を差し替えることで、S3 を使って同じクエリを再現できるはずです。

`ENGINE` 設定内の S3 エンドポイントでは、S3 オブジェクト（ファイル名）の一部としてパラメータトークン `{_partition_id}` を使用しており、SELECT クエリはその結果として生成されるオブジェクト名（例: `test_3.csv`）を対象に実行されます。


:::note
この例に示されているように、パーティション化された S3 テーブルに対するクエリは
現時点では直接サポートされていませんが、S3 テーブル関数を使用して個々のパーティションに対してクエリを実行することで
実現できます。

S3 にパーティション化されたデータを書き込む主なユースケースは、そのデータを別の
ClickHouse システムに転送できるようにすることです（たとえば、オンプレミスシステムから ClickHouse
Cloud への移行など）。ClickHouse のデータセットは非常に大きいことが多く、ネットワーク
の信頼性も常に完璧とは限らないため、データセットを小さな単位（サブセット）に分割して転送するのが理にかなっており、
そのためにパーティション化した書き込みを行います。
:::

#### テーブルを作成する {#create-the-table}

```sql
CREATE TABLE p
(
    `column1` UInt32,
    `column2` UInt32,
    `column3` UInt32
)
ENGINE = S3(
-- highlight-next-line
           'http://minio:10000/clickhouse//test_{_partition_id}.csv',
           'minioadmin',
           'minioadminpassword',
           'CSV')
PARTITION BY column3
```

#### データを挿入する {#insert-data}

```sql
INSERT INTO p VALUES (1, 2, 3), (3, 2, 1), (78, 43, 45)
```

#### パーティション3から選択 {#select-from-partition-3}

:::tip
このクエリでは S3 テーブル関数を使用します
:::

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│  1 │  2 │  3 │
└────┴────┴────┘
```

#### パーティション1からのSELECT {#select-from-partition-1}

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_1.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│  3 │  2 │  1 │
└────┴────┴────┘
```

#### パーティション45からSELECTする {#select-from-partition-45}

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_45.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```

```response
┌─c1─┬─c2─┬─c3─┐
│ 78 │ 43 │ 45 │
└────┴────┴────┘
```

#### 制限事項 {#limitation}

つい `Select * from p` を試したくなるかもしれませんが、前述のとおりこのクエリは失敗します。直前のクエリを使用してください。

```sql
SELECT * FROM p
```

```response
Received exception from server (version 23.4.1):
Code: 48. DB::Exception: Received from localhost:9000. DB::Exception: Reading from a partitioned S3 storage is not implemented yet. (NOT_IMPLEMENTED)
```


## データの挿入 {#inserting-data}

行は新しいファイルにしか挿入できないことに注意してください。マージ処理やファイル分割処理はありません。いったんファイルが書き込まれると、その後の挿入は失敗します。これを回避するには、`s3_truncate_on_insert` および `s3_create_new_file_on_insert` の設定を使用します。詳細は[こちら](/integrations/s3#inserting-data)を参照してください。



## 仮想カラム {#virtual-columns}

- `_path` — ファイルへのパス。型: `LowCardinality(String)`.
- `_file` — ファイル名。型: `LowCardinality(String)`.
- `_size` — ファイルサイズ（バイト単位）。型: `Nullable(UInt64)`. サイズが不明な場合、値は `NULL`。
- `_time` — ファイルの最終更新時刻。型: `Nullable(DateTime)`. 時刻が不明な場合、値は `NULL`。
- `_etag` — ファイルの ETag。型: `LowCardinality(String)`. ETag が不明な場合、値は `NULL`。
- `_tags` — ファイルのタグ。型: `Map(String, String)`. タグが存在しない場合、値は空のマップ `{}'。

仮想カラムの詳細については[こちら](../../../engines/table-engines/index.md#table_engines-virtual_columns)を参照してください。



## 実装の詳細 {#implementation-details}

- 読み取りと書き込みは並列に実行可能
- 非サポート項目:
  - `ALTER` および `SELECT...SAMPLE` 操作。
  - インデックス。
  - [Zero-copy](../../../operations/storing-data.md#zero-copy) レプリケーションは利用自体は可能だが、サポート対象外。

  :::note Zero-copy レプリケーションは本番環境向けに十分に成熟していません
  Zero-copy レプリケーションは ClickHouse バージョン 22.8 以降ではデフォルトで無効化されています。この機能は本番環境での使用は推奨されません。
  :::



## パスでのワイルドカード {#wildcards-in-path}

`path` 引数では、bash 互換のワイルドカードを使って複数のファイルを指定できます。処理対象となるには、ファイルが存在し、パス全体のパターンに一致している必要があります。ファイルの列挙は `SELECT` の実行時に行われます（`CREATE` の時点ではありません）。

* `*` — `/` 以外の任意の文字列（空文字列を含む）に任意の長さでマッチします。
* `**` — `/` を含む任意の文字列（空文字列を含む）に任意の長さでマッチします。
* `?` — 任意の 1 文字にマッチします。
* `{some_string,another_string,yet_another_one}` — 文字列 `'some_string', 'another_string', 'yet_another_one'` のいずれかにマッチします。
* `{N..M}` — N から M までの範囲の数値（両端を含む）にマッチします。N と M には先頭にゼロを付けることができます（例: `000..078`）。

`{}` を使った構文は、[remote](../../../sql-reference/table-functions/remote.md) テーブル関数と似ています。

:::note
ファイル名の一覧に先頭にゼロを含む数値の範囲がある場合は、各桁ごとに波かっこ構文を使うか、`?` を使用してください。
:::

**ワイルドカードを使った例 1**

`file-000.csv`, `file-001.csv`, ... , `file-999.csv` という名前のファイルを対象とするテーブルを作成します:

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/my_folder/file-{000..999}.csv', 'CSV');
```

**ワイルドカードを用いた例 2**

S3 上に、次の URI を持つ複数の CSV 形式のファイルがあるとします:

* &#39;[https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some&#95;folder/some&#95;file&#95;1.csv](https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_1.csv)&#39;
* &#39;[https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some&#95;folder/some&#95;file&#95;2.csv](https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_2.csv)&#39;
* &#39;[https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some&#95;folder/some&#95;file&#95;3.csv](https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_3.csv)&#39;
* &#39;[https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another&#95;folder/some&#95;file&#95;1.csv](https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_1.csv)&#39;
* &#39;[https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another&#95;folder/some&#95;file&#95;2.csv](https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_2.csv)&#39;
* &#39;[https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another&#95;folder/some&#95;file&#95;3.csv](https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_3.csv)&#39;

これら 6 つすべてのファイルから成るテーブルを作成する方法はいくつかあります:

1. ファイル名のポストフィックスの範囲を指定する:

```sql
CREATE TABLE table_with_range (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_{1..3}', 'CSV');
```

2. `some_file_` というプレフィックスを持つすべてのファイルを取得します（両方のフォルダに、そのプレフィックスを持つ余分なファイルが存在しないことを確認してください）:

```sql
CREATE TABLE table_with_question_mark (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_?', 'CSV');
```

3. 両方のフォルダ内にあるすべてのファイルを対象とします（すべてのファイルはクエリで記述されている形式とスキーマを満たしている必要があります）:

```sql
CREATE TABLE table_with_asterisk (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/*', 'CSV');
```


## ストレージ設定 {#storage-settings}

- [s3_truncate_on_insert](/operations/settings/settings.md#s3_truncate_on_insert) - 挿入前にファイルを切り詰め（内容を削除し）できるようにします。デフォルトでは無効です。
- [s3_create_new_file_on_insert](/operations/settings/settings.md#s3_create_new_file_on_insert) - フォーマットにサフィックスがある場合、挿入ごとに新しいファイルを作成できるようにします。デフォルトでは無効です。
- [s3_skip_empty_files](/operations/settings/settings.md#s3_skip_empty_files) - 読み取り時に空のファイルをスキップできるようにします。デフォルトでは有効です。



## S3 関連の設定 {#settings}

次の設定は、クエリ実行前に設定するか、設定ファイルに記述できます。

- `s3_max_single_part_upload_size` — S3 への単一パートアップロードで使用するオブジェクトの最大サイズ。デフォルト値は `32Mb`。
- `s3_min_upload_part_size` — [S3 Multipart upload](https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html) を使用したマルチパートアップロード時にアップロードするパーツの最小サイズ。デフォルト値は `16Mb`。
- `s3_max_redirects` — 許可される S3 リダイレクトホップの最大数。デフォルト値は `10`。
- `s3_single_read_retries` — 単一の読み取り時の最大リトライ回数。デフォルト値は `4`。
- `s3_max_put_rps` — スロットルされる前の 1 秒あたりの最大 PUT リクエスト数。デフォルト値は `0`（無制限）。
- `s3_max_put_burst` — 1 秒あたりのリクエスト数制限に達する前に同時に発行できるリクエストの最大数。デフォルト値（`0`）では `s3_max_put_rps` と同じです。
- `s3_max_get_rps` — スロットルされる前の 1 秒あたりの最大 GET リクエスト数。デフォルト値は `0`（無制限）。
- `s3_max_get_burst` — 1 秒あたりのリクエスト数制限に達する前に同時に発行できるリクエストの最大数。デフォルト値（`0`）では `s3_max_get_rps` と同じです。
- `s3_upload_part_size_multiply_factor` - 単一の書き込みから S3 に対して `s3_multiply_parts_count_threshold` 個のパーツがアップロードされるたびに、`s3_min_upload_part_size` にこの係数を掛けます。デフォルト値は `2`。
- `s3_upload_part_size_multiply_parts_count_threshold` - この数のパーツが S3 にアップロードされるたびに、`s3_min_upload_part_size` は `s3_upload_part_size_multiply_factor` 倍になります。デフォルト値は `500`。
- `s3_max_inflight_parts_for_one_file` - 1 つのオブジェクトに対して同時に実行できる PUT リクエスト数を制限します。この数は制限しておくべきです。値 `0` は無制限を意味します。デフォルト値は `20`。各インフライトパーツは、最初の `s3_upload_part_size_multiply_factor` 個のパーツに対しては `s3_min_upload_part_size` のサイズのバッファを持ち、ファイルが十分に大きい場合はそれ以上になります。`upload_part_size_multiply_factor` を参照してください。デフォルト設定では、アップロードされる 1 つのファイルは、サイズが `8G` 未満の場合、`320Mb` を超えてメモリを消費しません。より大きなファイルの場合は消費量も増加します。

セキュリティ上の注意: 悪意のあるユーザーが任意の S3 URL を指定できる場合、[SSRF](https://en.wikipedia.org/wiki/Server-side_request_forgery) 攻撃を回避するには `s3_max_redirects` を 0 に設定する必要があります。あるいは、サーバー設定で `remote_host_filter` を指定する必要があります。



## エンドポイントベースの設定 {#endpoint-settings}

次の設定は、特定のエンドポイントに対して設定ファイル内で指定できます（URL の先頭プレフィックスで完全一致してマッチします）:

* `endpoint` — エンドポイントのプレフィックスを指定します。必須。
* `access_key_id` と `secret_access_key` — 指定したエンドポイントで使用する認証情報を指定します。任意。
* `use_environment_credentials` — `true` に設定すると、指定したエンドポイントについて、S3 クライアントは環境変数および [Amazon EC2](https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud) メタデータから認証情報の取得を試みます。任意。デフォルト値は `false`。
* `region` — S3 リージョン名を指定します。任意。
* `use_insecure_imds_request` — `true` に設定すると、S3 クライアントは Amazon EC2 メタデータから認証情報を取得する際に、安全でない IMDS リクエストを使用します。任意。デフォルト値は `false`。
* `expiration_window_seconds` — 有効期限ベースの認証情報が期限切れかどうかを確認するための猶予期間（秒）です。任意。デフォルト値は `120`。
* `no_sign_request` - すべての認証情報を無視し、リクエストに署名しません。パブリックなバケットへアクセスする場合に有用です。
* `header` — 指定した HTTP ヘッダーを、指定したエンドポイントへのリクエストに追加します。任意。複数回指定できます。
* `access_header` - 他のソースからの認証情報が存在しない場合に、指定した HTTP ヘッダーを指定したエンドポイントへのリクエストに追加します。
* `server_side_encryption_customer_key_base64` — 指定された場合、SSE-C で暗号化された S3 オブジェクトへアクセスするために必要なヘッダーが設定されます。任意。
* `server_side_encryption_kms_key_id` - 指定された場合、[SSE-KMS で暗号化](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html)された S3 オブジェクトへアクセスするために必要なヘッダーが設定されます。空文字列が指定された場合、AWS 管理の S3 キーが使用されます。任意。
* `server_side_encryption_kms_encryption_context` - `server_side_encryption_kms_key_id` と併せて指定された場合、SSE-KMS 用の暗号化コンテキストヘッダーが設定されます。任意。
* `server_side_encryption_kms_bucket_key_enabled` - `server_side_encryption_kms_key_id` と併せて指定された場合、SSE-KMS の S3 バケットキーを有効にするためのヘッダーが設定されます。任意で、`true` または `false` を指定できます。デフォルトでは何も設定されず（バケットレベルの設定に従います）。
* `max_single_read_retries` — 単一の読み取り処理における最大試行回数です。デフォルト値は `4`。任意。
* `max_put_rps`, `max_put_burst`, `max_get_rps` および `max_get_burst` - 特定のエンドポイントに対して、クエリ単位ではなく、そのエンドポイント専用に使用するスロットリング設定です（上記の説明を参照）。任意。

**例:**

```xml
<s3>
    <endpoint-name>
        <endpoint>https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/</endpoint>
        <!-- <access_key_id>ACCESS_KEY_ID</access_key_id> -->
        <!-- <secret_access_key>SECRET_ACCESS_KEY</secret_access_key> -->
        <!-- <region>us-west-1</region> -->
        <!-- <use_environment_credentials>false</use_environment_credentials> -->
        <!-- <use_insecure_imds_request>false</use_insecure_imds_request> -->
        <!-- <expiration_window_seconds>120</expiration_window_seconds> -->
        <!-- <no_sign_request>false</no_sign_request> -->
        <!-- <header>Authorization: Bearer SOME-TOKEN</header> -->
        <!-- <server_side_encryption_customer_key_base64>BASE64-ENCODED-KEY</server_side_encryption_customer_key_base64> -->
        <!-- <server_side_encryption_kms_key_id>KMS_KEY_ID</server_side_encryption_kms_key_id> -->
        <!-- <server_side_encryption_kms_encryption_context>KMS_ENCRYPTION_CONTEXT</server_side_encryption_kms_encryption_context> -->
        <!-- <server_side_encryption_kms_bucket_key_enabled>true</server_side_encryption_kms_bucket_key_enabled> -->
        <!-- <max_single_read_retries>4</max_single_read_retries> -->
    </endpoint-name>
</s3>
```


## アーカイブの操作 {#working-with-archives}

S3 上に、次の URI を持つ複数のアーカイブファイルがあるとします:

* &#39;[https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip](https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip)&#39;
* &#39;[https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip](https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip)&#39;
* &#39;[https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip](https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip)&#39;

これらのアーカイブからデータを抽出するには :: を使用できます。グロブは、URL 部分と、アーカイブ内のファイル名を指定する :: 以降の部分の両方で使用できます。

```sql
SELECT *
FROM s3(
   'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-1{0..2}.csv.zip :: *.csv'
);
```

:::note
ClickHouse は次の 3 種類のアーカイブ形式をサポートしています:
ZIP
TAR
7Z
ZIP および TAR アーカイブはサポートされている任意のストレージから利用できますが、7Z アーカイブは ClickHouse がインストールされているローカルのファイルシステムからのみ読み取ることができます。
:::


## パブリックバケットへのアクセス {#accessing-public-buckets}

ClickHouse は、さまざまな種類のソースから認証情報を取得しようとします。
その結果、パブリックな一部のバケットへアクセスする際に問題が発生し、クライアントが `403` エラーコードを返してしまう場合があります。
この問題は、`NOSIGN` キーワードを使用してクライアントにすべての認証情報を無視させ、リクエストに署名しないよう強制することで回避できます。

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv', NOSIGN, 'CSVWithNames');
```


## パフォーマンスの最適化 {#optimizing-performance}

`s3` 関数のパフォーマンスを最適化する方法の詳細については、[詳細ガイド](/integrations/s3/performance) を参照してください。



## 関連項目 {#see-also}

- [S3 テーブル関数](../../../sql-reference/table-functions/s3.md)
- [S3 と ClickHouse の統合](/integrations/s3)
