---
title: 'セッション設定'
sidebar_label: 'セッション設定'
slug: /operations/settings/settings
toc_max_heading_level: 2
description: 'system.settings テーブルに見つかる設定。'
---

import ExperimentalBadge from '@theme/badges/ExperimentalBadge';
import BetaBadge from '@theme/badges/BetaBadge';
import CloudAvailableBadge from '@theme/badges/CloudAvailableBadge';

<!-- Autogenerated -->
以下のすべての設定は、テーブル [system.settings](/docs/operations/system-tables/settings) でも利用可能です。これらの設定は [source](https://github.com/ClickHouse/ClickHouse/blob/master/src/Core/Settings.cpp) から自動生成されています。
## add_http_cors_header {#add_http_cors_header}



タイプ: Bool

デフォルト値: 0

HTTP CORS ヘッダーを追加します。
## additional_result_filter {#additional_result_filter}



タイプ: String

デフォルト値: 

`SELECT` クエリの結果に適用する追加のフィルター式です。
この設定は、サブクエリには適用されません。

**例**

``` sql
INSERT INTO table_1 VALUES (1, 'a'), (2, 'bb'), (3, 'ccc'), (4, 'dddd');
SElECT * FROM table_1;
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 2 │ bb   │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
```sql
SELECT *
FROM table_1
SETTINGS additional_result_filter = 'x != 2'
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
## additional_table_filters {#additional_table_filters}



タイプ: Map

デフォルト値: {}

指定されたテーブルから読み込んだ後に適用される追加のフィルター式です。

**例**

``` sql
INSERT INTO table_1 VALUES (1, 'a'), (2, 'bb'), (3, 'ccc'), (4, 'dddd');
SELECT * FROM table_1;
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 2 │ bb   │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
```sql
SELECT *
FROM table_1
SETTINGS additional_table_filters = {'table_1': 'x != 2'}
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
## aggregate_functions_null_for_empty {#aggregate_functions_null_for_empty}



タイプ: Bool

デフォルト値: 0

クエリ内のすべての集計関数を再書き換えることを有効または無効にします。それにより、[-OrNull](/sql-reference/aggregate-functions/combinators#-ornull) サフィックスが追加されます。SQL標準の互換性を確保するために有効にします。
これはクエリの再書き換えを介して実装されており、分散クエリに対して一貫した結果を得るために使用されます。

可能な値:

- 0 — 無効。
- 1 — 有効。

**例**

次の集計関数を含むクエリを考えてみましょう：
```sql
SELECT SUM(-1), MAX(0) FROM system.one WHERE 0;
```

`aggregate_functions_null_for_empty = 0` の場合、生成される結果は次のようになります：
```text
┌─SUM(-1)─┬─MAX(0)─┐
│       0 │      0 │
└─────────┴────────┘
```

`aggregate_functions_null_for_empty = 1` の場合、結果は次のようになります：
```text
┌─SUMOrNull(-1)─┬─MAXOrNull(0)─┐
│          NULL │         NULL │
└───────────────┴──────────────┘
```
## aggregation_in_order_max_block_bytes {#aggregation_in_order_max_block_bytes}



タイプ: UInt64

デフォルト値: 50000000

プライマリキーの順序で集計中に蓄積されるブロックの最大サイズ（バイト単位）です。ブロックサイズを小さくすることで、集計の最終的なマージステージをより並列化することができます。
## aggregation_memory_efficient_merge_threads {#aggregation_memory_efficient_merge_threads}



タイプ: UInt64

デフォルト値: 0

メモリ効率の良いモードで中間集計結果をマージするために使用するスレッドの数です。大きくなるほど、消費されるメモリが増えます。0 は 'max_threads' と同等です。
## allow_aggregate_partitions_independently {#allow_aggregate_partitions_independently}



タイプ: Bool

デフォルト値: 0

パーティションキーがグループバイキーに適合する場合、別々のスレッドでのパーティションの独立した集計を有効にします。パーティションの数がコアの数に近く、パーティションのサイズがほぼ同じである場合に有益です。
## allow_archive_path_syntax {#allow_archive_path_syntax}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 1

ファイル/S3 エンジン/テーブル関数は、アーカイブが正しい拡張子を持っている場合、'::' を持つパスを `<archive> :: <file>\` として解析します。
## allow_asynchronous_read_from_io_pool_for_merge_tree {#allow_asynchronous_read_from_io_pool_for_merge_tree}



タイプ: Bool

デフォルト値: 0

バックグラウンド I/O プールを使用して MergeTree テーブルから読み込みます。この設定は、I/O に制約のあるクエリのパフォーマンスを向上させる可能性があります。
## allow_changing_replica_until_first_data_packet {#allow_changing_replica_until_first_data_packet}



タイプ: Bool

デフォルト値: 0

これが有効化されている場合、ヘッジリクエストでは、最初のデータパケットを受信するまで新しい接続を開始できます。すでに進捗があってもです（ただし、進捗は `receive_data_timeout` タイムアウトまで更新されていません）。そうでなければ、最初に進捗を得た後はレプリカの変更を無効にします。
## allow_create_index_without_type {#allow_create_index_without_type}



タイプ: Bool

デフォルト値: 0

タイプなしで CREATE INDEX クエリを許可します。このクエリは無視されます。SQL 互換性テストのために作成されました。
## allow_custom_error_code_in_throwif {#allow_custom_error_code_in_throwif}



タイプ: Bool

デフォルト値: 0

関数 `throwIf()` でカスタムエラーコードを有効にします。true の場合、投げられた例外は予期しないエラーコードを持つ可能性があります。
## allow_ddl {#allow_ddl}



タイプ: Bool

デフォルト値: 1

これが true に設定されている場合、ユーザーは DDL クエリを実行できるようになります。
## allow_deprecated_database_ordinary {#allow_deprecated_database_ordinary}



タイプ: Bool

デフォルト値: 0

廃止された Ordinary エンジンを使用してデータベースを作成することを許可します。
## allow_deprecated_error_prone_window_functions {#allow_deprecated_error_prone_window_functions}



タイプ: Bool

デフォルト値: 0

廃止されたエラーの多いウィンドウ関数の使用を許可します（neighbor, runningAccumulate, runningDifferenceStartingWithFirstValue, runningDifference）。
## allow_deprecated_snowflake_conversion_functions {#allow_deprecated_snowflake_conversion_functions}



タイプ: Bool

デフォルト値: 0

`snowflakeToDateTime`、`snowflakeToDateTime64`、`dateTimeToSnowflake`、および `dateTime64ToSnowflake` 関数は廃止されており、デフォルトでは無効になっています。
代わりに、`snowflakeIDToDateTime`、`snowflakeIDToDateTime64`、`dateTimeToSnowflakeID`、および `dateTime64ToSnowflakeID` 関数を使用してください。

廃止された関数を再度有効にするには（例：移行期間中）、この設定を `true` に設定してください。
## allow_deprecated_syntax_for_merge_tree {#allow_deprecated_syntax_for_merge_tree}



タイプ: Bool

デフォルト値: 0

廃止されたエンジン定義構文で *MergeTree テーブルを作成することを許可します。
## allow_distributed_ddl {#allow_distributed_ddl}



タイプ: Bool

デフォルト値: 1

これが true に設定されている場合、ユーザーは分散 DDL クエリを実行できるようになります。
## allow_drop_detached {#allow_drop_detached}



タイプ: Bool

デフォルト値: 0

ALTER TABLE ... DROP DETACHED PART[ITION] ... クエリを許可します。
## allow_execute_multiif_columnar {#allow_execute_multiif_columnar}



タイプ: Bool

デフォルト値: 1

multiIf 関数を列指向で実行を許可します。
## allow_experimental_analyzer {#allow_experimental_analyzer}



タイプ: Bool

デフォルト値: 1

新しいクエリアナライザーを許可します。
## allow_experimental_codecs {#allow_experimental_codecs}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

これが true に設定されている場合、実験的な圧縮コーデックを指定できるようになります（しかし、現在はまだありません。このオプションは何もしません）。
## allow_experimental_database_iceberg {#allow_experimental_database_iceberg}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

実験的なデータベースエンジン Iceberg の使用を許可します。
## allow_experimental_database_materialized_postgresql {#allow_experimental_database_materialized_postgresql}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

Engine=MaterializedPostgreSQL(...) でデータベースを作成できるようにします。
## allow_experimental_dynamic_type {#allow_experimental_dynamic_type}
<BetaBadge/>


タイプ: Bool

デフォルト値: 0

[Dynamic](../../sql-reference/data-types/dynamic.md) データ型の作成を許可します。
## allow_experimental_full_text_index {#allow_experimental_full_text_index}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

これが true に設定されている場合、実験的な全文インデックスを使用できるようになります。
## allow_experimental_funnel_functions {#allow_experimental_funnel_functions}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

ファネル分析のための実験的な関数を有効にします。
## allow_experimental_hash_functions {#allow_experimental_hash_functions}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

実験的なハッシュ関数を有効にします。
## allow_experimental_inverted_index {#allow_experimental_inverted_index}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

これが true に設定されている場合、実験的な逆インデックスを使用できるようになります。
## allow_experimental_join_condition {#allow_experimental_join_condition}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

両方の左と右のテーブルからのカラムを含む不等条件での結合をサポートします。例えば `t1.y < t2.y`。
## allow_experimental_join_right_table_sorting {#allow_experimental_join_right_table_sorting}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

これが true に設定され、`join_to_sort_minimum_perkey_rows` と `join_to_sort_maximum_table_rows` の条件が満たされた場合、キーで右テーブルを再整列し、左または内部ハッシュ結合のパフォーマンスを向上させます。
## allow_experimental_json_type {#allow_experimental_json_type}
<BetaBadge/>


タイプ: Bool

デフォルト値: 0

[JSON](../../sql-reference/data-types/newjson.md) データ型の作成を許可します。
## allow_experimental_kafka_offsets_storage_in_keeper {#allow_experimental_kafka_offsets_storage_in_keeper}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

Kafka 関連のオフセットを ClickHouse Keeper に格納する実験的な機能を許可します。これが有効になると、ClickHouse Keeper パスとレプリカ名を Kafka テーブルエンジンに指定できます。その結果、通常の Kafka エンジンの代わりに、新しい種類のストレージエンジンが使用され、コミットされたオフセットは主に ClickHouse Keeper に格納されます。
## allow_experimental_kusto_dialect {#allow_experimental_kusto_dialect}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

Kusto Query Language (KQL) - SQL の代替を有効にします。
## allow_experimental_live_view {#allow_experimental_live_view}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

廃止された LIVE VIEW の作成を許可します。

可能な値:

- 0 — ライブビューの操作が無効になっています。
- 1 — ライブビューの操作が有効になっています。
## allow_experimental_materialized_postgresql_table {#allow_experimental_materialized_postgresql_table}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

MaterializedPostgreSQL テーブルエンジンを使用できるようにします。デフォルトでは無効ですが、この機能は実験的です。
## allow_experimental_nlp_functions {#allow_experimental_nlp_functions}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

自然言語処理のための実験的な関数を有効にします。
## allow_experimental_object_type {#allow_experimental_object_type}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

廃止された Object データ型を許可します。
## allow_experimental_parallel_reading_from_replicas {#allow_experimental_parallel_reading_from_replicas}
<BetaBadge/>


タイプ: UInt64

デフォルト値: 0

SELECT クエリの実行時に各シャードから最大 `max_parallel_replicas` のレプリカを使用します。読み込みは並列化され、動的に調整されます。0 - 無効、1 - 有効、失敗時には静かに無効、2 - 有効、失敗時には例外をスローします。
## allow_experimental_prql_dialect {#allow_experimental_prql_dialect}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

PRQL - SQL の代替を有効にします。
## allow_experimental_query_deduplication {#allow_experimental_query_deduplication}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

部品 UUID に基づく SELECT クエリの実験的データ重複排除。
## allow_experimental_shared_set_join {#allow_experimental_shared_set_join}
<ExperimentalBadge/>

<CloudAvailableBadge/>

タイプ: Bool

デフォルト値: 0

ClickHouse Cloud でのみ効果があります。ShareSet と SharedJoin を作成できるようにします。
## allow_experimental_statistics {#allow_experimental_statistics}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

[統計](../../engines/table-engines/mergetree-family/mergetree.md/#table_engine-mergetree-creating-a-table) を持つカラムを定義し、[統計を操作](../../engines/table-engines/mergetree-family/mergetree.md/#column-statistics)することを許可します。
## allow_experimental_time_series_table {#allow_experimental_time_series_table}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

[TimeSeries](../../engines/table-engines/integrations/time-series.md) テーブルエンジンを使用したテーブルの作成を許可します。

可能な値:

- 0 — [TimeSeries](../../engines/table-engines/integrations/time-series.md) テーブルエンジンは無効です。
- 1 — [TimeSeries](../../engines/table-engines/integrations/time-series.md) テーブルエンジンは有効です。
## allow_experimental_ts_to_grid_aggregate_function {#allow_experimental_ts_to_grid_aggregate_function}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

Prometheus のようなタイムシリーズの再サンプリングのための実験的 tsToGrid 集約関数。クラウド専用。
## allow_experimental_variant_type {#allow_experimental_variant_type}
<BetaBadge/>


タイプ: Bool

デフォルト値: 0

[Variant](../../sql-reference/data-types/variant.md) データ型の作成を許可します。
## allow_experimental_vector_similarity_index {#allow_experimental_vector_similarity_index}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

実験的なベクトル類似インデックスを許可します。
## allow_experimental_window_view {#allow_experimental_window_view}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

WINDOW VIEW を有効にします。十分に成熟していません。
## allow_general_join_planning {#allow_general_join_planning}



タイプ: Bool

デフォルト値: 1

より複雑な条件を処理できるより一般的な結合計画アルゴリズムを許可しますが、ハッシュ結合でのみ機能します。ハッシュ結合が有効でない場合は、通常の結合計画アルゴリズムが使用され、設定の値に関係なく適用されます。
## allow_get_client_http_header {#allow_get_client_http_header}



タイプ: Bool

デフォルト値: 0

`getClientHTTPHeader` 関数を使用して、現在の HTTP リクエストのヘッダーの値を取得できるようにします。これはデフォルトでは無効であり、セキュリティ上の理由からです。一部のヘッダー（例: `Cookie`）には機密情報が含まれる可能性があります。なお、`X-ClickHouse-*` 及び `Authentication` ヘッダーは常に制限されており、この関数で取得することはできません。
## allow_hyperscan {#allow_hyperscan}



タイプ: Bool

デフォルト値: 1

Hyperscan ライブラリを使用する関数を許可します。コンパイル時間が長くなる可能性があり、リソースを過剰に消費する原因となるため、無効にします。
## allow_introspection_functions {#allow_introspection_functions}



タイプ: Bool

デフォルト値: 0

クエリプロファイリングのための [イントロスペクション関数](../../sql-reference/functions/introspection.md) を有効または無効にします。

可能な値:

- 1 — イントロスペクション関数が有効。
- 0 — イントロスペクション関数が無効。

**関連情報**

- [サンプリングクエリプロファイラー](../../operations/optimizing-performance/sampling-query-profiler.md)
- システムテーブル [trace_log](/operations/system-tables/trace_log)
## allow_materialized_view_with_bad_select {#allow_materialized_view_with_bad_select}



タイプ: Bool

デフォルト値: 1

存在しないテーブルやカラムを参照する SELECT クエリを伴う CREATE MATERIALIZED VIEW を許可します。ただし、文法的には有効でなければなりません。リフレッシュ可能な MV には適用されません。MV スキーマが SELECT クエリから推測される必要がある場合（すなわち、CREATE にカラムリストや TO テーブルがない場合）には適用されません。ソーステーブルが作成される前に MV を作成する際に使用できます。
## allow_named_collection_override_by_default {#allow_named_collection_override_by_default}



タイプ: Bool

デフォルト値: 1

デフォルトで名前付きコレクションのフィールドを上書きできるようにします。
## allow_non_metadata_alters {#allow_non_metadata_alters}



タイプ: Bool

デフォルト値: 1

テーブルのメタデータだけでなく、ディスク上のデータにも影響を与えるALTERを実行できるようにします。
## allow_nonconst_timezone_arguments {#allow_nonconst_timezone_arguments}



タイプ: Bool

デフォルト値: 0

toTimeZone()、fromUnixTimestamp*()、snowflakeToDateTime*() などの特定の時間関連関数での非定数タイムゾーン引数を許可します。
## allow_nondeterministic_mutations {#allow_nondeterministic_mutations}



タイプ: Bool

デフォルト値: 0

ユーザーレベルの設定で、複製テーブルのミューテーションが `dictGet` のような非決定的関数を使用できるようにします。

たとえば、辞書はノード間で同期していない可能性があるため、それから値を引き出すミューテーションは、デフォルトで複製テーブルでは許可されません。この設定を有効にすることで、この動作が可能になり、ユーザーはすべてのノードでデータが同期していることを確認する責任を持ちます。

**例**

``` xml
<profiles>
    <default>
        <allow_nondeterministic_mutations>1</allow_nondeterministic_mutations>

        <!-- ... -->
    </default>

    <!-- ... -->

</profiles>
```
## allow_nondeterministic_optimize_skip_unused_shards {#allow_nondeterministic_optimize_skip_unused_shards}



タイプ: Bool

デフォルト値: 0

シャーディングキーに非決定的（例えば `rand` や `dictGet` のような関数）を許可します。

可能な値:

- 0 — 不許可。
- 1 — 許可。
## allow_not_comparable_types_in_comparison_functions {#allow_not_comparable_types_in_comparison_functions}



タイプ: Bool

デフォルト値: 0

比較関数 `equal/less/greater/etc` で比較できないタイプ（JSON/Object/AggregateFunction など）の使用を許可または制限します。
## allow_not_comparable_types_in_order_by {#allow_not_comparable_types_in_order_by}



タイプ: Bool

デフォルト値: 0

ORDER BY キーで比較できないタイプ（JSON/Object/AggregateFunction など）の使用を許可または制限します。
## allow_prefetched_read_pool_for_local_filesystem {#allow_prefetched_read_pool_for_local_filesystem}



タイプ: Bool

デフォルト値: 0

すべてのパーツがローカルファイルシステムにある場合、プリフェッチされたスレッドプールを優先します。
## allow_prefetched_read_pool_for_remote_filesystem {#allow_prefetched_read_pool_for_remote_filesystem}



タイプ: Bool

デフォルト値: 1

すべてのパーツがリモートファイルシステムにある場合、プリフェッチされたスレッドプールを優先します。
## allow_push_predicate_ast_for_distributed_subqueries {#allow_push_predicate_ast_for_distributed_subqueries}



タイプ: Bool

デフォルト値: 1

分析器が有効な分散サブクエリに対してASTレベルでプッシュプリディケートを許可します。
## allow_push_predicate_when_subquery_contains_with {#allow_push_predicate_when_subquery_contains_with}



タイプ: Bool

デフォルト値: 1

サブクエリに WITH 句が含まれている場合にプッシュプリディケートを許可します。
## allow_reorder_prewhere_conditions {#allow_reorder_prewhere_conditions}



タイプ: Bool

デフォルト値: 1

WHERE から PREWHERE への条件移動時に、それらを最適化するために再編成を許可します。
## allow_settings_after_format_in_insert {#allow_settings_after_format_in_insert}



タイプ: Bool

デフォルト値: 0

INSERT クエリの FORMAT の後に SETTINGS が許可されるかどうかを制御します。これを使用することは推奨されません。なぜなら、SETTINGS の一部が値として解釈される可能性があるからです。

例:

```sql
INSERT INTO FUNCTION null('foo String') SETTINGS max_threads=1 VALUES ('bar');
```

しかし、次のクエリは `allow_settings_after_format_in_insert` でのみ動作します：

```sql
SET allow_settings_after_format_in_insert=1;
INSERT INTO FUNCTION null('foo String') VALUES ('bar') SETTINGS max_threads=1;
```

可能な値:

- 0 — 不許可。
- 1 — 許可。

:::note
この設定は、使用ケースが古い構文に依存している場合、後方互換性のためにのみ使用してください。
:::
## allow_simdjson {#allow_simdjson}



タイプ: Bool

デフォルト値: 1

AVX2 命令が利用できる場合、'JSON*' 関数で simdjson ライブラリを使用できるようにします。無効にすると rapidjson が使用されます。
## allow_statistics_optimize {#allow_statistics_optimize}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

クエリを最適化するために統計を使用できるようにします。
## allow_suspicious_codecs {#allow_suspicious_codecs}



タイプ: Bool

デフォルト値: 0

これが true に設定されている場合、意味のない圧縮コーデックを指定できるようになります。
## allow_suspicious_fixed_string_types {#allow_suspicious_fixed_string_types}



タイプ: Bool

デフォルト値: 0

CREATE TABLE 文では、n > 256 の FixedString(n) 型のカラムを作成することを許可します。長さが 256 以上の FixedString は疑わしく、誤用を示す可能性が高いです。
## allow_suspicious_indices {#allow_suspicious_indices}



タイプ: Bool

デフォルト値: 0

同一の式を持つプライマリ/セカンダリインデックスとソートキーを拒否します。
## allow_suspicious_low_cardinality_types {#allow_suspicious_low_cardinality_types}



タイプ: Bool

デフォルト値: 0

8 バイト以下の固定サイズのデータ型（数値データ型および `FixedString(8_bytes_or_less)`）で [LowCardinality](../../sql-reference/data-types/lowcardinality.md) を使用することを許可または制限します。

小さな固定値の場合、`LowCardinality` を使用することは通常非効率的です。なぜなら ClickHouse が各行のために数値インデックスを保存するためです。その結果：

- ディスクスペースの使用量が増加する可能性があります。
- 辞書のサイズによっては RAM の消費が増加する可能性があります。
- 余分なコーディング/エンコーディング処理のために、一部の関数が遅くなる可能性があります。

[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)-エンジンテーブルのマージ時間が上記の理由から増加する可能性があります。

可能な値：

- 1 — `LowCardinality` の使用が制限されません。
- 0 — `LowCardinality` の使用が制限されます。
## allow_suspicious_primary_key {#allow_suspicious_primary_key}



タイプ: Bool

デフォルト値: 0

MergeTree 用の疑わしい `PRIMARY KEY` / `ORDER BY` を許可します（すなわち、SimpleAggregateFunctionなど）。
## allow_suspicious_ttl_expressions {#allow_suspicious_ttl_expressions}



タイプ: Bool

デフォルト値: 0

テーブルのカラムに依存しない TTL 式を拒否します。これは大抵、ユーザーエラーを示します。
## allow_suspicious_types_in_group_by {#allow_suspicious_types_in_group_by}



タイプ: Bool

デフォルト値: 0

GROUP BY キーで [Variant](../../sql-reference/data-types/variant.md) と [Dynamic](../../sql-reference/data-types/dynamic.md) タイプの使用を許可または制限します。
## allow_suspicious_types_in_order_by {#allow_suspicious_types_in_order_by}



タイプ: Bool

デフォルト値: 0

ORDER BY キーで [Variant](../../sql-reference/data-types/variant.md) と [Dynamic](../../sql-reference/data-types/dynamic.md) タイプの使用を許可または制限します。
## allow_suspicious_variant_types {#allow_suspicious_variant_types}



タイプ: Bool

デフォルト値: 0

CREATE TABLE 文では、類似のバリアントタイプ（例えば、異なる数値または日付型）を持つ Variant タイプを指定できるようにします。この設定を有効にすると、類似タイプの値を扱う際に曖昧さが生じる可能性があります。
## allow_unrestricted_reads_from_keeper {#allow_unrestricted_reads_from_keeper}



タイプ: Bool

デフォルト値: 0

システム.zookeeper テーブルからの制限なしの読み取りを許可します（パスに条件を付けずに）。便利ですが、zookeeper に対しては安全ではありません。
## alter_move_to_space_execute_async {#alter_move_to_space_execute_async}



タイプ: Bool

デフォルト値: 0

ALTER TABLE MOVE ... TO [DISK|VOLUME] を非同期で実行します。
## alter_partition_verbose_result {#alter_partition_verbose_result}



タイプ: Bool

デフォルト値: 0

パーティションやパーツに対する操作が成功したことを示す情報の表示を有効または無効にします。
[ATTACH PARTITION|PART](/sql-reference/statements/alter/partition#attach-partitionpart) と [FREEZE PARTITION](/sql-reference/statements/alter/partition#freeze-partition) に適用されます。

可能な値：

- 0 — 冗長性を無効にします。
- 1 — 冗長性を有効にします。

**例**

```sql
CREATE TABLE test(a Int64, d Date, s String) ENGINE = MergeTree PARTITION BY toYYYYMDECLARE(d) ORDER BY a;
INSERT INTO test VALUES(1, '2021-01-01', '');
INSERT INTO test VALUES(1, '2021-01-01', '');
ALTER TABLE test DETACH PARTITION ID '202101';

ALTER TABLE test ATTACH PARTITION ID '202101' SETTINGS alter_partition_verbose_result = 1;

┌─command_type─────┬─partition_id─┬─part_name────┬─old_part_name─┐
│ ATTACH PARTITION │ 202101       │ 202101_7_7_0 │ 202101_5_5_0  │
│ ATTACH PARTITION │ 202101       │ 202101_8_8_0 │ 202101_6_6_0  │
└──────────────────┴──────────────┴──────────────┴───────────────┘

ALTER TABLE test FREEZE SETTINGS alter_partition_verbose_result = 1;

┌─command_type─┬─partition_id─┬─part_name────┬─backup_name─┬─backup_path───────────────────┬─part_backup_path────────────────────────────────────────────┐
│ FREEZE ALL   │ 202101       │ 202101_7_7_0 │ 8           │ /var/lib/clickhouse/shadow/8/ │ /var/lib/clickhouse/shadow/8/data/default/test/202101_7_7_0 │
│ FREEZE ALL   │ 202101       │ 202101_8_8_0 │ 8           │ /var/lib/clickhouse/shadow/8/ │ /var/lib/clickhouse/shadow/8/data/default/test/202101_8_8_0 │
└──────────────┴──────────────┴──────────────┴─────────────┴───────────────────────────────┴─────────────────────────────────────────────────────────────┘
```
## alter_sync {#alter_sync}



タイプ: UInt64

デフォルト値: 1

[ALTER](../../sql-reference/statements/alter/index.md)、[OPTIMIZE](../../sql-reference/statements/optimize.md)、または [TRUNCATE](../../sql-reference/statements/truncate.md) クエリによってレプリカで実行されるアクションを待機する設定を可能にします。

可能な値：

- 0 — 待たない。
- 1 — 自分の実行を待つ。
- 2 — みんなを待つ。

クラウドのデフォルト値: `0`。

:::note
`alter_sync` は `Replicated` テーブルにのみ適用可能で、非 `Replicated` テーブルの ALTER に対しては何も行いません。
:::
## analyze_index_with_space_filling_curves {#analyze_index_with_space_filling_curves}



タイプ: Bool

デフォルト値: 1

テーブルのインデックスに空間充填曲線がある場合（例: `ORDER BY mortonEncode(x, y)` または `ORDER BY hilbertEncode(x, y)`）、クエリがその引数に条件を持つときは、その空間充填曲線をインデックス分析に使用します。
## analyzer_compatibility_join_using_top_level_identifier {#analyzer_compatibility_join_using_top_level_identifier}



タイプ: Bool

デフォルト値: 0

JOIN USING の識別子をプロジェクションから解決するよう強制します（例えば、`SELECT a + 1 AS b FROM t1 JOIN t2 USING (b)` では、`t1.a + 1 = t2.b` による結合が行われ、`t1.b = t2.b` ではありません）。
## any_join_distinct_right_table_keys {#any_join_distinct_right_table_keys}



タイプ: Bool

デフォルト値: 0

`ANY INNER|LEFT JOIN` 操作での従来の ClickHouse サーバーの動作を有効にします。

:::note
この設定は、レガシーの `JOIN` 動作に依存する使用ケースがある場合のみ使用してください。
:::

レガシーの動作が有効の場合：

- `t1 ANY LEFT JOIN t2` と `t2 ANY RIGHT JOIN t1` 操作の結果は等しくありません。なぜなら、ClickHouse は左から右のテーブルキーのマッピングに対して多対一のロジックを使用するからです。
- `ANY INNER JOIN` 操作の結果には、左のテーブルからのすべての行が含まれ、`SEMI LEFT JOIN` 操作の結果と同様です。

レガシーの動作が無効の場合：

- `t1 ANY LEFT JOIN t2` と `t2 ANY RIGHT JOIN t1` 操作の結果は等しくなります。なぜなら、ClickHouse は `ANY RIGHT JOIN` 操作に多対一のキーのマッピングを提供するロジックを使用するからです。
- `ANY INNER JOIN` 操作の結果には、左と右のテーブルそれぞれのキーごとに 1 行のみが含まれます。

可能な値：

- 0 — レガシー動作が無効。
- 1 — レガシー動作が有効。

参考：

- [JOIN の厳密性](/sql-reference/statements/select/join#settings)
## apply_deleted_mask {#apply_deleted_mask}



タイプ: Bool

デフォルト値: 1

軽量 DELETE で削除された行をフィルタリングすることを有効にします。無効にすると、クエリはそれらの行を読み取ることができるようになります。これはデバッグや「アンダーデリート」シナリオに役立ちます。
## apply_mutations_on_fly {#apply_mutations_on_fly}



タイプ: Bool

デフォルト値: 0

true の場合、データパーツにリマテリアライズされていないミューテーション（UPDATE および DELETE）が SELECT に適用されます。
## apply_settings_from_server {#apply_settings_from_server}



タイプ: Bool

デフォルト値: 1

クライアントがサーバーから設定を受け入れるべきかどうかを示します。

これは、INSERT 入力データの解析やクエリ結果のフォーマットなど、クライアント側で行われる操作にのみ影響を与えます。クエリの実行の大部分はサーバー上で行われ、この設定の影響は受けません。

通常、この設定はユーザープロファイル（users.xml または `ALTER USER` のようなクエリ）で設定されるべきであり、クライアントで設定されるべきではありません（クライアントのコマンドライン引数や `SET` クエリ、または `SELECT` クエリの `SETTINGS` セクションを介して）。クライアントを通じて、この設定を false に変更することはできますが、true に変更すること（サーバーのプロファイルに `apply_settings_from_server = false` が設定されている場合）はできません。

なお、最初は（24.12）サーバー設定（`send_settings_to_client`）がありましたが、その後、より使いやすくするためにこのクライアント設定に置き換えられました。
## asterisk_include_alias_columns {#asterisk_include_alias_columns}



タイプ: Bool

デフォルト値: 0

ワイルドカードクエリ（`SELECT *`）で [ALIAS](../../sql-reference/statements/create/table.md/#alias) カラムを含めます。

可能な値：

- 0 - 無効
- 1 - 有効
## asterisk_include_materialized_columns {#asterisk_include_materialized_columns}



タイプ: Bool

デフォルト値: 0

ワイルドカードクエリ（`SELECT *`）で [MATERIALIZED](/sql-reference/statements/create/view#materialized-view) カラムを含めます。

可能な値：

- 0 - 無効
- 1 - 有効
## async_insert {#async_insert}



タイプ: Bool

デフォルト値: 0

true の場合、INSERT クエリからのデータはキューに保存され、バックグラウンドでテーブルにフラッシュされます。wait_for_async_insert が false の場合、INSERT クエリはほぼ即座に処理されます。それ以外の場合は、データがテーブルにフラッシュされるまでクライアントが待機します。
## async_insert_busy_timeout_decrease_rate {#async_insert_busy_timeout_decrease_rate}



タイプ: Double

デフォルト値: 0.2

適応的非同期挿入タイムアウトが減少する指数成長率。
## async_insert_busy_timeout_increase_rate {#async_insert_busy_timeout_increase_rate



タイプ: Double

デフォルト値: 0.2

適応的非同期挿入タイムアウトが増加する指数成長率。
```yaml
title: '設定'
sidebar_label: '設定'
keywords: 'ClickHouse, 設定'
description: 'ClickHouseの設定に関する詳細情報'
```

## async_insert_busy_timeout_max_ms {#async_insert_busy_timeout_max_ms}

タイプ: ミリ秒

デフォルト値: 200

最初のデータが現れてからクエリごとに収集されたデータをダンプする前に待機する最大時間。

## async_insert_busy_timeout_min_ms {#async_insert_busy_timeout_min_ms}

タイプ: ミリ秒

デフォルト値: 50

async_insert_use_adaptive_busy_timeoutを通じて自動調整が有効になっている場合、最初のデータが現れてからクエリごとに収集されたデータをダンプする前に待機する最小時間。これは、適応アルゴリズムの初期値としても機能する。

## async_insert_deduplicate {#async_insert_deduplicate}

タイプ: Bool

デフォルト値: 0

複製テーブルでの非同期INSERTクエリに対して、挿入ブロックの重複排除を実行することを指定します。

## async_insert_max_data_size {#async_insert_max_data_size}

タイプ: UInt64

デフォルト値: 10485760

挿入される前に、クエリごとに収集される未解析データの最大サイズ（バイト単位）。

## async_insert_max_query_number {#async_insert_max_query_number}

タイプ: UInt64

デフォルト値: 450

挿入される前の最大INSERTクエリ数。

## async_insert_poll_timeout_ms {#async_insert_poll_timeout_ms}

タイプ: ミリ秒

デフォルト値: 10

非同期挿入キューからデータをポーリングするためのタイムアウト。

## async_insert_use_adaptive_busy_timeout {#async_insert_use_adaptive_busy_timeout}

タイプ: Bool

デフォルト値: 1

trueに設定されている場合、非同期挿入に対して適応的なビジータイムアウトを使用します。

## async_query_sending_for_remote {#async_query_sending_for_remote}

タイプ: Bool

デフォルト値: 1

リモートクエリを実行する際に、非同期接続を作成しクエリを送信を有効にします。

デフォルトで有効です。

## async_socket_for_remote {#async_socket_for_remote}

タイプ: Bool

デフォルト値: 1

リモートクエリを実行する際にソケットから非同期読み込みを有効にします。

デフォルトで有効です。

## azure_allow_parallel_part_upload {#azure_allow_parallel_part_upload}

タイプ: Bool

デフォルト値: 1

Azureマルチパートアップロードに複数のスレッドを使用します。

## azure_check_objects_after_upload {#azure_check_objects_after_upload}

タイプ: Bool

デフォルト値: 0

アップロードされた各オブジェクトをAzure Blobストレージで確認して、アップロードが成功したことを確認します。

## azure_create_new_file_on_insert {#azure_create_new_file_on_insert}

タイプ: Bool

デフォルト値: 0

Azureエンジンテーブルに挿入するたびに新しいファイルを作成することを有効または無効にします。

## azure_ignore_file_doesnt_exist {#azure_ignore_file_doesnt_exist}

タイプ: Bool

デフォルト値: 0

特定のキーを読み取る際にファイルが存在しない場合、ファイルの不在を無視します。

可能な値:
- 1 — `SELECT`が空の結果を返します。
- 0 — `SELECT`が例外をスローします。

## azure_list_object_keys_size {#azure_list_object_keys_size}

タイプ: UInt64

デフォルト値: 1000

ListObjectリクエストでバッチとして返される可能性のあるファイルの最大数。

## azure_max_blocks_in_multipart_upload {#azure_max_blocks_in_multipart_upload}

タイプ: UInt64

デフォルト値: 50000

Azureのマルチパートアップロードにおけるブロックの最大数。

## azure_max_inflight_parts_for_one_file {#azure_max_inflight_parts_for_one_file}

タイプ: UInt64

デフォルト値: 20

マルチパートアップロードリクエストで同時にロードされる部分の最大数。0は無制限を意味します。

## azure_max_single_part_copy_size {#azure_max_single_part_copy_size}

タイプ: UInt64

デフォルト値: 268435456

Azure Blobストレージにシングルパートコピーを使用してコピーするオブジェクトの最大サイズ。

## azure_max_single_part_upload_size {#azure_max_single_part_upload_size}

タイプ: UInt64

デフォルト値: 104857600

Azure Blobストレージにシングルパートアップロードを使用してアップロードするオブジェクトの最大サイズ。

## azure_max_single_read_retries {#azure_max_single_read_retries}

タイプ: UInt64

デフォルト値: 4

Azure Blobストレージでのシングルリードの最大リトライ回数。

## azure_max_unexpected_write_error_retries {#azure_max_unexpected_write_error_retries}

タイプ: UInt64

デフォルト値: 4

Azure Blobストレージでの書き込み時の予期しないエラーに対する最大リトライ回数。

## azure_max_upload_part_size {#azure_max_upload_part_size}

タイプ: UInt64

デフォルト値: 5368709120

Azure Blobストレージへのマルチパートアップロード中の部分の最大サイズ。

## azure_min_upload_part_size {#azure_min_upload_part_size}

タイプ: UInt64

デフォルト値: 16777216

Azure Blobストレージへのマルチパートアップロード中の部分の最小サイズ。

## azure_sdk_max_retries {#azure_sdk_max_retries}

タイプ: UInt64

デフォルト値: 10

Azure SDK内での最大リトライ回数。

## azure_sdk_retry_initial_backoff_ms {#azure_sdk_retry_initial_backoff_ms}

タイプ: UInt64

デフォルト値: 10

Azure SDKでのリトライ間の最小バックオフ時間（ミリ秒）。

## azure_sdk_retry_max_backoff_ms {#azure_sdk_retry_max_backoff_ms}

タイプ: UInt64

デフォルト値: 1000

Azure SDKでのリトライ間の最大バックオフ時間（ミリ秒）。

## azure_skip_empty_files {#azure_skip_empty_files}

タイプ: Bool

デフォルト値: 0

S3エンジンで空のファイルをスキップすることを有効または無効にします。

可能な値:
- 0 — 空のファイルがリクエストされたフォーマットに適合しない場合、`SELECT`が例外をスローします。
- 1 — 空のファイルの場合、`SELECT`が空の結果を返します。

## azure_strict_upload_part_size {#azure_strict_upload_part_size}

タイプ: UInt64

デフォルト値: 0

Azure Blobストレージへのマルチパートアップロード中にアップロードする部分の正確なサイズ。

## azure_throw_on_zero_files_match {#azure_throw_on_zero_files_match}

タイプ: Bool

デフォルト値: 0

グロブ展開規則に従って一致するファイルがゼロの場合、エラーをスローします。

可能な値:
- 1 — `SELECT`が例外をスローします。
- 0 — `SELECT`が空の結果を返します。

## azure_truncate_on_insert {#azure_truncate_on_insert}

タイプ: Bool

デフォルト値: 0

Azureエンジンテーブルへの挿入前のトランケートを有効または無効にします。

## azure_upload_part_size_multiply_factor {#azure_upload_part_size_multiply_factor}

タイプ: UInt64

デフォルト値: 2

azure_multiply_parts_count_thresholdからの単一の書き込みに対してアップロードされた各部分のサイズをこの係数で乗算します。

## azure_upload_part_size_multiply_parts_count_threshold {#azure_upload_part_size_multiply_parts_count_threshold}

タイプ: UInt64

デフォルト値: 500

この数の部分がAzure Blobストレージにアップロードされるたびに、azure_min_upload_part_sizeがazure_upload_part_size_multiply_factorで乗算されます。

## backup_restore_batch_size_for_keeper_multi {#backup_restore_batch_size_for_keeper_multi}

タイプ: UInt64

デフォルト値: 1000

バックアップまたは復元中に[Zoo]Keeperに対するマルチリクエストの最大バッチサイズ。

## backup_restore_batch_size_for_keeper_multiread {#backup_restore_batch_size_for_keeper_multiread}

タイプ: UInt64

デフォルト値: 10000

バックアップまたは復元中の[Zoo]Keeperに対するマルチリードリクエストの最大バッチサイズ。

## backup_restore_failure_after_host_disconnected_for_seconds {#backup_restore_failure_after_host_disconnected_for_seconds}

タイプ: UInt64

デフォルト値: 3600

BACKUP ON CLUSTERまたはRESTORE ON CLUSTER操作中にホストがこの時間の間にZooKeeper内の一時的な「alive」ノードを再作成しない場合、全体のバックアップまたは復元は失敗と見なされます。
この値は、ホストが障害後にZooKeeperに再接続するのに合理的な時間よりも大きい必要があります。
ゼロは無制限を意味します。

## backup_restore_finish_timeout_after_error_sec {#backup_restore_finish_timeout_after_error_sec}

タイプ: UInt64

デフォルト値: 180

イニシエーターが「エラー」ノードに反応するまで、他のホストが待機すべき時間。

## backup_restore_keeper_fault_injection_probability {#backup_restore_keeper_fault_injection_probability}

タイプ: Float

デフォルト値: 0

バックアップまたは復元中のKeeperリクエストの失敗の確率、おおよそ。[0.0f, 1.0f]の範囲内で有効な値です。

## backup_restore_keeper_fault_injection_seed {#backup_restore_keeper_fault_injection_seed}

タイプ: UInt64

デフォルト値: 0

0 - ランダムシード、そうでなければ設定値。

## backup_restore_keeper_max_retries {#backup_restore_keeper_max_retries}

タイプ: UInt64

デフォルト値: 1000

バックアップまたは復元操作の途中での[Zoo]Keeper操作の最大リトライ数。
一時的な[Zoo]Keeper障害によって全体の操作が失敗しないようにするために十分大きい必要があります。

## backup_restore_keeper_max_retries_while_handling_error {#backup_restore_keeper_max_retries_while_handling_error}

タイプ: UInt64

デフォルト値: 20

BACKUP ON CLUSTERまたはRESTORE ON CLUSTER操作のエラー処理中の[Zoo]Keeper操作の最大リトライ数。

## backup_restore_keeper_max_retries_while_initializing {#backup_restore_keeper_max_retries_while_initializing}

タイプ: UInt64

デフォルト値: 20

BACKUP ON CLUSTERまたはRESTORE ON CLUSTER操作の初期化中の[Zoo]Keeper操作の最大リトライ数。

## backup_restore_keeper_retry_initial_backoff_ms {#backup_restore_keeper_retry_initial_backoff_ms}

タイプ: UInt64

デフォルト値: 100

バックアップまたは復元中の[Zoo]Keeper操作の初期バックオフタイムアウト。

## backup_restore_keeper_retry_max_backoff_ms {#backup_restore_keeper_retry_max_backoff_ms}

タイプ: UInt64

デフォルト値: 5000

バックアップまたは復元中の[Zoo]Keeper操作の最大バックオフタイムアウト。

## backup_restore_keeper_value_max_size {#backup_restore_keeper_value_max_size}

タイプ: UInt64

デフォルト値: 1048576

バックアップ中の[Zoo]Keeperノードのデータの最大サイズ。

## backup_restore_s3_retry_attempts {#backup_restore_s3_retry_attempts}

タイプ: UInt64

デフォルト値: 1000

Aws::Client::RetryStrategyの設定、Aws::Clientは自動的にリトライします。0はリトライなしを意味します。これはバックアップ/復元のみに適用されます。

## cache_warmer_threads {#cache_warmer_threads}

<CloudAvailableBadge/>

タイプ: UInt64

デフォルト値: 4

ClickHouse Cloudにおいてのみ影響があります。[cache_populated_by_fetch](merge-tree-settings.md/#cache_populated_by_fetch)が有効な場合、新しいデータパーツをファイルキャッシュに推測的にダウンロードするためのバックグラウンドスレッドの数。ゼロは無効にします。

## calculate_text_stack_trace {#calculate_text_stack_trace}

タイプ: Bool

デフォルト値: 1

クエリ実行中に例外が発生した場合にテキストスタックトレースを計算します。これはデフォルトです。この設定を無効にすると、多数の間違ったクエリが実行される場合、ファズテストが遅くなる可能性があります。通常の状況では、このオプションを無効にしないでください。

## cancel_http_readonly_queries_on_client_close {#cancel_http_readonly_queries_on_client_close}

タイプ: Bool

デフォルト値: 0

クライアントが応答を待たずに接続を閉じたときに、HTTPの読み取り専用クエリ（例: SELECT）をキャンセルします。

Cloudのデフォルト値: `1`。

## cast_ipv4_ipv6_default_on_conversion_error {#cast_ipv4_ipv6_default_on_conversion_error}

タイプ: Bool

デフォルト値: 0

IPv4へのCAST演算子、IPV6型へのCAST演算子、toIPv4、toIPv6関数は、変換エラーの際に例外をスローする代わりにデフォルト値を返します。

## cast_keep_nullable {#cast_keep_nullable}

タイプ: Bool

デフォルト値: 0

[CST](/sql-reference/functions/type-conversion-functions#cast)操作の際に`Nullable`データ型を保持することを有効または無効にします。

この設定が有効で、`CAST`関数の引数が`Nullable`の場合、結果も`Nullable`型に変換されます。この設定が無効な場合、結果は常に正確な宛先型になります。

可能な値:

- 0 — `CAST`結果は指定された宛先型と厳密に一致します。
- 1 — 引数の型が`Nullable`の場合、`CAST`結果は`Nullable(DestinationDataType)`に変換されます。

**例**

次のクエリは宛先データ型に正確に変換されます:

```sql
SET cast_keep_nullable = 0;
SELECT CAST(toNullable(toInt32(0)) AS Int32) as x, toTypeName(x);
```

結果:

```text
┌─x─┬─toTypeName(CAST(toNullable(toInt32(0)), 'Int32'))─┐
│ 0 │ Int32                                             │
└───┴───────────────────────────────────────────────────┘
```

次のクエリは宛先データ型の`Nullable`変換が結果になります:

```sql
SET cast_keep_nullable = 1;
SELECT CAST(toNullable(toInt32(0)) AS Int32) as x, toTypeName(x);
```

結果:

```text
┌─x─┬─toTypeName(CAST(toNullable(toInt32(0)), 'Int32'))─┐
│ 0 │ Nullable(Int32)                                   │
└───┴───────────────────────────────────────────────────┘
```

**関連情報**

- [CAST](/sql-reference/functions/type-conversion-functions#cast)関数

## cast_string_to_dynamic_use_inference {#cast_string_to_dynamic_use_inference}

タイプ: Bool

デフォルト値: 0

StringからDynamicへの変換時に型の推論を使用します。

## check_query_single_value_result {#check_query_single_value_result}

タイプ: Bool

デフォルト値: 1

`MergeTree`ファミリーエンジンのための[CHECK TABLE](/sql-reference/statements/check-table)クエリ結果の詳細レベルを定義します。

可能な値:

- 0 — クエリはテーブルの各データパートのチェック状態を表示します。
- 1 — クエリは一般的なテーブルチェック状態を表示します。

## check_referential_table_dependencies {#check_referential_table_dependencies}

タイプ: Bool

デフォルト値: 0

DDLクエリ（DROP TABLEやRENAMEなど）が参照の依存関係を壊さないことを確認します。

## check_table_dependencies {#check_table_dependencies}

タイプ: Bool

デフォルト値: 1

DDLクエリ（DROP TABLEやRENAMEなど）が依存関係を壊さないことを確認します。

## checksum_on_read {#checksum_on_read}

タイプ: Bool

デフォルト値: 1

読み取り時にチェックサムを検証します。デフォルトでは有効であり、常に本番環境では有効にする必要があります。この設定を無効にすることで得られる利益は期待しないでください。この設定は実験やベンチマークにのみ使用される可能性があります。この設定はMergeTreeファミリーのテーブルにのみ適用され、他のテーブルエンジンやネットワーク越しのデータ受信時には常にチェックサムが検証されます。

## cloud_mode {#cloud_mode}

タイプ: Bool

デフォルト値: 0

クラウドモード。

## cloud_mode_database_engine {#cloud_mode_database_engine}

タイプ: UInt64

デフォルト値: 1

クラウドで許可されているデータベースエンジン。1 - DDLをReplicatedデータベースを使用するように書き換える、2 - DDLをSharedデータベースを使用するように書き換える。

## cloud_mode_engine {#cloud_mode_engine}

タイプ: UInt64

デフォルト値: 1

クラウドで許可されるエンジンファミリー。

- 0 - すべてを許可
- 1 - DDLを*ReplicatedMergeTreeを使用するように書き換える
- 2 - DDLをSharedMergeTreeを使用するように書き換える
- 3 - 明示的に指定されたリモートディスクがある場合を除いてSharedMergeTreeを使用するように書き換える

UInt64は公開部分を最小限にしています。

## cluster_for_parallel_replicas {#cluster_for_parallel_replicas}
<BetaBadge/>

タイプ: String

デフォルト値: 

現在のサーバーが位置するシャードのクラスター。

## collect_hash_table_stats_during_aggregation {#collect_hash_table_stats_during_aggregation}

タイプ: Bool

デフォルト値: 1

メモリ割り当てを最適化するためにハッシュテーブルの統計を収集することを有効にします。

## collect_hash_table_stats_during_joins {#collect_hash_table_stats_during_joins}

タイプ: Bool

デフォルト値: 1

メモリ割り当てを最適化するためにハッシュテーブルの統計を収集することを有効にします。

## compatibility {#compatibility}

タイプ: String

デフォルト値: 

`compatibility`設定は、ClickHouseが前のバージョンのデフォルト設定を使用することを引き起こします。前のバージョンは設定として提供されます。

設定がデフォルトでない値に設定されている場合、その設定は尊重されます（変更されていない設定のみが`compatibility`設定の影響を受けます）。

この設定はClickHouseのバージョン番号を文字列として受け取ります（例: `22.3`, `22.8`）。空の値はこの設定が無効であることを意味します。

デフォルトでは無効です。

:::note
ClickHouse Cloudでは、compatibility設定はClickHouse Cloudサポートによって設定する必要があります。設定を行うには、[ケースを開いてください](https://clickhouse.cloud/support)。
:::

## compatibility_ignore_auto_increment_in_create_table {#compatibility_ignore_auto_increment_in_create_table}

タイプ: Bool

デフォルト値: 0

真の場合は列宣言のAUTO_INCREMENTキーワードを無視します。そうでない場合はエラーを返します。これによりMySQLからの移行が簡素化されます。

## compatibility_ignore_collation_in_create_table {#compatibility_ignore_collation_in_create_table}

タイプ: Bool

デフォルト値: 1

テーブル作成時に照合を無視する互換性。

## compile_aggregate_expressions {#compile_aggregate_expressions}

タイプ: Bool

デフォルト値: 1

集約関数をネイティブコードにJITコンパイルすることを有効または無効にします。この設定を有効にすることで、パフォーマンスが向上する場合があります。

可能な値:

- 0 — 集約はJITコンパイルなしで行われます。
- 1 — 集約はJITコンパイルを使用して行われます。

**関連情報**

- [min_count_to_compile_aggregate_expression](#min_count_to_compile_aggregate_expression)

## compile_expressions {#compile_expressions}

タイプ: Bool

デフォルト値: 0

一部のスカラ関数と演算子をネイティブコードにコンパイルします。LLVMコンパイラインフラストラクチャのバグにより、AArch64マシンではnullptr参照が発生し、結果としてサーバーがクラッシュすることがあります。この設定は有効にしないでください。

## compile_sort_description {#compile_sort_description}

タイプ: Bool

デフォルト値: 1

ソート記述をネイティブコードにコンパイルします。

## connect_timeout {#connect_timeout}

タイプ: 秒

デフォルト値: 10

レプリカがない場合の接続タイムアウト。

## connect_timeout_with_failover_ms {#connect_timeout_with_failover_ms}

タイプ: ミリ秒

デフォルト値: 1000

Distributedテーブルエンジンのためのリモートサーバーへの接続タイムアウト。クラスター定義で『シャード』および『レプリカ』セクションが使用されている場合。
接続に失敗した場合、さまざまなレプリカへの接続を試みます。

## connect_timeout_with_failover_secure_ms {#connect_timeout_with_failover_secure_ms}

タイプ: ミリ秒

デフォルト値: 1000

最初の正常なレプリカを選択するための接続タイムアウト（セキュア接続用）。

## connection_pool_max_wait_ms {#connection_pool_max_wait_ms}

タイプ: ミリ秒

デフォルト値: 0

接続プールが満杯の場合の接続待機時間（ミリ秒）。

可能な値:

- 正の整数。
- 0 — 無限タイムアウト。

## connections_with_failover_max_tries {#connections_with_failover_max_tries}

タイプ: UInt64

デフォルト値: 3

Distributedテーブルエンジンの各レプリカへの接続試行の最大回数。

## convert_query_to_cnf {#convert_query_to_cnf}

タイプ: Bool

デフォルト値: 0

`true`に設定すると、`SELECT`クエリが論理和標準形（CNF）に変換されます。CNFでのクエリの書き換えがより高速に実行される場合があるシナリオがあります（詳細な説明はこの[Githubの問題](https://github.com/ClickHouse/ClickHouse/issues/11749)をご覧ください）。

以下の`SELECT`クエリが修正されないことに注意してください（デフォルトの動作）：

```sql
EXPLAIN SYNTAX
SELECT *
FROM
(
    SELECT number AS x
    FROM numbers(20)
) AS a
WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))
SETTINGS convert_query_to_cnf = false;
```

結果は次のようになります:

```response
┌─explain────────────────────────────────────────────────────────┐
│ SELECT x                                                       │
│ FROM                                                           │
│ (                                                              │
│     SELECT number AS x                                         │
│     FROM numbers(20)                                           │
│     WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15)) │
│ ) AS a                                                         │
│ WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))     │
│ SETTINGS convert_query_to_cnf = 0                              │
└────────────────────────────────────────────────────────────────┘
```

`convert_query_to_cnf`を`true`に設定すると、変更が見られます：

```sql
EXPLAIN SYNTAX
SELECT *
FROM
(
    SELECT number AS x
    FROM numbers(20)
) AS a
WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))
SETTINGS convert_query_to_cnf = true;
```

WHERE句がCNFに書き換えられることに注意してくださいが、結果セットは同一であり、論理は変わりません：

```response
┌─explain───────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ SELECT x                                                                                                              │
│ FROM                                                                                                                  │
│ (                                                                                                                     │
│     SELECT number AS x                                                                                                │
│     FROM numbers(20)                                                                                                  │
│     WHERE ((x <= 15) OR (x <= 5)) AND ((x <= 15) OR (x >= 1)) AND ((x >= 10) OR (x <= 5)) AND ((x >= 10) OR (x >= 1)) │
│ ) AS a                                                                                                                │
│ WHERE ((x >= 10) OR (x >= 1)) AND ((x >= 10) OR (x <= 5)) AND ((x <= 15) OR (x >= 1)) AND ((x <= 15) OR (x <= 5))     │
│ SETTINGS convert_query_to_cnf = 1                                                                                     │
└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

可能な値: true, false

## count_distinct_implementation {#count_distinct_implementation}

タイプ: String

デフォルト値: uniqExact

COUNT(DISTINCT ...)の構文を実行するために使用すべき`uniq*`関数の指定。

可能な値:

- [uniq](/sql-reference/aggregate-functions/reference/uniq)
- [uniqCombined](/sql-reference/aggregate-functions/reference/uniqcombined)
- [uniqCombined64](/sql-reference/aggregate-functions/reference/uniqcombined64)
- [uniqHLL12](/sql-reference/aggregate-functions/reference/uniqhll12)
- [uniqExact](/sql-reference/aggregate-functions/reference/uniqexact)

## count_distinct_optimization {#count_distinct_optimization}

タイプ: Bool

デフォルト値: 0

COUNT DISTINCTをGROUP BYのサブクエリに書き換えます。

## create_if_not_exists {#create_if_not_exists}

タイプ: Bool

デフォルト値: 0

`CREATE`ステートメントのデフォルトで`IF NOT EXISTS`を有効にします。この設定または`IF NOT EXISTS`が指定され、提供された名前のテーブルがすでに存在する場合、例外はスローされません。

## create_index_ignore_unique {#create_index_ignore_unique}

タイプ: Bool

デフォルト値: 0

UNIQUEキーワードをCREATE UNIQUE INDEXで無視します。SQL互換性テストのために作成されました。

## create_replicated_merge_tree_fault_injection_probability {#create_replicated_merge_tree_fault_injection_probability}

タイプ: Float

デフォルト値: 0

ZooKeeperにメタデータを作成した後のテーブル作成中の故障注入の確率。

## create_table_empty_primary_key_by_default {#create_table_empty_primary_key_by_default}

タイプ: Bool

デフォルト値: 0

ORDER BYおよびPRIMARY KEYが指定されていない場合に空の主キーで*MergeTreeテーブルを作成させる。

## cross_join_min_bytes_to_compress {#cross_join_min_bytes_to_compress}

タイプ: UInt64

デフォルト値: 1073741824

CROSS JOIN内で圧縮するための最小サイズ。ゼロの場合はこのしきい値を無効にします。このブロックは、行数またはバイト数のいずれかのしきい値が達成された場合に圧縮されます。

## cross_join_min_rows_to_compress {#cross_join_min_rows_to_compress}

タイプ: UInt64

デフォルト値: 10000000

CROSS JOIN内でブロックを圧縮するための最小行数。ゼロの場合はこのしきい値を無効にします。このブロックは、行数またはバイト数のいずれかのしきい値が達成された場合に圧縮されます。

## data_type_default_nullable {#data_type_default_nullable}

タイプ: Bool

デフォルト値: 0

列定義で明示的な修飾子[NULLまたはNOT NULL](/sql-reference/statements/create/table#null-or-not-null-modifiers)がないデータ型を、[Nullable](/sql-reference/data-types/nullable)として扱うことを許可します。

可能な値:

- 1 — 列定義のデータ型がデフォルトで`Nullable`に設定されます。
- 0 — 列定義のデータ型がデフォルトで`Nullable`でないように設定されます。

## database_atomic_wait_for_drop_and_detach_synchronously {#database_atomic_wait_for_drop_and_detach_synchronously}

タイプ: Bool

デフォルト値: 0

すべての`DROP`および`DETACH`クエリに`SYNC`修飾子を追加します。

可能な値:

- 0 — クエリは遅延して実行されます。
- 1 — クエリは遅延なしに実行されます。

## database_replicated_allow_explicit_uuid {#database_replicated_allow_explicit_uuid}

タイプ: UInt64

デフォルト値: 0

0 - 複製データベース内のテーブルに明示的にUUIDを指定することを許可しません。1 - 許可。2 - 許可するが、指定したUUIDを無視してランダムなUUIDを生成します。

## database_replicated_allow_heavy_create {#database_replicated_allow_heavy_create}

タイプ: Bool

デフォルト値: 0

複製データベースエンジンで長時間実行されるDDLクエリ（CREATE AS SELECTおよびPOPULATE）を許可します。この設定はDDLキューを長時間ブロックする可能性があります。

## database_replicated_allow_only_replicated_engine {#database_replicated_allow_only_replicated_engine}

タイプ: Bool

デフォルト値: 0

複製エンジンを使用するデータベースのみを作成することを許可します。

## database_replicated_allow_replicated_engine_arguments {#database_replicated_allow_replicated_engine_arguments}

タイプ: UInt64

デフォルト値: 0

0 - 複製データベース内の*MergeTreeテーブルに対してZooKeeperのパスおよびレプリカ名を明示的に指定することを許可しません。1 - 許可。2 - 許可するが、指定したパスを無視してデフォルトのものを使用します。3 - 許可し、警告をログに記録しません。

## database_replicated_always_detach_permanently {#database_replicated_always_detach_permanently}

タイプ: Bool

デフォルト値: 0

データベースエンジンが複製の場合、DETACH TABLEをDETACH TABLE PERMANENTLYとして実行します。

## database_replicated_enforce_synchronous_settings {#database_replicated_enforce_synchronous_settings}

タイプ: Bool

デフォルト値: 0

いくつかのクエリのための同期待機を強制します（詳細はdatabase_atomic_wait_for_drop_and_detach_synchronously、mutation_sync、alter_syncを参照）。これらの設定を有効にすることは推奨されません。

## database_replicated_initial_query_timeout_sec {#database_replicated_initial_query_timeout_sec}

タイプ: UInt64

デフォルト値: 300

初期DDLクエリが複製データベースにおいて前のDDLキューのエントリを処理するのを待機する時間を秒単位で設定します。

可能な値:

- 正の整数。
- 0 — 無限。

## decimal_check_overflow {#decimal_check_overflow}

タイプ: Bool

デフォルト値: 1

10進数の演算/比較操作のオーバーフローをチェックします。

## deduplicate_blocks_in_dependent_materialized_views {#deduplicate_blocks_in_dependent_materialized_views}

タイプ: Bool

デフォルト値: 0

複製テーブルからデータを受信するマテリアライズドビューに対する重複排除チェックを有効または無効にします。

可能な値:

      0 — 無効。
      1 — 有効。

使用法

デフォルトでは、マテリアライズドビューに対する重複排除は行われませんが、ソーステーブル内で実行されます。
挿入されたブロックがソーステーブル内での重複排除によりスキップされた場合、添付されたマテリアライズドビューへの挿入はありません。この動作は、マテリアライズドビューの集約後に挿入されたブロックが同じである場合でも機能しますが、ソーステーブルへの異なる挿入から派生したものです。
同時に、この動作は`INSERT`の冪等性を「壊します」。メインテーブルへの`INSERT`が成功し、マテリアライズドビューへの`INSERT`が失敗（例: ClickHouse Keeperとの通信障害のため）した場合、クライアントはエラーを取得し、操作をリトライできます。しかし、マテリアライズドビューは、源テーブルでの重複排除により、2回目の挿入を受け入れません。`deduplicate_blocks_in_dependent_materialized_views`設定を有効にすることで、この動作を変更できます。リトライ時に、マテリアライズドビューは再挿入を受け入れ、ソーステーブルのチェック結果を無視して重複排除のチェックを自身で行い、最初の失敗により失われた行を挿入します。

## default_materialized_view_sql_security {#default_materialized_view_sql_security}

タイプ: SQLSecurityType

デフォルト値: DEFINER

マテリアライズドビューを作成する際のSQL SECURITYオプションのデフォルト値を設定します。 [SQLセキュリティの詳細についてはこちら](../../sql-reference/statements/create/view.md/#sql_security)。

デフォルトの値は`DEFINER`です。

## default_max_bytes_in_join {#default_max_bytes_in_join}

タイプ: UInt64

デフォルト値: 1000000000

制限が必要だがmax_bytes_in_joinが設定されていない場合の右側テーブルの最大サイズ。

## default_normal_view_sql_security {#default_normal_view_sql_security}

タイプ: SQLSecurityType

デフォルト値: INVOKER

通常のビューを作成する際のデフォルトの`SQL SECURITY`オプションを設定します。 [SQLセキュリティの詳細についてはこちら](../../sql-reference/statements/create/view.md/#sql_security)。

デフォルトの値は`INVOKER`です。

## default_table_engine {#default_table_engine}

タイプ: DefaultTableEngine

デフォルト値: MergeTree

`CREATE`ステートメントで`ENGINE`が設定されていない場合に使用されるデフォルトのテーブルエンジン。

可能な値:

- 有効なテーブルエンジン名を表す文字列。

Cloudのデフォルト値: `SharedMergeTree`。

**例**

クエリ:

```sql
SET default_table_engine = 'Log';

SELECT name, value, changed FROM system.settings WHERE name = 'default_table_engine';
```

結果:

```response
┌─name─────────────────┬─value─┬─changed─┐
│ default_table_engine │ Log   │       1 │
└──────────────────────┴───────┴─────────┘
```

この例では、`Engine`を指定しない新しいテーブルはすべて`Log`テーブルエンジンを使用します。

クエリ:

```sql
CREATE TABLE my_table (
    x UInt32,
    y UInt32
);

SHOW CREATE TABLE my_table;
```

結果:

```response
┌─statement────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.my_table
(
    `x` UInt32,
    `y` UInt32
)
ENGINE = Log
└──────────────────────────────────────────────────────────────────────────┘
```

## default_temporary_table_engine {#default_temporary_table_engine}

Type: DefaultTableEngine

Default value: Memory

一時テーブル用の `default_table_engine` と同じです。

この例では、`Engine` を指定しない新しい一時テーブルは `Log` テーブルエンジンを使用します：

Query:

```sql
SET default_temporary_table_engine = 'Log';

CREATE TEMPORARY TABLE my_table (
    x UInt32,
    y UInt32
);

SHOW CREATE TEMPORARY TABLE my_table;
```

Result:

```response
┌─statement────────────────────────────────────────────────────────────────┐
│ CREATE TEMPORARY TABLE default.my_table
(
    `x` UInt32,
    `y` UInt32
)
ENGINE = Log
└──────────────────────────────────────────────────────────────────────────┘
```

## default_view_definer {#default_view_definer}

Type: String

Default value: CURRENT_USER

ビューを作成する際にデフォルトの `DEFINER` オプションを設定できます。[SQLセキュリティの詳細](../../sql-reference/statements/create/view.md/#sql_security)。

デフォルト値は `CURRENT_USER` です。

## describe_compact_output {#describe_compact_output}

Type: Bool

Default value: 0

trueの場合、DESCRIBE クエリの結果にカラム名とタイプのみを含めます。

## describe_extend_object_types {#describe_extend_object_types}

Type: Bool

Default value: 0

DESCRIBE クエリで Object 型のカラムの具体的な型を推測します。

## describe_include_subcolumns {#describe_include_subcolumns}

Type: Bool

Default value: 0

[DESCRIBE](../../sql-reference/statements/describe-table.md) クエリのためにサブカラムを記述することを可能にします。例えば、[Tuple](../../sql-reference/data-types/tuple.md)のメンバーや、[Map](/sql-reference/data-types/map#reading-subcolumns-of-map)、[Nullable](../../sql-reference/data-types/nullable.md/#finding-null)や[Array](../../sql-reference/data-types/array.md/#array-size)データ型のサブカラム。

可能な値：

- 0 — サブカラムは `DESCRIBE` クエリに含まれません。
- 1 — サブカラムは `DESCRIBE` クエリに含まれます。

**Example**

[DESCRIBE](../../sql-reference/statements/describe-table.md) ステートメントの例を参照してください。

## describe_include_virtual_columns {#describe_include_virtual_columns}

Type: Bool

Default value: 0

trueの場合、テーブルの仮想カラムが DESCRIBE クエリの結果に含まれます。

## dialect {#dialect}

Type: Dialect

Default value: clickhouse

クエリを解析するために使用されるダイアレクト。

## dictionary_validate_primary_key_type {#dictionary_validate_primary_key_type}

Type: Bool

Default value: 0

辞書のための主キータイプを検証します。デフォルトではシンプルなレイアウトの ID タイプは UInt64 に暗黙的に変換されます。

## distinct_overflow_mode {#distinct_overflow_mode}

Type: OverflowMode

Default value: throw

制限を超えた場合に何をするか。

## distributed_aggregation_memory_efficient {#distributed_aggregation_memory_efficient}

Type: Bool

Default value: 1

分散集計でのメモリ保存モードが有効です。

## distributed_background_insert_batch {#distributed_background_insert_batch}

Type: Bool

Default value: 0

挿入されたデータをバッチで送信することを有効または無効にします。

バッチ送信が有効な場合、[Distributed](../../engines/table-engines/special/distributed.md) テーブルエンジンは、個別に送信するのではなく、一度の操作で挿入されたデータの複数のファイルを送信しようとします。バッチ送信は、サーバーやネットワークリソースをより有効活用することでクラスタのパフォーマンスを向上させます。

可能な値：

- 1 — 有効。
- 0 — 無効。

## distributed_background_insert_max_sleep_time_ms {#distributed_background_insert_max_sleep_time_ms}

Type: Milliseconds

Default value: 30000

[Distributed](../../engines/table-engines/special/distributed.md) テーブルエンジンがデータを送信するための最大インターバル。 [distributed_background_insert_sleep_time_ms](#distributed_background_insert_sleep_time_ms) 設定で設定されたインターバルの指数的成長を制限します。

可能な値：

- 正の整数のミリ秒。

## distributed_background_insert_sleep_time_ms {#distributed_background_insert_sleep_time_ms}

Type: Milliseconds

Default value: 100

[Distributed](../../engines/table-engines/special/distributed.md) テーブルエンジンがデータを送信するためのベースインターバル。実際のインターバルはエラーが発生した場合、指数的に増加します。

可能な値：

- 正の整数のミリ秒。

## distributed_background_insert_split_batch_on_failure {#distributed_background_insert_split_batch_on_failure}

Type: Bool

Default value: 0

失敗した場合にバッチの分割を有効または無効にします。

特定のバッチをリモートシャードに送信する際、`Memory limit exceeded` やその他の類似のエラーのために失敗する場合があります。この場合、再試行しても役に立たない（これにより、テーブルへの分散送信が停止します）が、そのバッチからファイルを1つずつ送信すれば INSERT に成功する場合があります。

この設定を `1` にすることで、そうしたバッチに対するバッチ処理が無効になります（つまり、失敗したバッチについては `distributed_background_insert_batch` を一時的に無効にします）。

可能な値：

- 1 — 有効。
- 0 — 無効。

:::note
この設定は異常終了したサーバー（マシン）のために発生した壊れたバッチにも影響します（`fsync_after_insert`/`fsync_directories` がない場合）。
:::

:::note
自動バッチ分割に依存すべきではありません。パフォーマンスに悪影響を及ぼす可能性があります。
:::

## distributed_background_insert_timeout {#distributed_background_insert_timeout}

Type: UInt64

Default value: 0

分散への挿入クエリのタイムアウト。この設定は、insert_distributed_sync が有効な場合にのみ使用されます。ゼロ値はタイムアウトなしを意味します。

## distributed_cache_bypass_connection_pool {#distributed_cache_bypass_connection_pool}

<CloudAvailableBadge/>

Type: Bool

Default value: 0

ClickHouse Cloud でのみ効果があります。分散キャッシュ接続プールをバイパスします。

## distributed_cache_connect_max_tries {#distributed_cache_connect_max_tries}

<CloudAvailableBadge/>

Type: UInt64

Default value: 20

ClickHouse Cloud でのみ効果があります。接続に失敗した場合の分散キャッシュへの接続試行回数。

## distributed_cache_data_packet_ack_window {#distributed_cache_data_packet_ack_window}

<CloudAvailableBadge/>

Type: UInt64

Default value: 5

ClickHouse Cloud でのみ効果があります。単一の分散キャッシュ読み取り要求における DataPacket シーケンスに対する ACK を送信するためのウィンドウ。

## distributed_cache_discard_connection_if_unread_data {#distributed_cache_discard_connection_if_unread_data}

<CloudAvailableBadge/>

Type: Bool

Default value: 1

ClickHouse Cloud でのみ効果があります。未読データがある場合に接続を廃棄します。

## distributed_cache_fetch_metrics_only_from_current_az {#distributed_cache_fetch_metrics_only_from_current_az}

<CloudAvailableBadge/>

Type: Bool

Default value: 1

ClickHouse Cloud でのみ効果があります。system.distributed_cache_metrics、system.distributed_cache_events からのみ現在のアベイラビリティゾーンからメトリクスを取得します。

## distributed_cache_log_mode {#distributed_cache_log_mode}

<CloudAvailableBadge/>

Type: DistributedCacheLogMode

Default value: on_error

ClickHouse Cloud でのみ効果があります。system.distributed_cache_log への書き込みモード。

## distributed_cache_max_unacked_inflight_packets {#distributed_cache_max_unacked_inflight_packets}

<CloudAvailableBadge/>

Type: UInt64

Default value: 10

ClickHouse Cloud でのみ効果があります。単一の分散キャッシュ読み取り要求における未確認のインフライトパケットの最大数。

## distributed_cache_min_bytes_for_seek {#distributed_cache_min_bytes_for_seek}

<CloudAvailableBadge/>

Type: Bool

Default value: 0

ClickHouse Cloud でのみ効果があります。分散キャッシュでシークを行うための最小バイト数です。

## distributed_cache_pool_behaviour_on_limit {#distributed_cache_pool_behaviour_on_limit}

<CloudAvailableBadge/>

Type: DistributedCachePoolBehaviourOnLimit

Default value: wait

ClickHouse Cloud でのみ効果があります。プール制限に達したときの分散キャッシュ接続の動作を識別します。

## distributed_cache_read_alignment {#distributed_cache_read_alignment}

<CloudAvailableBadge/>

Type: UInt64

Default value: 0

ClickHouse Cloud でのみ効果があります。テスト目的のための設定で、変更しないでください。

## distributed_cache_receive_response_wait_milliseconds {#distributed_cache_receive_response_wait_milliseconds}

<CloudAvailableBadge/>

Type: UInt64

Default value: 60000

ClickHouse Cloud でのみ効果があります。分散キャッシュからリクエストのデータを受け取るための待機時間（ミリ秒）。

## distributed_cache_receive_timeout_milliseconds {#distributed_cache_receive_timeout_milliseconds}

<CloudAvailableBadge/>

Type: UInt64

Default value: 10000

ClickHouse Cloud でのみ効果があります。分散キャッシュからのレスポンスを受け取るための待機時間（ミリ秒）。

## distributed_cache_throw_on_error {#distributed_cache_throw_on_error}

<CloudAvailableBadge/>

Type: Bool

Default value: 0

ClickHouse Cloud でのみ効果があります。分散キャッシュとの通信中に発生した例外や分散キャッシュから受け取った例外を再スローします。そうでなければ、エラー時に分散キャッシュをスキップします。

## distributed_cache_wait_connection_from_pool_milliseconds {#distributed_cache_wait_connection_from_pool_milliseconds}

<CloudAvailableBadge/>

Type: UInt64

Default value: 100

ClickHouse Cloud でのみ効果があります。分散キャッシュプール制限の動作が wait の場合、接続プールから接続を受け取るための待機時間（ミリ秒）。

## distributed_connections_pool_size {#distributed_connections_pool_size}

Type: UInt64

Default value: 1024

単一の Distributed テーブルに対するすべてのクエリのリモートサーバーとの同時接続の最大数。クラスタ内のサーバーの数以上に設定することをお勧めします。

## distributed_ddl_entry_format_version {#distributed_ddl_entry_format_version}

Type: UInt64

Default value: 5

分散 DDL (ON CLUSTER) クエリの互換性バージョン。

## distributed_ddl_output_mode {#distributed_ddl_output_mode}

Type: DistributedDDLOutputMode

Default value: throw

分散 DDL クエリ結果のフォーマットを設定します。

可能な値：

- `throw` — クエリが完了したすべてのホストについてのクエリ実行状況の結果セットを返します。もしクエリが一部のホストで失敗した場合、最初の例外を再スローします。すべてのホストでクエリがまだ完了しておらず [distributed_ddl_task_timeout](#distributed_ddl_task_timeout) を超えた場合、`TIMEOUT_EXCEEDED` 例外をスローします。
- `none` — `throw` と似ていますが、分散 DDL クエリは結果セットを返しません。
- `null_status_on_timeout` — クエリが特定のホストでまだ完了していない場合、結果セットの一部の行で実行状況として `NULL` を返します。
- `never_throw` — `TIMEOUT_EXCEEDED` をスローせず、クエリが一部のホストで失敗した場合に例外を再スローしません。
- `none_only_active` - `none` と似ていますが、`Replicated` データベースの非アクティブレプリカを待たず、実行します。注意: このモードでは、クエリが一部のレプリカで実行されなかったことが判明せず、バックグラウンドで実行されます。
- `null_status_on_timeout_only_active` — `null_status_on_timeout` と似ていますが、`Replicated` データベースの非アクティブレプリカを待たず、実行します。
- `throw_only_active` — `throw` と似ていますが、`Replicated` データベースの非アクティブレプリカを待たず、実行します。

クラウドのデフォルト値: `none`。

## distributed_ddl_task_timeout {#distributed_ddl_task_timeout}

Type: Int64

Default value: 180

クラスタ内のすべてのホストからのDDLクエリ応答のタイムアウトを設定します。DDLリクエストがすべてのホストで実行されていない場合、応答にはタイムアウトエラーが含まれ、リクエストは非同期モードで実行されます。負の値は無限を意味します。

可能な値：

- 正の整数。
- 0 — 非同期モード。
- 負の整数 — 無限のタイムアウト。

## distributed_foreground_insert {#distributed_foreground_insert}

Type: Bool

Default value: 0

[Distributed](/engines/table-engines/special/distributed) テーブルへの同期データ挿入を有効または無効にします。

デフォルトでは、`Distributed` テーブルにデータを挿入すると、ClickHouse サーバーはバックグラウンドモードでクラスター ノードにデータを送信します。 `distributed_foreground_insert=1` の場合、データは同期的に処理され、`INSERT` 操作はすべてのシャードにデータが保存されてから（`internal_replication` が true の場合、各シャードの少なくとも1つのレプリカ）成功します。

可能な値：

- 0 — データはバックグラウンドモードで挿入されます。
- 1 — データは同期モードで挿入されます。

クラウドのデフォルト値: `1`。

**See Also**

- [Distributed Table Engine](/engines/table-engines/special/distributed)
- [Managing Distributed Tables](/sql-reference/statements/system#managing-distributed-tables)

## distributed_group_by_no_merge {#distributed_group_by_no_merge}

Type: UInt64

Default value: 0

分散クエリ処理のために異なるサーバーからの集計状態をマージしないようにします。特定のシャード上に異なるキーがあることが確実である場合に使用できます。

可能な値：

- `0` — 無効（最終クエリ処理はイニシエーター ノードで行われます）。
- `1` - 分散クエリ処理のために異なるサーバーからの集計状態をマージしないようにします（クエリはシャードで完全に処理され、イニシエーターはデータをプロキシします）。 

**Example**

```sql
SELECT *
FROM remote('127.0.0.{2,3}', system.one)
GROUP BY dummy
LIMIT 1
SETTINGS distributed_group_by_no_merge = 1
FORMAT PrettyCompactMonoBlock

┌─dummy─┐
│     0 │
│     0 │
└───────┘
```

```sql
SELECT *
FROM remote('127.0.0.{2,3}', system.one)
GROUP BY dummy
LIMIT 1
SETTINGS distributed_group_by_no_merge = 2
FORMAT PrettyCompactMonoBlock

┌─dummy─┐
│     0 │
└───────┘
```

## distributed_insert_skip_read_only_replicas {#distributed_insert_skip_read_only_replicas}

Type: Bool

Default value: 0

分散テーブルへの `INSERT` クエリのために読み取り専用レプリカをスキップすることを有効にします。

可能な値：

- 0 — 通常の `INSERT` を実行し、読み取り専用レプリカに到達すると失敗します。
- 1 — イニシエーターがデータをシャードに送信する前に、読み取り専用レプリカをスキップします。

## distributed_product_mode {#distributed_product_mode}

Type: DistributedProductMode

Default value: deny

[分散サブクエリ](../../sql-reference/operators/in.md)の動作を変更します。

ClickHouse は、クエリが分散テーブルの生産物を含む場合、つまり、分散テーブルに対するクエリが分散テーブルに対する非 GLOBAL サブクエリを含む場合にこの設定を適用します。

制限事項：

- IN および JOIN サブクエリにのみ適用されます。
- FROM セクションが 1 つ以上のシャードを持つ分散テーブルを使用している場合にのみ適用されます。
- サブクエリが 1 つ以上のシャードを含む分散テーブルに関連している場合。
- テーブル値の [remote](../../sql-reference/table-functions/remote.md) 関数には使用されません。

可能な値：

- `deny` — デフォルト値。これらのタイプのサブクエリの使用を禁止します（"Double-distributed in/JOIN subqueries is denied" 例外を返します）。
- `local` — 再帰サブクエリのデータベースとテーブルを宛先サーバー（シャード）のローカルなものと置き換え、通常の `IN`/`JOIN` を残します。
- `global` — `IN`/`JOIN` クエリを `GLOBAL IN`/`GLOBAL JOIN` に置き換えます。
- `allow` — これらのタイプのサブクエリの使用を許可します。

## distributed_push_down_limit {#distributed_push_down_limit}

Type: UInt64

Default value: 1

各シャードでの [LIMIT](#limit) の適用を有効または無効にします。

これにより、次のことを回避できます：

- ネットワークを介して余分な行を送信する。
- イニシエーターでリミットの後ろの行を処理する。

21.9 バージョン以降、この設定を変更しても不正確な結果を取得することはできません。なぜなら、`distributed_push_down_limit` は以下の条件の一つが満たされる場合のみ、クエリの実行を変更するからです：

- [distributed_group_by_no_merge](#distributed_group_by_no_merge) > 0。
- クエリに `GROUP BY` / `DISTINCT` / `LIMIT BY` が **含まれていない** が、`ORDER BY` / `LIMIT` が含まれている。
- クエリに `GROUP BY` / `DISTINCT` / `LIMIT BY` が `ORDER BY` / `LIMIT` で含まれており、次の条件が満たされます：
    - [optimize_skip_unused_shards](#optimize_skip_unused_shards) が有効。
    - [optimize_distributed_group_by_sharding_key](#optimize_distributed_group_by_sharding_key) が有効。

可能な値：

- 0 — 無効。
- 1 — 有効。

さらに詳しくは：

- [distributed_group_by_no_merge](#distributed_group_by_no_merge)
- [optimize_skip_unused_shards](#optimize_skip_unused_shards)
- [optimize_distributed_group_by_sharding_key](#optimize_distributed_group_by_sharding_key)

## distributed_replica_error_cap {#distributed_replica_error_cap}

Type: UInt64

Default value: 1000

- Type: unsigned int
- Default value: 1000

各レプリカのエラーカウントはこの値に制限されており、単一のレプリカが過剰なエラーを蓄積することを防ぎます。

さらに詳しく：

- [load_balancing](#load_balancing-round_robin)
- [テーブルエンジン Distributed](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_half_life](#distributed_replica_error_half_life)
- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)

## distributed_replica_error_half_life {#distributed_replica_error_half_life}

Type: Seconds

Default value: 60

- Type: seconds
- Default value: 60 seconds

分散テーブルのエラーがゼロになる速度を制御します。レプリカが一時的に利用できなくなり、5つのエラーを蓄積し、`distributed_replica_error_half_life` が1秒に設定されている場合、レプリカは最後のエラーから3秒後に正常と見なされます。

さらに詳しく：

- [load_balancing](#load_balancing-round_robin)
- [テーブルエンジン Distributed](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_cap](#distributed_replica_error_cap)
- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)

## distributed_replica_max_ignored_errors {#distributed_replica_max_ignored_errors}

Type: UInt64

Default value: 0

- Type: unsigned int
- Default value: 0

レプリカを選択する際に無視されるエラーの数（`load_balancing` アルゴリズムに従って）。

さらに詳しく：

- [load_balancing](#load_balancing-round_robin)
- [テーブルエンジン Distributed](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_cap](#distributed_replica_error_cap)
- [distributed_replica_error_half_life](#distributed_replica_error_half_life)

## do_not_merge_across_partitions_select_final {#do_not_merge_across_partitions_select_final}

Type: Bool

Default value: 0

データを選択する際に、1つのパーティション内でのみ部分をマージします。

## empty_result_for_aggregation_by_constant_keys_on_empty_set {#empty_result_for_aggregation_by_constant_keys_on_empty_set}

Type: Bool

Default value: 1

空のセットに対して定数キーで集計する場合、空の結果を返します。

## empty_result_for_aggregation_by_empty_set {#empty_result_for_aggregation_by_empty_set}

Type: Bool

Default value: 0

空のセットに対してキーなしで集計する場合、空の結果を返します。

## enable_adaptive_memory_spill_scheduler {#enable_adaptive_memory_spill_scheduler}
<ExperimentalBadge/>

Type: Bool

Default value: 0

プロセッサにデータを外部ストレージに適応的にスピルさせます。現在、グレース結合がサポートされています。

## enable_blob_storage_log {#enable_blob_storage_log}

Type: Bool

Default value: 1

blob ストレージ操作に関する情報を `system.blob_storage_log` テーブルに書き込みます。

## enable_deflate_qpl_codec {#enable_deflate_qpl_codec}

Type: Bool

Default value: 0

有効にすると、DEFLATE_QPL コーデックを使用してカラムを圧縮できます。

## enable_early_constant_folding {#enable_early_constant_folding}

Type: Bool

Default value: 1

関数とサブクエリの結果を分析し、そこに定数がある場合はクエリを再構築するクエリ最適化を有効にします。

## enable_extended_results_for_datetime_functions {#enable_extended_results_for_datetime_functions}

Type: Bool

Default value: 0

次の型の結果を返すことを有効または無効にします：

- `Date32`（`Date` 型と比較して拡張範囲）のための関数 [toStartOfYear](../../sql-reference/functions/date-time-functions.md/#tostartofyear)、[toStartOfISOYear](../../sql-reference/functions/date-time-functions.md/#tostartofisoyear)、[toStartOfQuarter](../../sql-reference/functions/date-time-functions.md/#tostartofquarter)、[toStartOfMonth](../../sql-reference/functions/date-time-functions.md/#tostartofmonth)、[toLastDayOfMonth](../../sql-reference/functions/date-time-functions.md/#tolastdayofmonth)、[toStartOfWeek](../../sql-reference/functions/date-time-functions.md/#tostartofweek)、[toLastDayOfWeek](../../sql-reference/functions/date-time-functions.md/#tolastdayofweek) および [toMonday](../../sql-reference/functions/date-time-functions.md/#tomonday)。
- `DateTime64`（`DateTime` 型と比較して拡張範囲）のための関数 [toStartOfDay](../../sql-reference/functions/date-time-functions.md/#tostartofday)、[toStartOfHour](../../sql-reference/functions/date-time-functions.md/#tostartofhour)、[toStartOfMinute](../../sql-reference/functions/date-time-functions.md/#tostartofminute)、[toStartOfFiveMinutes](../../sql-reference/functions/date-time-functions.md/#tostartoffiveminutes)、[toStartOfTenMinutes](../../sql-reference/functions/date-time-functions.md/#tostartoftenminutes)、[toStartOfFifteenMinutes](../../sql-reference/functions/date-time-functions.md/#tostartoffifteenminutes) および [timeSlot](../../sql-reference/functions/date-time-functions.md/#timeslot)。

可能な値：

- 0 — 関数はすべての引数の型について `Date` または `DateTime` を返します。
- 1 — 関数は `Date32` または `DateTime64` 引数に対して `Date32` または `DateTime64` を返し、それ以外は `Date` または `DateTime` を返します。

## enable_filesystem_cache {#enable_filesystem_cache}

Type: Bool

Default value: 1

リモートファイルシステムのキャッシュを使用します。この設定はディスクのキャッシュをオン/オフにするものではなく（それはディスク設定で行う必要があります）、意図的にいくつかのクエリに対してキャッシュをバイパスすることを可能にします。

## enable_filesystem_cache_log {#enable_filesystem_cache_log}

Type: Bool

Default value: 0

各クエリのファイルシステムキャッシングログを記録できるようにします。

## enable_filesystem_cache_on_write_operations {#enable_filesystem_cache_on_write_operations}

Type: Bool

Default value: 0

書き込み操作時にキャッシュに書き込みます。この設定が機能するには、ディスク設定にも追加する必要があります。

## enable_filesystem_read_prefetches_log {#enable_filesystem_read_prefetches_log}

Type: Bool

Default value: 0

クエリ中に `system.filesystem` の prefetch_log にログを記録します。テストやデバッグのためにのみ使用し、デフォルトで有効にすることは推奨しません。

## enable_global_with_statement {#enable_global_with_statement}

Type: Bool

Default value: 1

UNION クエリやすべてのサブクエリに WITH ステートメントを伝播します。

## enable_http_compression {#enable_http_compression}

Type: Bool

Default value: 0

HTTP リクエストに対する応答データの圧縮を有効または無効にします。

詳細については、[HTTP インターフェースの説明](../../interfaces/http.md)を参照してください。

可能な値：

- 0 — 無効。
- 1 — 有効。

## enable_job_stack_trace {#enable_job_stack_trace}

Type: Bool

Default value: 1

ジョブが例外を発生させたときに、ジョブクリエイターのスタックトレースを出力します。

## enable_lightweight_delete {#enable_lightweight_delete}

Type: Bool

Default value: 1

MergeTree テーブル用の軽量 DELETE 変異を有効にします。

## enable_memory_bound_merging_of_aggregation_results {#enable_memory_bound_merging_of_aggregation_results}

Type: Bool

Default value: 1

集計のためにメモリバウンド結合戦略を有効にします。

## enable_multiple_prewhere_read_steps {#enable_multiple_prewhere_read_steps}

Type: Bool

Default value: 1

WHERE から PREWHERE に多くの条件を移動させ、AND で組み合わされた複数の条件がある場合に、ディスクからの読み取りおよびフィルタリングを複数のステップで行います。

## enable_named_columns_in_function_tuple {#enable_named_columns_in_function_tuple}

Type: Bool

Default value: 0

すべての名前がユニークで、引用符なしの識別子として扱うことができる場合、関数 tuple() で名前付きタプルを生成します。

## enable_optimize_predicate_expression {#enable_optimize_predicate_expression}

Type: Bool

Default value: 1

`SELECT` クエリでの述語プッシュダウンを有効にします。

述語プッシュダウンは、分散クエリのネットワークトラフィックを大幅に削減する可能性があります。

可能な値：

- 0 — 無効。
- 1 — 有効。

Usage

次のクエリを考慮してください：

1.  `SELECT count() FROM test_table WHERE date = '2018-10-10'`
2.  `SELECT count() FROM (SELECT * FROM test_table) WHERE date = '2018-10-10'`

`enable_optimize_predicate_expression = 1` の場合、これらのクエリの実行時間は等しくなります。なぜなら ClickHouse はサブクエリを処理する際に `WHERE` を適用するからです。

`enable_optimize_predicate_expression = 0` の場合、2番目のクエリの実行時間ははるかに長くなります。なぜなら、`WHERE` 句はサブクエリが終了した後にすべてのデータに適用されるからです。

## enable_optimize_predicate_expression_to_final_subquery {#enable_optimize_predicate_expression_to_final_subquery}

Type: Bool

Default value: 1

述語を最終サブクエリにプッシュすることを許可します。

## enable_order_by_all {#enable_order_by_all}

Type: Bool

Default value: 1

[`ORDER BY ALL`](../../sql-reference/statements/select/order-by.md) 構文でのソートの有効または無効を設定します。

可能な値：

- 0 — `ORDER BY ALL` を無効。
- 1 — `ORDER BY ALL` を有効。

**Example**

Query:

```sql
CREATE TABLE TAB(C1 Int, C2 Int, ALL Int) ENGINE=Memory();

INSERT INTO TAB VALUES (10, 20, 30), (20, 20, 10), (30, 10, 20);

SELECT * FROM TAB ORDER BY ALL; -- returns an error that ALL is ambiguous

SELECT * FROM TAB ORDER BY ALL SETTINGS enable_order_by_all = 0;
```

Result:

```text
┌─C1─┬─C2─┬─ALL─┐
│ 20 │ 20 │  10 │
│ 30 │ 10 │  20 │
│ 10 │ 20 │  30 │
└────┴────┴─────┘
```

## enable_parsing_to_custom_serialization {#enable_parsing_to_custom_serialization}

Type: Bool

Default value: 1

trueの場合、データはテーブルから得たシリアル化のヒントに従ってカラムにカスタムシリアル化（例：Sparse）として直接解析できます。

## enable_positional_arguments {#enable_positional_arguments}

Type: Bool

Default value: 1

[GROUP BY](/sql-reference/statements/select/group-by)、[LIMIT BY](../../sql-reference/statements/select/limit-by.md)、[ORDER BY](../../sql-reference/statements/select/order-by.md) ステートメントでの位置引数のサポートを有効または無効にします。

可能な値：

- 0 — 位置引数はサポートされていません。
- 1 — 位置引数がサポートされます。カラム番号はカラム名の代わりに使用できます。

**Example**

Query:

```sql
CREATE TABLE positional_arguments(one Int, two Int, three Int) ENGINE=Memory();

INSERT INTO positional_arguments VALUES (10, 20, 30), (20, 20, 10), (30, 10, 20);

SELECT * FROM positional_arguments ORDER BY 2,3;
```

Result:

```text
┌─one─┬─two─┬─three─┐
│  30 │  10 │   20  │
│  20 │  20 │   10  │
│  10 │  20 │   30  │
└─────┴─────┴───────┘
```

## enable_reads_from_query_cache {#enable_reads_from_query_cache}

Type: Bool

Default value: 1

有効にすると、`SELECT` クエリの結果が [クエリキャッシュ](../query-cache.md) から取得されます。

可能な値：

- 0 - 無効
- 1 - 有効

## enable_s3_requests_logging {#enable_s3_requests_logging}

Type: Bool

Default value: 0

S3 リクエストの非常に明示的なログを有効にします。デバッグ専用で意味があります。

## enable_scalar_subquery_optimization {#enable_scalar_subquery_optimization}

Type: Bool

Default value: 1

trueの場合、スカラーサブクエリは大規模なスカラー値の (de)シリアル化を防ぎ、同じサブクエリを複数回実行することを回避できます。

## enable_sharing_sets_for_mutations {#enable_sharing_sets_for_mutations}

Type: Bool

Default value: 1

IN サブクエリのために構築された共有集合オブジェクトを同じ変異の異なるタスク間で共有できるようにします。これにより、メモリ使用量と CPU 消費を削減できます。

## enable_software_prefetch_in_aggregation {#enable_software_prefetch_in_aggregation}

Type: Bool

Default value: 1

集計におけるソフトウェアプリフェッチの使用を有効にします。

## enable_unaligned_array_join {#enable_unaligned_array_join}

Type: Bool

Default value: 0

異なるサイズの複数の配列を使用した ARRAY JOIN を許可します。この設定が有効な場合、配列は最長のものにサイズを変更します。

## enable_url_encoding {#enable_url_encoding}

Type: Bool

Default value: 1

[URL](../../engines/table-engines/special/url.md)エンジンテーブル内の URI パスのデコード/エンコードを有効/無効にします。

デフォルトで有効です。

## enable_vertical_final {#enable_vertical_final}

Type: Bool

Default value: 1

有効にすると、マージするのではなく、行を削除としてマークして後でフィルタリングすることにより FINAL の際に重複行を削除します。

## enable_writes_to_query_cache {#enable_writes_to_query_cache}

Type: Bool

Default value: 1

有効にすると、`SELECT` クエリの結果が [クエリキャッシュ](../query-cache.md) に格納されます。

可能な値：

- 0 - 無効
- 1 - 有効

## enable_zstd_qat_codec {#enable_zstd_qat_codec}

Type: Bool

Default value: 0

有効にすると、ZSTD_QAT コーデックを使用してカラムを圧縮できます。

## enforce_strict_identifier_format {#enforce_strict_identifier_format}

Type: Bool

Default value: 0

有効にすると、アルファベットと数字、アンダースコアの文字のみを含む識別子を許可します。

## engine_file_allow_create_multiple_files {#engine_file_allow_create_multiple_files}

Type: Bool

Default value: 0

ファイルエンジンテーブルで、形式が接尾辞（`JSON`、`ORC`、`Parquet` など）を持つ場合、各挿入で新しいファイルを作成することを有効または無効にします。有効な場合、各挿入ごとに次のパターンに従った名前の新しいファイルが作成されます：

`data.Parquet` -> `data.1.Parquet` -> `data.2.Parquet` など。

可能な値：
- 0 — `INSERT` クエリはファイルの末尾に新しいデータを追加します。
- 1 — `INSERT` クエリは新しいファイルを作成します。
```
```yaml
title: 'エンジン設定: ファイルとHDFS'
sidebar_label: 'エンジン設定: ファイルとHDFS'
keywords: 'ClickHouse, 設定, エンジン, ファイル, HDFS'
description: 'ClickHouseのファイルおよびHDFSエンジンの設定に関する詳細な説明。'
```

## engine_file_empty_if_not_exists {#engine_file_empty_if_not_exists}

Type: Bool

Default value: 0

ファイルのないファイルエンジンテーブルからデータを選択できるようにします。

Possible values:
- 0 — `SELECT` は例外を投げます。
- 1 — `SELECT` は空の結果を返します。

## engine_file_skip_empty_files {#engine_file_skip_empty_files}

Type: Bool

Default value: 0

[File](../../engines/table-engines/special/file.md)エンジンテーブルで空のファイルをスキップするかどうかを有効または無効にします。

Possible values:
- 0 — 空のファイルが要求された形式と互換性がない場合、`SELECT` は例外を投げます。
- 1 — 空のファイルの場合、`SELECT` は空の結果を返します。

## engine_file_truncate_on_insert {#engine_file_truncate_on_insert}

Type: Bool

Default value: 0

[File](../../engines/table-engines/special/file.md)エンジンテーブルに挿入する前にトランケートを有効または無効にします。

Possible values:
- 0 — `INSERT` クエリはファイルの末尾に新しいデータを追加します。
- 1 — `INSERT` クエリは新しいデータでファイルの既存の内容を置き換えます。

## engine_url_skip_empty_files {#engine_url_skip_empty_files}

Type: Bool

Default value: 0

[URL](../../engines/table-engines/special/url.md)エンジンテーブルで空のファイルをスキップするかどうかを有効または無効にします。

Possible values:
- 0 — 空のファイルが要求された形式と互換性がない場合、`SELECT` は例外を投げます。
- 1 — 空のファイルの場合、`SELECT` は空の結果を返します。

## except_default_mode {#except_default_mode}

Type: SetOperationMode

Default value: ALL

EXCEPTクエリのデフォルトモードを設定します。Possible values: 空の文字列、'ALL'、'DISTINCT'。空の場合、モードなしのクエリは例外を投げます。

## external_storage_connect_timeout_sec {#external_storage_connect_timeout_sec}

Type: UInt64

Default value: 10

接続タイムアウト（秒）。現在、MySQLのみでサポートされています。

## external_storage_max_read_bytes {#external_storage_max_read_bytes}

Type: UInt64

Default value: 0

外部エンジンを持つテーブルが履歴データをフラッシュするときの最大バイト数の制限。現在、MySQLテーブルエンジン、データベースエンジン、辞書にのみ対応しています。0に等しい場合、この設定は無効です。

## external_storage_max_read_rows {#external_storage_max_read_rows}

Type: UInt64

Default value: 0

外部エンジンを持つテーブルが履歴データをフラッシュするときの最大行数の制限。現在、MySQLテーブルエンジン、データベースエンジン、辞書にのみ対応しています。0に等しい場合、この設定は無効です。

## external_storage_rw_timeout_sec {#external_storage_rw_timeout_sec}

Type: UInt64

Default value: 300

読み取り/書き込みのタイムアウト（秒）。現在、MySQLのみでサポートされています。

## external_table_functions_use_nulls {#external_table_functions_use_nulls}

Type: Bool

Default value: 1

[mysql](../../sql-reference/table-functions/mysql.md)、[postgresql](../../sql-reference/table-functions/postgresql.md)、および [odbc](../../sql-reference/table-functions/odbc.md) テーブル関数がNullableカラムをどのように使用するかを定義します。

Possible values:

- 0 — テーブル関数は明示的にNullableカラムを使用します。
- 1 — テーブル関数は暗黙的にNullableカラムを使用します。

**Usage**

この設定が`0`に設定されている場合、テーブル関数はNullableカラムを作成せず、NULLの代わりにデフォルト値を挿入します。この設定は、配列内のNULL値にも適用されます。

## external_table_strict_query {#external_table_strict_query}

Type: Bool

Default value: 0

trueに設定されている場合、外部テーブルへのクエリに対してローカルフィルタへの変換式が禁止されます。

## extract_key_value_pairs_max_pairs_per_row {#extract_key_value_pairs_max_pairs_per_row}

Type: UInt64

Default value: 1000

`extractKeyValuePairs`関数によって生成される最大ペア数。この制限は、過剰なメモリの消費を防ぐために使用されます。

## extremes {#extremes}

Type: Bool

Default value: 0

クエリ結果のカラムにおける極値（最小値と最大値）を数えるかどうか。0または1を受け入れます。デフォルトは0（無効）。

詳細については、「極値」セクションを参照してください。

## fallback_to_stale_replicas_for_distributed_queries {#fallback_to_stale_replicas_for_distributed_queries}

Type: Bool

Default value: 1

更新データが使用できない場合、古いレプリカへのクエリを強制します。[Replication](../../engines/table-engines/mergetree-family/replication.md)を参照してください。

ClickHouseは、テーブルの古いレプリカの中から最も関連性の高いものを選択します。

これは、レプリカテーブルを指す分散テーブルから`SELECT`を実行する際に使用されます。

デフォルトでは1（有効）です。

## filesystem_cache_boundary_alignment {#filesystem_cache_boundary_alignment}

Type: UInt64

Default value: 0

ファイルシステムキャッシュの境界アライメント。この設定は、非ディスク読み取り（リモートテーブルエンジン/テーブル関数のキャッシュなど）にのみ適用され、MergeTreeテーブルのストレージ設定には適用されません。値0はアライメントなしを意味します。

## filesystem_cache_enable_background_download_during_fetch {#filesystem_cache_enable_background_download_during_fetch}

<CloudAvailableBadge/>

Type: Bool

Default value: 1

ClickHouse Cloudのみに影響があります。ファイルシステムキャッシュ内のスペース予約のためにキャッシュをロックするまでの待機時間。

## filesystem_cache_enable_background_download_for_metadata_files_in_packed_storage {#filesystem_cache_enable_background_download_for_metadata_files_in_packed_storage}

<CloudAvailableBadge/>

Type: Bool

Default value: 1

ClickHouse Cloudのみに影響があります。ファイルシステムキャッシュ内のスペース予約のためにキャッシュをロックするまでの待機時間。

## filesystem_cache_max_download_size {#filesystem_cache_max_download_size}

Type: UInt64

Default value: 137438953472

単一のクエリによってダウンロード可能な最大リモートファイルシステムキャッシュサイズ。

## filesystem_cache_name {#filesystem_cache_name}

Type: String

Default value: 

状態を持たないテーブルエンジンやデータレイクで使用するファイルシステムキャッシュ名。

## filesystem_cache_prefer_bigger_buffer_size {#filesystem_cache_prefer_bigger_buffer_size}

Type: Bool

Default value: 1

ファイルシステムキャッシュが有効な場合、小さなファイルセグメントの書き込みを避けるために、より大きなバッファサイズを優先します。一方で、この設定を有効にするとメモリ使用量が増加する可能性があります。

## filesystem_cache_reserve_space_wait_lock_timeout_milliseconds {#filesystem_cache_reserve_space_wait_lock_timeout_milliseconds}

Type: UInt64

Default value: 1000

ファイルシステムキャッシュ内にスペースを予約するためにキャッシュをロックするまでの待機時間。

## filesystem_cache_segments_batch_size {#filesystem_cache_segments_batch_size}

Type: UInt64

Default value: 20

読み込みバッファがキャッシュから要求できるファイルセグメントの単一バッチのサイズに対する制限。値が低すぎるとキャッシュへの過剰なリクエストが発生し、値が大きすぎるとキャッシュからの追い出しが遅くなる可能性があります。

## filesystem_cache_skip_download_if_exceeds_per_query_cache_write_limit {#filesystem_cache_skip_download_if_exceeds_per_query_cache_write_limit}

Type: Bool

Default value: 1

クエリキャッシュサイズを超える場合、リモートファイルシステムからのダウンロードをスキップします。

## filesystem_prefetch_max_memory_usage {#filesystem_prefetch_max_memory_usage}

Type: UInt64

Default value: 1073741824

プリフェッチの最大メモリ使用量。

## filesystem_prefetch_step_bytes {#filesystem_prefetch_step_bytes}

Type: UInt64

Default value: 0

バイト単位でのプリフェッチステップ。ゼロは`auto`を意味します。最適なプリフェッチステップがおおよそ自動的に推論されますが、100%最適でない可能性があります。実際の値は、`filesystem_prefetch_min_bytes_for_single_read_task`の設定により異なる場合があります。

## filesystem_prefetch_step_marks {#filesystem_prefetch_step_marks}

Type: UInt64

Default value: 0

マーク単位でのプリフェッチステップ。ゼロは`auto`を意味します。最適なプリフェッチステップがおおよそ自動的に推論されますが、100%最適でない可能性があります。実際の値は、`filesystem_prefetch_min_bytes_for_single_read_task`の設定により異なる場合があります。

## filesystem_prefetches_limit {#filesystem_prefetches_limit}

Type: UInt64

Default value: 200

最大プリフェッチ数。ゼロは無制限を意味します。プリフェッチの数を制限したい場合は、`filesystem_prefetches_max_memory_usage`の設定を推奨します。

## final {#final}

Type: Bool

Default value: 0

クエリ内のすべてのテーブルに[FINAL](../../sql-reference/statements/select/from.md/#final-modifier)修飾子を自動的に適用します。これは、[FINAL](../../sql-reference/statements/select/from.md/#final-modifier)を適用できるテーブル、結合テーブル、サブクエリ内のテーブル、および分散テーブルに適用されます。

Possible values:

- 0 - 無効
- 1 - 有効

Example:

```sql
CREATE TABLE test
(
    key Int64,
    some String
)
ENGINE = ReplacingMergeTree
ORDER BY key;

INSERT INTO test FORMAT Values (1, 'first');
INSERT INTO test FORMAT Values (1, 'second');

SELECT * FROM test;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘
┌─key─┬─some──┐
│   1 │ first │
└─────┴───────┘

SELECT * FROM test SETTINGS final = 1;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘

SET final = 1;
SELECT * FROM test;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘
```

## flatten_nested {#flatten_nested}

Type: Bool

Default value: 1

[nested](../../sql-reference/data-types/nested-data-structures/index.md)カラムのデータ形式を設定します。

Possible values:

- 1 — ネストされたカラムが別々の配列にフラット化されます。
- 0 — ネストされたカラムはタプルの単一配列として保持されます。

**Usage**

この設定が`0`に設定されている場合、任意のネストレベルを使用できます。

**Examples**

Query:

``` sql
SET flatten_nested = 1;
CREATE TABLE t_nest (`n` Nested(a UInt32, b UInt32)) ENGINE = MergeTree ORDER BY tuple();

SHOW CREATE TABLE t_nest;
```

Result:

``` text
┌─statement───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.t_nest
(
    `n.a` Array(UInt32),
    `n.b` Array(UInt32)
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS index_granularity = 8192 │
└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

Query:

``` sql
SET flatten_nested = 0;

CREATE TABLE t_nest (`n` Nested(a UInt32, b UInt32)) ENGINE = MergeTree ORDER BY tuple();

SHOW CREATE TABLE t_nest;
```

Result:

``` text
┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.t_nest
(
    `n` Nested(a UInt32, b UInt32)
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS index_granularity = 8192 │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

## force_aggregate_partitions_independently {#force_aggregate_partitions_independently}

Type: Bool

Default value: 0

適用可能な場合には最適化の使用を強制しますが、ヒューリスティクスが使用しないことを決定した場合に適用されます。

## force_aggregation_in_order {#force_aggregation_in_order}

Type: Bool

Default value: 0

この設定は、サーバー自体によって分散クエリをサポートするために使用されます。手動で変更しないでください。正常な操作が妨げられるからです。（リモートノードでの分散集計時に順序での集計の使用を強制します）。

## force_data_skipping_indices {#force_data_skipping_indices}

Type: String

Default value: 

指定されたデータスキッピングインデックスが使用されていない場合、クエリの実行を無効にします。

次の例を考えてみましょう：

```sql
CREATE TABLE data
(
    key Int,
    d1 Int,
    d1_null Nullable(Int),
    INDEX d1_idx d1 TYPE minmax GRANULARITY 1,
    INDEX d1_null_idx assumeNotNull(d1_null) TYPE minmax GRANULARITY 1
)
Engine=MergeTree()
ORDER BY key;

SELECT * FROM data_01515;
SELECT * FROM data_01515 SETTINGS force_data_skipping_indices=''; -- クエリはCANNOT_PARSE_TEXTエラーを生成します。
SELECT * FROM data_01515 SETTINGS force_data_skipping_indices='d1_idx'; -- クエリはINDEX_NOT_USEDエラーを生成します。
SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='d1_idx'; -- Ok。
SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='`d1_idx`'; -- Ok（完全機能パーサーの例）。
SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='`d1_idx`, d1_null_idx'; -- クエリはd1_null_idxが使用されていないため、INDEX_NOT_USEDエラーを生成します。
SELECT * FROM data_01515 WHERE d1 = 0 AND assumeNotNull(d1_null) = 0 SETTINGS force_data_skipping_indices='`d1_idx`, d1_null_idx'; -- Ok。
```

## force_grouping_standard_compatibility {#force_grouping_standard_compatibility}

Type: Bool

Default value: 1

GROUPING関数が引数が集計キーとして使用されていない場合に1を返すようにします。

## force_index_by_date {#force_index_by_date}

Type: Bool

Default value: 0

インデックスが日付によって使用できない場合、クエリの実行を無効にします。

MergeTreeファミリーのテーブルで機能します。

`force_index_by_date=1`の場合、ClickHouseはクエリがデータ範囲を制限するために使用できる日付キー条件を持っているかどうかを確認します。適切な条件がない場合、例外がスローされます。しかしながら、条件が読み込むデータの量を減少させるかどうかは確認しません。たとえば、条件 `Date != ' 2000-01-01 '` は、テーブル内のすべてのデータと一致していても許可されます（すなわち、クエリを実行するには完全なスキャンが必要です）。MergeTreeテーブルにおけるデータ範囲の詳細については[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)を参照してください。

## force_optimize_projection {#force_optimize_projection}

Type: Bool

Default value: 0

`SELECT`クエリで[プロジェクション](../../engines/table-engines/mergetree-family/mergetree.md/#projections)の使用を強制するかどうかを有効または無効にします。プロジェクション最適化が有効な場合（[optimize_use_projections](#optimize_use_projections)設定を参照）。

Possible values:

- 0 — プロジェクション最適化は義務ではありません。
- 1 — プロジェクション最適化は義務です。

## force_optimize_projection_name {#force_optimize_projection_name}

Type: String

Default value: 

空でない文字列に設定されると、このプロジェクションがクエリで少なくとも一度は使用されるかをチェックします。

Possible values:

- 文字列: クエリで使用されているプロジェクションの名前。

## force_optimize_skip_unused_shards {#force_optimize_skip_unused_shards}

Type: UInt64

Default value: 0

[optimize_skip_unused_shards](#optimize_skip_unused_shards)が有効で、未使用のシャードをスキップできない場合には、クエリの実行を有効または無効にします。スキップが不可能な場合で、この設定が有効な場合は例外がスローされます。

Possible values:

- 0 — 無効。ClickHouseは例外をスローしません。
- 1 — 有効。テーブルにシャーディングキーがある場合のみクエリの実行が無効になります。
- 2 — 有効。テーブルにシャーディングキーが定義されているかどうかに関係なく、クエリの実行が無効になります。

## force_optimize_skip_unused_shards_nesting {#force_optimize_skip_unused_shards_nesting}

Type: UInt64

Default value: 0

[`force_optimize_skip_unused_shards`](#force_optimize_skip_unused_shards)（したがって、[`force_optimize_skip_unused_shards`](#force_optimize_skip_unused_shards)が必要）を分散クエリのネストレベルに応じて制御します（分散テーブルが別の分散テーブルを参照する場合のケース）。

Possible values:

- 0 - 無効、`force_optimize_skip_unused_shards` は常に動作します。
- 1 — `force_optimize_skip_unused_shards` は最初のレベルでのみ有効になります。
- 2 — `force_optimize_skip_unused_shards` は2番目のレベルまで有効になります。

## force_primary_key {#force_primary_key}

Type: Bool

Default value: 0

プライマリキーによるインデックス付けが不可能な場合、クエリの実行を無効にします。

MergeTreeファミリーのテーブルで機能します。

`force_primary_key=1`の場合、ClickHouseはクエリがデータ範囲を制限するために使用できるプライマリキー条件を持っているかどうかを確認します。適切な条件がない場合は、例外がスローされます。ただし、その条件が読み込むデータの量を減少させるかどうかは確認しません。MergeTreeテーブルにおけるデータ範囲の詳細については[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)を参照してください。

## force_remove_data_recursively_on_drop {#force_remove_data_recursively_on_drop}

Type: Bool

Default value: 0

DROPクエリでデータを再帰的に削除します。「ディレクトリが空でない」というエラーを回避しますが、デタッチされたデータが静かに削除される可能性があります。

## formatdatetime_f_prints_scale_number_of_digits {#formatdatetime_f_prints_scale_number_of_digits}

Type: Bool

Default value: 0

関数 'formatDateTime' のフォーマッタ '%f' は、固定6桁の代わりに DateTime64 に対して桁数スケールを印刷します。

## formatdatetime_f_prints_single_zero {#formatdatetime_f_prints_single_zero}

Type: Bool

Default value: 0

関数 'formatDateTime' のフォーマッタ '%f' は、フォーマットされた値に小数秒が存在しない場合、6つのゼロの代わりに1つのゼロを印刷します。

## formatdatetime_format_without_leading_zeros {#formatdatetime_format_without_leading_zeros}

Type: Bool

Default value: 0

関数 'formatDateTime' のフォーマッタ '%c', '%l' および '%k' は、先頭ゼロなしで月と時間を印刷します。

## formatdatetime_parsedatetime_m_is_month_name {#formatdatetime_parsedatetime_m_is_month_name}

Type: Bool

Default value: 1

関数 'formatDateTime' と 'parseDateTime' におけるフォーマッタ '%M' は、分の代わりに月の名前を印刷/解析します。

## fsync_metadata {#fsync_metadata}

Type: Bool

Default value: 1

.sqlファイルを書き込む際に[fsync](http://pubs.opengroup.org/onlinepubs/9699919799/functions/fsync.html)を有効または無効にします。デフォルトで有効です。

サーバーに何百万もの小さなテーブルがあり、それらが常に生成および削除されている場合には、無効にする意味があります。

## function_implementation {#function_implementation}

Type: String

Default value: 

特定のターゲットまたはバリアント（実験的）のために関数の実装を選択します。空の場合は、すべてが有効になります。

## function_json_value_return_type_allow_complex {#function_json_value_return_type_allow_complex}

Type: Bool

Default value: 0

json_value関数で複雑な型（構造体、配列、マップなど）を返すことを許可するかどうかを制御します。

```sql
SELECT JSON_VALUE('{"hello":{"world":"!"}}', '$.hello') settings function_json_value_return_type_allow_complex=true

┌─JSON_VALUE('{"hello":{"world":"!"}}', '$.hello')─┐
│ {"world":"!"}                                    │
└──────────────────────────────────────────────────┘

1 row in set. Elapsed: 0.001 sec.
```

Possible values:

- true — 許可。
- false — 不許可。

## function_json_value_return_type_allow_nullable {#function_json_value_return_type_allow_nullable}

Type: Bool

Default value: 0

JSON_VALUE関数の値が存在しない場合に`NULL`を返すことを許可するかどうかを制御します。

```sql
SELECT JSON_VALUE('{"hello":"world"}', '$.b') settings function_json_value_return_type_allow_nullable=true;

┌─JSON_VALUE('{"hello":"world"}', '$.b')─┐
│ ᴺᵁᴸᴸ                                   │
└────────────────────────────────────────┘

1 row in set. Elapsed: 0.001 sec.
```

Possible values:

- true — 許可。
- false — 不許可。

## function_locate_has_mysql_compatible_argument_order {#function_locate_has_mysql_compatible_argument_order}

Type: Bool

Default value: 1

関数 [locate](../../sql-reference/functions/string-search-functions.md/#locate) の引数の順序を制御します。

Possible values:

- 0 — Function `locate` は引数 `(haystack, needle[, start_pos])` を受け付けます。
- 1 — Function `locate` は引数 `(needle, haystack, [, start_pos])` を受け付けます（MySQL互換の動作）。

## function_range_max_elements_in_block {#function_range_max_elements_in_block}

Type: UInt64

Default value: 500000000

関数 [range](/sql-reference/functions/array-functions#rangeend-rangestart--end--step)によって生成されるデータボリュームの安全閾値を設定します。データブロック内の各行の配列サイズの合計によって生成される最大値を定義します。

Possible values:

- 正の整数。

**See Also**

- [max_block_size](#max_block_size)
- [min_insert_block_size_rows](#min_insert_block_size_rows)

## function_sleep_max_microseconds_per_block {#function_sleep_max_microseconds_per_block}

Type: UInt64

Default value: 3000000

関数 `sleep` が各ブロックごとに許可される最大マイクロ秒数。ユーザーが大きな値で呼び出すと、例外がスローされます。この設定は安全のための閾値です。

## function_visible_width_behavior {#function_visible_width_behavior}

Type: UInt64

Default value: 1

`visibleWidth` の動作のバージョン。0 - コードポイントの数をカウントするのみ；1 - ゼロ幅および合成文字を正しくカウントし、全角文字を2としてカウントし、タブ幅を推定し、削除文字をカウントします。

## geo_distance_returns_float64_on_float64_arguments {#geo_distance_returns_float64_on_float64_arguments}

Type: Bool

Default value: 1

`geoDistance`、`greatCircleDistance`、`greatCircleAngle`関数のすべての4つの引数がFloat64の場合、Float64を返し、内部計算にダブル精度を使用します。以前のClickHouseバージョンでは、これらの関数は常にFloat32を返していました。

## glob_expansion_max_elements {#glob_expansion_max_elements}

Type: UInt64

Default value: 1000

許可される最大アドレス数（外部ストレージ、テーブル関数など）。

## grace_hash_join_initial_buckets {#grace_hash_join_initial_buckets}
<ExperimentalBadge/>

Type: NonZeroUInt64

Default value: 1

グレースハッシュ結合の初期バケツ数。

## grace_hash_join_max_buckets {#grace_hash_join_max_buckets}
<ExperimentalBadge/>

Type: NonZeroUInt64

Default value: 1024

グレースハッシュ結合のバケツの数の制限。

## group_by_overflow_mode {#group_by_overflow_mode}

Type: OverflowModeGroupBy

Default value: throw

リミットを超えた場合に何をするか。

## group_by_two_level_threshold {#group_by_two_level_threshold}

Type: UInt64

Default value: 100000

どのキーの数から、2レベル集計が始まるか。0 - 制限は設定されていません。

## group_by_two_level_threshold_bytes {#group_by_two_level_threshold_bytes}

Type: UInt64

Default value: 50000000

集計状態のサイズがバイトでどのサイズから、2レベル集計が使用されるか。0 - 制限は設定されていません。1つ以上の制限がトリガーされると2レベル集計が使用されます。

## group_by_use_nulls {#group_by_use_nulls}

Type: Bool

Default value: 0

[GROUP BY句](/sql-reference/statements/select/group-by)が集計キーの型をどのように扱うかを変更します。
`ROLLUP`、`CUBE`、または`GROUPING SETS`指定子が使用される場合、一部の集計キーが結果行を生成するために使用されないことがあります。
これらのキーのカラムは、この設定に応じて、対応する行にデフォルト値または`NULL`で埋められます。

Possible values:

- 0 — 集計キー型のデフォルト値が欠落している値を生成するために使用されます。
- 1 — ClickHouseはSQL標準が言う通りに`GROUP BY`を実行します。集計キーの型は[Nullable](/sql-reference/data-types/nullable) に変換されます。対応する集計キーのカラムは、それが使用されなかった行に対して[NULL](/sql-reference/syntax#null)で埋められます。

See also:

- [GROUP BY句](/sql-reference/statements/select/group-by)

## h3togeo_lon_lat_result_order {#h3togeo_lon_lat_result_order}

Type: Bool

Default value: 0

関数 'h3ToGeo' が(true)の場合は(lon, lat)を返し、そうでない場合は(lat, lon)を返します。

## handshake_timeout_ms {#handshake_timeout_ms}

Type: Milliseconds

Default value: 10000

ハンドシェイク中にレプリカからHelloパケットを受信するためのタイムアウト（ミリ秒）。

## hdfs_create_new_file_on_insert {#hdfs_create_new_file_on_insert}

Type: Bool

Default value: 0

HDFSエンジンテーブルに挿入のたびに新しいファイルを作成するかどうかを有効または無効にします。有効な場合、各挿入で次のようにパターンに類似した新しいHDFSファイルが作成されます：

初期: `data.Parquet.gz` -> `data.1.Parquet.gz` -> `data.2.Parquet.gz` など。

Possible values:
- 0 — `INSERT` クエリはファイルの末尾に新しいデータを追加します。
- 1 — `INSERT` クエリは新しいファイルを作成します。

## hdfs_ignore_file_doesnt_exist {#hdfs_ignore_file_doesnt_exist}

Type: Bool

Default value: 0

特定のキーを読み込むときにファイルが存在しない場合にその不在を無視します。

Possible values:
- 1 — `SELECT` は空の結果を返します。
- 0 — `SELECT` は例外を投げます。

## hdfs_replication {#hdfs_replication}

Type: UInt64

Default value: 0

HDFSファイルが作成されるときに指定できる実際のレプリケーション数。

## hdfs_skip_empty_files {#hdfs_skip_empty_files}

Type: Bool

Default value: 0

[HDFS](../../engines/table-engines/integrations/hdfs.md)エンジンテーブルで空のファイルをスキップするかどうかを有効または無効にします。

Possible values:
- 0 — 空のファイルが要求された形式と互換性がない場合、`SELECT` は例外を投げます。
- 1 — 空のファイルの場合、`SELECT` は空の結果を返します。

## hdfs_throw_on_zero_files_match {#hdfs_throw_on_zero_files_match}

Type: Bool

Default value: 0

グロブ展開ルールに基づいて一致するゼロファイルがある場合にエラーを投げます。

Possible values:
- 1 — `SELECT` は例外を投げます。
- 0 — `SELECT` は空の結果を返します。

## hdfs_truncate_on_insert {#hdfs_truncate_on_insert}

Type: Bool

Default value: 0

hdfsエンジンテーブルへの挿入前にトランケーションを有効または無効にします。無効の場合、HDFSにファイルが既に存在する場合、挿入を試みると例外がスローされます。

Possible values:
- 0 — `INSERT` クエリはファイルの末尾に新しいデータを追加します。
- 1 — `INSERT` クエリは新しいデータでファイルの既存の内容を置き換えます。

## hedged_connection_timeout_ms {#hedged_connection_timeout_ms}

Type: Milliseconds

Default value: 50

ヘッジリクエストでレプリカとの接続を確立するための接続タイムアウト。

## hnsw_candidate_list_size_for_search {#hnsw_candidate_list_size_for_search}
<ExperimentalBadge/>

Type: UInt64

Default value: 256

ベクトル類似性インデックスに対する検索時の動的候補リストのサイズ、別名 'ef_search'。

## hsts_max_age {#hsts_max_age}

Type: UInt64

Default value: 0

HSTSの有効期限。0はHSTSを無効にします。

## http_connection_timeout {#http_connection_timeout}

Type: Seconds

Default value: 1

HTTP接続タイムアウト（秒）。

Possible values:

- 任意の正の整数。
- 0 - 無効（無限のタイムアウト）。

## http_headers_progress_interval_ms {#http_headers_progress_interval_ms}

Type: UInt64

Default value: 100

HTTPヘッダーX-ClickHouse-Progressを指定された間隔でそれ以上頻繁に送信しないようにします。

## http_make_head_request {#http_make_head_request}

Type: Bool

Default value: 1

`http_make_head_request`設定は、データをHTTPから読み込む際に、読み込むファイルに関する情報（サイズなど）を取得するために`HEAD`リクエストを実行することを許可します。デフォルトで有効なため、サーバーが`HEAD`リクエストをサポートしていない場合にはこの設定を無効にすることが望ましい場合があります。

## http_max_field_name_size {#http_max_field_name_size}

Type: UInt64

Default value: 131072

HTTPヘッダー内のフィールド名の最大長。

## http_max_field_value_size {#http_max_field_value_size}

Type: UInt64

Default value: 131072

HTTPヘッダー内のフィールド値の最大長。

## http_max_fields {#http_max_fields}

Type: UInt64

Default value: 1000000

HTTPヘッダー内のフィールドの最大数。

## http_max_multipart_form_data_size {#http_max_multipart_form_data_size}

Type: UInt64

Default value: 1073741824

multipart/form-dataコンテンツのサイズに制限。これはURLパラメータから解析できず、ユーザープロファイルに設定する必要があります。コンテンツは、クエリ実行開始前にメモリ内に解析され、外部テーブルが作成されます。これは、この段階で効果がある唯一の制限です（最大メモリ使用量および最大実行時間に関する制限は、HTTPフォームデータの読み込み中には影響しません）。

## http_max_request_param_data_size {#http_max_request_param_data_size}

Type: UInt64

Default value: 10485760

HTTPリクエストにおけるクエリパラメータとして使用されるリクエストデータのサイズの制限。

## http_max_tries {#http_max_tries}

Type: UInt64

Default value: 10

HTTP経由での最大リトライ数。

## http_max_uri_size {#http_max_uri_size}

Type: UInt64

Default value: 1048576

HTTPリクエストのURIの最大長を設定します。

Possible values:

- 正の整数。

## http_native_compression_disable_checksumming_on_decompress {#http_native_compression_disable_checksumming_on_decompress}

Type: Bool

Default value: 0

クライアントからのHTTP POSTデータを解凍する際にチェックサムの確認を有効または無効にします。ClickHouseのネイティブ圧縮形式にのみ使用され（`gzip`や`deflate`では使用されません）。

詳細については、[HTTPインターフェースの説明](../../interfaces/http.md)を参照してください。

Possible values:

- 0 — 無効。
- 1 — 有効。

## http_receive_timeout {#http_receive_timeout}

Type: Seconds

Default value: 30

HTTP受信タイムアウト（秒）。

Possible values:

- 任意の正の整数。
- 0 - 無効（無限のタイムアウト）。

## http_response_buffer_size {#http_response_buffer_size}

Type: UInt64

Default value: 0

HTTPレスポンスをクライアントに送信する前にサーバーメモリ内にバッファリングするバイト数（http_wait_end_of_queryが有効な場合）。

## http_response_headers {#http_response_headers}

Type: Map

Default value: {}

サーバーが成功したクエリ結果とともにレスポンスで返すHTTPヘッダーを追加または上書きすることを許可します。

これはHTTPインターフェースのみに影響します。

デフォルトで設定されているヘッダーは、提供された値で上書きされます。
デフォルトで設定されていないヘッダーは、ヘッダーリストに追加されます。
デフォルトでサーバーによって設定されており、この設定で上書きされていないヘッダーはそのまま残ります。

この設定により、ヘッダーを定数値に設定することが可能です。現在、動的に計算された値にヘッダーを設定する方法はありません。

設定を変更するUIアプリケーションを実装する場合、ユーザーが設定を変更できるようにしつつ、返されたヘッダーに基づいて意思決定を行う場合には、この設定を読み取り専用に制限することが推奨されます。

Example: `SET http_response_headers = '{"Content-Type": "image/png"}'`

## http_retry_initial_backoff_ms {#http_retry_initial_backoff_ms}

Type: UInt64

Default value: 100

リトライ時のバックオフの最小ミリ秒数、HTTP経由での読み取りをリトライする場合。
```yaml
title: '設定: HTTP リトライとタイムアウト'
sidebar_label: 'HTTP リトライとタイムアウト'
keywords: '設定, HTTP, リトライ, タイムアウト'
description: 'HTTP に関連する設定とパラメータ、リトライ、タイムアウトなどを説明します。' 
```
## http_retry_max_backoff_ms {#http_retry_max_backoff_ms}

タイプ: UInt64

デフォルト値: 10000

HTTP経由での読み取りをリトライする際のバックオフの最大ミリ秒。

## http_send_timeout {#http_send_timeout}

タイプ: 秒

デフォルト値: 30

HTTP送信のタイムアウト（秒単位）。

可能な値:

- 任意の正の整数。
- 0 - 無効 (タイムアウト無制限)。

:::note
これはデフォルトプロファイルにのみ適用されます。変更を反映させるにはサーバの再起動が必要です。
:::

## http_skip_not_found_url_for_globs {#http_skip_not_found_url_for_globs}

タイプ: Bool

デフォルト値: 1

HTTP_NOT_FOUND エラーが発生した場合、グロブに対するURLをスキップします。

## http_wait_end_of_query {#http_wait_end_of_query}

タイプ: Bool

デフォルト値: 0

サーバー側でのHTTPレスポンスバッファリングを有効にします。

## http_write_exception_in_output_format {#http_write_exception_in_output_format}

タイプ: Bool

デフォルト値: 1

有効な出力を生成するために出力形式で例外を書き込みます。JSONおよびXML形式で動作します。

## http_zlib_compression_level {#http_zlib_compression_level}

タイプ: Int64

デフォルト値: 3

[enable_http_compression = 1](#enable_http_compression)の場合、HTTPリクエストに対するレスポンスのデータ圧縮レベルを設定します。

可能な値: 1から9までの数字。

## idle_connection_timeout {#idle_connection_timeout}

タイプ: UInt64

デフォルト値: 3600

指定された秒数後にアイドルTCP接続を閉じるためのタイムアウト。

可能な値:

- 正の整数（0 - 直ちに閉じる、0秒後）。

## ignore_cold_parts_seconds {#ignore_cold_parts_seconds}

<CloudAvailableBadge/>

タイプ: Int64

デフォルト値: 0

ClickHouse Cloudでのみ効果があります。新しいデータパーツをSELECTクエリから除外し、事前にウォームアップされるまで（[cache_populated_by_fetch](merge-tree-settings.md/#cache_populated_by_fetch)を参照）またはこの秒数が経過した後にのみクエリ結果に表示されます。Replicated-/SharedMergeTree専用です。

## ignore_data_skipping_indices {#ignore_data_skipping_indices}

タイプ: String

デフォルト値: 

クエリで使用された場合、指定されたスキッピングインデックスを無視します。

以下の例を考慮してください。

```sql
CREATE TABLE data
(
    key Int,
    x Int,
    y Int,
    INDEX x_idx x TYPE minmax GRANULARITY 1,
    INDEX y_idx y TYPE minmax GRANULARITY 1,
    INDEX xy_idx (x,y) TYPE minmax GRANULARITY 1
)
Engine=MergeTree()
ORDER BY key;

INSERT INTO data VALUES (1, 2, 3);

SELECT * FROM data;
SELECT * FROM data SETTINGS ignore_data_skipping_indices=''; -- クエリはCANNOT_PARSE_TEXTエラーを生成します。
SELECT * FROM data SETTINGS ignore_data_skipping_indices='x_idx'; -- OK。
SELECT * FROM data SETTINGS ignore_data_skipping_indices='na_idx'; -- OK。

SELECT * FROM data WHERE x = 1 AND y = 1 SETTINGS ignore_data_skipping_indices='xy_idx',force_data_skipping_indices='xy_idx' ; -- クエリはINDEX_NOT_USEDエラーを生成します。xy_idxが明示的に無視されるためです。
SELECT * FROM data WHERE x = 1 AND y = 2 SETTINGS ignore_data_skipping_indices='xy_idx';
```

インデックスを無視しないクエリ：
```sql
EXPLAIN indexes = 1 SELECT * FROM data WHERE x = 1 AND y = 2;

Expression ((Projection + Before ORDER BY))
  Filter (WHERE)
    ReadFromMergeTree (default.data)
    Indexes:
      PrimaryKey
        Condition: true
        Parts: 1/1
        Granules: 1/1
      Skip
        Name: x_idx
        Description: minmax GRANULARITY 1
        Parts: 0/1
        Granules: 0/1
      Skip
        Name: y_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
      Skip
        Name: xy_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
```

`xy_idx`インデックスを無視する：
```sql
EXPLAIN indexes = 1 SELECT * FROM data WHERE x = 1 AND y = 2 SETTINGS ignore_data_skipping_indices='xy_idx';

Expression ((Projection + Before ORDER BY))
  Filter (WHERE)
    ReadFromMergeTree (default.data)
    Indexes:
      PrimaryKey
        Condition: true
        Parts: 1/1
        Granules: 1/1
      Skip
        Name: x_idx
        Description: minmax GRANULARITY 1
        Parts: 0/1
        Granules: 0/1
      Skip
        Name: y_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
```

MergeTreeファミリーのテーブルで動作します。

## ignore_drop_queries_probability {#ignore_drop_queries_probability}

タイプ: Float

デフォルト値: 0

有効にすると、指定された確率で全てのDROPテーブルクエリを無視します（メモリとJOINエンジンの場合、DROPをTRUNCATEに置き換えます）。テスト目的で使用されます。

## ignore_materialized_views_with_dropped_target_table {#ignore_materialized_views_with_dropped_target_table}

タイプ: Bool

デフォルト値: 0

ビューにプッシュ中に、ドロップされたターゲットテーブルを持つMVを無視します。

## ignore_on_cluster_for_replicated_access_entities_queries {#ignore_on_cluster_for_replicated_access_entities_queries}

タイプ: Bool

デフォルト値: 0

レプリケートされたアクセスエンティティ管理クエリのためのON CLUSTER句を無視します。

## ignore_on_cluster_for_replicated_named_collections_queries {#ignore_on_cluster_for_replicated_named_collections_queries}

タイプ: Bool

デフォルト値: 0

レプリケートされた名前付きコレクション管理クエリのためのON CLUSTER句を無視します。

## ignore_on_cluster_for_replicated_udf_queries {#ignore_on_cluster_for_replicated_udf_queries}

タイプ: Bool

デフォルト値: 0

レプリケートされたUDF管理クエリのためのON CLUSTER句を無視します。

## implicit_select {#implicit_select}

タイプ: Bool

デフォルト値: 0

先頭にSELECTキーワードのないシンプルなSELECTクエリの記述を許可します。計算機スタイルの使用に便利です。例えば、`1 + 2`は有効なクエリとなります。

`clickhouse-local`ではデフォルトで有効であり、明示的に無効にできます。

## implicit_transaction {#implicit_transaction}
<ExperimentalBadge/>

タイプ: Bool

デフォルト値: 0

有効にして、すでにトランザクション内にない場合は、クエリを完全なトランザクション（開始+コミットまたはロールバック）でラップします。

## input_format_parallel_parsing {#input_format_parallel_parsing}

タイプ: Bool

デフォルト値: 1

データフォーマットの順序を保持した並列パースを有効または無効にします。[TSV](../../interfaces/formats.md/#tabseparated)、[TSKV](../../interfaces/formats.md/#tskv)、[CSV](../../interfaces/formats.md/#csv)、および[JSONEachRow](../../interfaces/formats.md/#jsoneachrow)形式のみサポートされます。

可能な値：

- 1 — 有効。
- 0 — 無効。

## insert_allow_materialized_columns {#insert_allow_materialized_columns}

タイプ: Bool

デフォルト値: 0

この設定が有効な場合、INSERT文でマテリアライズされたカラムを許可します。

## insert_deduplicate {#insert_deduplicate}

タイプ: Bool

デフォルト値: 1

`INSERT`のブロック重複排除を有効または無効にします（Replicated* テーブル用）。

可能な値：

- 0 — 無効。
- 1 — 有効。

デフォルトでは、Replicatedテーブルに挿入されたブロックは重複排除されます（[データレプリケーション](../../engines/table-engines/mergetree-family/replication.md)を参照）。
Replicatedテーブルの場合、デフォルトでは各パーティションごとに最新の100のブロックのみが重複排除されます（[replicated_deduplication_window](merge-tree-settings.md/#replicated-deduplication-window)、[replicated_deduplication_window_seconds](merge-tree-settings.md/#replicated-deduplication-window-seconds)を参照）。
非レプリケートテーブルの場合は、[non_replicated_deduplication_window](merge-tree-settings.md/#non-replicated-deduplication-window)を参照してください。

## insert_deduplication_token {#insert_deduplication_token}

タイプ: String

デフォルト値:

この設定により、ユーザーはMergeTree/ReplicatedMergeTreeで独自の重複排除セマンティクスを提供できます。
例えば、各INSERT文に対して一意の値を設定することで、挿入されたデータが重複排除されるのを防ぎます。

可能な値：

- 任意の文字列

`insert_deduplication_token`は、空でない場合にのみ重複排除に使用されます。

Replicatedテーブルの場合、デフォルトでは各パーティションごとに最新の100のINSERTのみが重複排除されます（[replicated_deduplication_window](merge-tree-settings.md/#replicated-deduplication-window)、[replicated_deduplication_window_seconds](merge-tree-settings.md/#replicated-deduplication-window-seconds)を参照）。
非レプリケートテーブルの場合は、[non_replicated_deduplication_window](merge-tree-settings.md/#non-replicated-deduplication-window)を参照してください。

:::note
`insert_deduplication_token`はパーティションレベルで動作します（`insert_deduplication`チェックサムと同様です）。複数のパーティションが同じ`insert_deduplication_token`を持つことができます。
:::

例:

```sql
CREATE TABLE test_table
( A Int64 )
ENGINE = MergeTree
ORDER BY A
SETTINGS non_replicated_deduplication_window = 100;

INSERT INTO test_table SETTINGS insert_deduplication_token = 'test' VALUES (1);

-- 次の挿入はinsert_deduplication_tokenが異なるため、重複排除されません
INSERT INTO test_table SETTINGS insert_deduplication_token = 'test1' VALUES (1);

-- 次の挿入はinsert_deduplication_tokenが以前のものと同じであるため、重複排除されます
INSERT INTO test_table SETTINGS insert_deduplication_token = 'test' VALUES (2);

SELECT * FROM test_table

┌─A─┐
│ 1 │
└───┘
┌─A─┐
│ 1 │
└───┘
```

## insert_keeper_fault_injection_probability {#insert_keeper_fault_injection_probability}

タイプ: Float

デフォルト値: 0

挿入中のKeeperリクエストの失敗の約確率。妥当な値は[0.0f, 1.0f]の間である必要があります。

## insert_keeper_fault_injection_seed {#insert_keeper_fault_injection_seed}

タイプ: UInt64

デフォルト値: 0

0 - ランダムシード、その他は設定値。

## insert_keeper_max_retries {#insert_keeper_max_retries}

タイプ: UInt64

デフォルト値: 20

この設定により、Replicated MergeTreeへの挿入中のClickHouse Keeper（またはZooKeeper）リクエストの最大リトライ回数が設定されます。リトライの対象は、ネットワークエラー、Keeperセッションタイムアウト、またはリクエストタイムアウトにより失敗したKeeperリクエストのみです。

可能な値：

- 正の整数。
- 0 — リトライ無効。

クラウドのデフォルト値: `20`。

Keeperリクエストのリトライは、いくつかのタイムアウト後に行われます。タイムアウトは次の設定によって制御されます: `insert_keeper_retry_initial_backoff_ms`, `insert_keeper_retry_max_backoff_ms`。
最初のリトライは`insert_keeper_retry_initial_backoff_ms`タイムアウト後に行われます。次に来るタイムアウトは次のように計算されます：
```
timeout = min(insert_keeper_retry_max_backoff_ms, latest_timeout * 2)
```

例えば、`insert_keeper_retry_initial_backoff_ms=100`、`insert_keeper_retry_max_backoff_ms=10000`、`insert_keeper_max_retries=8`の場合、タイムアウトは`100, 200, 400, 800, 1600, 3200, 6400, 10000`になります。

故障耐性の他に、リトライはより良いユーザーエクスペリエンスを提供し、例えば、Keeperがアップグレード中に再起動してもINSERTの実行中にエラーを返さないようにします。

## insert_keeper_retry_initial_backoff_ms {#insert_keeper_retry_initial_backoff_ms}

タイプ: UInt64

デフォルト値: 100

INSERTクエリ実行中に失敗したKeeperリクエストをリトライするための初期タイムアウト（ミリ秒単位）

可能な値：

- 正の整数。
- 0 — タイムアウトなし。

## insert_keeper_retry_max_backoff_ms {#insert_keeper_retry_max_backoff_ms}

タイプ: UInt64

デフォルト値: 10000

INSERTクエリ実行中に失敗したKeeperリクエストをリトライするための最大タイムアウト（ミリ秒単位）

可能な値：

- 正の整数。
- 0 — 最大タイムアウトは制限されない。

## insert_null_as_default {#insert_null_as_default}

タイプ: Bool

デフォルト値: 1

[nullable](/sql-reference/data-types/nullable)データ型のカラムに対して[NULL](/sql-reference/syntax#null)の代わりに[デフォルト値](/sql-reference/statements/create/table#default_values)を挿入するかどうかを有効または無効にします。
カラム型がnullableでない場合、この設定が無効になっていると、`NULL`を挿入すると例外が発生します。カラム型がnullableの場合、`NULL`値はこの設定に関係なくそのまま挿入されます。

この設定は[INSERT ... SELECT](../../sql-reference/statements/insert-into.md/#inserting-the-results-of-select)クエリに適用されます。なお、`SELECT`サブクエリは`UNION ALL`句で連結できることに注意してください。

可能な値：

- 0 — 非nullableカラムに`NULL`を挿入すると例外が発生します。
- 1 — デフォルトのカラム値が`NULL`の代わりに挿入されます。

## insert_quorum {#insert_quorum}

タイプ: UInt64Auto

デフォルト値: 0

:::note
この設定はSharedMergeTreeには適用されません。詳細は[SharedMergeTreeの整合性](/cloud/reference/shared-merge-tree#consistency)を参照してください。
:::

クォーラム書き込みを有効にします。

- `insert_quorum < 2`の場合、クォーラム書き込みは無効です。
- `insert_quorum >= 2`の場合、クォーラム書き込みは有効です。
- `insert_quorum = 'auto'`の場合、過半数（`number_of_replicas / 2 + 1`）をクォーラム数として使用します。

クォーラム書き込み

`INSERT`は、ClickHouseが`insert_quorum_timeout`内に`insert_quorum`のレプリカにデータを書き込むのに成功した場合のみ成功します。何らかの理由で、成功した書き込みのあるレプリカの数が`insert_quorum`に達しない場合、その書き込みは失敗と見なされ、ClickHouseはデータがすでに書き込まれているすべてのレプリカから挿入されたブロックを削除します。

`insert_quorum_parallel`が無効になっている場合、クォーラム内のすべてのレプリカは整合的であり、すなわちすべての以前の`INSERT`クエリからのデータを含みます（`INSERT`シーケンスが線形化されます）。`insert_quorum`を使用して書き込まれたデータを読む際に、`insert_quorum_parallel`が無効になっている場合は、[select_sequential_consistency](#select_sequential_consistency)を使用して`SELECT`クエリのための逐次整合性を有効にできます。

ClickHouseは例外を生成します。

- クエリ実行時に利用可能なレプリカの数が`insert_quorum`未満の場合。
- `insert_quorum_parallel`が無効で、前のブロックが`insert_quorum`のレプリカにまだ挿入されていない場合にデータを書き込もうとした場合。この状況は、ユーザーが前の`INSERT`クエリが`insert_quorum`で完了する前に同じテーブルに対して別の`INSERT`クエリを実行しようとしたときに発生する可能性があります。

詳細は以下を参照してください：

- [insert_quorum_timeout](#insert_quorum_timeout)
- [insert_quorum_parallel](#insert_quorum_parallel)
- [select_sequential_consistency](#select_sequential_consistency)

## insert_quorum_parallel {#insert_quorum_parallel}

タイプ: Bool

デフォルト値: 1

:::note
この設定はSharedMergeTreeには適用されません。詳細は[SharedMergeTreeの整合性](/cloud/reference/shared-merge-tree#consistency)を参照してください。
:::

クォーラム`INSERT`クエリに対する並列性を有効または無効にします。有効にすると、前のクエリがまだ完了していない間に追加の`INSERT`クエリを送信できます。無効にすると、同じテーブルへの追加の書き込みは拒否されます。

可能な値：

- 0 — 無効。
- 1 — 有効。

詳細は以下を参照してください：

- [insert_quorum](#insert_quorum)
- [insert_quorum_timeout](#insert_quorum_timeout)
- [select_sequential_consistency](#select_sequential_consistency)

## insert_quorum_timeout {#insert_quorum_timeout}

タイプ: ミリ秒

デフォルト値: 600000

クォーラムへの書き込みタイムアウト（ミリ秒単位）。タイムアウトが過ぎても書き込みが行われていない場合、ClickHouseは例外を生成し、クライアントは同じブロックを同じまたは他のレプリカに書き込むためにクエリを繰り返す必要があります。

詳細は以下を参照してください：

- [insert_quorum](#insert_quorum)
- [insert_quorum_parallel](#insert_quorum_parallel)
- [select_sequential_consistency](#select_sequential_consistency)

## insert_shard_id {#insert_shard_id}

タイプ: UInt64

デフォルト値: 0

0でない場合、データを同期的に挿入する[Distributed](/engines/table-engines/special/distributed)テーブルのシャードを指定します。

`insert_shard_id`の値が正しくない場合、サーバーは例外をスローします。

`requested_cluster`のシャードの数を取得するには、サーバーの設定をチェックするか、このクエリを使用します：

``` sql
SELECT uniq(shard_num) FROM system.clusters WHERE cluster = 'requested_cluster';
```

可能な値：

- 0 — 無効。
- 対応する[Distributed](/engines/table-engines/special/distributed)テーブルの`1`から`shards_num`までの任意の数字。

**例**

クエリ：

```sql
CREATE TABLE x AS system.numbers ENGINE = MergeTree ORDER BY number;
CREATE TABLE x_dist AS x ENGINE = Distributed('test_cluster_two_shards_localhost', currentDatabase(), x);
INSERT INTO x_dist SELECT * FROM numbers(5) SETTINGS insert_shard_id = 1;
SELECT * FROM x_dist ORDER BY number ASC;
```

結果：

``` text
┌─number─┐
│      0 │
│      0 │
│      1 │
│      1 │
│      2 │
│      2 │
│      3 │
│      3 │
│      4 │
│      4 │
└────────┘
```

## interactive_delay {#interactive_delay}

タイプ: UInt64

デフォルト値: 100000

リクエスト実行がキャンセルされているかどうかをチェックし、進捗を送信するためのマイクロ秒単位の間隔。

## intersect_default_mode {#intersect_default_mode}

タイプ: SetOperationMode

デフォルト値: ALL

INTERSECTクエリのデフォルトモードを設定します。可能な値: 空文字列、'ALL'、'DISTINCT'。空の場合、モードなしのクエリは例外をスローします。

## join_algorithm {#join_algorithm}

タイプ: JoinAlgorithm

デフォルト値: direct,parallel_hash,hash

使用される[JOIN](../../sql-reference/statements/select/join.md)アルゴリズムを指定します。

複数のアルゴリズムを指定でき、利用可能なものが特定のクエリで選択されます。選択は種類/厳格さおよびテーブルエンジンに基づきます。

可能な値：

- grace_hash

 [Grace hash join](https://en.wikipedia.org/wiki/Hash_join#Grace_hash_join)が使用されます。Grace hashは、メモリ使用を制限しながらパフォーマンスの良い複雑なJOINを提供します。

 最初のフェーズは、キー列のハッシュ値に基づいて右テーブルをNバケツに分割します（最初は、Nは`grace_hash_join_initial_buckets`です）。これは、各バケツが独立して処理できることを保証するように行われます。最初のバケツからの行はメモリ内ハッシュテーブルに追加され、他のバケツはディスクに保存されます。ハッシュテーブルがメモリ制限を超えると（例：[`max_bytes_in_join`](/operations/settings/query-complexity#settings-max_bytes_in_join)によって設定されたように）、バケツの数が増加し各行に割り当てられます。現在のバケツに属さない行はフラッシュされ、再割り当てされます。

 `INNER/LEFT/RIGHT/FULL ALL/ANY JOIN`をサポートします。

- hash

 [Hash join algorithm](https://en.wikipedia.org/wiki/Hash_join)が使用されます。種類と厳格さのすべての組み合わせと、`JOIN ON`セクションで`OR`で結合された複数のJOINキーをサポートする最も一般的な実装です。

 `hash`アルゴリズムを使用する場合、JOINの右部分はRAMにアップロードされます。

- parallel_hash

 `hash` JOINのバリエーションで、データをバケツに分割して、同時に複数のハッシュテーブルを構築して、このプロセスを加速します。

 `parallel_hash`アルゴリズムを使用する場合、JOINの右部分はRAMにアップロードされます。

- partial_merge

 [sort-merge algorithm](https://en.wikipedia.org/wiki/Sort-merge_join)の変種で、右テーブルのみが完全にソートされます。

 `RIGHT JOIN`と`FULL JOIN`は、`ALL`の厳密性でのみサポートされます（`SEMI`、`ANTI`、`ANY`、および`ASOF`はサポートされていません）。

 `partial_merge`アルゴリズムを使用する場合、ClickHouseはデータをソートし、ディスクにダンプします。ClickHouseの`partial_merge`アルゴリズムは、従来の実装とは少し異なります。まず、ClickHouseは右テーブルを結合キーでブロックごとにソートし、ソートされたブロックのためのmin-maxインデックスを作成します。そして、左テーブルの部分を`join key`でソートし、右テーブルと結合します。min-maxインデックスも、不要な右テーブルのブロックをスキップするために使用されます。

- direct

 このアルゴリズムは、右テーブルがキー-バリューリクエストをサポートしているストレージに適用できます。

 `direct`アルゴリズムは、左テーブルの行をキーとして使用して右テーブルでルックアップを行います。これは、[Dictionary](/engines/table-engines/special/dictionary)や[EmbeddedRocksDB](../../engines/table-engines/integrations/embedded-rocksdb.md)のような特殊なストレージでのみサポートされ、`LEFT`および`INNER` JOINのみで用いられます。

- auto

 `auto`に設定すると、最初に`hash` JOINを試し、メモリ制限が違反された場合は自動的に別のアルゴリズムに切り替えます。

- full_sorting_merge

 [Sort-merge algorithm](https://en.wikipedia.org/wiki/Sort-merge_join)で、結合する前にソートされたテーブルを完全にソートします。

- prefer_partial_merge

 ClickHouseは可能な場合は常に`partial_merge` JOINを使用しようとし、それ以外の場合は`hash`を使用します。*非推奨*、`partial_merge`、`hash`と同様です。

- default (非推奨)

 レガシー値で、もはや使用しないでください。
 `direct,hash`、すなわち、最初に直接JOINを試み、次にハッシュJOIN（この順序で）を使用します。

## join_any_take_last_row {#join_any_take_last_row}

タイプ: Bool

デフォルト値: 0

`ANY`厳格性を持つJOIN操作の挙動を変更します。

:::note
この設定は、[Join](../../engines/table-engines/special/join.md)エンジンテーブルを使用した`JOIN`操作のみに適用されます。
:::

可能な値：

- 0 — 右テーブルに一致する行が複数ある場合、最初に見つかった行のみが結合されます。
- 1 — 右テーブルに一致する行が複数ある場合、最後に見つかった行のみが結合されます。

詳細は以下を参照してください：

- [JOIN句](/sql-reference/statements/select/join)
- [Joinテーブルエンジン](../../engines/table-engines/special/join.md)
- [join_default_strictness](#join_default_strictness)

## join_default_strictness {#join_default_strictness}

タイプ: JoinStrictness

デフォルト値: ALL

[JOIN句](/sql-reference/statements/select/join)のデフォルトの厳格性を設定します。

可能な値：

- `ALL` — 右テーブルに一致する行が複数ある場合、ClickHouseは一致する行の[デカルト積](https://en.wikipedia.org/wiki/Cartesian_product)を作成します。これは標準SQLからの通常の`JOIN`の動作です。
- `ANY` — 右テーブルに一致する行が複数ある場合、最初に見つかった行のみが結合されます。右テーブルに一致する行が1つしかない場合、`ANY`と`ALL`の結果は同じです。
- `ASOF` — 不確実な一致のシーケンスを結合します。
- 空文字列 — クエリに`ALL`または`ANY`が指定されていない場合、ClickHouseは例外をスローします。

## join_on_disk_max_files_to_merge {#join_on_disk_max_files_to_merge}

タイプ: UInt64

デフォルト値: 64

ディスク上で実行されるMergeJoin操作において、並行ソートで許可されるファイルの数を制限します。

設定の値が大きいほど、使用されるRAMが増え、ディスクI/Oが減少します。

可能な値：

- 2から始まる任意の正の整数。

## join_output_by_rowlist_perkey_rows_threshold {#join_output_by_rowlist_perkey_rows_threshold}

タイプ: UInt64

デフォルト値: 5

ハッシュJOINにおいて、行リストで出力するかどうかを判断するための右テーブルのキーごとの平均行の下限。

## join_overflow_mode {#join_overflow_mode}

タイプ: OverflowMode

デフォルト値: throw

制限を超えた場合の動作。

## join_to_sort_maximum_table_rows {#join_to_sort_maximum_table_rows}
<ExperimentalBadge/>

タイプ: UInt64

デフォルト値: 10000

左または内部JOINで、右テーブルをキーで再整理するかどうかを判断するための右テーブルの最大行数。

## join_to_sort_minimum_perkey_rows {#join_to_sort_minimum_perkey_rows}
<ExperimentalBadge/>

タイプ: UInt64

デフォルト値: 40

左または内部JOINで右テーブルをキーで再整理するかどうかを判断するための、右テーブルのキーごとの平均行の下限。この設定により、スパーステーブルキーに対して最適化が適用されないことが保証されます。

## join_use_nulls {#join_use_nulls}

タイプ: Bool

デフォルト値: 0

[JOIN](../../sql-reference/statements/select/join.md)の挙動のタイプを設定します。テーブルをマージする際に、空のセルが現れることがあります。ClickHouseはこの設定に基づいてそれらを異なる方法で埋めます。

可能な値：

- 0 — 空のセルは対応するフィールドタイプのデフォルト値で埋まります。
- 1 — `JOIN`は標準SQLと同様に動作します。対応するフィールドのタイプは[Nullable](/sql-reference/data-types/nullable)に変換され、空のセルは[NULL](/sql-reference/syntax)で埋まります。

## joined_subquery_requires_alias {#joined_subquery_requires_alias}

タイプ: Bool

デフォルト値: 1

正しい名前の資格のために、結合されたサブクエリおよびテーブル関数にエイリアスを持たせることを強制します。

## kafka_disable_num_consumers_limit {#kafka_disable_num_consumers_limit}

タイプ: Bool

デフォルト値: 0

利用可能なCPUコアの数に依存するkafka_num_consumersに対する制限を無効にします。

## kafka_max_wait_ms {#kafka_max_wait_ms}

タイプ: ミリ秒

デフォルト値: 5000

リトライの前に[Kafka](/engines/table-engines/integrations/kafka)からメッセージを読むための待機時間（ミリ秒単位）。

可能な値：

- 正の整数。
- 0 — 無限のタイムアウト。

詳細は以下を参照してください：

- [Apache Kafka](https://kafka.apache.org/)

## keeper_map_strict_mode {#keeper_map_strict_mode}

タイプ: Bool

デフォルト値: 0

KeeperMap上の操作中に追加のチェックを強制します。例えば、既存のキーに対して挿入時に例外をスローします。

## keeper_max_retries {#keeper_max_retries}

タイプ: UInt64

デフォルト値: 10

一般的なKeeper操作の最大リトライ数。

## keeper_retry_initial_backoff_ms {#keeper_retry_initial_backoff_ms}

タイプ: UInt64

デフォルト値: 100

一般的なKeeper操作のための初期バックオフタイムアウト。

## keeper_retry_max_backoff_ms {#keeper_retry_max_backoff_ms}

タイプ: UInt64

デフォルト値: 5000

一般的なKeeper操作のための最大バックオフタイムアウト。

## least_greatest_legacy_null_behavior {#least_greatest_legacy_null_behavior}

タイプ: Bool

デフォルト値: 0

有効になっている場合、関数'least'および'greatest'は、引数のいずれかがNULLの場合にNULLを返します。

## legacy_column_name_of_tuple_literal {#legacy_column_name_of_tuple_literal}

タイプ: Bool

デフォルト値: 0

大きなタプルリテラルの要素名をハッシュの代わりにそのカラム名としてリストします。この設定は互換性の理由でのみ存在します。バージョン21.7未満から高いバージョンへのクラスタのローリングアップデート中に'true'に設定することが意味があります。

## lightweight_deletes_sync {#lightweight_deletes_sync}

タイプ: UInt64

デフォルト値: 2

[`mutations_sync`](#mutations_sync)と同じですが、軽量削除の実行のみを制御します。

可能な値：

- 0 - ミューテーションは非同期で実行されます。
- 1 - クエリは現在のサーバーで軽量削除が完了するまで待機します。
- 2 - クエリはすべてのレプリカで軽量削除が完了するまで待機します（存在する場合）。

**関連情報**

- [ALTERクエリの同期性](../../sql-reference/statements/alter/index.md/#synchronicity-of-alter-queries)
- [ミューテーション](../../sql-reference/statements/alter/index.md/#mutations)

## limit {#limit}

タイプ: UInt64

デフォルト値: 0

クエリ結果から取得する最大行数を設定します。[LIMIT](/sql-reference/statements/select/limit)句で設定された値を調整するため、クエリに指定された制限は、この設定で設定された制限を超えることはできません。

可能な値：

- 0 — 行数に制限はありません。
- 正の整数。

## live_view_heartbeat_interval {#live_view_heartbeat_interval}
<ExperimentalBadge/>

タイプ: 秒

デフォルト値: 15

ライブクエリが生存していることを示すためのハートビート間隔（秒単位）。

## load_balancing {#load_balancing}

タイプ: LoadBalancing

デフォルト値: random

分散クエリ処理に使用されるレプリカ選択のアルゴリズムを指定します。

ClickHouseは、以下のレプリカ選択アルゴリズムをサポートします。

- [ランダム](#load_balancing-random)（デフォルト）
- [最も近いホスト名](#load_balancing-nearest_hostname)
- [ホスト名のレーベンシュタイン距離](#load_balancing-hostname_levenshtein_distance)
- [順番に](#load_balancing-in_order)
- [最初またはランダム](#load_balancing-first_or_random)
- [ラウンドロビン](#load_balancing-round_robin)

詳細は以下を参照してください：

- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)

### ランダム（デフォルト） {#load_balancing-random}

``` sql
load_balancing = random
```

各レプリカのエラー数がカウントされます。クエリはエラーが最も少ないレプリカに送信され、同じ場合にはその中のいずれかに送信されます。
短所: サーバの近接性は考慮されず、レプリカに異なるデータがある場合、異なるデータが得られます。

### 最も近いホスト名 {#load_balancing-nearest_hostname}

``` sql
load_balancing = nearest_hostname
```

各レプリカのエラー数がカウントされます。5分ごとにエラー数は2で整合的に割り算されます。したがって、最近のエラー数が指数的に平滑化されます。最小のエラー数を持つレプリカが1つある場合、クエリはそれに送信されます。同じ最小のエラー数を持つ複数のレプリカがある場合、クエリは設定ファイルのサーバのホスト名に最も近いホスト名のレプリカに送信されます（同じ文字が同じ位置に存在する数まで、両ホスト名の最小長さまで）。

例えば、example01-01-1とexample01-01-2は1箇所異なり、example01-01-1とexample01-02-2は2箇所異なります。
この方法は単純に思えるかもしれませんが、ネットワークトポロジーに関する外部データを必要とせず、IPアドレスを比較することで、私たちのIPv6アドレスに対して複雑になることはありません。

したがって、同等のレプリカがある場合には、名前によって最も近いレプリカが優先されます。
同じサーバにクエリを送信する際、障害がない限り、分散クエリも同じサーバに送信されると仮定できます。したがって、レプリカに異なるデータが配置されていても、クエリはほぼ同じ結果を返します。

### ホスト名のレーベンシュタイン距離 {#load_balancing-hostname_levenshtein_distance}

``` sql
load_balancing = hostname_levenshtein_distance
```

`nearest_hostname`と同様ですが、ホスト名を[レーベンシュタイン距離](https://en.wikipedia.org/wiki/Levenshtein_distance)の観点から比較します。例えば：

``` text
example-clickhouse-0-0 ample-clickhouse-0-0
1

example-clickhouse-0-0 example-clickhouse-1-10
2

example-clickhouse-0-0 example-clickhouse-12-0
3
```
```yaml
title: 'ロードバランシング設定'
sidebar_label: 'ロードバランシング設定'
keywords: 'ロードバランシング, ClickHouse, 設定'
description: 'ClickHouse のロードバランシング設定の詳細について'
```

### In Order {#load_balancing-in_order}

``` sql
load_balancing = in_order
```

同じ数のエラーを持つレプリカは、設定ファイルに指定された順序でアクセスされます。この方法は、どのレプリカが好ましいか正確にわかっている場合に適しています。

### First or Random {#load_balancing-first_or_random}

``` sql
load_balancing = first_or_random
```

このアルゴリズムは、セット内の最初のレプリカを選択するか、最初のレプリカが利用できない場合にはランダムなレプリカを選択します。クロスレプリケーショントポロジーのセットアップでは効果的ですが、他の構成では無駄です。

`first_or_random` アルゴリズムは、`in_order` アルゴリズムの問題を解決します。`in_order` を使用すると、1つのレプリカがダウンすると次のレプリカにダブルロードがかかり、残りのレプリカは通常のトラフィックを処理します。`first_or_random` アルゴリズムを使用すると、利用可能なレプリカ間で負荷が均等に分配されます。

最初のレプリカを明示的に定義することも可能で、その際は `load_balancing_first_offset` 設定を使用します。これにより、クエリのワークロードをレプリカ間で再バランスさせる制御が向上します。

### Round Robin {#load_balancing-round_robin}

``` sql
load_balancing = round_robin
```

このアルゴリズムは、同じ数のエラーを持つレプリカに対してラウンドロビンポリシーを使用します（`round_robin` ポリシーのクエリのみが考慮されます）。

## load_balancing_first_offset {#load_balancing_first_offset}

タイプ: UInt64

デフォルト値: 0

FIRST_OR_RANDOM ロードバランシング戦略が使用されるときに、クエリを優先して送信するレプリカを指定します。

## load_marks_asynchronously {#load_marks_asynchronously}

タイプ: Bool

デフォルト値: 0

MergeTree マークを非同期でロードします。

## local_filesystem_read_method {#local_filesystem_read_method}

タイプ: String

デフォルト値: pread_threadpool

ローカルファイルシステムからデータを読み取る方法の1つ：read、pread、mmap、io_uring、pread_threadpool。 'io_uring' メソッドは実験的であり、Log、TinyLog、StripeLog、File、Set、Join、および同時読み取りと書き込みが存在する場合の追加可能なファイルを持つ他のテーブルには機能しません。

## local_filesystem_read_prefetch {#local_filesystem_read_prefetch}

タイプ: Bool

デフォルト値: 0

ローカルファイルシステムからデータを読み取るときにプレフェッチを使用すべきかどうか。

## lock_acquire_timeout {#lock_acquire_timeout}

タイプ: Seconds

デフォルト値: 120

ロック要求が失敗するまでの待機時間（秒）を定義します。

ロックタイムアウトは、テーブルの読み取り/書き込み操作を実行する際にデッドロックから保護するために使用されます。タイムアウトが切れると、ロック要求が失敗し、ClickHouse サーバーは「ロック試行がタイムアウトしました！デッドロックを回避しました。クライアントは再試行する必要があります。」という例外を投げます。

可能な値:

- 正の整数（秒単位）。
- 0 — ロックタイムアウトなし。

## log_comment {#log_comment}

タイプ: String

デフォルト値: 

[system.query_log](../system-tables/query_log.md) テーブルの `log_comment` フィールドに指定する値とサーバーログのコメントテキストを指定します。

これにより、サーバーログの可読性が向上します。さらに、[clickhouse-test](../../development/tests.md)を実行した後に、`system.query_log` からテストに関連するクエリを選択するのに役立ちます。

可能な値:

- [max_query_size](#max_query_size) を超えない限り、任意の文字列。最大クエリサイズを超えると、サーバーは例外を投げます。

**例**

クエリ:

``` sql
SET log_comment = 'log_comment test', log_queries = 1;
SELECT 1;
SYSTEM FLUSH LOGS;
SELECT type, query FROM system.query_log WHERE log_comment = 'log_comment test' AND event_date >= yesterday() ORDER BY event_time DESC LIMIT 2;
```

結果:

``` text
┌─type────────┬─query─────┐
│ QueryStart  │ SELECT 1; │
│ QueryFinish │ SELECT 1; │
└─────────────┴───────────┘
```

## log_formatted_queries {#log_formatted_queries}

タイプ: Bool

デフォルト値: 0

[system.query_log](../../operations/system-tables/query_log.md) システムテーブルにフォーマットされたクエリをログに記録します（`formatted_query` カラムがポピュレートされます）。

可能な値:

- 0 — フォーマットされたクエリはシステムテーブルに記録されません。
- 1 — フォーマットされたクエリがシステムテーブルに記録されます。

## log_processors_profiles {#log_processors_profiles}

タイプ: Bool

デフォルト値: 1

テーブル処理中にプロセッサーが費やした時間を `system.processors_profile_log` テーブルに記録します。

詳細情報:

- [`system.processors_profile_log`](../../operations/system-tables/processors_profile_log.md)
- [`EXPLAIN PIPELINE`](../../sql-reference/statements/explain.md/#explain-pipeline)

## log_profile_events {#log_profile_events}

タイプ: Bool

デフォルト値: 1

クエリ性能統計を `query_log`、`query_thread_log` および `query_views_log` にログします。

## log_queries {#log_queries}

タイプ: Bool

デフォルト値: 1

クエリログを設定します。

この設定で ClickHouse に送信されたクエリは、[query_log](../../operations/server-configuration-parameters/settings.md/#query-log) サーバー設定パラメータのルールに従ってログに記録されます。

例:

``` text
log_queries=1
```

## log_queries_cut_to_length {#log_queries_cut_to_length}

タイプ: UInt64

デフォルト値: 100000

クエリの長さが指定されたしきい値（バイト）を超える場合、クエリをクエリログに書き込むときにカットします。また、通常のテキストログに印刷されるクエリの長さも制限します。

## log_queries_min_query_duration_ms {#log_queries_min_query_duration_ms}

タイプ: Milliseconds

デフォルト値: 0

有効にすると（非ゼロ）、この設定の値よりも速いクエリはログに記録されません（これは [MySQL Slow Query Log](https://dev.mysql.com/doc/refman/5.7/slow-query-log.html) のための `long_query_time` と考えることができます）。基本的に、以下のテーブルには見つからないことを意味します:

- `system.query_log`
- `system.query_thread_log`

ログに入るのは以下のタイプのクエリのみです:

- `QUERY_FINISH`
- `EXCEPTION_WHILE_PROCESSING`

- タイプ: ミリ秒
- デフォルト値: 0（すべてのクエリ）

## log_queries_min_type {#log_queries_min_type}

タイプ: LogQueriesType

デフォルト値: QUERY_START

ログに記録する `query_log` 最小タイプ。

可能な値:

- `QUERY_START` (`=1`)
- `QUERY_FINISH` (`=2`)
- `EXCEPTION_BEFORE_START` (`=3`)
- `EXCEPTION_WHILE_PROCESSING` (`=4`)

これは、どのエンティティが `query_log` に記録されるかを制限するために使用できます。たとえば、エラーのみを興味がある場合は、`EXCEPTION_WHILE_PROCESSING` を使用できます。

``` text
log_queries_min_type='EXCEPTION_WHILE_PROCESSING'
```

## log_queries_probability {#log_queries_probability}

タイプ: Float

デフォルト値: 1

ユーザーが、指定された確率でランダムに選択されたクエリのサンプルのみを [query_log](../../operations/system-tables/query_log.md)、[query_thread_log](../../operations/system-tables/query_thread_log.md)、および [query_views_log](../../operations/system-tables/query_views_log.md) システムテーブルに書き込むことを許可します。これにより、1秒あたりの大量のクエリの負荷を軽減するのに役立ちます。

可能な値:

- 0 — クエリはシステムテーブルに記録されません。
- 正の浮動小数点数 [0..1] の範囲。たとえば、設定値が `0.5` の場合、約半分のクエリがシステムテーブルにログされます。
- 1 — すべてのクエリがシステムテーブルに記録されます。

## log_query_settings {#log_query_settings}

タイプ: Bool

デフォルト値: 1

クエリ設定を `query_log` および OpenTelemetry span log にログします。

## log_query_threads {#log_query_threads}

タイプ: Bool

デフォルト値: 0

クエリスレッドのログ設定。

この設定は、[log_queries](#log_queries) が true の場合にのみ効果があります。この設定が有効な場合、ClickHouse によって実行されるクエリのスレッドが [system.query_thread_log](../../operations/system-tables/query_thread_log.md) テーブルにログされます。

可能な値:

- 0 — 無効。
- 1 — 有効。

**例**

``` text
log_query_threads=1
```

## log_query_views {#log_query_views}

タイプ: Bool

デフォルト値: 1

クエリビューのログ設定。

この設定が有効な ClickHouse によって実行されるクエリに関連するビュー（マテリアライズドビューまたはライブビュー）がある場合、これらは [query_views_log](/operations/server-configuration-parameters/settings#query_views_log) サーバー設定パラメータに記録されます。

例:

``` text
log_query_views=1
```

## low_cardinality_allow_in_native_format {#low_cardinality_allow_in_native_format}

タイプ: Bool

デフォルト値: 1

[LowCardinality](../../sql-reference/data-types/lowcardinality.md) データ型を [Native](../../interfaces/formats.md/#native) フォーマットで使用することを許可または制限します。

`LowCardinality` の使用が制限されている場合、ClickHouse サーバーは `SELECT` クエリの場合に `LowCardinality` カラムを通常のカラムに変換し、`INSERT` クエリの場合には通常のカラムを `LowCardinality` カラムに変換します。

この設定は主に、`LowCardinality` データ型をサポートしていないサードパーティのクライアント向けに必要です。

可能な値:

- 1 — `LowCardinality` の使用が制限されていません。
- 0 — `LowCardinality` の使用が制限されています。

## low_cardinality_max_dictionary_size {#low_cardinality_max_dictionary_size}

タイプ: UInt64

デフォルト値: 8192

[LowCardinality](../../sql-reference/data-types/lowcardinality.md) データ型の共有グローバル辞書の最大行数を、ストレージファイルシステムに書き込むことができます。この設定は、辞書の無制限な成長に対処するためにRAMの問題を防ぎます。最大辞書サイズの制限によりエンコードできないすべてのデータは、ClickHouseは普通の方法で書き込みます。

可能な値:

- 任意の正の整数。

## low_cardinality_use_single_dictionary_for_part {#low_cardinality_use_single_dictionary_for_part}

タイプ: Bool

デフォルト値: 0

データパーツの単一辞書の使用を有効または無効にします。

デフォルトでは、ClickHouse サーバーは辞書のサイズを監視しており、辞書がオーバーフローすると、サーバーは次の辞書を書き始めます。複数の辞書を作成しないようにするには、`low_cardinality_use_single_dictionary_for_part = 1` を設定します。

可能な値:

- 1 — データパーツの複数の辞書を作成することが禁止されます。
- 0 — データパーツの複数の辞書を作成することが禁止されません。

## materialize_skip_indexes_on_insert {#materialize_skip_indexes_on_insert}

タイプ: Bool

デフォルト値: 1

INSERT の際にスキップインデックスを構築および保存します。無効にすると、スキップインデックスはマージ中または明示的な MATERIALIZE INDEX によって構築および保存されます。

## materialize_statistics_on_insert {#materialize_statistics_on_insert}

タイプ: Bool

デフォルト値: 1

INSERT の際に統計を構築して挿入します。無効にすると、統計はマージ中または明示的な MATERIALIZE STATISTICS によって構築および保存されます。

## materialize_ttl_after_modify {#materialize_ttl_after_modify}

タイプ: Bool

デフォルト値: 1

ALTER MODIFY TTL クエリの後に古いデータに TTL を適用します。

## materialized_views_ignore_errors {#materialized_views_ignore_errors}

タイプ: Bool

デフォルト値: 0

MATERIALIZED VIEW のエラーを無視し、MV に関係なくオリジナルのブロックをテーブルに提供します。

## max_analyze_depth {#max_analyze_depth}

タイプ: UInt64

デフォルト値: 5000

インタープリタによって実行される最大解析数。

## max_ast_depth {#max_ast_depth}

タイプ: UInt64

デフォルト値: 1000

クエリ構文木の最大深さです。解析後にチェックされます。

## max_ast_elements {#max_ast_elements}

タイプ: UInt64

デフォルト値: 50000

ノード数におけるクエリ構文木の最大サイズです。解析後にチェックされます。

## max_autoincrement_series {#max_autoincrement_series}

タイプ: UInt64

デフォルト値: 1000

`generateSeriesID` 関数によって作成されるシリーズの最大数。

各シリーズは Keeper 内のノードを表すため、数百万人を超えないことを推奨します。

## max_backup_bandwidth {#max_backup_bandwidth}

タイプ: UInt64

デフォルト値: 0

特定のバックアップに対してサーバー上での最大読み取り速度（バイト毎秒）。ゼロは無制限を意味します。

## max_block_size {#max_block_size}

タイプ: UInt64

デフォルト値: 65409

ClickHouse では、データはブロックによって処理され、これはカラムパーツのセットです。単一ブロックの内部処理サイクルは効率的ですが、各ブロックを処理する際には目立ったコストがかかります。

`max_block_size` 設定は、テーブルからデータをロードするときに単一ブロックに含めることが推奨される最大行数を示します。`max_block_size` のサイズのブロックは、テーブルから常にロードされるわけではなく、ClickHouse がより少ないデータを取得する必要があると判断した場合には、小さなブロックが処理されます。

ブロックサイズは、各ブロックを処理する際の目立ったコストを避けるためには大きすぎてはいけません。また、LIMIT 条項を持つクエリが最初のブロックを処理した後に迅速に実行されることを確保するためには、小さすぎてもいけません。`max_block_size` を設定する際は、大量のカラムを複数スレッドで抽出する際にあまり多くのメモリを消費せず、ある程度のキャッシュローカリティが保たれることを目指すべきです。

## max_bytes_before_external_group_by {#max_bytes_before_external_group_by}

タイプ: UInt64

デフォルト値: 0

GROUP BY 操作中のメモリ使用量がバイトでこのしきい値を超える場合、'外部集約' モードをアクティブにします（データをディスクにスピルします）。推奨値は利用可能なシステムメモリの半分です。

## max_bytes_before_external_sort {#max_bytes_before_external_sort}

タイプ: UInt64

デフォルト値: 0

ORDER BY 操作中のメモリ使用量がバイトでこのしきい値を超える場合、'外部ソート' モードをアクティブにします（データをディスクにスピルします）。推奨値は利用可能なシステムメモリの半分です。

## max_bytes_before_remerge_sort {#max_bytes_before_remerge_sort}

タイプ: UInt64

デフォルト値: 1000000000

ORDER BY と LIMIT の場合に、メモリ使用量が指定されたしきい値を超えると、最終的なマージの前にブロックを追加でマージする手順を実行して、上位の LIMIT 行だけを保持します。

## max_bytes_in_distinct {#max_bytes_in_distinct}

タイプ: UInt64

デフォルト値: 0

DISTINCT の実行のための状態の最大サイズ（未圧縮バイト）です。

## max_bytes_in_join {#max_bytes_in_join}

タイプ: UInt64

デフォルト値: 0

JOIN のためのハッシュテーブルの最大サイズ（メモリ内のバイト数）です。

## max_bytes_in_set {#max_bytes_in_set}

タイプ: UInt64

デフォルト値: 0

IN セクションの実行結果として得られるセットの最大サイズ（メモリ内のバイト数）です。

## max_bytes_ratio_before_external_group_by {#max_bytes_ratio_before_external_group_by}

タイプ: Double

デフォルト値: 0.5

外部 GROUP BY を有効にする前の使用メモリの比率。これを 0.6 に設定すると、メモリ使用量がクエリの許可メモリの 60% に達した時点で外部 GROUP BY が使用されます。

## max_bytes_ratio_before_external_sort {#max_bytes_ratio_before_external_sort}

タイプ: Double

デフォルト値: 0.5

外部 ORDER BY を有効にする前の使用メモリの比率。これを 0.6 に設定すると、メモリ使用量がクエリの許可メモリの 60% に達した時点で外部 ORDER BY が使用されます。

## max_bytes_to_read {#max_bytes_to_read}

タイプ: UInt64

デフォルト値: 0

最も「深い」ソースからの読み取りバイト（圧縮後）の制限。それは、最も深いサブクエリにのみ適用されます。リモートサーバーから読み取る場合は、リモートサーバーでのみチェックされます。

## max_bytes_to_read_leaf {#max_bytes_to_read_leaf}

タイプ: UInt64

デフォルト値: 0

分散クエリのリーフノードでの読み取りバイト（圧縮後）の制限。制限はローカル読み取りにのみ適用され、ルートノードでの最終的なマージステージは除外されます。この設定は `prefer_localhost_replica=1` で不安定です。

## max_bytes_to_sort {#max_bytes_to_sort}

タイプ: UInt64

デフォルト値: 0

ORDER BY 操作のために処理される必要がある（未圧縮）バイト数が指定された量を超える場合、その動作は 'sort_overflow_mode' によって決定され、デフォルトでは例外を投げます。

## max_bytes_to_transfer {#max_bytes_to_transfer}

タイプ: UInt64

デフォルト値: 0

GLOBAL IN/JOIN セクションが実行されたときに、送信される外部テーブルの最大サイズ（未圧縮バイト数）です。

## max_columns_to_read {#max_columns_to_read}

タイプ: UInt64

デフォルト値: 0

クエリが指定された数のカラムを超えて読み取る必要がある場合、例外が投げられます。ゼロの値は無制限を意味します。この設定は、あまりにも複雑なクエリを防ぐのに役立ちます。

## max_compress_block_size {#max_compress_block_size}

タイプ: UInt64

デフォルト値: 1048576

テーブルへの書き込みのために圧縮する前の未圧縮データブロックの最大サイズです。デフォルトは1,048,576（1 MiB）です。小さなブロックサイズを指定すると、一般的に圧縮率がわずかに減少し、圧縮および解凍速度がわずかに向上し、メモリ消費が削減されます。

:::note
これはエキスパートレベルの設定であり、ClickHouseを初めて使用する場合は変更しないでください。
:::

圧縮用のブロック（バイトの塊）をクエリ処理用のブロック（テーブルからの行のセット）と混同しないでください。

## max_concurrent_queries_for_all_users {#max_concurrent_queries_for_all_users}

タイプ: UInt64

デフォルト値: 0

この設定の値が、同時に処理されているクエリの数と同じか少ない場合、例外を投げます。

例: `max_concurrent_queries_for_all_users` を99に設定し、データベース管理者は自己のために100に設定して、サーバーが過負荷の際にも調査のためのクエリを実行できるようにします。

この設定を1つのクエリまたはユーザーに対して変更しても、他のクエリには影響しません。

可能な値:

- 正の整数。
- 0 — 限度なし。

**例**

```xml
<max_concurrent_queries_for_all_users>99</max_concurrent_queries_for_all_users>
```

**関連情報**

- [max_concurrent_queries](/operations/server-configuration-parameters/settings#max_concurrent_queries)

## max_concurrent_queries_for_user {#max_concurrent_queries_for_user}

タイプ: UInt64

デフォルト値: 0

ユーザーごとに同時に処理されるクエリの最大数。

可能な値:

- 正の整数。
- 0 — 限度なし。

**例**

``` xml
<max_concurrent_queries_for_user>5</max_concurrent_queries_for_user>
```

## max_distributed_connections {#max_distributed_connections}

タイプ: UInt64

デフォルト値: 1024

単一の Distributed テーブルへの単一クエリのために、リモートサーバーとの同時接続の最大数。クラスタ内のサーバーの数以上の値を設定することを推奨します。

以下のパラメータは、Distributed テーブルの作成時（およびサーバーの起動時）にのみ使用されるため、ランタイム中に変更する理由はありません。

## max_distributed_depth {#max_distributed_depth}

タイプ: UInt64

デフォルト値: 5

[Distributed](../../engines/table-engines/special/distributed.md) テーブルに対する再帰クエリの最大深さを制限します。

値が超えた場合、サーバーは例外を投げます。

可能な値:

- 正の整数。
- 0 — 無制限の深さ。

## max_download_buffer_size {#max_download_buffer_size}

タイプ: UInt64

デフォルト値: 10485760

各スレッドごとの並列ダウンロードの最大バッファサイズ（例：URLエンジン用）。

## max_download_threads {#max_download_threads}

タイプ: MaxThreads

デフォルト値: 4

データをダウンロードするためのスレッドの最大数（例：URLエンジン用）。

## max_estimated_execution_time {#max_estimated_execution_time}

タイプ: Seconds

デフォルト値: 0

クエリの推定実行時間の最大値（秒）。

## max_execution_speed {#max_execution_speed}

タイプ: UInt64

デフォルト値: 0

毎秒の最大実行行数。

## max_execution_speed_bytes {#max_execution_speed_bytes}

タイプ: UInt64

デフォルト値: 0

毎秒の最大実行バイト数。

## max_execution_time {#max_execution_time}

タイプ: Seconds

デフォルト値: 0

クエリの実行時間が指定された秒数を超えた場合、その動作は 'timeout_overflow_mode' によって決定され、デフォルトでは例外を投げます。タイムアウトはチェックされ、クエリはデータ処理中の指定された場所でのみ停止できます。現在、集約状態のマージ中やクエリ分析中に停止することはできず、実際の実行時間はこの設定の値よりも大きくなります。

## max_execution_time_leaf {#max_execution_time_leaf}

タイプ: Seconds

デフォルト値: 0

max_execution_time と同様の意味ですが、分散クエリのリーフノードでのみ適用され、タイムアウト動作は 'timeout_overflow_mode_leaf' によって決定され、デフォルトでは例外を投げます。

## max_expanded_ast_elements {#max_expanded_ast_elements}

タイプ: UInt64

デフォルト値: 500000

エイリアスとアスタリスクの展開後のノード数におけるクエリ構文木の最大サイズです。

## max_fetch_partition_retries_count {#max_fetch_partition_retries_count}

タイプ: UInt64

デフォルト値: 5

別のホストからパーティションを取得する際のリトライ回数です。

## max_final_threads {#max_final_threads}

タイプ: MaxThreads

デフォルト値: 'auto(12)'

[FINAL](/sql-reference/statements/select/from#final-modifier) 修飾子を使用した `SELECT` クエリのデータ読み取りフェーズに対しての最大並列スレッド数を設定します。

可能な値:

- 正の整数。
- 0 または 1 — 無効。`SELECT` クエリは単一スレッドで実行されます。

## max_http_get_redirects {#max_http_get_redirects}

タイプ: UInt64

デフォルト値: 0

許可される最大の HTTP GET リダイレクトホップ数。悪意のあるサーバーが不明なサービスにリクエストをリダイレクトするのを防ぐために追加のセキュリティ対策が講じられます。

外部サーバーが他のアドレスにリダイレクトする場合、ただし、そのアドレスが企業のインフラストラクチャ内部に見える場合、内部サーバーにHTTPリクエストを送信することで、認証をバイパスしたり、Redis や Memcached などの他のサービスを要求することができます。内部インフラストラクチャ（ローカルホスト上で実行されている何かを含む）がない場合、またはサーバーを信頼している場合、リダイレクトの使用は安全です。ただし、URL が HTTP を使用している場合、リモートサーバーだけでなく、ISP および中間のすべてのネットワークも信頼しなければならないことを念頭に置いてください。

## max_hyperscan_regexp_length {#max_hyperscan_regexp_length}

タイプ: UInt64

デフォルト値: 0

[hyperscan マルチマッチ関数](/sql-reference/functions/string-search-functions#multimatchany)における各正規表現の最大長を定義します。

可能な値:

- 正の整数。
- 0 - 長さは制限されていません。

**例**

クエリ:

```sql
SELECT multiMatchAny('abcd', ['ab','bcd','c','d']) SETTINGS max_hyperscan_regexp_length = 3;
```

結果:

```text
┌─multiMatchAny('abcd', ['ab', 'bcd', 'c', 'd'])─┐
│                                              1 │
└────────────────────────────────────────────────┘
```

クエリ:

```sql
SELECT multiMatchAny('abcd', ['ab','bcd','c','d']) SETTINGS max_hyperscan_regexp_length = 2;
```

結果:

```text
Exception: Regexp length too large.
```

**関連情報**

- [max_hyperscan_regexp_total_length](#max_hyperscan_regexp_total_length)

## max_hyperscan_regexp_total_length {#max_hyperscan_regexp_total_length}

タイプ: UInt64

デフォルト値: 0

各 [hyperscan マルチマッチ関数](/sql-reference/functions/string-search-functions#multimatchany) におけるすべての正規表現の最大総長を設定します。

可能な値:

- 正の整数。
- 0 - 長さは制限されていません。

**例**

クエリ:

```sql
SELECT multiMatchAny('abcd', ['a','b','c','d']) SETTINGS max_hyperscan_regexp_total_length = 5;
```

結果:

```text
┌─multiMatchAny('abcd', ['a', 'b', 'c', 'd'])─┐
│                                           1 │
└─────────────────────────────────────────────┘
```

クエリ:

```sql
SELECT multiMatchAny('abcd', ['ab','bc','c','d']) SETTINGS max_hyperscan_regexp_total_length = 5;
```

結果:

```text
Exception: Total regexp lengths too large.
```

**関連情報**

- [max_hyperscan_regexp_length](#max_hyperscan_regexp_length)

## max_insert_block_size {#max_insert_block_size}

タイプ: UInt64

デフォルト値: 1048449

テーブルに挿入するために形成する行数のブロックサイズです。この設定は、サーバーがブロックを形成する場合にのみ適用されます。たとえば、HTTPインターフェース経由のINSERTの場合、サーバーはデータ形式を解析し、指定されたサイズのブロックを形成します。しかし、clickhouse-client を使用する場合、クライアントがデータを自ら解析し、サーバーの 'max_insert_block_size' 設定は挿入されるブロックのサイズに影響しません。この設定は、INSERT SELECT を使用する場合には目的がありません。なぜなら、データは SELECT によって形成された同じブロックを使用して挿入されるからです。

デフォルトは、`max_block_size` よりもわずかに大きくなっています。これは、特定のテーブルエンジン（`*MergeTree`）が、挿入された各ブロックのためにディスク上にデータパートを形成する、大規模なエンティティであるためです。同様に、`*MergeTree` テーブルは挿入中にデータをソートするため、十分なサイズのブロックサイズにより、RAM内でより多くのデータをソートすることができます。

## max_insert_delayed_streams_for_parallel_write {#max_insert_delayed_streams_for_parallel_write}

タイプ: UInt64

デフォルト値: 0

最終部分フラッシュを遅延させるための最大ストリーム（カラム）の数。デフォルト - auto（基盤となるストレージが並列書き込みをサポートする場合は1000、それ以外は無効）。

## max_insert_threads {#max_insert_threads}

タイプ: UInt64

デフォルト値: 0

`INSERT SELECT` クエリを実行するための最大スレッド数。

可能な値:

- 0（または1） — `INSERT SELECT` は並列実行されません。
- 正の整数。1より大きい。

クラウドのデフォルト値: サービスサイズに応じて `2` から `4` まで。

並列 `INSERT SELECT` は、`SELECT` 部分が並列で実行されている場合にのみ効果があります。このための設定は [max_threads](#max_threads) です。
高い値は、より多くのメモリ使用につながります。

## max_joined_block_size_rows {#max_joined_block_size_rows}

タイプ: UInt64

デフォルト値: 65409

JOIN 結果の最大ブロックサイズ（結合アルゴリズムがサポートする場合）。0 は無制限を意味します。

## max_limit_for_ann_queries {#max_limit_for_ann_queries}
<ExperimentalBadge/>

タイプ: UInt64

デフォルト値: 1000000

この設定より大きい LIMIT を持つ SELECT クエリは、ベクトル類似インデックスを使用できません。ベクトル類似インデックスにおけるメモリのオーバーフローを防ぐのに役立ちます。

## max_live_view_insert_blocks_before_refresh {#max_live_view_insert_blocks_before_refresh}
<ExperimentalBadge/>

タイプ: UInt64

デフォルト値: 64

マージ可能なブロックがドロップされ、クエリが再実行される最大挿入ブロック数を制限します。

## max_local_read_bandwidth {#max_local_read_bandwidth}

タイプ: UInt64

デフォルト値: 0

ローカル読み取りの最大速度（バイト毎秒）です。

## max_local_write_bandwidth {#max_local_write_bandwidth}

タイプ: UInt64

デフォルト値: 0

ローカル書き込みの最大速度（バイト毎秒）です。

## max_memory_usage {#max_memory_usage}

タイプ: UInt64

デフォルト値: 0

単一クエリの処理のための最大メモリ使用量。ゼロは無制限を意味します。

## max_memory_usage_for_user {#max_memory_usage_for_user}

タイプ: UInt64

デフォルト値: 0

ユーザーが同時に実行されているクエリの最大メモリ使用量。ゼロは無制限を意味します。

## max_network_bandwidth {#max_network_bandwidth}

タイプ: UInt64

デフォルト値: 0

ネットワーク越しのデータ交換速度をバイト毎秒で制限します。この設定はすべてのクエリに適用されます。

可能な値:

- 正の整数。
- 0 — 帯域幅制御は無効です。

## max_network_bandwidth_for_all_users {#max_network_bandwidth_for_all_users}

タイプ: UInt64

デフォルト値: 0

ネットワーク上でのデータ交換速度をバイト毎秒で制限します。この設定は、サーバー上で同時に実行されるすべてのクエリに適用されます。

可能な値:

- 正の整数。
- 0 — データ速度制御は無効です。

## max_network_bandwidth_for_user {#max_network_bandwidth_for_user}

タイプ: UInt64

デフォルト値: 0

ネットワーク越しのデータ交換速度をバイト毎秒で制限します。この設定は、単一のユーザーによって実行されるすべての同時クエリに適用されます。

可能な値:

- 正の整数。
- 0 — データ速度制御は無効です。

## max_network_bytes {#max_network_bytes}

タイプ: UInt64

デフォルト値: 0

クエリを実行する際にネットワーク上で受信または送信されるデータ量（バイト）を制限します。この設定は、各個別のクエリに適用されます。

可能な値:

- 正の整数。
- 0 — データ量制御は無効です。

## max_number_of_partitions_for_independent_aggregation {#max_number_of_partitions_for_independent_aggregation}

タイプ: UInt64

デフォルト値: 128

最適化を適用するためのテーブル内のパーティションの最大数です。

## max_parallel_replicas {#max_parallel_replicas}

タイプ: NonZeroUInt64

デフォルト値: 1000

クエリを実行する際の各シャードにおけるレプリカの最大数です。

可能な値:

- 正の整数。

**追加情報**

このオプションは、使用される設定によって異なる結果を生じます。

:::note
この設定は、結合やサブクエリが含まれている場合に不正確な結果を生成します。すべてのテーブルが特定の要件を満たしていない場合。詳細は、[Distributed Subqueries and max_parallel_replicas](/operations/settings/settings#max_parallel_replicas) を参照してください。
:::

### Parallel processing using `SAMPLE` key

クエリは、複数のサーバーで並列に実行されるとより速く処理される可能性があります。しかし、次の場合にはクエリ性能が低下することがあります：

- サンプリングキーの位置がパーティションキー内で効率的な範囲スキャンを許可しない場合。
- テーブルにサンプリングキーを追加することが、他のカラムによるフィルタリングを非効率的にする場合。
- サンプリングキーが計算コストの高い式の場合。
- クラスタの遅延分布に長い尾があるため、より多くのサーバーをクエリすると全体のクエリ遅延が増加する場合。

### Parallel processing using [parallel_replicas_custom_key](#parallel_replicas_custom_key)

この設定は、すべてのレプリケートテーブルにとって有用です。
```

## max_parser_backtracks {#max_parser_backtracks}

タイプ: UInt64

デフォルト値: 1000000

最大パーサーバックトラック（再帰的に下降するパースプロセスで異なる選択肢を試す回数）。

## max_parser_depth {#max_parser_depth}

タイプ: UInt64

デフォルト値: 1000

再帰的な下降パーサーにおける最大再帰深度の制限。スタックサイズを制御できます。

可能な値:

- 正の整数。
- 0 — 再帰の深度は無制限。

## max_parsing_threads {#max_parsing_threads}

タイプ: MaxThreads

デフォルト値: 'auto(12)'

並行パーシングをサポートする入力フォーマットでデータをパースするためのスレッドの最大数。デフォルトでは、自動的に決定されます。

## max_partition_size_to_drop {#max_partition_size_to_drop}

タイプ: UInt64

デフォルト値: 50000000000

クエリ時にパーティションをドロップする際の制限。値0は、制限なしでパーティションをドロップできることを意味します。

クラウドのデフォルト値: 1 TB。

:::note
このクエリ設定は、そのサーバー設定の同等物を上書きします。詳細は [max_partition_size_to_drop](/operations/server-configuration-parameters/settings#max_partition_size_to_drop) を参照してください。
:::

## max_partitions_per_insert_block {#max_partitions_per_insert_block}

タイプ: UInt64

デフォルト値: 100

単一のINSERTブロック内の最大パーティション数の制限。ゼロは無制限を意味します。ブロックにパーティションが多すぎる場合、例外をスローします。この設定は安全性の閾値です。なぜなら、パーティション数が多すぎることは一般的に誤解されているためです。

## max_partitions_to_read {#max_partitions_to_read}

タイプ: Int64

デフォルト値: -1

1つのクエリでアクセスできる最大パーティション数の制限。&lt;= 0 は無制限を意味します。

## max_parts_to_move {#max_parts_to_move}

タイプ: UInt64

デフォルト値: 1000

1つのクエリで移動できるパーツの数の制限。ゼロは無制限を意味します。

## max_query_size {#max_query_size}

タイプ: UInt64

デフォルト値: 262144

SQLパーサーによってパースされるクエリ文字列の最大バイト数。
INSERTクエリのVALUES句のデータは、別のストリームパーサーによって処理され（O(1) RAMを消費します）、この制限には影響されません。

:::note
`max_query_size` はSQLクエリ内で設定できません（例: `SELECT now() SETTINGS max_query_size=10000`）。ClickHouseはクエリをパースするためのバッファを割り当てる必要があり、このバッファサイズは `max_query_size` 設定によって決定され、クエリが実行される前に設定する必要があります。
:::

## max_read_buffer_size {#max_read_buffer_size}

タイプ: UInt64

デフォルト値: 1048576

ファイルシステムから読み込むためのバッファの最大サイズ。

## max_read_buffer_size_local_fs {#max_read_buffer_size_local_fs}

タイプ: UInt64

デフォルト値: 131072

ローカルファイルシステムから読み込むためのバッファの最大サイズ。0に設定されている場合、max_read_buffer_size が使用されます。

## max_read_buffer_size_remote_fs {#max_read_buffer_size_remote_fs}

タイプ: UInt64

デフォルト値: 0

リモートファイルシステムから読み込むためのバッファの最大サイズ。0に設定されている場合、max_read_buffer_size が使用されます。

## max_recursive_cte_evaluation_depth {#max_recursive_cte_evaluation_depth}

タイプ: UInt64

デフォルト値: 1000

再帰的CTE評価の深さに関する最大制限。

## max_remote_read_network_bandwidth {#max_remote_read_network_bandwidth}

タイプ: UInt64

デフォルト値: 0

読み込みのためのネットワーク上のデータ交換速度（バイト毎秒）の最大値。

## max_remote_write_network_bandwidth {#max_remote_write_network_bandwidth}

タイプ: UInt64

デフォルト値: 0

書き込みのためのネットワーク上のデータ交換速度（バイト毎秒）の最大値。

## max_replica_delay_for_distributed_queries {#max_replica_delay_for_distributed_queries}

タイプ: UInt64

デフォルト値: 300

分散クエリのための遅延レプリカを無効にします。詳細は [Replication](../../engines/table-engines/mergetree-family/replication.md) を参照してください。

秒単位での設定です。レプリカの遅延が設定値以上の場合、そのレプリカは使用されません。

可能な値:

- 正の整数。
- 0 — レプリカの遅延はチェックされません。

非ゼロの遅延を持つレプリカを使用しないようにするには、このパラメータを1に設定します。

レプリケートテーブルを指す分散テーブルから `SELECT` を実行する際に使用されます。

## max_result_bytes {#max_result_bytes}

タイプ: UInt64

デフォルト値: 0

結果サイズ（解凍後）の制限。しきい値に達した場合、クエリはデータブロックの処理を停止しますが、最後の結果ブロックを切り捨てることはありません。そのため結果サイズはしきい値を超えることがあります。注意点：このしきい値にはメモリ内の結果サイズも考慮されます。結果サイズが小さくても、LowCardinalityカラムの辞書やAggregateFunctionカラムのアリーナに関連すると、しきい値を超える可能性があります。この設定はかなり低レベルであり、慎重に使用する必要があります。

## max_result_rows {#max_result_rows}

タイプ: UInt64

デフォルト値: 0

結果サイズ（行数）の制限。しきい値に達した場合、クエリはデータブロックの処理を停止しますが、最後の結果ブロックを切り捨てることはありません。そのため結果サイズはしきい値を超えることがあります。

## max_rows_in_distinct {#max_rows_in_distinct}

タイプ: UInt64

デフォルト値: 0

DISTINCTを実行中の最大要素数。

## max_rows_in_join {#max_rows_in_join}

タイプ: UInt64

デフォルト値: 0

JOINのためのハッシュテーブルの最大サイズ（行数）。

## max_rows_in_set {#max_rows_in_set}

タイプ: UInt64

デフォルト値: 0

INセクションの実行結果として得られるセットの最大サイズ（要素数）。

## max_rows_in_set_to_optimize_join {#max_rows_in_set_to_optimize_join}

タイプ: UInt64

デフォルト値: 0

結合前に互いの行セットによってフィルタリングするためのセットの最大サイズ。

可能な値:

- 0 — 無効。
- 任意の正の整数。

## max_rows_to_group_by {#max_rows_to_group_by}

タイプ: UInt64

デフォルト値: 0

GROUP BY中に指定された行数（ユニークなGROUP BYキー）が生成される場合、動作は 'group_by_overflow_mode' によって決定されます。デフォルトでは例外をスローしますが、近似GROUP BYモードに切り替えることもできます。

## max_rows_to_read {#max_rows_to_read}

タイプ: UInt64

デフォルト値: 0

最も「深い」ソースから読み取る行数の制限。すなわち、最も深いサブクエリ内でのみ。リモートサーバーから読み取る場合、これはリモートサーバーでのみチェックされます。

## max_rows_to_read_leaf {#max_rows_to_read_leaf}

タイプ: UInt64

デフォルト値: 0

分散クエリに対するリーフノードでの読み取り行数の制限。制限はローカル読み込みのみに適用され、ルートノードでの最終的なマージステージは除外されます。この設定は、prefer_localhost_replica=1 で不安定です。

## max_rows_to_sort {#max_rows_to_sort}

タイプ: UInt64

デフォルト値: 0

ORDER BY操作のために処理する必要があるレコードが指定数を超える場合、動作は 'sort_overflow_mode' によって決定されます。デフォルトでは例外をスローします。

## max_rows_to_transfer {#max_rows_to_transfer}

タイプ: UInt64

デフォルト値: 0

GLOBAL IN/JOINセクションが実行されるときに転送される外部テーブルの最大サイズ（行数）。

## max_sessions_for_user {#max_sessions_for_user}

タイプ: UInt64

デフォルト値: 0

ユーザーの同時セッションの最大数。

## max_size_to_preallocate_for_aggregation {#max_size_to_preallocate_for_aggregation}

タイプ: UInt64

デフォルト値: 1000000000000

集計の前に、すべてのハッシュテーブルで許可される要素の合計をプリエイリアライズします。

## max_size_to_preallocate_for_joins {#max_size_to_preallocate_for_joins}

タイプ: UInt64

デフォルト値: 1000000000000

結合前に、すべてのハッシュテーブルで許可される要素の合計をプリエイリアライズします。

## max_streams_for_merge_tree_reading {#max_streams_for_merge_tree_reading}

タイプ: UInt64

デフォルト値: 0

ゼロでない場合、MergeTreeテーブルのための読み取りストリームの数を制限します。

## max_streams_multiplier_for_merge_tables {#max_streams_multiplier_for_merge_tables}

タイプ: Float

デフォルト値: 5

マージテーブルから読み込む際に、より多くのストリームを要求します。ストリームは、マージテーブルが使用するテーブルに分配されます。これにより、スレッド間での作業の均一な分配が可能になり、マージされたテーブルがサイズが異なる場合に特に役立ちます。

## max_streams_to_max_threads_ratio {#max_streams_to_max_threads_ratio}

タイプ: Float

デフォルト値: 1

スレッド数より多くのソースを使用できるようにし、スレッド間での作業をより均等に分配します。これは一時的な解決策と考えられます。将来的にはソース数とスレッド数を等しくすることが可能になると予想されており、各ソースが自動的に利用可能な作業を選択するようになるでしょう。

## max_subquery_depth {#max_subquery_depth}

タイプ: UInt64

デフォルト値: 100

クエリに指定された数のネストされたサブクエリが含まれている場合、例外をスローします。これにより、クラスタのユーザーをクエリの混乱から保護するための健全性チェックが行われます。

## max_table_size_to_drop {#max_table_size_to_drop}

タイプ: UInt64

デフォルト値: 50000000000

クエリ時にテーブルを削除する際の制限。値0は、制限なしで全テーブルを削除できることを意味します。

クラウドのデフォルト値: 1 TB。

:::note
このクエリ設定は、そのサーバー設定の同等物を上書きします。詳細は [max_table_size_to_drop](/operations/server-configuration-parameters/settings#max_table_size_to_drop) を参照してください。
:::

## max_temporary_columns {#max_temporary_columns}

タイプ: UInt64

デフォルト値: 0

クエリが中間計算の結果としてメモリ内に生成される一時カラムの数が指定を超える場合、例外がスローされます。ゼロの値は無制限を意味します。この設定は、複雑すぎるクエリを防ぐために有用です。

## max_temporary_data_on_disk_size_for_query {#max_temporary_data_on_disk_size_for_query}

タイプ: UInt64

デフォルト値: 0

同時に実行されるすべてのクエリの一時ファイルがディスク上で消費する最大データ量（バイト）。ゼロは無制限を意味します。

## max_temporary_data_on_disk_size_for_user {#max_temporary_data_on_disk_size_for_user}

タイプ: UInt64

デフォルト値: 0

同時に実行されるユーザークエリがディスク上で消費する一時ファイルの最大データ量（バイト）。ゼロは無制限を意味します。

## max_temporary_non_const_columns {#max_temporary_non_const_columns}

タイプ: UInt64

デフォルト値: 0

'max_temporary_columns' 設定に似ていますが、非定数カラムにのみ適用されます。定数カラムはコストが低いため、より多くの定数を許可するのが合理的です。

## max_threads {#max_threads}

タイプ: MaxThreads

デフォルト値: 'auto(12)'

クエリ処理スレッドの最大数です。リモートサーバーからデータを取得するためのスレッドは除外されます（'max_distributed_connections' パラメータを参照）。

このパラメータは、クエリ処理パイプラインの同じステージを並行して実行するスレッドに適用されます。
例えば、テーブルから読み込む際に、関数付きの式を評価し、WHEREでフィルタリングし、GROUP BYのために事前集約が、少なくとも 'max_threads' 数のスレッドを使用して並行して実行できる場合、'max_threads' が使用されます。

LIMITのためにクエリが速やかに終了する場合は、より低い 'max_threads' を設定できます。例えば、必要な数のエントリがすべてのブロックに存在し、max_threads = 8 の場合、8つのブロックが取得されますが、実際には1つだけを読み取れば十分です。

`max_threads` の値が小さいほど、消費するメモリは少なくなります。

## max_threads_for_indexes {#max_threads_for_indexes}

タイプ: UInt64

デフォルト値: 0

インデックスを処理するための最大スレッド数。

## max_untracked_memory {#max_untracked_memory}

タイプ: UInt64

デフォルト値: 4194304

小さな割り当てと解放は、スレッドローカル変数にグループ化され、指定された値より大きくなるまで追跡またはプロファイルされません。値が 'memory_profiler_step' より大きい場合、実質的には 'memory_profiler_step' に下げられます。

## memory_overcommit_ratio_denominator {#memory_overcommit_ratio_denominator}

タイプ: UInt64

デフォルト値: 1073741824

これは、ハードリミットがグローバルレベルで達成されたときのソフトメモリ制限を表します。
この値は、クエリのオーバーコミット比率を計算するために使用されます。
ゼロはクエリをスキップすることを意味します。
[メモリオーバーコミット](/memory-overcommit.md) について詳しく読むことができます。

## memory_overcommit_ratio_denominator_for_user {#memory_overcommit_ratio_denominator_for_user}

タイプ: UInt64

デフォルト値: 1073741824

これは、ハードリミットがユーザーレベルで達成されたときのソフトメモリ制限を表します。
この値は、クエリのオーバーコミット比率を計算するために使用されます。
ゼロはクエリをスキップすることを意味します。
[メモリオーバーコミット](/memory-overcommit.md) について詳しく読むことができます。

## memory_profiler_sample_max_allocation_size {#memory_profiler_sample_max_allocation_size}

タイプ: UInt64

デフォルト値: 0

指定された値以下のサイズのランダムな割り当てを、`memory_profiler_sample_probability` の確率で収集します。0は無効を意味します。このしきい値を期待通りに機能させるためには、'max_untracked_memory' を0に設定する必要があります。

## memory_profiler_sample_min_allocation_size {#memory_profiler_sample_min_allocation_size}

タイプ: UInt64

デフォルト値: 0

指定された値以上のサイズのランダムな割り当てを、`memory_profiler_sample_probability` の確率で収集します。0は無効を意味します。このしきい値を期待通りに機能させるためには、'max_untracked_memory' を0に設定する必要があります。

## memory_profiler_sample_probability {#memory_profiler_sample_probability}

タイプ: Float

デフォルト値: 0

ランダムな割り当てと解放を収集し、それを system.trace_log に 'MemorySample' trace_type で書き込みます。確率は、割り当てのサイズに関係なく、すべての割り当て/解放に対して適用されます（`memory_profiler_sample_min_allocation_size` と `memory_profiler_sample_max_allocation_size` で変更できます）。サンプリングは、追跡されないメモリの量が 'max_untracked_memory' を超えたときにのみ発生します。追加の詳細なサンプリングを行いたい場合は、'max_untracked_memory' を0に設定してください。

## memory_profiler_step {#memory_profiler_step}

タイプ: UInt64

デフォルト値: 4194304

メモリプロファイラのステップを設定します。クエリのメモリ使用量が、各次のステップのバイト数より大きくなるたびに、メモリプロファイラは割り当てスタックトレースを収集し、[trace_log](/operations/system-tables/trace_log) に書き込みます。

可能な値:

- 正の整数のバイト数。

- メモリプロファイラをオフにするための0。

## memory_tracker_fault_probability {#memory_tracker_fault_probability}

タイプ: Float

デフォルト値: 0

`例外の安全性` をテストするために、指定された確率でメモリを割り当てるたびに例外をスローします。

## memory_usage_overcommit_max_wait_microseconds {#memory_usage_overcommit_max_wait_microseconds}

タイプ: UInt64

デフォルト値: 5000000

ユーザーレベルでメモリオーバーコミットが発生した場合に、スレッドがメモリが解放されるのを待つ最大時間（マイクロ秒）。

タイムアウトが reached し、メモリが解放されない場合、例外がスローされます。
[メモリオーバーコミット](/memory-overcommit.md) について詳しく読むことができます。

## merge_table_max_tables_to_look_for_schema_inference {#merge_table_max_tables_to_look_for_schema_inference}

タイプ: UInt64

デフォルト値: 1000

明示的なスキーマなしで `Merge` テーブルを作成する際、または `merge` テーブル関数を使用する際に、指定された数の一致するテーブルの中からスキーマを推測します。
より多くのテーブルが存在する場合、最初に指定された数のテーブルからスキーマを推測します。

## merge_tree_coarse_index_granularity {#merge_tree_coarse_index_granularity}

タイプ: UInt64

デフォルト値: 8

データを検索する際、ClickHouseはインデックスファイル内のデータマークを確認します。ClickHouseが必要なキーが範囲内にあることを確認すると、その範囲を `merge_tree_coarse_index_granularity` サブレンジに分割し、必要なキーを再帰的に検索します。

可能な値:

- 任意の正の偶整数。

## merge_tree_compact_parts_min_granules_to_multibuffer_read {#merge_tree_compact_parts_min_granules_to_multibuffer_read}

<CloudAvailableBadge/>

タイプ: UInt64

デフォルト値: 16

ClickHouse Cloud のみで効果があります。MergeTreeテーブルのコンパクト部分のストライプ内で、並行読み込みとプリフェッチをサポートするために使用するグラニュールの数。リモートファイルシステムから読み込む場合、マルチバッファリーダーの使用が読み取り要求の数を増加させます。

## merge_tree_determine_task_size_by_prewhere_columns {#merge_tree_determine_task_size_by_prewhere_columns}

タイプ: Bool

デフォルト値: 1

読み取りタスクのサイズを決定するために、プレウエアカラムのサイズのみを使用するかどうか。

## merge_tree_max_bytes_to_use_cache {#merge_tree_max_bytes_to_use_cache}

タイプ: UInt64

デフォルト値: 2013265920

ClickHouseが1クエリで `merge_tree_max_bytes_to_use_cache` バイト以上を読み込む必要がある場合、未圧縮ブロックのキャッシュを使用しません。

未圧縮ブロックのキャッシュは、クエリ用に抽出されたデータを格納します。ClickHouseはこのキャッシュを使用して、再度の小さなクエリに対する応答を迅速化します。この設定は、大量のデータを読み取るクエリによるキャッシュの破棄から保護します。[uncompressed_cache_size](/operations/server-configuration-parameters/settings#uncompressed_cache_size) サーバー設定が未圧縮ブロックのキャッシュのサイズを定義します。

可能な値:

- 任意の正の整数。

## merge_tree_max_rows_to_use_cache {#merge_tree_max_rows_to_use_cache}

タイプ: UInt64

デフォルト値: 1048576

ClickHouseが1クエリで `merge_tree_max_rows_to_use_cache` 行以上を読み込む必要がある場合、未圧縮ブロックのキャッシュを使用しません。

未圧縮ブロックのキャッシュは、クエリ用に抽出されたデータを格納します。ClickHouseはこのキャッシュを使用して、再度の小さなクエリに対する応答を迅速化します。この設定は、大量のデータを読み取るクエリによるキャッシュの破棄から保護します。[uncompressed_cache_size](/operations/server-configuration-parameters/settings#uncompressed_cache_size) サーバー設定が未圧縮ブロックのキャッシュのサイズを定義します。

可能な値:

- 任意の正の整数。

## merge_tree_min_bytes_for_concurrent_read {#merge_tree_min_bytes_for_concurrent_read}

タイプ: UInt64

デフォルト値: 251658240

[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)エンジンテーブルの1ファイルから読み取るバイト数が `merge_tree_min_bytes_for_concurrent_read` を超える場合、ClickHouseはこのファイルに対して並行して読み取ることを試みます。

可能な値:

- 正の整数。

## merge_tree_min_bytes_for_concurrent_read_for_remote_filesystem {#merge_tree_min_bytes_for_concurrent_read_for_remote_filesystem}

タイプ: UInt64

デフォルト値: 0

リモートファイルシステムから読み込む際に、[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)エンジンが読みを並行化できるようになるまでの最低バイト数。使用をお勧めしません。

可能な値:

- 正の整数。

## merge_tree_min_bytes_for_seek {#merge_tree_min_bytes_for_seek}

タイプ: UInt64

デフォルト値: 0

ファイル内で読み取る2つのデータブロック間の距離が `merge_tree_min_bytes_for_seek` バイト未満の場合、ClickHouseは両方のブロックを含むファイルの範囲を逐次的に読み取ります。これにより、余分なシークを回避します。

可能な値:

- 任意の正の整数。

## merge_tree_min_bytes_per_task_for_remote_reading {#merge_tree_min_bytes_per_task_for_remote_reading}

タイプ: UInt64

デフォルト値: 2097152

タスクあたりの最小読み取りバイト数。

## merge_tree_min_read_task_size {#merge_tree_min_read_task_size}

タイプ: UInt64

デフォルト値: 8

タスクサイズのハード下限（グラニュールの数が少なく、使用可能なスレッド数が多い場合でも、小さなタスクを割り当てません）。

## merge_tree_min_rows_for_concurrent_read {#merge_tree_min_rows_for_concurrent_read}

タイプ: UInt64

デフォルト値: 163840

[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)テーブルのファイルから読み取る行数が `merge_tree_min_rows_for_concurrent_read` を超える場合、ClickHouseはこのファイルから並行して読み取ることを試みます。

可能な値:

- 正の整数。

## merge_tree_min_rows_for_concurrent_read_for_remote_filesystem {#merge_tree_min_rows_for_concurrent_read_for_remote_filesystem}

タイプ: UInt64

デフォルト値: 0

リモートファイルシステムから読み込む際に、[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)エンジンが読みを並行化できるようになるまでの最低行数。使用をお勧めしません。

可能な値:

- 正の整数。

## merge_tree_min_rows_for_seek {#merge_tree_min_rows_for_seek}

タイプ: UInt64

デフォルト値: 0

ファイル内で読み取る2つのデータブロック間の距離が `merge_tree_min_rows_for_seek` 行未満の場合、ClickHouseはファイルをシークすることなく、データを逐次的に読み取ります。

可能な値:

- 任意の正の整数。

## merge_tree_read_split_ranges_into_intersecting_and_non_intersecting_injection_probability {#merge_tree_read_split_ranges_into_intersecting_and_non_intersecting_injection_probability}

タイプ: Float

デフォルト値: 0

`PartsSplitter` のテストのため - MergeTreeから読み取る際に、指定された確率で読み取る範囲を交差している範囲と非交差している範囲に分割します。

## merge_tree_use_const_size_tasks_for_remote_reading {#merge_tree_use_const_size_tasks_for_remote_reading}

タイプ: Bool

デフォルト値: 1

リモートテーブルから読み取る際に、定数サイズのタスクを使用するかどうか。

## merge_tree_use_deserialization_prefixes_cache {#merge_tree_use_deserialization_prefixes_cache}

タイプ: Bool

デフォルト値: 1

MergeTree内のワイドパーツから読み取る際に、ファイルプレフィックスからのカラムメタデータキャッシュを有効にします。

## merge_tree_use_prefixes_deserialization_thread_pool {#merge_tree_use_prefixes_deserialization_thread_pool}

タイプ: Bool

デフォルト値: 1

MergeTree内のワイドパーツで並行してプレフィックスを読み取るためのスレッドプールの使用を有効にします。このスレッドプールのサイズは、サーバー設定 `max_prefixes_deserialization_thread_pool_size` によって制御されます。

## merge_tree_use_v1_object_and_dynamic_serialization {#merge_tree_use_v1_object_and_dynamic_serialization}

タイプ: Bool

デフォルト値: 0

有効にすると、MergeTree内のJSONおよびDynamic型のV1シリアライズバージョンがV2の代わりに使用されます。この設定を変更するには、サーバーを再起動する必要があります。

## metrics_perf_events_enabled {#metrics_perf_events_enabled}

タイプ: Bool

デフォルト値: 0

有効にすると、クエリ実行中にパフォーマンスイベントの一部が測定されます。

## metrics_perf_events_list {#metrics_perf_events_list}

タイプ: String

デフォルト値: 

カンマ区切りのパフォーマンスメトリクスのリストで、クエリの実行中に測定されます。空の場合はすべてのイベントを意味します。利用可能なイベントについては、ソース内の PerfEventInfo を参照してください。

## min_bytes_to_use_direct_io {#min_bytes_to_use_direct_io}

タイプ: UInt64

デフォルト値: 0

ストレージディスクへの直接I/Oアクセスを使用するために必要な最小データ量。

ClickHouseは、データをテーブルから読み取るときにこの設定を使用します。読み取るデータの総ストレージボリュームが `min_bytes_to_use_direct_io` バイトを超える場合、ClickHouseは `O_DIRECT` オプションを使用してストレージディスクからデータを読み取ります。

可能な値:

- 0 — 直接I/Oが無効。
- 正の整数。

## min_bytes_to_use_mmap_io {#min_bytes_to_use_mmap_io}

タイプ: UInt64

デフォルト値: 0

これは実験的な設定です。カーネルからユーザースペースにデータをコピーせずに大きなファイルを読み取るための最小メモリ量を設定します。推奨しきい値は約64 MBで、[mmap/munmap](https://en.wikipedia.org/wiki/Mmap) は遅いためです。これは大きなファイルのみに意味があり、データがページキャッシュに存在する場合にのみ助けになります。

可能な値:

- 正の整数。
- 0 — 大きなファイルはカーネルからユーザースペースにデータをコピーして読み込まれます。

## min_chunk_bytes_for_parallel_parsing {#min_chunk_bytes_for_parallel_parsing}

タイプ: NonZeroUInt64

デフォルト値: 10485760

- タイプ: unsigned int
- デフォルト値: 1 MiB

各スレッドが並行してパースする最小チャンクサイズ（バイト数）。

## min_compress_block_size {#min_compress_block_size}

タイプ: UInt64

デフォルト値: 65536

[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)テーブル用です。クエリの処理時の待機時間を減少させるために、次のマークを書き込む際にサイズが少なくとも `min_compress_block_size` の場合、ブロックが圧縮されます。デフォルトは65,536です。

圧縮されていないデータの実際のサイズが `max_compress_block_size` 未満の場合、ブロックのサイズはこの値以上で、かつ1マークのデータボリューム以上となります。

例を見てみましょう。テーブル作成時に `index_granularity` が8192に設定されていたと仮定します。

UInt32型のカラムを書き込む場合（値あたり4バイト）。8192行を書き込むと、合計32 KBのデータとなります。`min_compress_block_size` = 65,536 のため、2つのマークごとに圧縮ブロックが形成されます。

文字列型のURLカラムを書き込む場合（値あたり平均60バイト）。8192行を書き込むと、平均で500 KB未満のデータとなります。65,536よりも大きいため、各マークごとに圧縮ブロックが形成されます。この場合、ディスクからのデータ読み取り時に、単一のマークの範囲内の余分なデータは解凍されません。

:::note
これは専門家レベルの設定であり、ClickHouseを始めたばかりの場合は変更しないでください。
:::

## min_count_to_compile_aggregate_expression {#min_count_to_compile_aggregate_expression}

タイプ: UInt64

デフォルト値: 3

JITコンパイルを開始するための同一の集約式の最小数。`compile_aggregate_expressions` 設定が有効になっている場合にのみ機能します。

可能な値:

- 正の整数。
- 0 — 同一の集約式は常にJITコンパイルされます。

## min_count_to_compile_expression {#min_count_to_compile_expression}

タイプ: UInt64

デフォルト値: 3

同一の式を実行するための最小カウント。コンパイルされる前。

## min_count_to_compile_sort_description {#min_count_to_compile_sort_description}

タイプ: UInt64

デフォルト値: 3

同一のソート記述の数。JITコンパイルの前。

## min_execution_speed {#min_execution_speed}

タイプ: UInt64

デフォルト値: 0

最小行の実行速度（毎秒）。

## min_execution_speed_bytes {#min_execution_speed_bytes}

タイプ: UInt64

デフォルト値: 0

最小バイトの実行速度（毎秒）。

## min_external_sort_block_bytes {#min_external_sort_block_bytes}

タイプ: UInt64

デフォルト値: 104857600

ディスクにダンプされる外部ソートの最小ブロックサイズ（バイト）。ファイル数が多すぎるのを避けるため。

## min_external_table_block_size_bytes {#min_external_table_block_size_bytes}

タイプ: UInt64

デフォルト値: 268402944

ブロックが十分に大きくない場合、外部テーブルに渡されるブロックを指定されたバイトサイズに圧縮します。

## min_external_table_block_size_rows {#min_external_table_block_size_rows}

タイプ: UInt64

デフォルト値: 1048449

ブロックが十分に大きくない場合、外部テーブルに渡されるブロックを指定された行数に圧縮します。

## min_free_disk_bytes_to_perform_insert {#min_free_disk_bytes_to_perform_insert}

タイプ: UInt64

デフォルト値: 0

挿入を行うために必要な最低空きディスクスペースバイト数。

## min_free_disk_ratio_to_perform_insert {#min_free_disk_ratio_to_perform_insert}

タイプ: Float

デフォルト値: 0

挿入を行うために必要な最小空きディスクスペース比率。

## min_free_disk_space_for_temporary_data {#min_free_disk_space_for_temporary_data}

タイプ: UInt64

デフォルト値: 0

外部ソートと集計で使用する一時データを書き込む際に維持するための最小ディスクスペース。

## min_hit_rate_to_use_consecutive_keys_optimization {#min_hit_rate_to_use_consecutive_keys_optimization}

タイプ: Float

デフォルト値: 0.5

集約における連続キーの最適化に使用されるキャッシュの最小ヒット率。

## min_insert_block_size_bytes {#min_insert_block_size_bytes}

タイプ: UInt64

デフォルト値: 268402944

`INSERT` クエリによってテーブルに挿入できるブロック内の最小バイト数を設定します。サイズの小さいブロックは大きなものに圧縮されます。

可能な値:

- 正の整数。
- 0 — 圧縮無効。

## min_insert_block_size_bytes_for_materialized_views {#min_insert_block_size_bytes_for_materialized_views}

タイプ: UInt64

デフォルト値: 0

`INSERT` クエリによってテーブルに挿入できるブロック内の最小バイト数を設定します。サイズの小さいブロックは大きなものに圧縮されます。この設定は、[materialized view](../../sql-reference/statements/create/view.md) に挿入されるブロックにのみ適用されます。この設定を調整することで、materialized view にプッシュする際のブロックの圧縮を制御し、過剰なメモリ使用を回避します。

可能な値:

- 任意の正の整数。
- 0 — 圧縮無効。

**参照**

- [min_insert_block_size_bytes](#min_insert_block_size_bytes)

## min_insert_block_size_rows {#min_insert_block_size_rows}

タイプ: UInt64

デフォルト値: 1048449

`INSERT` クエリによってテーブルに挿入できるブロック内の最小行数を設定します。サイズの小さいブロックは大きなものに圧縮されます。

可能な値:

- 正の整数。
- 0 — 圧縮無効。
```
```yaml
title: '設定オプション'
sidebar_label: '設定オプション'
keywords: 'ClickHouse, 設定, オプション'
description: 'ClickHouseの設定オプションの詳細'
```

## min_insert_block_size_rows_for_materialized_views {#min_insert_block_size_rows_for_materialized_views}

Type: UInt64

Default value: 0

`INSERT` クエリによってテーブルに挿入できるブロック内の最小行数を設定します。サイズの小さいブロックは大きいブロックに圧縮されます。この設定は、[マテリアライズドビュー](../../sql-reference/statements/create/view.md)に挿入されるブロックにのみ適用されます。この設定を調整することで、マテリアライズドビューにプッシュする際のブロック圧縮を制御し、過剰なメモリ使用を回避します。

Possible values:

- 任意の正の整数。
- 0 — 圧縮無効。

**See Also**

- [min_insert_block_size_rows](#min_insert_block_size_rows)

## min_joined_block_size_bytes {#min_joined_block_size_bytes}

Type: UInt64

Default value: 524288

JOIN結果の最小ブロックサイズ（JOINアルゴリズムがサポートしている場合）。0は無制限を意味します。

## mongodb_throw_on_unsupported_query {#mongodb_throw_on_unsupported_query}

Type: Bool

Default value: 1

有効にすると、MongoDBクエリを生成できない場合、MongoDBテーブルはエラーを返します。そうでない場合、ClickHouseはテーブル全体を読み取り、ローカルで処理します。このオプションは、レガシー実装や 'allow_experimental_analyzer=0' の場合には適用されません。

## move_all_conditions_to_prewhere {#move_all_conditions_to_prewhere}

Type: Bool

Default value: 1

WHEREからPREWHEREへすべての有効な条件を移動します。

## move_primary_key_columns_to_end_of_prewhere {#move_primary_key_columns_to_end_of_prewhere}

Type: Bool

Default value: 1

主キー列を含むPREWHERE条件をANDチェーンの末尾に移動します。これらの条件は主キー分析中に考慮される可能性が高く、PREWHEREフィルタリングに大きく寄与しない可能性があります。

## multiple_joins_try_to_keep_original_names {#multiple_joins_try_to_keep_original_names}

Type: Bool

Default value: 0

複数のJOIN書き換え時にトップレベルの表現リストにエイリアスを追加しない。

## mutations_execute_nondeterministic_on_initiator {#mutations_execute_nondeterministic_on_initiator}

Type: Bool

Default value: 0

trueの場合、定数非決定論的関数（例えば、`now()`関数）はイニシエーターで実行され、`UPDATE`および`DELETE`クエリ内のリテラルに置き換えられます。これにより、定数非決定論的関数を使用してミューテーションを実行する際に、レプリカ間でデータを同期させることができます。デフォルト値: `false`。

## mutations_execute_subqueries_on_initiator {#mutations_execute_subqueries_on_initiator}

Type: Bool

Default value: 0

trueの場合、スカラーサブクエリがイニシエーターで実行され、`UPDATE`および`DELETE`クエリ内のリテラルに置き換えられます。デフォルト値: `false`。

## mutations_max_literal_size_to_replace {#mutations_max_literal_size_to_replace}

Type: UInt64

Default value: 16384

`UPDATE`および`DELETE`クエリ内で置き換える最大バイト数の直列化リテラルサイズ。上記の2つの設定のうち少なくとも1つが有効な場合のみ影響します。デフォルト値: 16384（16 KiB）。

## mutations_sync {#mutations_sync}

Type: UInt64

Default value: 0

`synchronously`に `ALTER TABLE ... UPDATE|DELETE|MATERIALIZE INDEX|MATERIALIZE PROJECTION|MATERIALIZE COLUMN|MATERIALIZE STATISTICS`クエリを実行できます（[ミューテーション](../../sql-reference/statements/alter/index.md/#mutations)）。

Possible values:

- 0 - ミューテーションは非同期で実行されます。
- 1 - クエリは現在のサーバー上のすべてのミューテーションが完了するまで待機します。
- 2 - クエリはすべてのレプリカでのすべてのミューテーションが完了するまで待機します（存在する場合）。 

## mysql_datatypes_support_level {#mysql_datatypes_support_level}

Type: MySQLDataTypesSupport

Default value: 

MySQLタイプが対応するClickHouseタイプにどのように変換されるかを定義します。`decimal`、`datetime64`、`date2Date32`、または`date2String`の任意の組み合わせでカンマ区切りのリストです。
- `decimal`: 精度が可能な場合、`NUMERIC`および`DECIMAL`タイプを`Decimal`に変換します。
- `datetime64`: 精度が`0`でない場合、`DATETIME`および`TIMESTAMP`タイプを`DateTime64`に変換します。
- `date2Date32`: `DATE`を`Date`の代わりに`Date32`に変換します。これは`date2String`よりも優先されます。
- `date2String`: `DATE`を`Date`の代わりに`String`に変換します。`datetime64`によりオーバーライドされます。

## mysql_map_fixed_string_to_text_in_show_columns {#mysql_map_fixed_string_to_text_in_show_columns}

Type: Bool

Default value: 1

有効にすると、[FixedString](../../sql-reference/data-types/fixedstring.md) ClickHouseデータ型は[SHOW COLUMNS](../../sql-reference/statements/show.md/#show_columns)で`TEXT`として表示されます。

MySQLワイヤプロトコルを介して接続が行われている場合のみ効果があります。

- 0 - `BLOB`を使用。
- 1 - `TEXT`を使用。

## mysql_map_string_to_text_in_show_columns {#mysql_map_string_to_text_in_show_columns}

Type: Bool

Default value: 1

有効にすると、[String](../../sql-reference/data-types/string.md) ClickHouseデータ型は[SHOW COLUMNS](../../sql-reference/statements/show.md/#show_columns)で`TEXT`として表示されます。

MySQLワイヤプロトコルを介して接続が行われている場合のみ効果があります。

- 0 - `BLOB`を使用。
- 1 - `TEXT`を使用。

## mysql_max_rows_to_insert {#mysql_max_rows_to_insert}

Type: UInt64

Default value: 65536

MySQLストレージエンジンのバッチ挿入時の最大行数です。

## network_compression_method {#network_compression_method}

Type: String

Default value: LZ4

サーバー間、およびサーバーと[clickhouse-client](../../interfaces/cli.md)間の通信に使用されるデータ圧縮方法を設定します。

Possible values:

- `LZ4` — LZ4圧縮方法を設定します。
- `ZSTD` — ZSTD圧縮方法を設定します。

**See Also**

- [network_zstd_compression_level](#network_zstd_compression_level)

## network_zstd_compression_level {#network_zstd_compression_level}

Type: Int64

Default value: 1

ZSTD圧縮のレベルを調整します。[network_compression_method](#network_compression_method)が`ZSTD`に設定されているときのみ使用されます。

Possible values:

- 1から15までの正の整数。

## normalize_function_names {#normalize_function_names}

Type: Bool

Default value: 1

関数名をその標準名に正規化します。

## number_of_mutations_to_delay {#number_of_mutations_to_delay}

Type: UInt64

Default value: 0

変異したテーブルが未完了の変異を少なくともその数だけ含む場合、テーブルの変異を人工的に遅らせます。0 - 無効。

## number_of_mutations_to_throw {#number_of_mutations_to_throw}

Type: UInt64

Default value: 0

変異したテーブルが未完了の変異を少なくともその数だけ含む場合、'Too many mutations ...'例外をスローします。0 - 無効。

## odbc_bridge_connection_pool_size {#odbc_bridge_connection_pool_size}

Type: UInt64

Default value: 16

ODBCブリッジ内の各接続設定文字列の接続プールサイズです。

## odbc_bridge_use_connection_pooling {#odbc_bridge_use_connection_pooling}

Type: Bool

Default value: 1

ODBCブリッジで接続プーリングを使用します。falseに設定すると、毎回新しい接続が作成されます。

## offset {#offset}

Type: UInt64

Default value: 0

クエリから行を返す前にスキップする行数を設定します。[OFFSET](/sql-reference/statements/select/offset)句によって設定されたオフセットを調整します。

Possible values:

- 0 — 行はスキップされません。
- 正の整数。

**Example**

入力テーブル:

``` sql
CREATE TABLE test (i UInt64) ENGINE = MergeTree() ORDER BY i;
INSERT INTO test SELECT number FROM numbers(500);
```

クエリ:

``` sql
SET limit = 5;
SET offset = 7;
SELECT * FROM test LIMIT 10 OFFSET 100;
```
結果:

``` text
┌───i─┐
│ 107 │
│ 108 │
│ 109 │
└─────┘
```

## opentelemetry_start_trace_probability {#opentelemetry_start_trace_probability}

Type: Float

Default value: 0

ClickHouseが実行されたクエリのためにトレースを開始できる確率を設定します（親の[トレースコンテキスト](https://www.w3.org/TR/trace-context/)が供給されていない場合）。

Possible values:

- 0 — すべての実行されたクエリのトレースが無効になります（親のトレースコンテキストが供給されていない場合）。
- 0から1の範囲の正の浮動小数点数。たとえば、設定値が`0.5`の場合、ClickHouseは平均してクエリの半分でトレースを開始できます。
- 1 — すべての実行されたクエリのトレースが有効です。

## opentelemetry_trace_processors {#opentelemetry_trace_processors}

Type: Bool

Default value: 0

プロセッサー用のOpenTelemetryスパンを収集します。

## optimize_aggregation_in_order {#optimize_aggregation_in_order}

Type: Bool

Default value: 0

集約データを対応する順序で[GROUP BY](/sql-reference/statements/select/group-by)最適化を有効にするための[SELECT](../../sql-reference/statements/select/index.md)クエリです。

Possible values:

- 0 — `GROUP BY`最適化は無効。
- 1 — `GROUP BY`最適化は有効。

**See Also**

- [GROUP BY optimization](/sql-reference/statements/select/group-by#group-by-optimization-depending-on-table-sorting-key)

## optimize_aggregators_of_group_by_keys {#optimize_aggregators_of_group_by_keys}

Type: Bool

Default value: 1

SELECTセクションのGROUP BYキーのmin/max/any/anyLast集約器を排除します。

## optimize_and_compare_chain {#optimize_and_compare_chain}

Type: Bool

Default value: 1

フィルタリング能力を向上させるためにANDチェーン内の定数比較を埋め込みます。サポートする演算子は`<`、`<=`、`>`、`>=`、`=`およびそれらの混合です。たとえば、`(a < b) AND (b < c) AND (c < 5)`は`(a < b) AND (b < c) AND (c < 5) AND (b < 5) AND (a < 5)`に置き換えられます。

## optimize_append_index {#optimize_append_index}

Type: Bool

Default value: 0

[制約](../../sql-reference/statements/create/table.md/#constraints)を使用してインデックス追加条件を追加します。デフォルトは`false`です。

Possible values:

- true, false

## optimize_arithmetic_operations_in_aggregate_functions {#optimize_arithmetic_operations_in_aggregate_functions}

Type: Bool

Default value: 1

演算機能を集約関数の外に移動します。

## optimize_count_from_files {#optimize_count_from_files}

Type: Bool

Default value: 1

異なる入力形式のファイルから行数を数える最適化を有効または無効にします。テーブル関数/エンジン`file`/`s3`/`url`/`hdfs`/`azureBlobStorage`に適用されます。

Possible values:

- 0 — 最適化無効。
- 1 — 最適化有効。

## optimize_distinct_in_order {#optimize_distinct_in_order}

Type: Bool

Default value: 1

DISTINCTの列の順序のプレフィックスがある場合、DISTINCT最適化を有効にします。たとえば、マージツリーでのソートキーのプレフィックスやORDER BYステートメント。

## optimize_distributed_group_by_sharding_key {#optimize_distributed_group_by_sharding_key}

Type: Bool

Default value: 1

イニシエーターサーバー上でのコストのかかる集約を避けることによって、`GROUP BY sharding_key`クエリを最適化します（これにより、イニシエーターサーバーでのクエリメモリ使用量が削減されます）。

サポートされるクエリの種類は次のとおりです（およびそれらの組み合わせ）：

- `SELECT DISTINCT [..., ]sharding_key[, ...] FROM dist`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...]`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] ORDER BY x`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] LIMIT 1`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] LIMIT 1 BY x`

サポートされないクエリの種類は次のとおりです（後々サポートが追加されるかもしれません）：

- `SELECT ... GROUP BY sharding_key[, ...] WITH TOTALS`
- `SELECT ... GROUP BY sharding_key[, ...] WITH ROLLUP`
- `SELECT ... GROUP BY sharding_key[, ...] WITH CUBE`
- `SELECT ... GROUP BY sharding_key[, ...] SETTINGS extremes=1`

Possible values:

- 0 — 無効。
- 1 — 有効。

See also:

- [distributed_group_by_no_merge](#distributed_group_by_no_merge)
- [distributed_push_down_limit](#distributed_push_down_limit)
- [optimize_skip_unused_shards](#optimize_skip_unused_shards)

:::note
現在、`optimize_skip_unused_shards`が必要です（その理由は、将来的にはデフォルトで有効にされ、データがDistributedテーブルを通じて挿入された場合にのみ正しく機能します。つまり、データがsharding_keyに従って分散されます）。
:::

## optimize_extract_common_expressions {#optimize_extract_common_expressions}

Type: Bool

Default value: 1

WHERE、PREWHERE、ON、HAVING、QUALIFYの表現から共通の表現を抽出できるようにします。論理式のような`(A AND B) OR (A AND C)`は`A AND (B OR C)`に書き換えることができ、これが以下に役立ちます：
- 単純なフィルタリングエクスプレッションにおけるインデックスの利用
- クロスから内側JOINの最適化

## optimize_functions_to_subcolumns {#optimize_functions_to_subcolumns}

Type: Bool

Default value: 1

いくつかの関数をサブカラムを読み取るように変換して最適化します。これにより、読み込むデータの量が削減されます。

次の関数が変換できます：

- [length](/sql-reference/functions/array-functions#length) を[サイズ0](../../sql-reference/data-types/array.md/#array-size)のサブカラムを読み取るように変換します。
- [empty](/sql-reference/functions/array-functions#empty) を[サイズ0](../../sql-reference/data-types/array.md/#array-size)のサブカラムを読み取るように変換します。
- [notEmpty](/sql-reference/functions/array-functions#notempty) を[サイズ0](../../sql-reference/data-types/array.md/#array-size)のサブカラムを読み取るように変換します。
- [isNull](/sql-reference/functions/functions-for-nulls#isnull) を[null](../../sql-reference/data-types/nullable.md/#finding-null)のサブカラムを読み取るように変換します。
- [isNotNull](/sql-reference/functions/functions-for-nulls#isnotnull) を[null](../../sql-reference/data-types/nullable.md/#finding-null)のサブカラムを読み取るように変換します。
- [count](/sql-reference/aggregate-functions/reference/count) を[null](../../sql-reference/data-types/nullable.md/#finding-null)のサブカラムを読み取るように変換します。
- [mapKeys](/sql-reference/functions/tuple-map-functions#mapkeys) を[keys](/sql-reference/data-types/map#reading-subcolumns-of-map)のサブカラムを読み取るように変換します。
- [mapValues](/sql-reference/functions/tuple-map-functions#mapvalues) を[values](/sql-reference/data-types/map#reading-subcolumns-of-map)のサブカラムを読み取るように変換します。

Possible values:

- 0 — 最適化無効。
- 1 — 最適化有効。

## optimize_group_by_constant_keys {#optimize_group_by_constant_keys}

Type: Bool

Default value: 1

すべてのブロック内のキーが定数である場合、GROUP BYを最適化します。

## optimize_group_by_function_keys {#optimize_group_by_function_keys}

Type: Bool

Default value: 1

GROUP BYセクション内での他のキーの関数を排除します。

## optimize_if_chain_to_multiif {#optimize_if_chain_to_multiif}

Type: Bool

Default value: 0

if(cond1, then1, if(cond2, ...) のチェーンをmultiIfに置き換えます。現在、この機能は数値タイプにとって有益ではありません。

## optimize_if_transform_strings_to_enum {#optimize_if_transform_strings_to_enum}

Type: Bool

Default value: 0

IfおよびTransform内の文字列タイプ引数をenumに置き換えます。これはデフォルトで無効であり、分散クエリの不整合な変更を引き起こし、その失敗を招く可能性があります。

## optimize_injective_functions_in_group_by {#optimize_injective_functions_in_group_by}

Type: Bool

Default value: 1

GROUP BYセクション内で自身を持つinjective関数を引数に置き換えます。

## optimize_injective_functions_inside_uniq {#optimize_injective_functions_inside_uniq}

Type: Bool

Default value: 1

uniq*() 関数内の単一引数のinjective関数を削除します。

## optimize_min_equality_disjunction_chain_length {#optimize_min_equality_disjunction_chain_length}

Type: UInt64

Default value: 3

最適化のための表現`expr = x1 OR ... expr = xN`の最小長。

## optimize_min_inequality_conjunction_chain_length {#optimize_min_inequality_conjunction_chain_length}

Type: UInt64

Default value: 3

最適化のための表現`expr <> x1 AND ... expr <> xN`の最小長。

## optimize_move_to_prewhere {#optimize_move_to_prewhere}

Type: Bool

Default value: 1

[SELECT](../../sql-reference/statements/select/index.md)クエリにおける自動[PREWHERE](../../sql-reference/statements/select/prewhere.md)最適化を有効または無効にします。

MergeTreeテーブルにのみ機能します。

Possible values:

- 0 — 自動`PREWHERE`最適化は無効。
- 1 — 自動`PREWHERE`最適化は有効。

## optimize_move_to_prewhere_if_final {#optimize_move_to_prewhere_if_final}

Type: Bool

Default value: 0

[FINAL](/sql-reference/statements/select/from#final-modifier)修飾子を持つ[SELECT](../../sql-reference/statements/select/index.md)クエリにおける自動[PREWHERE](../../sql-reference/statements/select/prewhere.md)最適化を有効または無効にします。

MergeTreeテーブルにのみ機能します。

Possible values:

- 0 — `FINAL`修飾子を持つ`SELECT`クエリの自動`PREWHERE`最適化は無効。
- 1 — `FINAL`修飾子を持つ`SELECT`クエリの自動`PREWHERE`最適化は有効。

**See Also**

- [optimize_move_to_prewhere](#optimize_move_to_prewhere)設定

## optimize_multiif_to_if {#optimize_multiif_to_if}

Type: Bool

Default value: 1

'multiIf'を一つの条件だけを持つ'if'に置き換えます。

## optimize_normalize_count_variants {#optimize_normalize_count_variants}

Type: Bool

Default value: 1

semantically count()と等しい集約関数をcount()に書き換えます。

## optimize_on_insert {#optimize_on_insert}

Type: Bool

Default value: 1

挿入前にデータ変換を有効または無効にし、このブロックでマージが行われたかのように扱います（テーブルエンジンに従います）。

Possible values:

- 0 — 無効。
- 1 — 有効。

**Example**

有効と無効の違い：

クエリ:

```sql
SET optimize_on_insert = 1;

CREATE TABLE test1 (`FirstTable` UInt32) ENGINE = ReplacingMergeTree ORDER BY FirstTable;

INSERT INTO test1 SELECT number % 2 FROM numbers(5);

SELECT * FROM test1;

SET optimize_on_insert = 0;

CREATE TABLE test2 (`SecondTable` UInt32) ENGINE = ReplacingMergeTree ORDER BY SecondTable;

INSERT INTO test2 SELECT number % 2 FROM numbers(5);

SELECT * FROM test2;
```

結果:

``` text
┌─FirstTable─┐
│          0 │
│          1 │
└────────────┘

┌─SecondTable─┐
│           0 │
│           0 │
│           0 │
│           1 │
│           1 │
└─────────────┘
```

この設定は[マテリアライズドビュー](/sql-reference/statements/create/view#materialized-view)の動作にも影響します。

## optimize_or_like_chain {#optimize_or_like_chain}

Type: Bool

Default value: 0

複数のOR LIKEをmultiMatchAnyに最適化します。この最適化は、インデックス分析を一部のケースで無効にするため、デフォルトでは有効にすべきではありません。

## optimize_read_in_order {#optimize_read_in_order}

Type: Bool

Default value: 1

MergeTreeテーブルからデータを読むための[SELECT](../../sql-reference/statements/select/index.md)クエリの[ORDER BY](/sql-reference/statements/select/order-by#optimization-of-data-reading)最適化を有効にします。

Possible values:

- 0 — `ORDER BY`最適化は無効。
- 1 — `ORDER BY`最適化は有効。

**See Also**

- [ORDER BY Clause](/sql-reference/statements/select/order-by#optimization-of-data-reading)

## optimize_read_in_window_order {#optimize_read_in_window_order}

Type: Bool

Default value: 1

MergeTreeテーブルでの対応する順序でデータを読むためのウィンドウ句内におけるORDER BY最適化を有効にします。

## optimize_redundant_functions_in_order_by {#optimize_redundant_functions_in_order_by}

Type: Bool

Default value: 1

ORDER BY内で、その引数もORDER BYにある場合、関数を削除します。

## optimize_respect_aliases {#optimize_respect_aliases}

Type: Bool

Default value: 1

これがtrueに設定されている場合、WHERE/GROUP BY/ORDER BYでエイリアスを尊重します。これにより、パーティションプルーニング/二次インデックス/optimize_aggregation_in_order/optimize_read_in_order/optimize_trivial_countが助けられます。

## optimize_rewrite_aggregate_function_with_if {#optimize_rewrite_aggregate_function_with_if}

Type: Bool

Default value: 1

論理的に等しい場合、if式を引数に持つ集約関数を再書き換えます。
たとえば、`avg(if(cond, col, null))`は`avgOrNullIf(cond, col)`に書き換えることができます。これにより、パフォーマンスが向上する可能性があります。

:::note
アナライザーがサポートされている場合にのみ有効です（`enable_analyzer = 1`）。
:::

## optimize_rewrite_array_exists_to_has {#optimize_rewrite_array_exists_to_has}

Type: Bool

Default value: 0

論理的に等しい場合、arrayExists()関数をhas()に書き換えます。たとえば、arrayExists(x -> x = 1, arr)はhas(arr, 1)に置き換えられる場合があります。

## optimize_rewrite_sum_if_to_count_if {#optimize_rewrite_sum_if_to_count_if}

Type: Bool

Default value: 1

論理的に等しい場合、sumIf()およびsum(if())関数をcountIf()関数に書き換えます。

## optimize_skip_merged_partitions {#optimize_skip_merged_partitions}

Type: Bool

Default value: 0

単一のレベルが`> 0`で、期限切れTTLがない場合に、[OPTIMIZE TABLE ... FINAL](../../sql-reference/statements/optimize.md)クエリの最適化を有効または無効にします。

- `OPTIMIZE TABLE ... FINAL SETTINGS optimize_skip_merged_partitions=1`

デフォルトでは、`OPTIMIZE TABLE ... FINAL`クエリは単一のパーツがあっても再書き換えされます。

Possible values:

- 1 - 最適化を有効にします。
- 0 - 最適化を無効にします。

## optimize_skip_unused_shards {#optimize_skip_unused_shards}

Type: Bool

Default value: 0

`WHERE/PREWHERE`にshardingキー条件がある[SELECT](../../sql-reference/statements/select/index.md)クエリに対して、未使用のシャードのスキップの有効または無効を設定します（データがshardingキーによって分散されていることを前提としています、そうでなければクエリの結果は不正確になります）。

Possible values:

- 0 — 無効。
- 1 — 有効。

## optimize_skip_unused_shards_limit {#optimize_skip_unused_shards_limit}

Type: UInt64

Default value: 1000

shardingキー値の数の制限、制限を超えた場合は`optimize_skip_unused_shards`をオフにします。

値が多すぎると、処理にかなりの時間がかかる可能性がある一方で、利益は疑わしいです。大量の値が`IN (...)`にある場合、その場合も多くのシャードに送信される可能性が高いです。

## optimize_skip_unused_shards_nesting {#optimize_skip_unused_shards_nesting}

Type: UInt64

Default value: 0

[`optimize_skip_unused_shards`](#optimize_skip_unused_shards)を制御します（したがって、まだ[`optimize_skip_unused_shards`](#optimize_skip_unused_shards)が必要です）。分散クエリのネストレベルに依存する（Distributedテーブルが別のDistributedテーブルを参照する場合）。

Possible values:

- 0 — 無効、`optimize_skip_unused_shards`は常に機能。
- 1 — 最初のレベルでのみ`optimize_skip_unused_shards`を有効にする。
- 2 — 2番目のレベルまで`optimize_skip_unused_shards`を有効にする。

## optimize_skip_unused_shards_rewrite_in {#optimize_skip_unused_shards_rewrite_in}

Type: Bool

Default value: 1

リモートシャードのクエリでINを再書き換え、シャードに属さない値を除外します（`optimize_skip_unused_shards`が必要です）。

Possible values:

- 0 — 無効。
- 1 — 有効。

## optimize_sorting_by_input_stream_properties {#optimize_sorting_by_input_stream_properties}

Type: Bool

Default value: 1

入力ストリームのソートプロパティによるソートを最適化します。

## optimize_substitute_columns {#optimize_substitute_columns}

Type: Bool

Default value: 0

[制約](../../sql-reference/statements/create/table.md/#constraints)を使用してカラムの置き換えを行います。デフォルトは`false`です。

Possible values:

- true, false

## optimize_syntax_fuse_functions {#optimize_syntax_fuse_functions}

Type: Bool

Default value: 0

同一の引数を持つ集約関数を融合させることを可能にします。同一の引数を持つ少なくとも2つの集約関数が含まれるクエリを再書き換え、[sum](/sql-reference/aggregate-functions/reference/sum)、[count](/sql-reference/aggregate-functions/reference/count)、または[avg](/sql-reference/aggregate-functions/reference/avg)から[sumCount](/sql-reference/aggregate-functions/reference/sumcount)に変換されます。

Possible values:

- 0 — 同一の引数を持つ関数は融合されません。
- 1 — 同一の引数を持つ関数は融合されます。

**Example**

クエリ:

``` sql
CREATE TABLE fuse_tbl(a Int8, b Int8) Engine = Log;
SET optimize_syntax_fuse_functions = 1;
EXPLAIN SYNTAX SELECT sum(a), sum(b), count(b), avg(b) from fuse_tbl FORMAT TSV;
```

結果:

``` text
SELECT
    sum(a),
    sumCount(b).1,
    sumCount(b).2,
    (sumCount(b).1) / (sumCount(b).2)
FROM fuse_tbl
```

## optimize_throw_if_noop {#optimize_throw_if_noop}

Type: Bool

Default value: 0

[OPTIMIZE](../../sql-reference/statements/optimize.md)クエリがマージを行わなかった場合に例外をスローするか無効にします。

デフォルトでは、`OPTIMIZE`は何も行わなくても正常に戻ります。この設定により、これらの状況を識別し、例外メッセージで理由を得ることができます。

Possible values:

- 1 — 例外をスローすることが有効になります。
- 0 — 例外をスローすることが無効になります。

## optimize_time_filter_with_preimage {#optimize_time_filter_with_preimage}

Type: Bool

Default value: 1

関数を変換することなく比較に置き換えることによって、日付および日時の述語を最適化します（例：`toYear(col) = 2023 -> col >= '2023-01-01' AND col <= '2023-12-31'`）

## optimize_trivial_approximate_count_query {#optimize_trivial_approximate_count_query}

Type: Bool

Default value: 0

そのような推定をサポートするストレージのための簡易なカウント最適化に対して近似値を使用します。例えば、EmbeddedRocksDB。

Possible values:

   - 0 — 最適化無効。
   - 1 — 最適化有効。

## optimize_trivial_count_query {#optimize_trivial_count_query}

Type: Bool

Default value: 1

メタデータを使用して、`SELECT count() FROM table`のトリビアルクエリの最適化を有効または無効にします。行レベルのセキュリティを使用する必要がある場合、この設定を無効にします。

Possible values:

   - 0 — 最適化無効。
   - 1 — 最適化有効。

See also:

- [optimize_functions_to_subcolumns](#optimize_functions_to_subcolumns)

## optimize_trivial_insert_select {#optimize_trivial_insert_select}

Type: Bool

Default value: 0

トリビアルな 'INSERT INTO table SELECT ... FROM TABLES' クエリを最適化します。

## optimize_uniq_to_count {#optimize_uniq_to_count}

Type: Bool

Default value: 1

独自のクエリがdistinctまたはgroup by句を持つサブクエリを含む場合、uniqおよびその変種（uniqUpToを除く）を書き換え、countにします。

## optimize_use_implicit_projections {#optimize_use_implicit_projections}

Type: Bool

Default value: 1

SELECTクエリを実行するために暗黙の投影を自動的に選択します。

## optimize_use_projections {#optimize_use_projections}

Type: Bool

Default value: 1

SELECTクエリを処理するときの[プロジェクション](../../engines/table-engines/mergetree-family/mergetree.md/#projections)最適化を有効にします。

Possible values:

- 0 — プロジェクション最適化無効。
- 1 — プロジェクション最適化有効。

## optimize_using_constraints {#optimize_using_constraints}

Type: Bool

Default value: 0

クエリ最適化のために[制約](../../sql-reference/statements/create/table.md/#constraints)を使用します。デフォルトは`false`です。

Possible values:

- true, false

## os_thread_priority {#os_thread_priority}

Type: Int64

Default value: 0

クエリを実行するスレッドの優先度（[nice](https://en.wikipedia.org/wiki/Nice_(Unix)))を設定します。OSスケジューラは次に実行するスレッドを選択する際にこの優先度を考慮します。

:::note
この設定を使用するには、`CAP_SYS_NICE`の権限を設定する必要があります。`clickhouse-server`パッケージはインストール中にこれを設定します。一部の仮想環境では、`CAP_SYS_NICE`の権限を設定できません。この場合、`clickhouse-server`は起動時にこれについてのメッセージを表示します。
:::

Possible values:

- `[-20, 19]`の範囲で値を設定できます。

低い値はより高い優先度を意味します。低い`nice`優先度値を持つスレッドは、高い値を持つスレッドよりも頻繁に実行されます。高い値は、長時間実行される非対話型クエリに対して望ましく、短い対話型クエリが到着するときにリソースを迅速に放棄することを可能にします。

## output_format_compression_level {#output_format_compression_level}

Type: UInt64

Default value: 3

クエリ出力が圧縮されている場合のデフォルト圧縮レベル。この設定は、`SELECT`クエリが`INTO OUTFILE`を持つか、テーブル関数`file`、`url`、`hdfs`、`s3`、または`azureBlobStorage`に書き込むときに適用されます。

Possible values: `1`から`22`まで。

## output_format_compression_zstd_window_log {#output_format_compression_zstd_window_log}

Type: UInt64

Default value: 0

出力圧縮方法が`zstd`の場合に使用できます。0より大きい場合、この設定は圧縮ウィンドウサイズ（`2`の累乗）を明示的に設定し、zstd圧縮のロングレンジモードを有効にします。これにより、より良い圧縮率を達成できる可能性があります。

Possible values: 非負の数。値が小さすぎるか大きすぎると、`zstdlib`は例外をスローします。通常の値は`20`（ウィンドウサイズ=`1MB`）から`30`（ウィンドウサイズ=`1GB`）です。

## output_format_parallel_formatting {#output_format_parallel_formatting}

Type: Bool

Default value: 1

データ形式の並列フォーマットを有効または無効にします。[TSV](../../interfaces/formats.md/#tabseparated)、[TSKV](../../interfaces/formats.md/#tskv)、[CSV](../../interfaces/formats.md/#csv)、および[JSONEachRow](../../interfaces/formats.md/#jsoneachrow)形式でのみサポートされています。

Possible values:

- 1 — 有効。
- 0 — 無効。

## page_cache_inject_eviction {#page_cache_inject_eviction}

Type: Bool

Default value: 0

ユーザースペースページキャッシュは、時折ランダムでいくつかのページを無効にします。テスト用の機能です。
```yaml
title: '並列分散挿入選択'
sidebar_label: '並列分散挿入選択'
keywords: 'ClickHouse, 並列, 分散, 挿入, 選択, 設定'
description: 'ClickHouseの並列分散挿入選択機能についての設定ガイド'
```

## parallel_distributed_insert_select {#parallel_distributed_insert_select}

Type: UInt64

Default value: 0

並列分散 `INSERT ... SELECT` クエリを有効にします。

`INSERT INTO distributed_table_a SELECT ... FROM distributed_table_b` クエリを実行し、両方のテーブルが同じクラスターを使用し、両方のテーブルが [レプリケートされた](../../engines/table-engines/mergetree-family/replication.md) または非レプリケートである場合、このクエリは各シャードでローカルに処理されます。

Possible values:

- 0 — 無効。
- 1 — `SELECT` は分散エンジンの基盤テーブルの各シャードで実行されます。
- 2 — `SELECT` と `INSERT` は分散エンジンの基盤テーブルの各シャードで実行されます。

## parallel_replica_offset {#parallel_replica_offset}
<BetaBadge/>

Type: UInt64

Default value: 0

これは内部設定であり、直接使用すべきではなく、「並列レプリカ」モードの実装の詳細を示します。この設定は、並列レプリカ間でクエリ処理に参加するレプリカのインデックスへの分散クエリのために、イニシエーターサーバーによって自動的に設定されます。

## parallel_replicas_allow_in_with_subquery {#parallel_replicas_allow_in_with_subquery}
<BetaBadge/>

Type: Bool

Default value: 1

true の場合、IN のサブクエリは各フォロワーレプリカで実行されます。

## parallel_replicas_count {#parallel_replicas_count}
<BetaBadge/>

Type: UInt64

Default value: 0

これは内部設定であり、直接使用すべきではなく、「並列レプリカ」モードの実装の詳細を示します。この設定は、並列レプリカでクエリ処理に参加する並列レプリカの数のために、イニシエーターサーバーによって自動的に設定されます。

## parallel_replicas_custom_key {#parallel_replicas_custom_key}
<BetaBadge/>

Type: String

Default value: 

特定のテーブルのレプリカ間で作業を分割するために使用できる任意の整数式。
値は任意の整数式で構いません。

主キーを使用した単純な式が好まれます。

この設定が、複数のレプリカを持つ単一シャードからなるクラスターで使用されると、これらのレプリカは仮想シャードに変換されます。
それ以外の場合、`SAMPLE` キーと同様に機能し、各シャードの複数のレプリカを使用します。

## parallel_replicas_custom_key_range_lower {#parallel_replicas_custom_key_range_lower}
<BetaBadge/>

Type: UInt64

Default value: 0

フィルタータイプ `range` がカスタム範囲 `[parallel_replicas_custom_key_range_lower, INT_MAX]` に基づいてレプリカ間で作業を均等に分割することを許可します。

[parallel_replicas_custom_key_range_upper](#parallel_replicas_custom_key_range_upper) と併用すると、範囲 `[parallel_replicas_custom_key_range_lower, parallel_replicas_custom_key_range_upper]` でレプリカ間で作業を均等に分割します。

注意: この設定は、クエリ処理中に追加のデータがフィルタリングされることはなく、並列処理のために範囲フィルターが範囲 `[0, INT_MAX]` を分割するポイントを変更します。

## parallel_replicas_custom_key_range_upper {#parallel_replicas_custom_key_range_upper}
<BetaBadge/>

Type: UInt64

Default value: 0

フィルタータイプ `range` がカスタム範囲 `[0, parallel_replicas_custom_key_range_upper]` に基づいてレプリカ間で作業を均等に分割することを許可します。0の値は上限を無効にし、カスタムキー式の最大値を設定します。

[parallel_replicas_custom_key_range_lower](#parallel_replicas_custom_key_range_lower) と併用すると、範囲 `[parallel_replicas_custom_key_range_lower, parallel_replicas_custom_key_range_upper]` でレプリカ間で作業を均等に分割します。

注意: この設定は、クエリ処理中に追加のデータがフィルタリングされることはなく、並列処理のために範囲フィルターが範囲 `[0, INT_MAX]` を分割するポイントを変更します。

## parallel_replicas_for_non_replicated_merge_tree {#parallel_replicas_for_non_replicated_merge_tree}
<BetaBadge/>

Type: Bool

Default value: 0

true の場合、ClickHouse は非レプリケートMergeTreeテーブルに対しても並列レプリカアルゴリズムを使用します。

## parallel_replicas_index_analysis_only_on_coordinator {#parallel_replicas_index_analysis_only_on_coordinator}
<BetaBadge/>

Type: Bool

Default value: 1

インデックス分析はレプリカコーディネーターでのみ実行され、他のレプリカではスキップされます。これは、`parallel_replicas_local_plan` が有効な場合にのみ効果があります。

## parallel_replicas_local_plan {#parallel_replicas_local_plan}
<BetaBadge/>

Type: Bool

Default value: 1

ローカルレプリカのためのローカルプランを構築します。

## parallel_replicas_mark_segment_size {#parallel_replicas_mark_segment_size}
<BetaBadge/>

Type: UInt64

Default value: 0

パーツを仮想的にセグメントに分割し、それをレプリカ間で並列読み取りのために分配します。この設定は、これらのセグメントのサイズを制御します。何をしているか確実でない限り変更は推奨されません。値は範囲 [128; 16384] であるべきです。

## parallel_replicas_min_number_of_rows_per_replica {#parallel_replicas_min_number_of_rows_per_replica}
<BetaBadge/>

Type: UInt64

Default value: 0

クエリで使用されるレプリカの数を制限します。(推定行数 / min_number_of_rows_per_replica)。最大は依然として 'max_parallel_replicas' に制限されます。

## parallel_replicas_mode {#parallel_replicas_mode}
<BetaBadge/>

Type: ParallelReplicasMode

Default value: read_tasks

カスタムキーで並列レプリカに使用するフィルターのタイプ。デフォルト - カスタムキーに対して剰余演算を使用、範囲 - カスタムキーに対して範囲フィルターを使用し、カスタムキーの値タイプのすべての可能な値を使用。

## parallel_replicas_only_with_analyzer {#parallel_replicas_only_with_analyzer}
<BetaBadge/>

Type: Bool

Default value: 1

アナライザーが有効でないと、クエリ実行はローカル実行にフォールバックし、レプリカからの並列読み取りが有効であっても、アナライザーが有効でない限り並列レプリカの使用はサポートされません。

## parallel_replicas_prefer_local_join {#parallel_replicas_prefer_local_join}
<BetaBadge/>

Type: Bool

Default value: 1

true の場合、JOIN を並列レプリカアルゴリズムで実行でき、右 JOIN 部分のすべてのストレージが *MergeTree の場合、ローカル JOIN が使用されます。グローバル JOIN の代わりに。

## parallel_view_processing {#parallel_view_processing}

Type: Bool

Default value: 0

添付されたビューに対して順次ではなく同時にプッシュすることを有効にします。

## parallelize_output_from_storages {#parallelize_output_from_storages}

Type: Bool

Default value: 1

ストレージからの読み取りステップの出力を並列化します。可能な場合、ストレージから読み込んだ後すぐにクエリ処理の並列化を可能にします。

## parsedatetime_parse_without_leading_zeros {#parsedatetime_parse_without_leading_zeros}

Type: Bool

Default value: 1

フォーマッタ '%c', '%l' および '%k' は、'parseDateTime' 関数で月および時間をゼロパディングなしで解析します。

## partial_merge_join_left_table_buffer_bytes {#partial_merge_join_left_table_buffer_bytes}

Type: UInt64

Default value: 0

0でない場合、部分的マージジョインの左側のテーブルに対して、左側のテーブルのブロックを大きいものにグループ化します。それは、結合スレッドごとに指定されたメモリの最大 2 倍を使用します。

## partial_merge_join_rows_in_right_blocks {#partial_merge_join_rows_in_right_blocks}

Type: UInt64

Default value: 65536

部分的マージジョインアルゴリズムにおける右側の結合データブロックのサイズに制限を設けます。これは、[JOIN](../../sql-reference/statements/select/join.md) クエリ用です。

ClickHouse サーバー:

1.  右側の結合データを指定された行数のブロックに分割します。
2.  各ブロックをその最小値および最大値でインデックスします。
3.  可能であれば、準備されたブロックをディスクにアンロードします。

Possible values:

- 任意の正の整数。推奨される値の範囲: \[1000, 100000\]。

## partial_result_on_first_cancel {#partial_result_on_first_cancel}

Type: Bool

Default value: 0

クエリがキャンセルされた後に部分的な結果を返すことを許可します。

## parts_to_delay_insert {#parts_to_delay_insert}

Type: UInt64

Default value: 0

宛先テーブルが単一パーティション内にアクティブなパーツをその数だけ含む場合、テーブルへの挿入を人工的に遅延させます。

## parts_to_throw_insert {#parts_to_throw_insert}

Type: UInt64

Default value: 0

宛先テーブルの単一パーティション内にこの数よりも多くのアクティブなパーツがある場合、「パーツが多すぎる ...」例外をスローします。

## periodic_live_view_refresh {#periodic_live_view_refresh}

Type: Seconds

Default value: 60

定期的に更新されるライブビューが強制的に更新されるまでの間隔。

## poll_interval {#poll_interval}

Type: UInt64

Default value: 10

指定された秒数の間、サーバーでクエリ待機ループをブロックします。

## postgresql_connection_attempt_timeout {#postgresql_connection_attempt_timeout}

Type: UInt64

Default value: 2

PostgreSQL エンドポイントへの接続の単一試行における接続タイムアウト（秒）。

この値は接続URLの `connect_timeout` パラメータとして渡されます。

## postgresql_connection_pool_auto_close_connection {#postgresql_connection_pool_auto_close_connection}

Type: Bool

Default value: 0

接続をプールに返す前に閉じます。

## postgresql_connection_pool_retries {#postgresql_connection_pool_retries}

Type: UInt64

Default value: 2

PostgreSQLテーブルエンジンおよびデータベースエンジンの接続プールのプッシュ/ポップ試行回数。

## postgresql_connection_pool_size {#postgresql_connection_pool_size}

Type: UInt64

Default value: 16

PostgreSQLテーブルエンジンおよびデータベースエンジンの接続プールのサイズ。

## postgresql_connection_pool_wait_timeout {#postgresql_connection_pool_wait_timeout}

Type: UInt64

Default value: 5000

PostgreSQLテーブルエンジンおよびデータベースエンジンの空のプールにおける接続プールのプッシュ/ポップタイムアウト。デフォルトでは、空のプールでブロックされます。

## postgresql_fault_injection_probability {#postgresql_fault_injection_probability}

Type: Float

Default value: 0

内部（レプリケーション用）のPostgreSQLクエリが失敗する確率の概算。妥当な値は [0.0f, 1.0f] の範囲です。

## prefer_column_name_to_alias {#prefer_column_name_to_alias}

Type: Bool

Default value: 0

クエリ表現や節においてエイリアスの代わりに元のカラム名を使用するかどうかを有効または無効にします。特に、エイリアスがカラム名と同じ場合は重要です。[Expression Aliases](/sql-reference/syntax#notes-on-usage)を参照してください。この設定を有効にすることで、ClickHouseのエイリアス構文ルールが他のほとんどのデータベースエンジンとより互換性を持つようになります。

Possible values:

- 0 — カラム名はエイリアスに置き換えられます。
- 1 — カラム名はエイリアスに置き換えられません。

**Example**

有効と無効の違い:

クエリ:

```sql
SET prefer_column_name_to_alias = 0;
SELECT avg(number) AS number, max(number) FROM numbers(10);
```

結果:

```text
Received exception from server (version 21.5.1):
Code: 184. DB::Exception: Received from localhost:9000. DB::Exception: Aggregate function avg(number) is found inside another aggregate function in query: While processing avg(number) AS number.
```

クエリ:

```sql
SET prefer_column_name_to_alias = 1;
SELECT avg(number) AS number, max(number) FROM numbers(10);
```

結果:

```text
┌─number─┬─max(number)─┐
│    4.5 │           9 │
└────────┴─────────────┘
```

## prefer_external_sort_block_bytes {#prefer_external_sort_block_bytes}

Type: UInt64

Default value: 16744704

外部ソートのための最大ブロックバイトを優先し、マージ中のメモリ使用量を削減します。

## prefer_global_in_and_join {#prefer_global_in_and_join}

Type: Bool

Default value: 0

`IN`/`JOIN` 演算子を `GLOBAL IN`/`GLOBAL JOIN` に置き換えることを有効にします。

Possible values:

- 0 — 無効。`IN`/`JOIN` 演算子は `GLOBAL IN`/`GLOBAL JOIN` に置き換えられません。
- 1 — 有効。`IN`/`JOIN` 演算子は `GLOBAL IN`/`GLOBAL JOIN` に置き換えられます。

**Usage**

`SET distributed_product_mode=global` は、分散テーブルのクエリの動作を変更できますが、ローカルテーブルや外部リソースからのテーブルには適していません。ここで `prefer_global_in_and_join` 設定が登場します。

例えば、ローカルテーブルを含むクエリサービスノードがある場合、これらは分散に適していません。分散処理中にデータをフライで散布する必要があります。`GLOBAL`キーワードを使用して `GLOBAL IN`/`GLOBAL JOIN` です。

`prefer_global_in_and_join` の別の使用ケースは、外部エンジンによって作成されたテーブルへのアクセスです。この設定は、そのようなテーブルを結合する際の外部ソースへのコールの回数を減らすのに役立ちます：クエリごとに1回の呼び出しのみ。

**See also:**

- [Distributed subqueries](/sql-reference/operators/in#distributed-subqueries) で `GLOBAL IN`/`GLOBAL JOIN` の使用方法についての詳細を確認できます。

## prefer_localhost_replica {#prefer_localhost_replica}

Type: Bool

Default value: 1

分散クエリの処理時にローカルホストレプリカの使用を好むかどうかを有効または無効にします。

Possible values:

- 1 — ClickHouse はローカルホストレプリカが存在する場合、常にクエリをそのレプリカに送信します。
- 0 — ClickHouse は [load_balancing](#load_balancing) 設定で指定されたバランス戦略を使用します。

:::note
[parallel_replicas_custom_key](#parallel_replicas_custom_key) を使用せずに [max_parallel_replicas](#max_parallel_replicas) を使用する場合は、この設定を無効にします。
[parallel_replicas_custom_key](#parallel_replicas_custom_key) が設定されている場合は、複数のレプリカを含む複数のシャードのクラスターで使用されている場合にのみ、この設定を無効にします。
単一シャードと複数のレプリカのクラスターで使用されている場合、この設定を無効にすると悪影響があります。
:::

## prefer_warmed_unmerged_parts_seconds {#prefer_warmed_unmerged_parts_seconds}
<CloudAvailableBadge/>

Type: Int64

Default value: 0

ClickHouse Cloudでのみ影響します。マージされたパーツがこの秒数未満の新しいもので、未プレウォーム（[cache_populated_by_fetch](merge-tree-settings.md/#cache_populated_by_fetch)を参照）の場合、すべてのソースパーツが利用可能でプレウォームされているとき、SELECTクエリはそれらのパーツから読み取ります。Replicated-/SharedMergeTree のみ。これは、CacheWarmer がパーツを処理したかどうかを確認するだけであり、何か別のものでキャッシュに取得された場合でも、それはまだ冷たく、CacheWarmer がそれに到達しなければなりません。ウォームされた場合、キャッシュから排除されると、まだウォームと見なされます。

## preferred_block_size_bytes {#preferred_block_size_bytes}

Type: UInt64

Default value: 1000000

この設定は、クエリ処理のためのデータブロックサイズを調整し、より粗い 'max_block_size' 設定に対する追加的な微調整を表します。カラムが大きく、'max_block_size' 行がある場合、ブロックサイズは指定されたバイト数よりも大きくなる可能性があり、その場合はより良いCPUキャッシュのローカリティのためにサイズが小さくなります。

## preferred_max_column_in_block_size_bytes {#preferred_max_column_in_block_size_bytes}

Type: UInt64

Default value: 0

読み取り時のブロック内の最大カラムサイズに制限を設けます。キャッシュミスの数を減少させるのに役立ちます。L2キャッシュサイズに近い必要があります。

## preferred_optimize_projection_name {#preferred_optimize_projection_name}

Type: String

Default value: 

非空の文字列に設定されると、ClickHouseはクエリ内で指定されたプロジェクションを適用しようとします。

Possible values:

- string: 推奨されるプロジェクションの名前

## prefetch_buffer_size {#prefetch_buffer_size}

Type: UInt64

Default value: 1048576

ファイルシステムから読み取るためのプレフェッチバッファの最大サイズです。

## print_pretty_type_names {#print_pretty_type_names}

Type: Bool

Default value: 1

`DESCRIBE` クエリや `toTypeName()` 関数で、深くネストされたタイプ名をインデント付きで美しく印刷できるようにします。

Example:

```sql
CREATE TABLE test (a Tuple(b String, c Tuple(d Nullable(UInt64), e Array(UInt32), f Array(Tuple(g String, h Map(String, Array(Tuple(i String, j UInt64))))), k Date), l Nullable(String))) ENGINE=Memory;
DESCRIBE TABLE test FORMAT TSVRaw SETTINGS print_pretty_type_names=1;
```

```
a   Tuple(
    b String,
    c Tuple(
        d Nullable(UInt64),
        e Array(UInt32),
        f Array(Tuple(
            g String,
            h Map(
                String,
                Array(Tuple(
                    i String,
                    j UInt64
                ))
            )
        )),
        k Date
    ),
    l Nullable(String)
)
```

## priority {#priority}

Type: UInt64

Default value: 0

クエリの優先度。1 - 最高、より高い値 - 低い優先度; 0 - 優先度を使用しません。

## push_external_roles_in_interserver_queries {#push_external_roles_in_interserver_queries}

Type: Bool

Default value: 1

クエリを実行する際に、元のユーザー役割を他のノードにプッシュすることを有効にします。

## query_cache_compress_entries {#query_cache_compress_entries}

Type: Bool

Default value: 1

[クエリキャッシュ](../query-cache.md)のエントリを圧縮します。クエリキャッシュのメモリ消費を抑え、挿入時や読み取り時の速度を遅くします。

Possible values:

- 0 - 無効
- 1 - 有効

## query_cache_max_entries {#query_cache_max_entries}

Type: UInt64

Default value: 0

現在のユーザーが[クエリキャッシュ](../query-cache.md)に保存できるクエリ結果の最大数。0は無制限を意味します。

Possible values:

- 正の整数 >= 0。

## query_cache_max_size_in_bytes {#query_cache_max_size_in_bytes}

Type: UInt64

Default value: 0

現在のユーザーが[クエリキャッシュ](../query-cache.md)に割り当てることができる最大のメモリ量（バイト単位）。0は無制限を意味します。

Possible values:

- 正の整数 >= 0。

## query_cache_min_query_duration {#query_cache_min_query_duration}

Type: Milliseconds

Default value: 0

クエリが結果を[クエリキャッシュ](../query-cache.md)に保存するために実行する必要がある最小の時間（ミリ秒）。

Possible values:

- 正の整数 >= 0。

## query_cache_min_query_runs {#query_cache_min_query_runs}

Type: UInt64

Default value: 0

クエリ結果を[クエリキャッシュ](../query-cache.md)に保存する前に、`SELECT` クエリが実行される必要がある最小回数。

Possible values:

- 正の整数 >= 0。

## query_cache_nondeterministic_function_handling {#query_cache_nondeterministic_function_handling}

Type: QueryResultCacheNondeterministicFunctionHandling

Default value: throw

[クエリキャッシュ](../query-cache.md)が `rand()` や `now()` のような非決定論的関数を持つ `SELECT` クエリをどのように扱うかを制御します。

Possible values:

- `'throw'` - 例外をスローし、クエリ結果をキャッシュしません。
- `'save'` - クエリ結果をキャッシュします。
- `'ignore'` - クエリ結果をキャッシュせず、例外をスローしません。

## query_cache_share_between_users {#query_cache_share_between_users}

Type: Bool

Default value: 0

有効にした場合、[クエリキャッシュ](../query-cache.md)にキャッシュされた`SELECT`クエリの結果を他のユーザーが読み取ることができます。
セキュリティ上の理由から、この設定を有効にすることは推奨されません。

Possible values:

- 0 - 無効
- 1 - 有効

## query_cache_squash_partial_results {#query_cache_squash_partial_results}

Type: Bool

Default value: 1

部分的な結果ブロックを[最大ブロックサイズ](#max_block_size)のサイズのブロックへと圧縮します。これにより、[クエリキャッシュ](../query-cache.md)への挿入のパフォーマンスが低下しますが、キャッシュエントリの圧縮可能性が向上します（[query_cache_compress-entries](#query_cache_compress_entries)を参照）。

Possible values:

- 0 - 無効
- 1 - 有効

## query_cache_system_table_handling {#query_cache_system_table_handling}

Type: QueryResultCacheSystemTableHandling

Default value: throw

[クエリキャッシュ](../query-cache.md)がシステムテーブル、すなわち `system.*` および `information_schema.*` データベースのテーブルに対する `SELECT` クエリをどのように扱うかを制御します。

Possible values:

- `'throw'` - 例外をスローし、クエリ結果をキャッシュしません。
- `'save'` - クエリ結果をキャッシュします。
- `'ignore'` - クエリ結果をキャッシュせず、例外をスローしません。

## query_cache_tag {#query_cache_tag}

Type: String

Default value: 

[クエリキャッシュ](../query-cache.md)エントリのラベルとして機能する文字列。
タグが異なる同じクエリは、クエリキャッシュによって別々のものと見なされます。

Possible values:

- 任意の文字列

## query_cache_ttl {#query_cache_ttl}

Type: Seconds

Default value: 60

この時間（秒）が経過すると、[クエリキャッシュ](../query-cache.md)内のエントリは古くなります。

Possible values:

- 正の整数 >= 0。

## query_metric_log_interval {#query_metric_log_interval}

Type: Int64

Default value: -1

個別のクエリに対する [query_metric_log](../../operations/system-tables/query_metric_log.md) を収集する間隔（ミリ秒）。

負の値が設定された場合、[query_metric_log setting](/operations/server-configuration-parameters/settings#query_metric_log)からの `collect_interval_milliseconds` の値を取るか、存在しない場合はデフォルトで1000の値になります。

単一のクエリの収集を無効にするには、`query_metric_log_interval`を0に設定します。

Default value: -1

## query_plan_aggregation_in_order {#query_plan_aggregation_in_order}

Type: Bool

Default value: 1

クエリプランレベル最適化での順序ごとの集約を切り替えます。
[query_plan_enable_optimizations](#query_plan_enable_optimizations) 設定が 1 の場合のみ効果があります。

:::note
これは、開発者によるデバッグ専用の専門家レベルの設定です。この設定は、将来後方互換性のない方法で変更されるか、削除される可能性があります。
:::

Possible values:

- 0 - 無効
- 1 - 有効

## query_plan_convert_outer_join_to_inner_join {#query_plan_convert_outer_join_to_inner_join}

Type: Bool

Default value: 1

結合後のフィルタが常にデフォルトの値をフィルタリングする場合、OUTER JOINをINNER JOINに変換することを許可します。

## query_plan_enable_multithreading_after_window_functions {#query_plan_enable_multithreading_after_window_functions}

Type: Bool

Default value: 1

ウィンドウ関数の評価後にマルチスレッド処理を有効にして並列ストリーム処理を可能にします。

## query_plan_enable_optimizations {#query_plan_enable_optimizations}

Type: Bool

Default value: 1

クエリプランレベルでのクエリ最適化を切り替えます。

:::note
これは、開発者によるデバッグ専用の専門家レベルの設定です。この設定は、将来後方互換性のない方法で変更されるか、削除される可能性があります。
:::

Possible values:

- 0 - クエリプランレベルでのすべての最適化を無効にする
- 1 - クエリプランレベルでの最適化を有効にする（ただし、個々の最適化は各自の設定を介して無効にされることもあります）

## query_plan_execute_functions_after_sorting {#query_plan_execute_functions_after_sorting}

Type: Bool

Default value: 1

ソートステップの後に演算を実行するクエリプランレベルの最適化を切り替えます。
[query_plan_enable_optimizations](#query_plan_enable_optimizations) 設定が 1 の場合のみ効果があります。

:::note
これは、開発者によるデバッグ専用の専門家レベルの設定です。この設定は、将来後方互換性のない方法で変更されるか、削除される可能性があります。
:::

Possible values:

- 0 - 無効
- 1 - 有効

## query_plan_filter_push_down {#query_plan_filter_push_down}

Type: Bool

Default value: 1

クエリプランにおけるフィルターを実行計画内に移動させる最適化を切り替えます。
[query_plan_enable_optimizations](#query_plan_enable_optimizations) 設定が 1 の場合のみ効果があります。

:::note
これは、開発者によるデバッグ専用の専門家レベルの設定です。この設定は、将来後方互換性のない方法で変更されるか、削除される可能性があります。
:::

Possible values:

- 0 - 無効
- 1 - 有効

## query_plan_join_swap_table {#query_plan_join_swap_table}

Type: BoolAuto

Default value: auto

    結合でビルドテーブル（ハッシュ結合のハッシュテーブルに挿入されるもの）としてどちらの側のテーブルを使用するかを決定します。この設定は、`JOIN ON` 節を持つ `ALL` 結合のみでサポートされます。可能な値は次のとおりです。
    - 'auto': プランナーがビルドテーブルとして使用するテーブルを決定します。
    - 'false': テーブルを入れ替えません（右のテーブルがビルドテーブルです）。
    - 'true': テーブルを常に入れ替えます（左のテーブルがビルドテーブルです）。

## query_plan_lift_up_array_join {#query_plan_lift_up_array_join}

Type: Bool

Default value: 1

ARRAY JOINを実行計画内に移動させるクエリプランレベルの最適化を切り替えます。
[query_plan_enable_optimizations](#query_plan_enable_optimizations) 設定が 1 の場合のみ効果があります。

:::note
これは、開発者によるデバッグ専用の専門家レベルの設定です。この設定は、将来後方互換性のない方法で変更されるか、削除される可能性があります。
:::

Possible values:

- 0 - 無効
- 1 - 有効

## query_plan_lift_up_union {#query_plan_lift_up_union}

Type: Bool

Default value: 1

より大きなクエリプランの部分木を結合に移動させ、さらなる最適化を有効にするクエリプランレベルの最適化を切り替えます。
[query_plan_enable_optimizations](#query_plan_enable_optimizations) 設定が 1 の場合のみ効果があります。

:::note
これは、開発者によるデバッグ専用の専門家レベルの設定です。この設定は、将来後方互換性のない方法で変更されるか、削除される可能性があります。
:::

Possible values:

- 0 - 無効
- 1 - 有効

## query_plan_max_optimizations_to_apply {#query_plan_max_optimizations_to_apply}

Type: UInt64

Default value: 10000

クエリプランに適用される最適化の総数を制限します。[query_plan_enable_optimizations](#query_plan_enable_optimizations) 設定を見てください。
複雑なクエリに対して長い最適化時間を避けるのに役立ちます。
EXPLAIN PLAN クエリでは、この制限に達した後は最適化を適用するのを止めて、そのままのプランを返します。
通常のクエリ実行では、実際の最適化数がこの設定を超えた場合、例外がスローされます。

:::note
これは、開発者によるデバッグ専用の専門家レベルの設定です。この設定は、将来後方互換性のない方法で変更されるか、削除される可能性があります。
:::

## query_plan_merge_expressions {#query_plan_merge_expressions}

Type: Bool

Default value: 1

連続するフィルターをマージするクエリプランレベルの最適化を切り替えます。
[query_plan_enable_optimizations](#query_plan_enable_optimizations) 設定が 1 の場合のみ効果があります。

:::note
これは、開発者によるデバッグ専用の専門家レベルの設定です。この設定は、将来後方互換性のない方法で変更されるか、削除される可能性があります。
:::

Possible values:

- 0 - 無効
- 1 - 有効

## query_plan_merge_filters {#query_plan_merge_filters}

Type: Bool

Default value: 1

クエリプランの中でフィルターをマージできるようにします。

## query_plan_optimize_prewhere {#query_plan_optimize_prewhere}

Type: Bool

Default value: 1

サポートされているストレージのプレイングエクスプレッションへのフィルターをプッシュダウンできるようにします。

## query_plan_push_down_limit {#query_plan_push_down_limit}

Type: Bool

Default value: 1

実行計画の中でLIMITを下に移動させるクエリプランレベルの最適化を切り替えます。
[query_plan_enable_optimizations](#query_plan_enable_optimizations) 設定が 1 の場合のみ効果があります。

:::note
これは、開発者によるデバッグ専用の専門家レベルの設定です。この設定は、将来後方互換性のない方法で変更されるか、削除される可能性があります。
:::

Possible values:

- 0 - 無効
- 1 - 有効

## query_plan_read_in_order {#query_plan_read_in_order}

Type: Bool

Default value: 1

読み取り順序の最適化クエリプランレベルの最適化を切り替えます。
[query_plan_enable_optimizations](#query_plan_enable_optimizations) 設定が 1 の場合のみ効果があります。

:::note
これは、開発者によるデバッグ専用の専門家レベルの設定です。この設定は、将来後方互換性のない方法で変更されるか、削除される可能性があります。
:::

Possible values:

- 0 - 無効
- 1 - 有効

## query_plan_remove_redundant_distinct {#query_plan_remove_redundant_distinct}

Type: Bool

Default value: 1

冗長な DISTINCT ステップを削除するクエリプランレベルの最適化を切り替えます。
[query_plan_enable_optimizations](#query_plan_enable_optimizations) 設定が 1 の場合のみ効果があります。

:::note
これは、開発者によるデバッグ専用の専門家レベルの設定です。この設定は、将来後方互換性のない方法で変更されるか、削除される可能性があります。
:::

Possible values:

- 0 - 無効
- 1 - 有効

## query_plan_remove_redundant_sorting {#query_plan_remove_redundant_sorting}

Type: Bool

Default value: 1

冗長なソートステップを削除するクエリプランレベルの最適化を切り替えます（例えば、サブクエリの場合）。
[query_plan_enable_optimizations](#query_plan_enable_optimizations) 設定が 1 の場合のみ効果があります。

:::note
これは、開発者によるデバッグ専用の専門家レベルの設定です。この設定は、将来後方互換性のない方法で変更されるか、削除される可能性があります。
:::

Possible values:

- 0 - 無効
- 1 - 有効

## query_plan_reuse_storage_ordering_for_window_functions {#query_plan_reuse_storage_ordering_for_window_functions}

Type: Bool

Default value: 1

ウィンドウ関数のソートに合わせてストレージソートを使用するクエリプランレベルの最適化を切り替えます。
[query_plan_enable_optimizations](#query_plan_enable_optimizations) 設定が 1 の場合のみ効果があります。

:::note
これは、開発者によるデバッグ専用の専門家レベルの設定です。この設定は、将来後方互換性のない方法で変更されるか、削除される可能性があります。
:::

Possible values:

- 0 - 無効
- 1 - 有効

## query_plan_split_filter {#query_plan_split_filter}

Type: Bool

Default value: 1

:::note
これは、開発者によるデバッグ専用の専門家レベルの設定です。この設定は、将来後方互換性のない方法で変更されるか、削除される可能性があります。
:::

クエリプランレベルの最適化でフィルタを式に分割する機能を切り替えます。
[query_plan_enable_optimizations](#query_plan_enable_optimizations) 設定が 1 の場合のみ効果があります。

Possible values:

- 0 - 無効
- 1 - 有効
```
```yaml
title: '設定の説明'
sidebar_label: '設定の説明'
keywords: 'ClickHouse,設定,最適化,クエリ,ファイル'
description: 'ClickHouseの設定オプションに関する詳細な説明。'
```

## query_plan_try_use_vector_search {#query_plan_try_use_vector_search}

Type: Bool

Default value: 1

クエリプランレベルの最適化を切り替えます。これはベクトル類似インデックスを使用しようとします。
設定[query_plan_enable_optimizations](#query_plan_enable_optimizations)が1の場合にのみ効果があります。

:::note
これは専門家向けの設定で、開発者によるデバッグのためにのみ使用する必要があります。この設定は、今後互換性のない方法で変更されるか、削除される可能性があります。
:::

Possible values:

- 0 - 無効
- 1 - 有効

## query_plan_use_new_logical_join_step {#query_plan_use_new_logical_join_step}

Type: Bool

Default value: 1

クエリプランに新しい論理結合ステップを使用します。

## query_profiler_cpu_time_period_ns {#query_profiler_cpu_time_period_ns}

Type: UInt64

Default value: 1000000000

[クエリプロファイラ](../../operations/optimizing-performance/sampling-query-profiler.md)のCPUクロックタイマーの期間を設定します。このタイマーはCPU時間のみをカウントします。

Possible values:

- 正の整数ナノ秒。

    推奨値:

            - 単一クエリの場合は10000000（1秒間に100回）ナノ秒以上。
            - クラスター全体のプロファイリングには1000000000（1秒に1回）。

- タイマーをオフにするには0を使用します。

**ClickHouse Cloudでは一時的に無効にされています。**

See also:

- システムテーブル[trace_log](/operations/system-tables/trace_log)

## query_profiler_real_time_period_ns {#query_profiler_real_time_period_ns}

Type: UInt64

Default value: 1000000000

[クエリプロファイラ](../../operations/optimizing-performance/sampling-query-profiler.md)のリアルクロックタイマーの期間を設定します。リアルクロックタイマーはウォールクロック時間をカウントします。

Possible values:

- 正の整数ナノ秒。

    推奨値:

            - 単一クエリの場合は10000000（1秒間に100回）ナノ秒以下。
            - クラスター全体のプロファイリングには1000000000（1秒に1回）。

- タイマーをオフにするには0を使用します。

**ClickHouse Cloudでは一時的に無効にされています。**

See also:

- システムテーブル[trace_log](/operations/system-tables/trace_log)

## queue_max_wait_ms {#queue_max_wait_ms}

Type: Milliseconds

Default value: 0

同時リクエストの数が最大を超えた場合のリクエストキューでの待機時間。

## rabbitmq_max_wait_ms {#rabbitmq_max_wait_ms}

Type: Milliseconds

Default value: 5000

RabbitMQから読み取る待機時間、再試行前。

## read_backoff_max_throughput {#read_backoff_max_throughput}

Type: UInt64

Default value: 1048576

スローリード時のスレッド数を減少させるための設定。読み取り帯域幅が指定バイト/秒未満の場合にイベントをカウントします。

## read_backoff_min_concurrency {#read_backoff_min_concurrency}

Type: UInt64

Default value: 1

スローリード時に最小スレッド数を維持しようとするための設定。

## read_backoff_min_events {#read_backoff_min_events}

Type: UInt64

Default value: 2

スローリード時のスレッド数を減少させるための設定。スレッド数が減少するイベントの数。

## read_backoff_min_interval_between_events_ms {#read_backoff_min_interval_between_events_ms}

Type: Milliseconds

Default value: 1000

スローリード時のスレッド数を減少させるための設定。前のイベントから一定時間未満が経過している場合、そのイベントに注意を払わない。

## read_backoff_min_latency_ms {#read_backoff_min_latency_ms}

Type: Milliseconds

Default value: 1000

スローリード時のスレッド数を減少させるための設定。指定された時間以上かかったリードのみに注意を払う。

## read_from_filesystem_cache_if_exists_otherwise_bypass_cache {#read_from_filesystem_cache_if_exists_otherwise_bypass_cache}

Type: Bool

Default value: 0

ファイルシステムキャッシュをパッシブモードで使用することを許可します - 既存のキャッシュエントリから利益を得ますが、キャッシュに新しいエントリは追加しません。この設定をヘビーなアドホッククエリ用に設定し、リアルタイムクエリ用に無効にすると、重すぎるクエリによるキャッシュスラッシングを避け、全体のシステム効率を向上させることができます。

## read_from_page_cache_if_exists_otherwise_bypass_cache {#read_from_page_cache_if_exists_otherwise_bypass_cache}

Type: Bool

Default value: 0

ユーザースペースページキャッシュをパッシブモードで使用します。これはread_from_filesystem_cache_if_exists_otherwise_bypass_cacheに似ています。

## read_in_order_two_level_merge_threshold {#read_in_order_two_level_merge_threshold}

Type: UInt64

Default value: 100

プライマリキー順にマルチスレッド読み取りを行う際の予備マージステップを実行するために読み取る必要がある最小パーツ数。

## read_in_order_use_buffering {#read_in_order_use_buffering}

Type: Bool

Default value: 1

プライマリキーの順序で読み取る際にマージ前にバッファリングを使用します。クエリ実行の並列性を向上させます。

## read_in_order_use_virtual_row {#read_in_order_use_virtual_row}

Type: Bool

Default value: 0

プライマリキーまたはその単調関数の順序で読み取る際に、仮想行を使用します。これは、複数のパーツを検索する際に、関連するものにのみ触れるため便利です。

## read_overflow_mode {#read_overflow_mode}

Type: OverflowMode

Default value: throw

制限が超えた場合の対応を指定します。

## read_overflow_mode_leaf {#read_overflow_mode_leaf}

Type: OverflowMode

Default value: throw

葉の制限が超えた場合の対応を指定します。

## read_priority {#read_priority}

Type: Int64

Default value: 0

ローカルファイルシステムまたはリモートファイルシステムからデータを読み取る際の優先度。ローカルファイルシステムに対しては 'pread_threadpool' メソッド、リモートファイルシステムに対しては `threadpool` メソッドにのみ対応しています。

## read_through_distributed_cache {#read_through_distributed_cache}

<CloudAvailableBadge/>

Type: Bool

Default value: 0

ClickHouse Cloudでのみ効果があります。分散キャッシュからの読み取りを許可します。

## readonly {#readonly}

Type: UInt64

Default value: 0

0 - 読み取り専用制限なし。1 - 読み取りリクエストと、明示的に許可された設定の変更のみ。2 - 読み取りリクエストと、設定の変更が許可されていますが、'readonly'設定は除外されます。

## receive_data_timeout_ms {#receive_data_timeout_ms}

Type: Milliseconds

Default value: 2000

最初のデータパケットまたはレプリカからの進捗状況が正のデータパケットを受信するための接続タイムアウト。

## receive_timeout {#receive_timeout}

Type: Seconds

Default value: 300

ネットワークからデータを受信する際のタイムアウト（秒）。この間にバイトが受信されなかった場合、例外がスローされます。この設定をクライアントで行うと、ソケットの 'send_timeout' もサーバーの対応する接続端で設定されます。

## regexp_max_matches_per_row {#regexp_max_matches_per_row}

Type: UInt64

Default value: 1000

単一の正規表現に対する最大一致数を行ごとに設定します。これは、[extractAllGroupsHorizontal](/sql-reference/functions/string-search-functions#extractallgroupshorizontal)関数で貪欲な正規表現を使用する際にメモリ過負荷から保護するために使用します。

Possible values:

- 正の整数。

## reject_expensive_hyperscan_regexps {#reject_expensive_hyperscan_regexps}

Type: Bool

Default value: 1

ハイパースキャンで評価するのが高コストになる可能性のあるパターンを拒否します（NFA状態の爆発のため）。

## remerge_sort_lowered_memory_bytes_ratio {#remerge_sort_lowered_memory_bytes_ratio}

Type: Float

Default value: 2

再マージ後のメモリ使用量がこの比率で減少しなかった場合、再マージは無効になります。

## remote_filesystem_read_method {#remote_filesystem_read_method}

Type: String

Default value: threadpool

リモートファイルシステムからデータを読み取る際のメソッド。readまたはthreadpoolのいずれかです。

## remote_filesystem_read_prefetch {#remote_filesystem_read_prefetch}

Type: Bool

Default value: 1

リモートファイルシステムからデータを読み取る際にプリフェッチを使用する必要があります。

## remote_fs_read_backoff_max_tries {#remote_fs_read_backoff_max_tries}

Type: UInt64

Default value: 5

バックオフ時の最大リトライ回数。

## remote_fs_read_max_backoff_ms {#remote_fs_read_max_backoff_ms}

Type: UInt64

Default value: 10000

リモートディスクからデータを読み取る際の最大待機時間。

## remote_read_min_bytes_for_seek {#remote_read_min_bytes_for_seek}

Type: UInt64

Default value: 4194304

リモートリード（url, s3）でシークを行うために必要な最小バイト数。リードを無視する代わりにシークを行います。

## rename_files_after_processing {#rename_files_after_processing}

Type: String

Default value: 

- **Type:** String

- **Default value:** 空文字列

この設定は、`file`テーブル関数で処理されたファイルの名前変更パターンを指定することを許可します。このオプションが設定されると、`file`テーブル関数で読み取られたすべてのファイルは、指定されたパターンに従って、プレースホルダーに従って名前が変更されます。ファイル処理が成功した場合のみです。
### Placeholders

- `%a` — 完全な元のファイル名（例："sample.csv"）。
- `%f` — 拡張子なしの元のファイル名（例："sample"）。
- `%e` — ドット付きの元のファイル拡張子（例: ".csv"）。
- `%t` — タイムスタンプ（マイクロ秒単位）。
- `%%` — パーセント記号 ("%")。

### Example

- オプション: `--rename_files_after_processing="processed_%f_%t%e"`

- クエリ: `SELECT * FROM file('sample.csv')`


`sample.csv`の読み取りが成功した場合、ファイルは`processed_sample_1683473210851438.csv`に名前が変更されます。

## replace_running_query {#replace_running_query}

Type: Bool

Default value: 0

HTTPインターフェースを使用する際、`query_id`パラメータを渡すことができます。これはクエリ識別子として機能する任意の文字列です。
同じユーザーから同じ`query_id`のクエリがすでに存在する場合、動作は`replace_running_query`パラメータに依存します。

`0`（デフォルト） – 例外をスローします（同じ`query_id`のクエリがすでに実行中の場合はクエリを実行できません）。

`1` – 古いクエリをキャンセルし、新しいクエリを実行します。

このパラメータを1に設定することで、セグメンテーション条件の提案を実装します。次の文字を入力した後、古いクエリがまだ完了していない場合はキャンセルされるべきです。

## replace_running_query_max_wait_ms {#replace_running_query_max_wait_ms}

Type: Milliseconds

Default value: 5000

[replace_running_query](#replace_running_query)設定がアクティブな場合、同じ`query_id`のクエリが終了するまでの待機時間。

Possible values:

- 正の整数。
- 0 — サーバーがすでに同じ`query_id`のクエリを実行している場合、新しいクエリが実行されないように例外をスローします。

## replication_wait_for_inactive_replica_timeout {#replication_wait_for_inactive_replica_timeout}

Type: Int64

Default value: 120

[ALTER](../../sql-reference/statements/alter/index.md)、[OPTIMIZE](../../sql-reference/statements/optimize.md)、または[TRUNCATE](../../sql-reference/statements/truncate.md)クエリを実行するまで非アクティブレプリカが待機する秒数を指定します。

Possible values:

- 0 — 待機しない。
- 負の整数 — 無制限で待機する。
- 正の整数 — 待機する秒数。

## restore_replace_external_dictionary_source_to_null {#restore_replace_external_dictionary_source_to_null}

Type: Bool

Default value: 0

復元時に外部辞書ソースをNullに置き換えます。テスト目的で便利です。

## restore_replace_external_engines_to_null {#restore_replace_external_engines_to_null}

Type: Bool

Default value: 0

テスト目的で。すべての外部エンジンをNullに置き換えて外部接続を開始しないようにします。

## restore_replace_external_table_functions_to_null {#restore_replace_external_table_functions_to_null}

Type: Bool

Default value: 0

テスト目的で。すべての外部テーブル関数をNullに置き換えて外部接続を開始しないようにします。

## restore_replicated_merge_tree_to_shared_merge_tree {#restore_replicated_merge_tree_to_shared_merge_tree}

Type: Bool

Default value: 0

RESTORE中にテーブルエンジンをReplicated*MergeTreeからShared*MergeTreeに置き換えます。

## result_overflow_mode {#result_overflow_mode}

Type: OverflowMode

Default value: throw

制限が超えた場合の対応を指定します。

## rewrite_count_distinct_if_with_count_distinct_implementation {#rewrite_count_distinct_if_with_count_distinct_implementation}

Type: Bool

Default value: 0

`countDistcintIf`を[count_distinct_implementation](#count_distinct_implementation)設定で書き換えることを許可します。

Possible values:

- true — 許可。
- false — 不許可。

## s3_allow_multipart_copy {#s3_allow_multipart_copy}

Type: Bool

Default value: 1

S3でのマルチパートコピーを許可します。

## s3_allow_parallel_part_upload {#s3_allow_parallel_part_upload}

Type: Bool

Default value: 1

s3のマルチパートアップロードに対して複数のスレッドを使用します。これにより、わずかにメモリ使用量が増加する可能性があります。

## s3_check_objects_after_upload {#s3_check_objects_after_upload}

Type: Bool

Default value: 0

アップロードが成功したことを確認するために、ヘッドリクエストでS3にアップロードされた各オブジェクトをチェックします。

## s3_connect_timeout_ms {#s3_connect_timeout_ms}

Type: UInt64

Default value: 1000

S3ディスクからホストへの接続タイムアウト。

## s3_create_new_file_on_insert {#s3_create_new_file_on_insert}

Type: Bool

Default value: 0

S3エンジンテーブルへの各挿入時に新しいファイルの作成を有効または無効にします。有効にすると、各挿入で新しいS3オブジェクトが作成され、次のようなパターンのキーが使われます：

初期: `data.Parquet.gz` -> `data.1.Parquet.gz` -> `data.2.Parquet.gz`など。

Possible values:
- 0 — `INSERT`クエリはファイルの末尾に新しいデータを追加します。
- 1 — `INSERT`クエリは新しいファイルを作成します。

## s3_disable_checksum {#s3_disable_checksum}

Type: Bool

Default value: 0

ファイルをS3に送信する際にチェックサムを計算しません。これにより、ファイルに対する過剰な処理を回避することで書き込み速度が向上します。MergeTreeテーブルのデータはClickHouseによってチェックサムされているため、これはほとんど安全です。HTTPSでアクセスする際には、TLSレイヤーがすでに転送中の整合性を提供します。追加のチェックサムがS3上での防御を強化します。

## s3_ignore_file_doesnt_exist {#s3_ignore_file_doesnt_exist}

Type: Bool

Default value: 0

特定のキーの読み取り時にファイルが存在しない場合の無視を許可します。

Possible values:
- 1 — `SELECT`は空の結果を返します。
- 0 — `SELECT`は例外をスローします。

## s3_list_object_keys_size {#s3_list_object_keys_size}

Type: UInt64

Default value: 1000

ListObjectリクエストでバッチ的に返される可能性のある最大ファイル数。

## s3_max_connections {#s3_max_connections}

Type: UInt64

Default value: 1024

サーバーごとの最大接続数。

## s3_max_get_burst {#s3_max_get_burst}

Type: UInt64

Default value: 0

リクエストごとの制限に達する前に一度に発行できる最大リクエスト数。デフォルト（0）は`s3_max_get_rps`に等しくなります。

## s3_max_get_rps {#s3_max_get_rps}

Type: UInt64

Default value: 0

スロットルがかかる前のS3 GETリクエストの毎秒制限。ゼロは無制限を意味します。

## s3_max_inflight_parts_for_one_file {#s3_max_inflight_parts_for_one_file}

Type: UInt64

Default value: 20

マルチパートアップロードリクエストで同時に読み込まれる最大パーツ数。0は無制限を意味します。

## s3_max_part_number {#s3_max_part_number}

Type: UInt64

Default value: 10000

S3アップロードパートの最大部分番号。

## s3_max_put_burst {#s3_max_put_burst}

Type: UInt64

Default value: 0

リクエストごとの制限に達する前に一度に発行できる最大リクエスト数。デフォルト（0）は`s3_max_put_rps`に等しくなります。

## s3_max_put_rps {#s3_max_put_rps}

Type: UInt64

Default value: 0

スロットルがかかる前のS3 PUTリクエストの毎秒制限。ゼロは無制限を意味します。

## s3_max_redirects {#s3_max_redirects}

Type: UInt64

Default value: 10

許可される最大S3リダイレクトホップ数。

## s3_max_single_operation_copy_size {#s3_max_single_operation_copy_size}

Type: UInt64

Default value: 33554432

s3での単一操作コピーの最大サイズ。この設定は、s3_allow_multipart_copyがtrueの場合にのみ使用されます。

## s3_max_single_part_upload_size {#s3_max_single_part_upload_size}

Type: UInt64

Default value: 33554432

単一パートアップロードを使用する際にS3にアップロードするオブジェクトの最大サイズ。

## s3_max_single_read_retries {#s3_max_single_read_retries}

Type: UInt64

Default value: 4

単一S3読み取り時の最大リトライ回数。

## s3_max_unexpected_write_error_retries {#s3_max_unexpected_write_error_retries}

Type: UInt64

Default value: 4

S3書き込み時の予期しないエラーが発生した場合の最大リトライ回数。

## s3_max_upload_part_size {#s3_max_upload_part_size}

Type: UInt64

Default value: 5368709120

マルチパートアップロード中にS3にアップロードするパートの最大サイズ。

## s3_min_upload_part_size {#s3_min_upload_part_size}

Type: UInt64

Default value: 16777216

マルチパートアップロード中にS3にアップロードするパートの最小サイズ。

## s3_request_timeout_ms {#s3_request_timeout_ms}

Type: UInt64

Default value: 30000

S3へのデータの送信および受信の際のアイドルタイムアウト。単一のTCP読み取りまたは書き込み呼び出しがこの間にブロックされると失敗します。

## s3_retry_attempts {#s3_retry_attempts}

Type: UInt64

Default value: 100

Aws::Client::RetryStrategy用の設定。Aws::Clientは自分自身でリトライを行います。0はリトライしないことを意味します。

## s3_skip_empty_files {#s3_skip_empty_files}

Type: Bool

Default value: 1

[S3](../../engines/table-engines/integrations/s3.md)エンジンテーブル内の空のファイルをスキップすることを有効または無効にします。

Possible values:
- 0 — 空のファイルがリクエストされた形式と互換性がない場合、`SELECT`は例外をスローします。
- 1 — 空のファイルに対して`SELECT`が空の結果を返します。

## s3_strict_upload_part_size {#s3_strict_upload_part_size}

Type: UInt64

Default value: 0

マルチパートアップロード中にS3にアップロードするパートの正確なサイズ（いくつかの実装は可変サイズパーツをサポートしていません）。

## s3_throw_on_zero_files_match {#s3_throw_on_zero_files_match}

Type: Bool

Default value: 0

ListObjectsリクエストがファイルに一致しない場合にエラーをスローします。

## s3_truncate_on_insert {#s3_truncate_on_insert}

Type: Bool

Default value: 0

S3エンジンテーブルへの挿入の前にトランケートを有効または無効にします。無効にすると、S3オブジェクトがすでに存在する場合、挿入を試みると例外がスローされます。

Possible values:
- 0 — `INSERT`クエリはファイルの末尾に新しいデータを追加します。
- 1 — `INSERT`クエリはファイルの既存の内容を新しいデータで置き換えます。

## s3_upload_part_size_multiply_factor {#s3_upload_part_size_multiply_factor}

Type: UInt64

Default value: 2

s3_multiply_parts_count_thresholdからの単一書き込みからアップロードされた各s3_min_upload_part_sizeをこの係数で乗算します。

## s3_upload_part_size_multiply_parts_count_threshold {#s3_upload_part_size_multiply_parts_count_threshold}

Type: UInt64

Default value: 500

この数のパーツがS3にアップロードされるたびに、s3_min_upload_part_sizeがs3_upload_part_size_multiply_factorで乗算されます。

## s3_use_adaptive_timeouts {#s3_use_adaptive_timeouts}

Type: Bool

Default value: 1

`true`に設定されている場合、すべてのs3リクエストに対して最初の2回の試行を低スピードの送信および受信タイムアウトで行います。
`false`に設定されている場合、すべての試行は同一のタイムアウトで行われます。

## s3_validate_request_settings {#s3_validate_request_settings}

Type: Bool

Default value: 1

s3リクエスト設定の検証を有効にします。

Possible values:
- 1 — 設定を検証します。
- 0 — 設定を検証しません。

## s3queue_default_zookeeper_path {#s3queue_default_zookeeper_path}

Type: String

Default value: /clickhouse/s3queue/

S3Queueエンジンのデフォルトのzookeeperパスプレフィックス。

## s3queue_enable_logging_to_s3queue_log {#s3queue_enable_logging_to_s3queue_log}

Type: Bool

Default value: 0

system.s3queue_logへの書き込みを有効にします。この値はテーブル設定で上書きできます。

## s3queue_migrate_old_metadata_to_buckets {#s3queue_migrate_old_metadata_to_buckets}

Type: Bool

Default value: 0

S3Queueテーブルの古いメタデータ構造を新しいものに移行します。

## schema_inference_cache_require_modification_time_for_url {#schema_inference_cache_require_modification_time_for_url}

Type: Bool

Default value: 1

最終更新時刻の検証を伴うURLのためにキャッシュからスキーマを使用します（Last-Modifiedヘッダーを持つURL用）。

## schema_inference_use_cache_for_azure {#schema_inference_use_cache_for_azure}

Type: Bool

Default value: 1

Azureテーブル関数を使用する際にスキーマ推論でキャッシュを使用します。

## schema_inference_use_cache_for_file {#schema_inference_use_cache_for_file}

Type: Bool

Default value: 1

ファイルテーブル関数を使用する際にスキーマ推論でキャッシュを使用します。

## schema_inference_use_cache_for_hdfs {#schema_inference_use_cache_for_hdfs}

Type: Bool

Default value: 1

HDFSテーブル関数を使用する際にスキーマ推論でキャッシュを使用します。

## schema_inference_use_cache_for_s3 {#schema_inference_use_cache_for_s3}

Type: Bool

Default value: 1

S3テーブル関数を使用する際にスキーマ推論でキャッシュを使用します。

## schema_inference_use_cache_for_url {#schema_inference_use_cache_for_url}

Type: Bool

Default value: 1

URLテーブル関数を使用する際にスキーマ推論でキャッシュを使用します。

## select_sequential_consistency {#select_sequential_consistency}

Type: UInt64

Default value: 0

:::note
この設定はSharedMergeTreeとReplicatedMergeTreeでの動作が異なります。SharedMergeTreeでの`select_sequential_consistency`の動作についての詳細は、[SharedMergeTreeの一貫性](/cloud/reference/shared-merge-tree#consistency)を参照してください。
:::

`SELECT`クエリのための逐次一貫性を有効または無効にします。`insert_quorum_parallel`を無効にする必要があります（デフォルトでは有効です）。

Possible values:

- 0 — 無効。
- 1 — 有効。

Usage

逐次一貫性が有効になっている場合、ClickHouseは、クライアントが`insert_quorum`を用いて実行されたすべての以前の`INSERT`クエリのデータを含むレプリカに対してのみ`SELECT`クエリを実行することを許可します。クライアントが部分的なレプリカに言及した場合、ClickHouseは例外を生成します。SELECTクエリは、まだクォーラムのレプリカに書かれていないデータを含みません。

`insert_quorum_parallel`が有効（デフォルト）になっている場合、`select_sequential_consistency`は機能しません。これは、並列の`INSERT`クエリが異なるクォーラムレプリカのセットに書き込むことができるため、単一のレプリカがすべての書き込みを受け取った保証がないからです。

See also:

- [insert_quorum](#insert_quorum)
- [insert_quorum_timeout](#insert_quorum_timeout)
- [insert_quorum_parallel](#insert_quorum_parallel)

## send_logs_level {#send_logs_level}

Type: LogsLevel

Default value: fatal

指定した最小レベルのサーバーテキストログをクライアントに送信します。 有効な値: 'trace', 'debug', 'information', 'warning', 'error', 'fatal', 'none'

## send_logs_source_regexp {#send_logs_source_regexp}

Type: String

Default value: 

指定した正規表現でログソース名を一致させるサーバーテキストログを送信します。空の場合はすべてのソースとなります。

## send_progress_in_http_headers {#send_progress_in_http_headers}

Type: Bool

Default value: 0

`clickhouse-server`の応答で`X-ClickHouse-Progress` HTTPレスポンスヘッダーを有効または無効にします。

詳細については、[HTTPインターフェースの説明](../../interfaces/http.md)を参照してください。

Possible values:

- 0 — 無効。
- 1 — 有効。

## send_timeout {#send_timeout}

Type: Seconds

Default value: 300

ネットワークへのデータ送信のためのタイムアウト（秒）。クライアントがデータを送信する必要があるが、この間にバイトを送ることができない場合、例外がスローされます。この設定をクライアントで設定すると、接続するサーバーの端でのソケットも`receive_timeout`が設定されます。

## session_timezone {#session_timezone}
<BetaBadge/>

Type: Timezone

Default value: 

現在のセッションまたはクエリの暗黙的なタイムゾーンを設定します。
暗黙的なタイムゾーンは、明示的に指定されたタイムゾーンがないDateTime/DateTime64型の値に適用されるタイムゾーンです。
この設定は、グローバルに設定された（サーバーレベルの）暗黙的なタイムゾーンに優先します。
''（空文字列）の値は、現在のセッションまたはクエリの暗黙的なタイムゾーンが[サーバーのタイムゾーン](../server-configuration-parameters/settings.md/#timezone)と等しいことを意味します。

`timeZone()`および`serverTimeZone()`関数を使用して、セッションタイムゾーンおよびサーバータイムゾーンを取得できます。

Possible values:

- `system.time_zones`からの任意のタイムゾーン名（例: `Europe/Berlin`、`UTC`、または`Zulu`）。

例:

```sql
SELECT timeZone(), serverTimeZone() FORMAT CSV

"Europe/Berlin","Europe/Berlin"
```

```sql
SELECT timeZone(), serverTimeZone() SETTINGS session_timezone = 'Asia/Novosibirsk' FORMAT CSV

"Asia/Novosibirsk","Europe/Berlin"
```

明示的にタイムゾーンが指定されていない内蔵DateTimeにセッションタイムゾーン"America/Denver"を割り当てます:

```sql
SELECT toDateTime64(toDateTime64('1999-12-12 23:23:23.123', 3), 3, 'Europe/Zurich') SETTINGS session_timezone = 'America/Denver' FORMAT TSV

1999-12-13 07:23:23.123
```

:::warning
すべての関数がDateTime/DateTime64を解析するときに`session_timezone`を尊重するわけではありません。これにより、微妙なエラーが発生する可能性があります。
以下の例と説明を参照してください。
:::

```sql
CREATE TABLE test_tz (`d` DateTime('UTC')) ENGINE = Memory AS SELECT toDateTime('2000-01-01 00:00:00', 'UTC');

SELECT *, timeZone() FROM test_tz WHERE d = toDateTime('2000-01-01 00:00:00') SETTINGS session_timezone = 'Asia/Novosibirsk'
0 rows in set.

SELECT *, timeZone() FROM test_tz WHERE d = '2000-01-01 00:00:00' SETTINGS session_timezone = 'Asia/Novosibirsk'
┌───────────────────d─┬─timeZone()───────┐
│ 2000-01-01 00:00:00 │ Asia/Novosibirsk │
└─────────────────────┴──────────────────┘
```

これは異なる解析パイプラインによるものです:

- 明示的に指定されたタイムゾーンを持たない`toDateTime()`は最初の`SELECT`クエリで`session_timezone`とグローバルタイムゾーンを尊重します。
- 2番目のクエリでは、文字列からDateTimeが解析され、既存のカラム`d`の型とタイムゾーンを継承します。したがって、`session_timezone`設定とグローバルタイムゾーンは尊重されません。

**See also**

- [timezone](../server-configuration-parameters/settings.md/#timezone)

## set_overflow_mode {#set_overflow_mode}

Type: OverflowMode

Default value: throw

制限が超えた場合の対応を指定します。

## shared_merge_tree_sync_parts_on_partition_operations {#shared_merge_tree_sync_parts_on_partition_operations}

Type: Bool

Default value: 1

SMTテーブル内でのMOVE|REPLACE|ATTACHパーティション操作後にデータパーツのセットを自動的に同期します。クラウド専用。

## short_circuit_function_evaluation {#short_circuit_function_evaluation}

Type: ShortCircuitFunctionEvaluation

Default value: enable

[if](../../sql-reference/functions/conditional-functions.md/#if)、[multiIf](../../sql-reference/functions/conditional-functions.md/#multiif)、[and](/sql-reference/functions/logical-functions#and)、および[or](/sql-reference/functions/logical-functions#or)関数を[短絡スキーム](https://en.wikipedia.org/wiki/Short-circuit_evaluation)に従って計算することを許可します。これにより、これらの関数の複雑な式の実行を最適化し、予期しない場合の可能な例外（ゼロ除算など）を防ぐのに役立ちます。

Possible values:

- `enable` — 短絡関数評価を適用可能な関数に対して有効にします（例外をスローする可能性があるか、計算が重い）。
- `force_enable` — すべての関数に対して短絡関数評価を有効にします。
- `disable` — 短絡関数評価を無効にします。

## short_circuit_function_evaluation_for_nulls {#short_circuit_function_evaluation_for_nulls}

Type: Bool

Default value: 1

任意の引数がNULLである場合にNULLを返す関数の評価を最適化します。有効化されると、関数の引数でNULL値の割合がshort_circuit_function_evaluation_for_nulls_thresholdを超えた場合、システムは行ごとに関数を評価するのをスキップします。代わりに、すべての行に対してNULLを即座に返し、無駄な計算を回避します。

## short_circuit_function_evaluation_for_nulls_threshold {#short_circuit_function_evaluation_for_nulls_threshold}

Type: Double

Default value: 1

NULL引数のある関数をすべての引数で非NULLの行でのみ実行するためのNULL値の割合しきい値。short_circuit_function_evaluation_for_nullsが有効な場合に適用されます。
NULL値を含む行の割合がこのしきい値を超えると、NULL値を含む行は評価されません。

## show_table_uuid_in_table_create_query_if_not_nil {#show_table_uuid_in_table_create_query_if_not_nil}

Type: Bool

Default value: 0

`SHOW TABLE`クエリの表示を設定します。

Possible values:

- 0 — テーブルUUIDなしでクエリが表示されます。
- 1 — テーブルUUID付きでクエリが表示されます。

## single_join_prefer_left_table {#single_join_prefer_left_table}

Type: Bool

Default value: 1

識別子のあいまいさがある場合に、単一のJOINで左側のテーブルを優先します。

## skip_redundant_aliases_in_udf {#skip_redundant_aliases_in_udf}

Type: Bool

Default value: 0

ユーザー定義関数内で冗長なエイリアスが使用されない（置き換えられない）ため、使用が簡素化されます。

Possible values:

- 1 — エイリアスがUDF内でスキップ（置き換え）されます。
- 0 — エイリアスはUDF内でスキップ（置き換え）されません。

**Example**

有効と無効の違い：

クエリ:

```sql
SET skip_redundant_aliases_in_udf = 0;
CREATE FUNCTION IF NOT EXISTS test_03274 AS ( x ) -> ((x + 1 as y, y + 2));

EXPLAIN SYNTAX SELECT test_03274(4 + 2);
```

結果:

```text
SELECT ((4 + 2) + 1 AS y, y + 2)
```

クエリ:

```sql
SET skip_redundant_aliases_in_udf = 1;
CREATE FUNCTION IF NOT EXISTS test_03274 AS ( x ) -> ((x + 1 as y, y + 2));

EXPLAIN SYNTAX SELECT test_03274(4 + 2);
```

結果:

```text
SELECT ((4 + 2) + 1, ((4 + 2) + 1) + 2)
```
```yaml
title: 'ClickHouse 設定オプション'
sidebar_label: '設定オプション'
keywords:
  - ClickHouse
  - 設定
  - オプション
description: 'ClickHouse のさまざまな設定オプションの説明。'
```

## skip_unavailable_shards {#skip_unavailable_shards}



Type: Bool

Default value: 0

利用できないシャードを静かにスキップするかどうかを有効または無効にします。

シャードは、そのすべてのレプリカが利用できない場合、利用できないと見なされます。レプリカが利用できない場合は、以下のいずれかのケースに該当します。

- ClickHouse がレプリカに接続できない場合、いかなる理由でも無効です。

    レプリカに接続する際、ClickHouse は複数回試みます。すべての試みが失敗した場合、そのレプリカは無効と見なされます。

- レプリカは DNS 経由で解決できません。

    レプリカのホスト名が DNS 経由で解決できない場合、以下の状況を示す場合があります。

    - レプリカのホストに DNS レコードがありません。これは、例えば [Kubernetes](https://kubernetes.io) のように、ダウンタイム中にノードが解決できないことがある動的 DNS システムで発生する可能性がありますが、これはエラーではありません。

    - 設定エラー。ClickHouse の設定ファイルに誤ったホスト名が含まれています。

可能な値:

- 1 — スキップが有効。

    シャードが利用できない場合、ClickHouse は部分データに基づいた結果を返し、ノードの可用性に関する問題を報告しません。

- 0 — スキップが無効。

    シャードが利用できない場合、ClickHouse は例外を投げます。

## sleep_after_receiving_query_ms {#sleep_after_receiving_query_ms}



Type: Milliseconds

Default value: 0

TCPHandler でクエリを受信した後のスリープ時間

## sleep_in_send_data_ms {#sleep_in_send_data_ms}



Type: Milliseconds

Default value: 0

TCPHandler でデータを送信する際のスリープ時間

## sleep_in_send_tables_status_ms {#sleep_in_send_tables_status_ms}



Type: Milliseconds

Default value: 0

TCPHandler でテーブルのステータス応答を送信する際のスリープ時間

## sort_overflow_mode {#sort_overflow_mode}



Type: OverflowMode

Default value: throw

制限が超えた場合の処理方法。

## split_intersecting_parts_ranges_into_layers_final {#split_intersecting_parts_ranges_into_layers_final}



Type: Bool

Default value: 1

FINAL 最適化の際に交差するパーツ範囲をレイヤーに分割します。

## split_parts_ranges_into_intersecting_and_non_intersecting_final {#split_parts_ranges_into_intersecting_and_non_intersecting_final}



Type: Bool

Default value: 1

FINAL 最適化の際にパーツ範囲を交差するものとしないものに分割します。

## splitby_max_substrings_includes_remaining_string {#splitby_max_substrings_includes_remaining_string}



Type: Bool

Default value: 0

引数 `max_substrings` > 0 の関数 [splitBy*()](../../sql-reference/functions/splitting-merging-functions.md) が結果の配列の最後の要素に残りの文字列を含むかどうかを制御します。

可能な値:

- `0` - 残りの文字列は結果の配列の最後の要素に含まれません。
- `1` - 残りの文字列は結果の配列の最後の要素に含まれます。これは、Spark の [`split()`](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.split.html) 関数および Python の ['string.split()'](https://docs.python.org/3/library/stdtypes.html#str.split) メソッドの動作です。

## stop_refreshable_materialized_views_on_startup {#stop_refreshable_materialized_views_on_startup}
<ExperimentalBadge/>


Type: Bool

Default value: 0

サーバー起動時に、まるで `SYSTEM STOP VIEWS` のように、リフレッシュ可能なマテリアライズドビューのスケジュールを防ぎます。その後、`SYSTEM START VIEWS` または `SYSTEM START VIEW <name>` で手動で開始できます。新しく作成されたビューにも適用されます。リフレッシュ不可能なマテリアライズドビューには影響しません。

## storage_file_read_method {#storage_file_read_method}



Type: LocalFSReadMethod

Default value: pread

ストレージファイルからデータを読み取る方法。次のいずれかです：`read`, `pread`, `mmap`。mmap メソッドは clickhouse-server には適用されず（clickhouse-local 用に意図されています）。

## storage_system_stack_trace_pipe_read_timeout_ms {#storage_system_stack_trace_pipe_read_timeout_ms}



Type: Milliseconds

Default value: 100

`system.stack_trace` テーブルをクエリした際にスレッドから情報を受信するためのパイプからの読み取りにかかる最大時間。この設定はテスト目的で使用されており、ユーザーによって変更されるべきではありません。

## stream_flush_interval_ms {#stream_flush_interval_ms}



Type: Milliseconds

Default value: 7500

ストリーミング対応のテーブルでタイムアウトが発生した場合や、スレッドが [max_insert_block_size](#max_insert_block_size) 行を生成した際に機能します。

デフォルト値は 7500 です。

値が小さいほど、データがテーブルにフラッシュされる頻度が高くなります。値を低くしすぎるとパフォーマンスが悪化します。

## stream_like_engine_allow_direct_select {#stream_like_engine_allow_direct_select}



Type: Bool

Default value: 0

Kafka、RabbitMQ、FileLog、Redis Streams、および NATS エンジンに対して直接の SELECT クエリを許可します。マテリアライズドビューが添付されている場合、この設定が有効であっても SELECT クエリは許可されません。

## stream_like_engine_insert_queue {#stream_like_engine_insert_queue}



Type: String

Default value: 

ストリームライクエンジンが複数のキューから読み取る際、書き込み時に挿入するキューを選択する必要があります。Redis Streams および NATS で使用されます。

## stream_poll_timeout_ms {#stream_poll_timeout_ms}



Type: Milliseconds

Default value: 500

ストリーミングストレージからのデータポーリングのタイムアウト。

## system_events_show_zero_values {#system_events_show_zero_values}



Type: Bool

Default value: 0

[`system.events`](../../operations/system-tables/events.md) からゼロ値のイベントを選択できるようにします。

一部の監視システムでは、メトリクスの値がゼロであっても、各チェックポイントに対してすべてのメトリクス値を渡す必要があります。

可能な値:

- 0 — 無効。
- 1 — 有効。

**例**

クエリ

```sql
SELECT * FROM system.events WHERE event='QueryMemoryLimitExceeded';
```

結果

```text
Ok.
```

クエリ
```sql
SET system_events_show_zero_values = 1;
SELECT * FROM system.events WHERE event='QueryMemoryLimitExceeded';
```

結果

```text
┌─event────────────────────┬─value─┬─description───────────────────────────────────────────┐
│ QueryMemoryLimitExceeded │     0 │ クエリのメモリ制限を超えた回数。                           │
└──────────────────────────┴───────┴───────────────────────────────────────────────────────┘
```

## table_function_remote_max_addresses {#table_function_remote_max_addresses}



Type: UInt64

Default value: 1000

[remote](../../sql-reference/table-functions/remote.md) 関数からパターンによって生成される最大アドレス数を設定します。

可能な値:

- 正の整数。

## tcp_keep_alive_timeout {#tcp_keep_alive_timeout}



Type: Seconds

Default value: 290

TCP がキープアライブプローブを送信し始める前に接続がアイドル状態である必要がある時間（秒）。

## temporary_data_in_cache_reserve_space_wait_lock_timeout_milliseconds {#temporary_data_in_cache_reserve_space_wait_lock_timeout_milliseconds}



Type: UInt64

Default value: 600000

ファイルシステムキャッシュ内の一時データの空間予約のためのキャッシュをロックする待機時間。

## temporary_files_codec {#temporary_files_codec}



Type: String

Default value: LZ4

ディスク上のソートおよび結合操作で使用する一時ファイルの圧縮コーデックを設定します。

可能な値:

- LZ4 — [LZ4](https://en.wikipedia.org/wiki/LZ4_(compression_algorithm)) 圧縮が適用されます。
- NONE — 圧縮は適用されません。

## throw_if_deduplication_in_dependent_materialized_views_enabled_with_async_insert {#throw_if_deduplication_in_dependent_materialized_views_enabled_with_async_insert}



Type: Bool

Default value: 1

設定 `deduplicate_blocks_in_dependent_materialized_views` が有効で、`async_insert` とともに使用される場合、INSERT クエリで例外を投げます。これは、これらの機能が同時に機能できないため、正当性を保証します。

## throw_if_no_data_to_insert {#throw_if_no_data_to_insert}



Type: Bool

Default value: 1

空の INSERT を許可または禁止します。デフォルトで有効（空の挿入時にエラーを投げます）。これは [`clickhouse-client`](/interfaces/cli) または [gRPC インターフェイス](/interfaces/grpc) を使用する INSERT にのみ適用されます。

## throw_on_error_from_cache_on_write_operations {#throw_on_error_from_cache_on_write_operations}



Type: Bool

Default value: 0

書き込み操作（INSERT、マージ）時のキャッシュからのエラーを無視します。

## throw_on_max_partitions_per_insert_block {#throw_on_max_partitions_per_insert_block}



Type: Bool

Default value: 1

max_partitions_per_insert_block とともに使用されます。true（デフォルト）の場合、max_partitions_per_insert_block に達した際に例外が投げられます。false の場合、この制限に達した挿入クエリの詳細がログに記録されます。これは、max_partitions_per_insert_block を変更する際のユーザーへの影響を理解するのに役立つ場合があります。

## throw_on_unsupported_query_inside_transaction {#throw_on_unsupported_query_inside_transaction}
<ExperimentalBadge/>


Type: Bool

Default value: 1

トランザクション内でサポートされていないクエリが使用された場合に例外を投げます。

## timeout_before_checking_execution_speed {#timeout_before_checking_execution_speed}



Type: Seconds

Default value: 10

指定された時間が経過した後、速度があまりにも低くないかをチェックします。

## timeout_overflow_mode {#timeout_overflow_mode}



Type: OverflowMode

Default value: throw

制限が超えた場合にどうするか。

## timeout_overflow_mode_leaf {#timeout_overflow_mode_leaf}



Type: OverflowMode

Default value: throw

リーフ制限が超えた場合にどうするか。

## totals_auto_threshold {#totals_auto_threshold}



Type: Float

Default value: 0.5

`totals_mode = 'auto'` の閾値。

「WITH TOTALS 修飾子」のセクションを参照してください。

## totals_mode {#totals_mode}



Type: TotalsMode

Default value: after_having_exclusive

HAVING が存在する場合、および max_rows_to_group_by と group_by_overflow_mode = 'any' が存在する場合の TOTALS の計算方法。

「WITH TOTALS 修飾子」のセクションを参照してください。

## trace_profile_events {#trace_profile_events}



Type: Bool

Default value: 0

プロファイルイベントの各更新時にスタックトレースを収集し、プロファイルイベントの名前およびインクリメントの値を [trace_log](/operations/system-tables/trace_log) に送信するかどうかを有効または無効にします。

可能な値:

- 1 — プロファイルイベントのトレースが有効。
- 0 — プロファイルイベントのトレースが無効。

## transfer_overflow_mode {#transfer_overflow_mode}



Type: OverflowMode

Default value: throw

制限が超えた場合にどうするか。

## transform_null_in {#transform_null_in}



Type: Bool

Default value: 0

[IN](../../sql-reference/operators/in.md) 演算子の [NULL](/sql-reference/syntax#null) 値の同等性を有効にします。

デフォルトでは、`NULL` 値は比較できません。なぜなら、`NULL` は定義されていない値を意味するからです。そのため、比較 `expr = NULL` は常に `false` を返す必要があります。この設定を有効にすると、`NULL = NULL` が `IN` 演算子で `true` を返します。

可能な値:

- 0 — `IN` 演算子での `NULL` 値の比較は `false` を返します。
- 1 — `IN` 演算子での `NULL` 値の比較は `true` を返します。

**例**

`null_in` テーブルを考えてみましょう：

``` text
┌──idx─┬─────i─┐
│    1 │     1 │
│    2 │  NULL │
│    3 │     3 │
└──────┴───────┘
```

クエリ：

``` sql
SELECT idx, i FROM null_in WHERE i IN (1, NULL) SETTINGS transform_null_in = 0;
```

結果：

``` text
┌──idx─┬────i─┐
│    1 │    1 │
└──────┴──────┘
```

クエリ：

``` sql
SELECT idx, i FROM null_in WHERE i IN (1, NULL) SETTINGS transform_null_in = 1;
```

結果：

``` text
┌──idx─┬─────i─┐
│    1 │     1 │
│    2 │  NULL │
└──────┴───────┘
```

**関連情報**

- [IN 演算子の NULL 処理](/sql-reference/operators/in#null-processing)

## traverse_shadow_remote_data_paths {#traverse_shadow_remote_data_paths}



Type: Bool

Default value: 0

クエリ system.remote_data_paths に対して、実際のテーブルデータに加えて凍結されたデータ（シャドウディレクトリ）をトラバースします。

## union_default_mode {#union_default_mode}



Type: SetOperationMode

Default value: 

`SELECT` クエリ結果を結合するモードを設定します。この設定は、`UNION ALL` または `UNION DISTINCT` を明示的に指定せずに [UNION](../../sql-reference/statements/select/union.md) と共有されているときにのみ使用されます。

可能な値:

- `'DISTINCT'` — ClickHouse は重複行を削除して、クエリの結合結果として行を出力します。
- `'ALL'` — ClickHouse は重複行を含めて、クエリの結合結果としてすべての行を出力します。
- `''` — ClickHouse は、`UNION` と共に使用されると例外を生成します。

[UNION](../../sql-reference/statements/select/union.md) における例を参照してください。

## unknown_packet_in_send_data {#unknown_packet_in_send_data}



Type: UInt64

Default value: 0

データ N 番目のデータパケットの代わりに不明なパケットを送信します。

## use_async_executor_for_materialized_views {#use_async_executor_for_materialized_views}



Type: Bool

Default value: 0

マテリアライズドビュークエリの非同期およびおそらくマルチスレッド実行を使用します。これにより、INSERT 中のビュー処理の速度が向上しますが、メモリを多く消費する可能性もあります。

## use_cache_for_count_from_files {#use_cache_for_count_from_files}



Type: Bool

Default value: 1

テーブル関数 `file`/`s3`/`url`/`hdfs`/`azureBlobStorage` からのカウント時に行数をキャッシュすることを有効にします。

デフォルトで有効です。

## use_client_time_zone {#use_client_time_zone}



Type: Bool

Default value: 0

DateTime 文字列値の解釈にクライアントタイムゾーンを使用し、サーバーのタイムゾーンを採用しません。

## use_compact_format_in_distributed_parts_names {#use_compact_format_in_distributed_parts_names}



Type: Bool

Default value: 1

`Distributed` エンジンを持つテーブルに対するバックグラウンド (`distributed_foreground_insert`) INSERT のためのブロックを格納するためにコンパクトフォーマットを使用します。

可能な値:

- 0 — `user[:password]@host:port#default_database` ディレクトリ形式を使用します。
- 1 — `[shard{shard_index}[_replica{replica_index}]]` ディレクトリ形式を使用します。

:::note
- `use_compact_format_in_distributed_parts_names=0` の場合、クラスター定義の変更はバックグラウンド INSERT に対して適用されません。
- `use_compact_format_in_distributed_parts_names=1` の場合、クラスター定義内のノードの順序を変更すると、`shard_index`/`replica_index` が変更されるため注意が必要です。
:::

## use_concurrency_control {#use_concurrency_control}



Type: Bool

Default value: 1

サーバーの並行制御を尊重します（`concurrent_threads_soft_limit_num` と `concurrent_threads_soft_limit_ratio_to_cores` のグローバルサーバー設定を参照）。無効にすると、サーバーが過負荷の状態であってもより多くのスレッドを使用することが可能になります（通常の使用には推奨されず、主にテスト用です）。

## use_hedged_requests {#use_hedged_requests}



Type: Bool

Default value: 1

リモートクエリのためのヘッジリクエストロジックを有効にします。これにより、クエリのために異なるレプリカとの多くの接続を確立できます。既存の接続が確立されなかった場合や、データが受信されなかった場合に新しい接続が有効になります。クエリは、最初の空でない進捗パケット（またはデータパケット、`allow_changing_replica_until_first_data_packet`が有効な場合）を送信する接続を使用します。他の接続はキャンセルされます。`max_parallel_replicas > 1` のクエリがサポートされています。

デフォルトで有効です。

Cloud ではデフォルトで無効です。

## use_hive_partitioning {#use_hive_partitioning}



Type: Bool

Default value: 1

有効な場合、ClickHouse はファイルライクテーブルエンジン [File](/sql-reference/table-functions/file#hive-style-partitioning)/[S3](/sql-reference/table-functions/s3#hive-style-partitioning)/[URL](/sql-reference/table-functions/url#hive-style-partitioning)/[HDFS](/sql-reference/table-functions/hdfs#hive-style-partitioning)/[AzureBlobStorage](/sql-reference/table-functions/azureBlobStorage#hive-style-partitioning) のパス内の Hive スタイルのパーティショニングを検出し、クエリ内で仮想カラムとしてパーティションカラムを使用できるようにします。これらの仮想カラムは、パーティションされたパスと同じ名前で始まりますが、`_` から始まります。

## use_iceberg_partition_pruning {#use_iceberg_partition_pruning}



Type: Bool

Default value: 0

Iceberg テーブルのための Iceberg パーティショニングプルーニングを使用します。

## use_index_for_in_with_subqueries {#use_index_for_in_with_subqueries}



Type: Bool

Default value: 1

IN 演算子の右側にサブクエリまたはテーブル式がある場合、インデックスを使用しようとします。

## use_index_for_in_with_subqueries_max_values {#use_index_for_in_with_subqueries_max_values}



Type: UInt64

Default value: 0

フィルタリングにテーブルインデックスを使用するために、IN 演算子の右側のセットの最大サイズ。大規模なクエリの準備のために追加のデータ構造を準備することで性能の低下とメモリ使用量の増加を回避できるようになります。ゼロは制限がないことを意味します。

## use_json_alias_for_old_object_type {#use_json_alias_for_old_object_type}



Type: Bool

Default value: 0

有効な場合、`JSON` データ型エイリアスは新しい [JSON](../../sql-reference/data-types/newjson.md) 型の代わりに古い [Object('json')](../../sql-reference/data-types/json.md) 型を作成するために使用されます。

## use_local_cache_for_remote_storage {#use_local_cache_for_remote_storage}



Type: Bool

Default value: 1

リモートストレージ（HDFS または S3 のような）用のローカルキャッシュを使用します。これはリモートテーブルエンジンのみに使用されます。

## use_page_cache_for_disks_without_file_cache {#use_page_cache_for_disks_without_file_cache}



Type: Bool

Default value: 0

ファイルシステムキャッシュを有効にしていないリモートディスク用にユーザースペースページキャッシュを使用します。

## use_page_cache_with_distributed_cache {#use_page_cache_with_distributed_cache}



Type: Bool

Default value: 0

分散キャッシュが使用されている場合にユーザースペースページキャッシュを使用します。

## use_query_cache {#use_query_cache}



Type: Bool

Default value: 0

これがオンになっている場合、`SELECT` クエリは [クエリキャッシュ](../query-cache.md) を利用できる場合があります。より詳細にキャッシュの使用が制御されるパラメータ [enable_reads_from_query_cache](#enable_reads_from_query_cache) と [enable_writes_to_query_cache](#enable_writes_to_query_cache) を参照してください。

可能な値:

- 0 - 無効
- 1 - 有効

## use_query_condition_cache {#use_query_condition_cache}



Type: Bool

Default value: 0

クエリ条件キャッシュを有効にします。

可能な値:

- 0 - 無効
- 1 - 有効

## use_skip_indexes {#use_skip_indexes}



Type: Bool

Default value: 1

クエリ実行中にデータスキッピングインデックスを使用します。

可能な値:

- 0 — 無効。
- 1 — 有効。

## use_skip_indexes_if_final {#use_skip_indexes_if_final}



Type: Bool

Default value: 0

FINAL 修飾子を持つクエリを実行する際にスキッピングインデックスが使用されるかどうかを制御します。

デフォルトでは、この設定は無効です。これは、スキップインデックスが最新のデータを含む行（グラニュール）を除外する可能性があり、誤った結果を引き起こす可能性があるためです。これを有効にすると、FINAL 修飾子があってもスキッピングインデックスが適用されるため、パフォーマンスが向上する可能性がありますが、最近の更新を見逃すリスクがあります。

可能な値:

- 0 — 無効。
- 1 — 有効。

## use_structure_from_insertion_table_in_table_functions {#use_structure_from_insertion_table_in_table_functions}



Type: UInt64

Default value: 2

データからのスキーマ推論の代わりに挿入テーブルからの構造を使用します。可能な値: 0 - 無効、1 - 有効、2 - 自動

## use_uncompressed_cache {#use_uncompressed_cache}



Type: Bool

Default value: 0

非圧縮のブロックのキャッシュを使用するかどうか。0 または 1 を受け入れます。デフォルトでは 0（無効）です。
非圧縮キャッシュ（MergeTree ファミリのテーブルのみ）は、大量の短いクエリを処理する際にレイテンシを大幅に減少させ、スループットを向上させる可能性があります。頻繁に短いリクエストを送信するユーザーにはこの設定を有効にすることをお勧めします。また、非圧縮キャッシュブロックのサイズを設定する [uncompressed_cache_size](/operations/server-configuration-parameters/settings#uncompressed_cache_size) 構成パラメーターにも注意してください（設定ファイルにのみ設定）。デフォルトでは 8 GiB です。非圧縮キャッシュは必要に応じて充填され、最も使用されていないデータは自動的に削除されます。

少しでも大きなデータ量（100 万行以上）を読み取るクエリの場合、非圧縮キャッシュは自動的に無効になり、非常に小さなクエリのためのスペースが確保されます。これは、常に 'use_uncompressed_cache' 設定を 1 に設定できることを意味します。

## use_variant_as_common_type {#use_variant_as_common_type}



Type: Bool

Default value: 0

`if` (../../sql-reference/functions/conditional-functions.md/#if) / `multiIf` (../../sql-reference/functions/conditional-functions.md/#multiif) / `array` (../../sql-reference/functions/array-functions.md) / `map` (../../sql-reference/functions/tuple-map-functions.md) 関数の引数タイプに共通のタイプがない場合に、`Variant` タイプを結果タイプとして使用できます。

例：

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(if(number % 2, number, range(number))) as variant_type FROM numbers(1);
SELECT if(number % 2, number, range(number)) as variant FROM numbers(5);
```

```text
┌─variant_type───────────────────┐
│ Variant(Array(UInt64), UInt64) │
└────────────────────────────────┘
┌─variant───┐
│ []        │
│ 1         │
│ [0,1]     │
│ 3         │
│ [0,1,2,3] │
└───────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(multiIf((number % 4) = 0, 42, (number % 4) = 1, [1, 2, 3], (number % 4) = 2, 'Hello, World!', NULL)) AS variant_type FROM numbers(1);
SELECT multiIf((number % 4) = 0, 42, (number % 4) = 1, [1, 2, 3], (number % 4) = 2, 'Hello, World!', NULL) AS variant FROM numbers(4);
```

```text
─variant_type─────────────────────────┐
│ Variant(Array(UInt8), String, UInt8) │
└──────────────────────────────────────┘

┌─variant───────┐
│ 42            │
│ [1,2,3]       │
│ Hello, World! │
│ ᴺᵁᴸᴸ          │
└───────────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(array(range(number), number, 'str_' || toString(number))) as array_of_variants_type from numbers(1);
SELECT array(range(number), number, 'str_' || toString(number)) as array_of_variants FROM numbers(3);
```

```text
┌─array_of_variants_type────────────────────────┐
│ Array(Variant(Array(UInt64), String, UInt64)) │
└───────────────────────────────────────────────┘

┌─array_of_variants─┐
│ [[],0,'str_0']    │
│ [[0],1,'str_1']   │
│ [[0,1],2,'str_2'] │
└───────────────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(map('a', range(number), 'b', number, 'c', 'str_' || toString(number))) as map_of_variants_type from numbers(1);
SELECT map('a', range(number), 'b', number, 'c', 'str_' || toString(number)) as map_of_variants FROM numbers(3);
```

```text
┌─map_of_variants_type────────────────────────────────┐
│ Map(String, Variant(Array(UInt64), String, UInt64)) │
└─────────────────────────────────────────────────────┘

┌─map_of_variants───────────────┐
│ {'a':[],'b':0,'c':'str_0'}    │
│ {'a':[0],'b':1,'c':'str_1'}   │
│ {'a':[0,1],'b':2,'c':'str_2'} │
└───────────────────────────────┘
```

## use_with_fill_by_sorting_prefix {#use_with_fill_by_sorting_prefix}



Type: Bool

Default value: 1

WITH FILL のカラムに preceding するカラムは、ORDER BY 句のソートプレフィックスを形成します。ソートプレフィックスの異なる値の行は独立してフィルされます。

## validate_enum_literals_in_operators {#validate_enum_literals_in_operators}



Type: Bool

Default value: 0

有効な場合、`IN`、`NOT IN`、`==`、`!=` のような演算子の enum リテラルを enum タイプに対して検証し、リテラルが無効な enum 値である場合は例外を投げます。

## validate_mutation_query {#validate_mutation_query}



Type: Bool

Default value: 1

ミューテーションクエリを受け入れる前に検証します。ミューテーションはバックグラウンドで実行されており、無効なクエリが実行されるとミューテーションがスタックし、手動による介入が必要になります。

後方互換性のないバグに遭遇した場合のみ、この設定を変更してください。

## validate_polygons {#validate_polygons}



Type: Bool

Default value: 1

ポリゴンが自己交差または自己接触している場合に [pointInPolygon](/sql-reference/functions/geo/coordinates#pointinpolygon) 関数内で例外を投げるかどうかを有効または無効にします。

可能な値:

- 0 — 例外を投げることが無効になります。`pointInPolygon` は無効なポリゴンを受け入れ、その結果が不正確である可能性があります。
- 1 — 例外を投げることが有効になります。

## wait_changes_become_visible_after_commit_mode {#wait_changes_become_visible_after_commit_mode}
<ExperimentalBadge/>


Type: TransactionsWaitCSNMode

Default value: wait_unknown

コミットされた変更が最新のスナップショットに実際に見えるようになるまで待ちます。

## wait_for_async_insert {#wait_for_async_insert}



Type: Bool

Default value: 1

true の場合、非同期挿入の処理を待ちます。

## wait_for_async_insert_timeout {#wait_for_async_insert_timeout}



Type: Seconds

Default value: 120

非同期挿入の処理を待つためのタイムアウト。

## wait_for_window_view_fire_signal_timeout {#wait_for_window_view_fire_signal_timeout}
<ExperimentalBadge/>


Type: Seconds

Default value: 10

イベントタイム処理におけるウィンドービューの発火シグナルを待つためのタイムアウト。

## window_view_clean_interval {#window_view_clean_interval}
<ExperimentalBadge/>


Type: Seconds

Default value: 60

古いデータを解放するためのウィンドービューのクリン間隔（秒）。

## window_view_heartbeat_interval {#window_view_heartbeat_interval}
<ExperimentalBadge/>


Type: Seconds

Default value: 15

ウォッチクエリが生きていることを示すためのハートビート間隔（秒）。

## workload {#workload}



Type: String

Default value: default

リソースにアクセスするために使用するワークロードの名前。

## write_through_distributed_cache {#write_through_distributed_cache}


<CloudAvailableBadge/>

Type: Bool

Default value: 0

ClickHouse Cloud でのみ効果があります。分散キャッシュへの書き込みを許可します（S3 への書き込みも分散キャッシュによって行われます）。

## zstd_window_log_max {#zstd_window_log_max}



Type: Int64

Default value: 0

ZSTD の最大ウィンドウログを選択できます（MergeTree ファミリには使用されません）。

