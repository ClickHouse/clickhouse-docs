---
date: 2025-01-29
title: 破損した Keeper スナップショットからの復旧方法
tags: ['トラブルシューティング']
keywords: ['Keeper', '破損したスナップショット']
description: 'この記事では、破損した Keeper スナップショットから復旧する方法を説明します。問題の現れ方、スナップショットの概要と保存場所、および想定される復旧戦略について解説します。'
---

{frontMatter.description}

{/* 省略 */}

<br />

<br />

破損または不良な ClickHouse Keeper のスナップショットは、メタデータの不整合、テーブルの読み取り専用状態、リソース枯渇、バックアップ失敗など、システムの深刻な不安定化を引き起こす可能性があります。本記事では次の内容を扱います。

* [スナップショットとは何か、どこにあるのか](#overview)
* [問題がどのような形で現れるか](#symptoms)
* [復旧のために取り得る戦略](#recovery-strategies) とそれぞれが意味すること


## Keeper スナップショットの概要 \{#overview\}

### スナップショットとは何ですか？ \{#what-is-snapshot\}

スナップショットとは、特定時点における Keeper の内部データ（クラスタに関するメタデータ、テーブルコーディネーション用のパス、設定情報など）の状態をシリアライズしたものです。スナップショットは、クラスタ内の Keeper ノードを再同期したり、障害発生時にメタデータを復旧したり、既知の正常な Keeper 状態に依存する起動や再起動処理を行うために不可欠です。

### スナップショットはどこにありますか？ \{#where-to-find-snapshots\}

スナップショットは、Keeper ノードのローカルファイルシステム上のファイルとして保存されます。デフォルトでは `/var/lib/clickhouse/coordination/snapshots/` に保存されるか、`keeper_server.xml` ファイル内の `snapshot_storage_path` で指定されたカスタムパスに保存されます。スナップショットは連番で命名され（例: snapshot.23）、新しいものほど番号が大きくなります。

マルチノードクラスタの場合、各 Keeper ノードはそれぞれ専用のスナップショットディレクトリを持ちます。

:::note
ノード間でスナップショットの内容が一貫していることは、リカバリにおいて極めて重要です。
:::

## 破損した Keeper スナップショットの主な症状と兆候 \{#symptoms\}

以下の表は、破損した Keeper スナップショットに一般的に見られる症状と兆候を示します。

| **カテゴリ** | **問題の種類** | **確認すべき点** |
|---|---|---|
| **運用上の問題** | 読み取り専用モード | テーブルが予期せず読み取り専用モードに切り替わる |
| | クエリ失敗 | `Coordination::Exception` エラーを伴う継続的なクエリ失敗 |
| **メタデータ破損** | 古いメタデータ | 削除済みテーブルが反映されない／陳腐化したメタデータが原因で操作が失敗する |
| **リソース過負荷** | システムリソース枯渇 | Keeper ノードが過剰な CPU・メモリ・ディスク容量を消費し、ダウンタイムが発生し得る |
| | ディスクフル | スナップショット作成中にディスクがフルになる |
| **バックアップとリストア** | バックアップ失敗 | 欠落または不整合な Keeper メタデータによりバックアップが失敗する |
| **スナップショット作成／転送** | Keeper のクラッシュ | スナップショット作成中の Keeper クラッシュ（"SEGFAULT" エラーを確認） |
| | スナップショット転送の破損 | レプリカ間のスナップショット転送中の破損 |
| | レースコンディション | ログ圧縮中のレースコンディション - バックグラウンドコミットスレッドが削除済みログへアクセスする |
| | ネットワーク同期 | リーダーからフォロワーへのスナップショット同期を妨げるネットワーク問題 |

**ログ上の兆候:**

スナップショット破損を診断する前に、特定のエラーパターンについて **Keeper ログ** を確認します。

| **ログの種類** | **確認すべき内容** |
|---|---|
| **スナップショット破損エラー** | • `Aborting because of failure to load from latest snapshot with index`<br/>• `Failure to load from latest snapshot with index {}: {}. Manual intervention is necessary for recovery`<br/>• `Failed to preprocess stored log at index {}, aborting to avoid inconsistent state`<br/>• 起動時のスナップショットのシリアライズ／ロード失敗 |
| **その他の Keeper 問題** | • `Coordination::Exception`<br/>• `Zookeeper::Session Timeout`<br/>• 同期またはリーダー選出に関する問題<br/>• ログ圧縮時のレースコンディション |

## 破損した Keeper スナップショットからの復旧 \{#recovery-strategies\}

ファイルに手を加える前に、必ず次を実施します:

1. さらなる破損を防ぐため、すべての Keeper ノードを停止する
2. coordination ディレクトリ全体を安全な場所にコピーして、すべてをバックアップする
3. 少なくとも 1 つのノードに正常なデータが存在することを確認するため、クラスタのクォーラムを検証する

---

### 1. 既存のバックアップから復元する \{#1-restore-from-an-existing-backup\}

次のいずれかに該当する場合は、この手順に従ってください。

- Keeper のメタデータまたはスナップショットの破損により、現在のデータが復旧不可能な状態になっている。
- 正常な Keeper 状態が確認されているバックアップが存在する。

既存のバックアップから復元するには、以下の手順に従います。

1. メタデータの整合性を確認しつつ、最新のバックアップを特定して検証します。
2. ClickHouse と Keeper のサービスを停止します。
3. 破損したスナップショットとログを、バックアップディレクトリ内のものと置き換えます。
4. Keeper クラスターを再起動し、メタデータの同期が取れていることを確認します。

:::tip[定期的なバックアップ]
バックアップが古い場合、直近のメタデータ変更が失われる可能性があります。そのため、定期的なバックアップの取得を推奨します。
:::

---

### 2. 以前のスナップショットへロールバックする \{#2-rollback-to-an-older-snapshot\}

次のような場合は、この手順に従ってください。

- 直近のスナップショットが破損しているが、より古いスナップショットは利用可能な場合。
- 一貫性のあるリカバリに必要なインクリメンタルログが無傷で残っている場合。

古いスナップショットへロールバックするには、以下の手順に従います。

1. Keeper ディレクトリから、有効な古いスナップショット（例: snapshot.19）を特定して選択します。
2. より新しいスナップショットとログを削除します。
3. Keeper を再起動し、ログをリプレイしてメタデータ状態を再構築させます。

:::warning[メタデータ不整合のリスク]
スナップショットやログが欠落している、または不完全な場合、メタデータに不整合が生じるおそれがあります。
:::

---

### 3. SYSTEM RESTORE REPLICA を使用してメタデータを復元する \{#3-restore-metadata-using-system-restore-replica\}

次のような場合に、この手順に従ってください。

* Keeper のメタデータが失われたか破損しているが、テーブルデータはディスク上に存在している場合
* ZooKeeper/Keeper メタデータの欠落により、テーブルが読み取り専用モードに切り替わった場合
* ローカルで利用可能なパーツに基づいて、Keeper 内のメタデータを再作成する必要がある場合

メタデータを復元するには、以下の手順に従います。

1. テーブルデータが、設定ファイル内の `<path>` で指定されている ClickHouse サーバーのデータパス（デフォルトでは `/var/lib/clickhouse/data/`）にローカルで存在していることを確認します。

2. 影響を受けた各テーブルに対して、次を実行します。

```sql
SYSTEM RESTART REPLICA [db.]table_name;
SYSTEM RESTORE REPLICA [db.]table_name;
```

3. データベース単位で復旧を行う場合（`Replicated` データベースエンジンを使用している場合）：

```sql
SYSTEM RESTORE DATABASE REPLICA db_name;
```

4. 同期が完了するまで待ちます：

```sql
SYSTEM SYNC REPLICA [db.]table_name;
```

5. `system.replicas` で `is_readonly = 0` になっていることを確認し、`system.detached_parts` を監視して復旧を検証します

:::info[動作の仕組み]
`SYSTEM RESTORE REPLICA` は既存のすべてのパーツを切り離し、Keeper 内のメタデータを（新しい空のテーブルであるかのように）再作成し、その後すべてのパーツを再度アタッチします。これにより、ネットワーク経由でデータを再ダウンロードする必要がなくなります。
:::

:::warning[前提条件]
これはローカルデータのパーツが正常な場合にのみ機能します。データ自体も破損している場合は、代わりに戦略 #5（クラスタの再構築）を使用してください。
:::

***


### 4. Keeper 内のレプリカメタデータの削除と再作成 \{#4-drop-and-recreate-replica-metadata-in-keeper\}

次のような場合は、この手順に従ってください。

* クラスター内の単一のレプリカでエラーが発生しており、Keeper 内のメタデータが破損または不整合な状態になっている場合
* 「Part XXXXX intersects previous part YYYYY」のようなエラーが発生した場合
* ローカルデータを保持したまま、特定のレプリカの Keeper メタデータを完全にリセットする必要がある場合

メタデータを削除して再作成するには、以下の手順に従います。

1. 影響を受けているレプリカで、テーブルを DETACH します。

```sql
DETACH TABLE [db.]table_name;
```

2. Keeper からレプリカのメタデータを削除します（任意のレプリカで実行）:

```sql
SYSTEM DROP REPLICA 'replica_name' FROM ZKPATH '/clickhouse/tables/{shard}/table_name';
```

正しい ZooKeeper パスを特定するには、次のようにします。

```sql
SELECT zookeeper_path, replica_name FROM system.replicas WHERE table = 'table_name';
```

3. テーブルを再度アタッチします（読み取り専用モードになります）:

```sql
ATTACH TABLE [db.]table_name;
```

4. レプリカのメタデータを復元します:

```sql
SYSTEM RESTORE REPLICA [db.]table_name;
```

5. 他のレプリカと同期させる：

```sql
SYSTEM SYNC REPLICA [db.]table_name;
```

6. 復旧後にすべてのレプリカで `system.detached_parts` を確認する

:::warning[影響を受けたすべてのレプリカで実行]
破損が複数のレプリカに影響している場合は、これらの手順を各レプリカで順番に繰り返してください。
:::

:::tip[データベース全体に適用する場合]
Replicated データベースを使用している場合は、代わりに `SYSTEM DROP REPLICA ... FROM DATABASE db_name` を使用できます。
:::

**代替手段: force&#95;restore&#95;data フラグの使用**

サーバー起動時に、すべての Replicated テーブルを自動的に復旧するには:

1. ClickHouse サーバーを停止する
2. 復旧用フラグを作成する:

```bash
sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data
```

3. ClickHouse サーバーを起動します
4. サーバーはフラグを自動的に削除し、すべてのレプリケートされたテーブルを復元します
5. 復旧状況をログで監視します

この方法は、複数のテーブルを一度に復旧する必要がある場合に有用です。

***


### 5. Keeper クラスターの再構築 \{#5-rebuild-keeper-cluster\}

次のような場合は、この手順に従ってください。

- 復旧に使用できる有効なスナップショット、ログ、またはバックアップが存在しない場合。
- Keeper クラスター全体およびそのメタデータを再作成する必要がある場合。

Keeper クラスターを再構築するには、以下の手順に従います。

1. ClickHouse および Keeper クラスターを完全に停止します。
2. スナップショットおよびログディレクトリをクリーンアップして、各 Keeper ノードをリセットします。
3. 1 つの Keeper ノードをリーダーとして初期化し、他のノードを段階的に追加します。
4. 外部の記録からメタデータが利用可能な場合は、再インポートします。

:::warning[時間のかかる手順]
この手順は時間を要し、長時間のサービス停止リスクを伴います。データ全体の再構築が必要です。
:::