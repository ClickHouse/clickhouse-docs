[
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/cloud/features/04_infrastructure/warehouses.md",
    "line": 20,
    "header": "What is compute-compute separation?",
    "anchor": "what-is-compute-compute-separation",
    "first_para": "Compute-compute separation is available for Scale and Enterprise tiers. Each ClickHouse Cloud service includes: - A group of two or more ClickHouse nodes (or replicas) is required, but the child services can be single replica. - An endpoint (or multiple endpoints created via ClickHouse Cloud UI console), which is a service URL that you use to connect to the service (for example, `https://dv2fzne24g.us-east-1.aws.clickhouse.cloud:8443`). - An object storage folder where the service stores all the data and partially metadata:",
    "category": "delayed_definition",
    "full_content_len": 1000
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/cloud/features/04_infrastructure/warehouses.md",
    "line": 51,
    "header": "What is a warehouse?",
    "anchor": "what-is-a-warehouse",
    "first_para": "In ClickHouse Cloud, a _warehouse_ is a set of services that share the same data. Each warehouse has a primary service (this service was created first) and secondary service(s). For example, in the screenshot below you can see a warehouse \"DWH Prod\" with two services: - Primary service `DWH Prod` - Secondary service `DWH Prod Subservice` <br /> All services in a warehouse share the same: - Region (for example, us-east1) - Cloud service provider (AWS, GCP or Azure) - ClickHouse database version You can sort services by the warehouse that they belong to.",
    "category": "no_clear_definition",
    "full_content_len": 709
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/cloud/features/09_AI_ML/langfuse.md",
    "line": 14,
    "header": "What is Langfuse?",
    "anchor": "what-is-langfuse",
    "first_para": "[Langfuse](https://langfuse.com) is an open-source LLM engineering platform that helps teams collaboratively debug, analyze, and iterate on their LLM applications. It is part of the ClickHouse ecosystem and relies on **ClickHouse** at its core to provide a scalable, high-performance observability backend. By leveraging ClickHouse's columnar storage and fast analytical capabilities, Langfuse can handle billions of traces and events with low latency, making it suitable for high-throughput production workloads.",
    "category": "no_clear_definition",
    "full_content_len": 514
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/cloud/guides/infrastructure/01_deployment_options/byoc/03_onboarding/01_standard.md",
    "line": 16,
    "header": "What is Standard Onboarding?",
    "anchor": "what-is-standard-onboarding",
    "first_para": "**Standard onboarding** is the default, guided workflow for deploying ClickHouse in your own cloud account using BYOC. In this approach, ClickHouse Cloud provisions all of the core cloud resources required for your deployment\u2014such as the VPC, subnets, security groups, Kubernetes (EKS/GKE) cluster, and supporting IAM roles/service accounts\u2014within your AWS account/GCP project. This ensures consistent, secure configuration, and minimizes the manual steps required from your team. With standard onboarding, you simply provide a dedicated AWS account/GCP project, and run an initial stack (via CloudFo",
    "category": "no_clear_definition",
    "full_content_len": 1000
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/cloud/onboard/01_discover/01_what_is.md",
    "line": 10,
    "header": "What is ClickHouse Cloud?",
    "anchor": "what-is-clickhouse-cloud",
    "first_para": "ClickHouse Cloud is a fully managed cloud service created by the original creators of ClickHouse, the fastest and most popular open-source columnar online analytical processing database. With Cloud, infrastructure, maintenance, scaling, and operations are taken care of for you, so that you can focus on what matters most to you, which is building value for your organization and your customers faster.",
    "category": "no_clear_definition",
    "full_content_len": 403
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/cloud/onboard/01_discover/02_use_cases/01_real-time-analytics.md",
    "line": 18,
    "header": "What is real-time analytics?",
    "anchor": "what-is-real-time-analytics",
    "first_para": "Real-time analytics refers to data processing that delivers insights to end users and customers as soon as the data is generated. It differs from traditional or batch analytics, where data is collected in batches and processed, often a long time after it was generated. Real-time analytics systems are built on top of event streams, which consist of a series of events ordered in time. An event is something that\u2019s already happened. It could be the addition of an item to the shopping cart on an e-commerce website,",
    "category": "no_clear_definition",
    "full_content_len": 1000
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/cloud/onboard/01_discover/02_use_cases/02_observability.md",
    "line": 22,
    "header": "What is Observability?",
    "anchor": "what-is-observability",
    "first_para": "Observability is understanding a system's internal state by examining its outputs. In software systems, this means understanding what's happening inside your applications and infrastructure through the data they generate. This field has evolved significantly and can be understood through two distinct generations of observability approaches. The first generation, often called Observability 1.0, was built around the traditional \"three pillars\" approach of metrics, logs, and traces. This approach required multiple tools and data stores for different types of telemetry. It",
    "category": "no_clear_definition",
    "full_content_len": 1000
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/cloud/onboard/01_discover/02_use_cases/03_data_warehousing.md",
    "line": 23,
    "header": "What are the components of the data lakehouse?",
    "anchor": "components-of-the-data-lakehouse",
    "first_para": "The modern data lakehouse architecture represents a convergence of data warehouse and data lake technologies, combining the best aspects of both approaches. This architecture comprises several distinct but interconnected layers providing a flexible, robust data storage, management, and analysis platform. Understanding these components is essential for organizations looking to implement or optimize their data lakehouse strategy. The layered approach allows for component substitution and independent evolution of each layer, providing",
    "category": "no_clear_definition",
    "full_content_len": 1000
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/cloud/onboard/01_discover/02_use_cases/03_data_warehousing.md",
    "line": 49,
    "header": "What are the benefits of the data lakehouse?",
    "anchor": "benefits-of-the-data-lakehouse",
    "first_para": "The data lakehouse architecture offers several significant advantages when compared directly to both traditional data warehouses and data lakes:",
    "category": "no_clear_definition",
    "full_content_len": 144
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/cloud/onboard/01_discover/02_use_cases/04_machine_learning_and_genAI/02_agent_facing_analytics.md",
    "line": 19,
    "header": "What are \"agents\"?",
    "anchor": "agents",
    "first_para": "One can think of AI agents as digital assistants that have evolved beyond simple task execution (or function calling): they can understand context, make decisions, and take meaningful actions toward specific goals. They operate in a \"sense-think-act\" loop (see ReAct agents), processing various inputs (text, media, data), analyzing situations, and then doing something useful with that information. Most importantly, depending on the application domain, they can theoretically operate at various levels of autonomy,",
    "category": "no_clear_definition",
    "full_content_len": 961
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/cloud/onboard/02_migrate/01_migration_guides/08_other_methods/01_clickhouse-local-etl.md",
    "line": 34,
    "header": "What is clickhouse-local?",
    "anchor": "what-is-clickhouse-local",
    "first_para": "Typically, ClickHouse is run in the form of a cluster, where several instances of the ClickHouse database engine are running in a distributed fashion on different servers. On a single server, the ClickHouse database engine is run as part of the `clickhouse-server` program. Database access (paths, users, security, ...) is configured with a server configuration file. The `clickhouse-local` tool allows you to use the ClickHouse database engine isolated in a command-line utility fashion for blazing-fast SQL data processing on an ample amount of inputs and outputs, without having to configure and s",
    "category": "no_clear_definition",
    "full_content_len": 707
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/cloud/reference/03_billing/01_billing_overview.md",
    "line": 180,
    "header": "What is a ClickHouse Credit (CHC)?",
    "anchor": "what-is-chc",
    "first_para": "A ClickHouse Credit is a unit of credit toward Customer's usage of ClickHouse Cloud equal to one (1) US dollar, to be applied based on ClickHouse's then-current published price list. If you are being billed through Stripe then you will see that 1 CHC is equal to \\$0.01 USD on your Stripe invoice. This is to allow accurate billing on Stripe due to their limitation on not being able to bill fractional quantities of our standard SKU of 1 CHC = \\$1 USD.",
    "category": "no_clear_definition",
    "full_content_len": 467
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/cloud/reference/03_billing/02_marketplace/overview.md",
    "line": 57,
    "header": "I subscribed to ClickHouse Cloud as a marketplace user, and then unsubscribed. Now I want to subscribe back, what is the process?\u200b",
    "anchor": "i-subscribed-to-clickhouse-cloud-as-a-marketplace-user-and-then-unsubscribed-now-i-want-to-subscribe-back-what-is-the-process",
    "first_para": "In that case please subscribe to the ClickHouse Cloud as usual (see sections on subscribing to ClickHouse Cloud via the marketplace). - For AWS marketplace a new ClickHouse Cloud organization will be created and connected to the marketplace. - For the GCP marketplace your old organization will be reactivated. If you have any trouble with reactivating your marketplace org, please contact [ClickHouse Cloud Support](https://clickhouse.com/support/program).",
    "category": "no_clear_definition",
    "full_content_len": 459
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/cloud/reference/03_billing/03_clickpipes/clickpipes_for_streaming_and_object_storage.md",
    "line": 28,
    "header": "What are ClickPipes replicas?",
    "anchor": "what-are-clickpipes-replicas",
    "first_para": "ClickPipes ingests data from remote data sources via a dedicated infrastructure that runs and scales independently of the ClickHouse Cloud service. For this reason, it uses dedicated compute replicas.",
    "category": "no_clear_definition",
    "full_content_len": 200
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/cloud/reference/03_billing/03_clickpipes/clickpipes_for_streaming_and_object_storage.md",
    "line": 34,
    "header": "What is the default number of replicas and their size?",
    "anchor": "what-is-the-default-number-of-replicas-and-their-size",
    "first_para": "Each ClickPipe defaults to 1 replica that is provided with 512 MiB of RAM and 0.125 vCPU (XS). This corresponds to **0.0625** ClickHouse compute units (1 unit = 8 GiB RAM, 2 vCPUs).",
    "category": "no_clear_definition",
    "full_content_len": 181
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/cloud/reference/03_billing/03_clickpipes/clickpipes_for_streaming_and_object_storage.md",
    "line": 39,
    "header": "What are the ClickPipes public prices?",
    "anchor": "what-are-the-clickpipes-public-prices",
    "first_para": "- Compute: \\$0.20 per unit per hour (\\$0.0125 per replica per hour for the default replica size) - Ingested data: \\$0.04 per GB The price for the Compute dimension depends on the **number** and **size** of replica(s) in a ClickPipe. The default replica size can be adjusted using vertical scaling, and each replica size is priced as follows: | Replica Size               | Compute Units | RAM     | vCPU   | Price per Hour | |----------------------------|---------------|---------|--------|----------------|",
    "category": "no_clear_definition",
    "full_content_len": 924
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/concepts/olap.md",
    "line": 11,
    "header": "What is OLAP?",
    "anchor": null,
    "first_para": "[OLAP](https://en.wikipedia.org/wiki/Online_analytical_processing) stands for Online Analytical Processing. It is a broad term that can be looked at from two perspectives: technical and business. At the highest level, you can just read these words backward: **Processing** \u2014 Some source data is processed\u2026 **Analytical** \u2014 \u2026to produce some analytical reports and insights\u2026 **Online** \u2014 \u2026in real-time.",
    "category": "delayed_definition",
    "full_content_len": 403
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/faq/general/columnar-database.md",
    "line": 15,
    "header": "What is a columnar database?",
    "anchor": "what-is-a-columnar-database",
    "first_para": "A columnar database stores the data of each column independently. This allows reading data from disk only for those columns that are used in any given query. The cost is that operations that affect whole rows become proportionally more expensive. The synonym for a columnar database is a column-oriented database management system. ClickHouse is a typical example of such a system. Key columnar database advantages are: - Queries that use only a few columns out of many. - Aggregating queries against large volumes of data.",
    "category": "delayed_definition",
    "full_content_len": 1000
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/faq/general/dependencies.md",
    "line": 11,
    "header": "What are the 3rd-party dependencies for running ClickHouse?",
    "anchor": null,
    "first_para": "ClickHouse does not have any runtime dependencies. It is distributed as a single binary application which is fully self-contained. This application provides all the functionality of the cluster, serves queries, acts as a worker node in the cluster, as a coordination system providing the RAFT consensus algorithm, as a client or a local query engine. This unique architecture choice differentiates it from other systems, that often have dedicated frontend, backend, or aggregation nodes, as it makes the deployment, cluster management, and monitoring easier.",
    "category": "no_clear_definition",
    "full_content_len": 757
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/faq/general/olap.md",
    "line": 11,
    "header": "What Is OLAP?",
    "anchor": "what-is-olap",
    "first_para": "[OLAP](https://en.wikipedia.org/wiki/Online_analytical_processing) stands for Online Analytical Processing. It is a broad term that can be looked at from two perspectives: technical and business. But at the very high level, you can just read these words backward: Processing :   Some source data is processed... Analytical :   ...to produce some analytical reports and insights... Online :   ...in real-time.",
    "category": "delayed_definition",
    "full_content_len": 411
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/getting-started/example-datasets/github.md",
    "line": 2080,
    "header": "What is the average time before code will be rewritten and the median (half-life of code decay)?",
    "anchor": "what-is-the-average-time-before-code-will-be-rewritten-and-the-median-half-life-of-code-decay",
    "first_para": "We can use the same principle as [List files that were rewritten most number of time or by most of authors](#list-files-that-were-rewritten-most-number-of-times) to identify rewrites but consider all files. A window function is used to compute the time between rewrites for each file. From this, we can calculate an average and median across all files. [play](https://sql.clickhouse.com?query_id=WSHUEPJP9TNJUH7QITWWOR)",
    "category": "no_clear_definition",
    "full_content_len": 1000
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/getting-started/example-datasets/github.md",
    "line": 2140,
    "header": "What is the worst time to write code in sense that the code has highest chance to be re-written?",
    "anchor": "what-is-the-worst-time-to-write-code-in-sense-that-the-code-has-highest-chance-to-be-re-written",
    "first_para": "Similar to [What is the average time before code will be rewritten and the median (half-life of code decay)?](#what-is-the-average-time-before-code-will-be-rewritten-and-the-median-half-life-of-code-decay) and [List files that were rewritten most number of time or by most of authors](#list-files-that-were-rewritten-most-number-of-times), except we aggregate by day of week. Adjust as required e.g. month of year. [play](https://sql.clickhouse.com?query_id=8PQNWEWHAJTGN6FTX59KH2)",
    "category": "no_clear_definition",
    "full_content_len": 1000
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/integrations/data-ingestion/clickpipes/postgres/faq.md",
    "line": 101,
    "header": "What are the costs for ClickPipes for Postgres CDC?",
    "anchor": "what-are-the-costs-for-clickpipes-for-postgres-cdc",
    "first_para": "For detailed pricing information, please refer to the [ClickPipes for Postgres CDC pricing section on our main billing overview page](/cloud/reference/billing/clickpipes).",
    "category": "no_clear_definition",
    "full_content_len": 171
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/integrations/data-ingestion/clickpipes/postgres/toast.md",
    "line": 14,
    "header": "What are TOAST columns in PostgreSQL?",
    "anchor": "what-are-toast-columns-in-postgresql",
    "first_para": "TOAST (The Oversized-Attribute Storage Technique) is PostgreSQL's mechanism for handling large field values. When a row exceeds the maximum row size (typically 2KB, but this can vary depending on the PostgreSQL version and exact settings), PostgreSQL automatically moves large field values into a separate TOAST table, storing only a pointer in the main table. It's important to note that during Change Data Capture (CDC), unchanged TOAST columns are not included in the replication stream. This can lead to incomplete data replication if not handled properly.",
    "category": "no_clear_definition",
    "full_content_len": 935
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/integrations/data-ingestion/streamkap/sql-server-clickhouse.md",
    "line": 104,
    "header": "Upsert Mode: What Is That?",
    "anchor": "upsert-mode-what-is-that",
    "first_para": "This is an important step: we want to use ClickHouse\u2019s \u201cupsert\u201d mode\u2014which (under the hood) uses the ReplacingMergeTree engine in ClickHouse. This lets us merge incoming records efficiently and handle updates after ingest, using what ClickHouse calls \u201cpart merging.\u201d - This makes sure your destination table doesn\u2019t fill up with duplicates when things change on the SQL Server side.",
    "category": "good_direct_answer",
    "full_content_len": 383
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/intro.md",
    "line": 17,
    "header": "What are analytics?",
    "anchor": "what-are-analytics",
    "first_para": "Analytics, also known as OLAP (Online Analytical Processing), refers to SQL queries with complex calculations (e.g., aggregations, string processing, arithmetic) over massive datasets. Unlike transactional queries (or OLTP, Online Transaction Processing) that read and write just a few rows per query and, therefore, complete in milliseconds, analytics queries routinely process billions and trillions of rows. In many use cases, [analytics queries must be \"real-time\"](https://clickhouse.com/engineering-resources/what-is-real-time-analytics), i.e., return a result in less than one second.",
    "category": "no_clear_definition",
    "full_content_len": 593
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/intro.md",
    "line": 96,
    "header": "What is OLAP?",
    "anchor": "what-is-olap",
    "first_para": "OLAP scenarios require real-time responses on top of large datasets for complex analytical queries with the following characteristics: - Datasets can be massive - billions or trillions of rows - Data is organized in tables that contain many columns - Only a few columns are selected to answer any particular query - Results must be returned in milliseconds or seconds",
    "category": "no_clear_definition",
    "full_content_len": 367
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/managing-data/core-concepts/parts.md",
    "line": 13,
    "header": "What are table parts in ClickHouse?",
    "anchor": "what-are-table-parts-in-clickhouse",
    "first_para": "<br /> The data from each table in the ClickHouse [MergeTree engine family](/engines/table-engines/mergetree-family) is organized on disk as a collection of immutable `data parts`. To illustrate this, we use [this](https://sql.clickhouse.com/?query=U0hPVyBDUkVBVEUgVEFCTEUgdWsudWtfcHJpY2VfcGFpZF9zaW1wbGU&run_query=true&tab=results) table (adapted from the [UK property prices dataset](/getting-started/example-datasets/uk-price-paid)) tracking the date, town, street, and price for sold properties in the United Kingdom:",
    "category": "no_clear_definition",
    "full_content_len": 1000
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/use-cases/AI_ML/MCP/index.md",
    "line": 25,
    "header": "What is MCP's architecture?",
    "anchor": "mcp-architecture",
    "first_para": "MCP follows a client-server architecture: * Clients (like Claude Desktop, Cursor, or VS Code) establish connections with MCP servers. You can see a collection of clients in the [awesome-mcp-clients](https://github.com/punkpeye/awesome-mcp-clients?tab=readme-ov-file#windsurf) GitHub repository. * Servers expose tools and capabilities through standardized interfaces. You can see a collection of servers in the [awesome-mcp-servers](https://github.com/punkpeye/awesome-mcp-servers) GitHub repository. * AI models can then use these tools to access external data and functionality when needed",
    "category": "no_clear_definition",
    "full_content_len": 692
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/use-cases/observability/clickstack/integration-partners/bindplane.md",
    "line": 35,
    "header": "What is Bindplane?",
    "anchor": "what-is-bindplane",
    "first_para": "Bindplane is an OpenTelemetry-native telemetry pipeline that provides centralized management for OpenTelemetry Collectors. It simplifies operating large collector fleets by offering visual configuration editing, safe rollouts, and pipeline intelligence.",
    "category": "good_direct_answer",
    "full_content_len": 253
  },
  {
    "file": "/home/runner/work/clickhouse-docs/clickhouse-docs/docs/use-cases/observability/clickstack/managing/materialized_views.md",
    "line": 34,
    "header": "What are incremental materialized views",
    "anchor": "what-are-incremental-materialized-views",
    "first_para": "Incremental materialized views allow you to shift the cost of computation from query time to insert time, resulting in significantly faster `SELECT` queries. Unlike transactional databases such as Postgres, a ClickHouse materialized view isn't a stored snapshot. Instead, it acts as a trigger that runs a query on blocks of data as they're inserted into a source table. The output of this query is written into a separate target table. As additional data is inserted, new partial results are appended and merged into the target table. The merged result is equivalent to running the aggregation over t",
    "category": "no_clear_definition",
    "full_content_len": 1000
  }
]